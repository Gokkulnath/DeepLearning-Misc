{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!doctype html><html lang=\"en\"><head><script>!function(c,f){var t,o,i,e=[],r={passive:!0,capture:!0},n=new Date,a=\"pointerup\",u=\"pointercancel\";function p(n,e){t||(t=e,o=n,i=new Date,w(f),s())}function s(){0<=o&&o<i-n&&(e.forEach(function(n){n(o,t)}),e=[])}function l(n){if(n.cancelable){var e=(1e12<n.timeStamp?new Date:performance.now())-n.timeStamp;\"pointerdown\"==n.type?function(n,e){function t(){p(n,e),i()}function o(){i()}function i(){f(a,t,r),f(u,o,r)}c(a,t,r),c(u,o,r)}(e,n):p(e,n)}}function\n"
     ]
    }
   ],
   "source": [
    "from requests import get\n",
    "url = 'https://towardsdatascience.com/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac'\n",
    "response = get(url)\n",
    "print(response.text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_soup=BeautifulSoup(response.text, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<div class=\"gg o hv hw hx dl\"></div>,\n",
       " <section class=\"hy hz ia ib ic\"><div class=\"ae fb ab di v w\"><div><div class=\"id ie ea ap if b ig ih ii ij ik il im\" id=\"1ef0\"><h1 class=\"if b ig in ea\">Review: ResNeXt — 1st Runner Up in ILSVRC 2016 (Image Classification)</h1></div></div><div class=\"io ie at ap ao cs ip iq ir is it iu iv\" id=\"4c9a\"><h2 class=\"ao cs iw ix at\"><strong class=\"bb\">Network-in-Neuron, A New Dimensionality: Cardinality</strong></h2></div><div class=\"iy\"><div class=\"ag af\"><div><a href=\"/@sh.tsang?source=post_page---------------------------\"><img alt=\"Sik-Ho Tsang\" class=\"l fn iz ja\" height=\"48\" src=\"https://miro.medium.com/fit/c/96/96/1*OxjNUHcLFU8-pp-j8su6pg.jpeg\" width=\"48\"/></a></div><div class=\"jb ab l\"><div class=\"af\"><div style=\"flex:1\"><span class=\"ao b ap aq ar as l ea al\"><div class=\"jc af ag jd\" data-test-id=\"postByline\"><span class=\"ao cs ed aq by je ef eg jf ei ea\"><a class=\"cy cz ax ay az ba bb bc bd be jg bh bi dw dx\" href=\"/@sh.tsang?source=post_page---------------------------\">Sik-Ho Tsang</a></span><div class=\"jh l am h\"><button class=\"ji ea al bm jj jk jl jm be dw jn jo jp jq jr js bp ao b ap jt ct as bq br ae bs bt bh\">Follow</button></div></div></span></div></div><span class=\"ao b ap aq ar as l at au\"><span class=\"ao cs ed aq by je ef eg jf ei at\"><div><a class=\"cy cz ax ay az ba bb bc bd be jg bh bi dw dx\" href=\"/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac?source=post_page---------------------------\">Dec 9, 2018</a> <!-- -->·<!-- --> <!-- -->7<!-- --> min read</div></span></span></div></div></div><figure class=\"ju jv jw jx jy ez jz x y paragraph-image\"><div class=\"kc l eo kd\"><div class=\"ke l\"><div class=\"dk ka gg n o gf ab by t kb\"><img class=\"gg n o gf ab kf kg\" height=\"467\" src=\"https://miro.medium.com/max/60/0*MvA7JX1Z1rAG5byo?q=20\" width=\"700\"/></div><img class=\"dk ka gg n o gf ab gd\" height=\"467\" width=\"700\"/><noscript><img class=\"gg n o gf ab\" height=\"467\" src=\"https://miro.medium.com/max/1400/0*MvA7JX1Z1rAG5byo\" width=\"700\"/></noscript></div></div></figure><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku kv\" id=\"ecb1\"><span class=\"l kw kx ky kz la lb lc ld le eo\">In</span> this story, <strong class=\"kj lf\">ResNeXt, </strong>by <strong class=\"kj lf\">UC San Diego </strong>and <strong class=\"kj lf\">Facebook AI Research (FAIR), </strong>is reviewed. The model name, ResNeXt, contains Next. It means the <em class=\"lg\">next </em>dimension, on top of the <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet</a>. This next dimension is called the “<strong class=\"kj lf\"><em class=\"lg\">cardinality</em></strong>” <strong class=\"kj lf\">dimension</strong>. And ResNeXt becomes the <strong class=\"kj lf\">1st Runner Up of ILSVRC classification task</strong>.</p><figure class=\"ju jv jw jx jy ez ll x y paragraph-image\"><div class=\"kc l eo kd\"><div class=\"lm l\"><div class=\"dk ka gg n o gf ab by t kb\"><img class=\"gg n o gf ab kf kg\" height=\"421\" src=\"https://miro.medium.com/max/60/1*RHpn70qFNCcqyVjkPdFtGA.png?q=20\" width=\"700\"/></div><img class=\"dk ka gg n o gf ab gd\" height=\"421\" width=\"700\"/><noscript><img class=\"gg n o gf ab\" height=\"421\" src=\"https://miro.medium.com/max/1400/1*RHpn70qFNCcqyVjkPdFtGA.png\" width=\"700\"/></noscript></div></div><figcaption class=\"at ed ln lo hi di x y lp ao cs\"><strong class=\"ao lf\">ILSVRC 2016 Classification Ranking </strong><a class=\"cy bt lh li lj lk\" href=\"http://image-net.org/challenges/LSVRC/2016/results#loc\">http://image-net.org/challenges/LSVRC/2016/results#loc</a></figcaption></figure><figure class=\"ju jv jw jx jy ez lq x y paragraph-image\"><div class=\"kc l eo kd\"><div class=\"lr l\"><div class=\"dk ka gg n o gf ab by t kb\"><img class=\"gg n o gf ab kf kg\" height=\"307\" src=\"https://miro.medium.com/max/60/1*LOoc11tkDoqv0pC6OH7mwA.png?q=20\" width=\"700\"/></div><img class=\"dk ka gg n o gf ab gd\" height=\"307\" width=\"700\"/><noscript><img class=\"gg n o gf ab\" height=\"307\" src=\"https://miro.medium.com/max/1400/1*LOoc11tkDoqv0pC6OH7mwA.png\" width=\"700\"/></noscript></div></div><figcaption class=\"at ed ln lo hi di x y lp ao cs\"><strong class=\"ao lf\">Residual Block in ResNet (Left), A Block of ResNeXt with Cardinality = 32 (Right)</strong></figcaption></figure><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"f37a\">Compared with <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet</a> (The winner in ILSVRC 2015, 3.57%) and <a class=\"cy bt lh li lj lk\" href=\"/review-polynet-2nd-runner-up-in-ilsvrc-2016-image-classification-8a1a941ce9ea\">PolyNet</a> (2nd Runner Up, 3.04%, Team name CU-DeepLink), ResNeXt got 3.03% Top-5 error rate, which is a large relative improvement of about 15%!!</p><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"a174\">And it is published in <strong class=\"kj lf\">2017 CVPR</strong>, which has already got over <strong class=\"kj lf\">500 citations</strong> while I was writing this story. (<a class=\"ls av bt\" href=\"/u/aff72a0c1243?source=post_page---------------------------\">Sik-Ho Tsang</a> @ Medium)</p></div></section>,\n",
       " <hr class=\"lt cs gy lu lv hi lw lx ly lz ma\"/>,\n",
       " <section class=\"hy hz ia ib ic\"><div class=\"ae fb ab di v w\"><h1 class=\"mb mc ea ap ao dy md me mf mg mh mi mj mk ml mm mn\" id=\"f006\">What Are Covered</h1><ol class=\"\"><li class=\"kh ki ea ap kj b kk mo km mp ko mq kq mr ks ms ku mt mu mv\" id=\"1929\"><strong class=\"kj lf\">Aggregated Transformation</strong></li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku mt mu mv\" id=\"801d\"><strong class=\"kj lf\">Relationship with </strong><a class=\"cy bt lh li lj lk\" href=\"/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc\"><strong class=\"kj lf\">Inception-ResNet</strong></a><strong class=\"kj lf\">, and Grouped Convolution in </strong><a class=\"cy bt lh li lj lk\" href=\"https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160\"><strong class=\"kj lf\">AlexNet</strong></a></li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku mt mu mv\" id=\"48b6\"><strong class=\"kj lf\">Full Architecture and Ablation Study</strong></li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku mt mu mv\" id=\"0375\"><strong class=\"kj lf\">Results</strong></li></ol></div></section>,\n",
       " <hr class=\"lt cs gy lu lv hi lw lx ly lz ma\"/>,\n",
       " <section class=\"hy hz ia ib ic\"><div class=\"ae fb ab di v w\"><h1 class=\"mb mc ea ap ao dy md me mf mg mh mi mj mk ml mm mn\" id=\"ad50\">1. Aggregated Transformation</h1><h2 class=\"nb mc ea ap ao dy nc nd ne nf ng nh ni nj nk nl nm\" id=\"f984\">1.1. Revisiting Simple Neuron</h2><figure class=\"ju jv jw jx jy ez nn x y paragraph-image\"><div class=\"kc l eo kd\"><div class=\"no l\"><div class=\"dk ka gg n o gf ab by t kb\"><img class=\"gg n o gf ab kf kg\" height=\"206\" src=\"https://miro.medium.com/max/60/1*lm62iiybACCoz4KR9w5Azg.png?q=20\" width=\"651\"/></div><img class=\"dk ka gg n o gf ab gd\" height=\"206\" width=\"651\"/><noscript><img class=\"gg n o gf ab\" height=\"206\" src=\"https://miro.medium.com/max/1302/1*lm62iiybACCoz4KR9w5Azg.png\" width=\"651\"/></noscript></div></div><figcaption class=\"at ed ln lo hi di x y lp ao cs\"><strong class=\"ao lf\">A Simple Neuron (Left), and the corresponding equation (Right)</strong></figcaption></figure><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"0a28\">As we should know, <strong class=\"kj lf\">a simple neuron</strong> as above, the output is the summation of wi times xi. The above operation can be recast as <strong class=\"kj lf\">a combination of splitting, transforming, and aggregating.</strong></p><ul class=\"\"><li class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku np mu mv\" id=\"beaf\"><strong class=\"kj lf\">Splitting</strong>: the vector x is sliced as a low-dimensional embedding, and in the above, it is a single-dimension subspace xi.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"28ea\"><strong class=\"kj lf\">Transforming</strong>: the low-dimensional representation is transformed, and in the above, it is simply scaled: wixi.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"6164\"><strong class=\"kj lf\">Aggregating</strong>: the transformations in all embeddings are aggregated by summation.</li></ul><h2 class=\"nb mc ea ap ao dy nc nd ne nf ng nh ni nj nk nl nm\" id=\"e151\">1.2. Aggregated Transformations</h2><figure class=\"ju jv jw jx jy ez nq x y paragraph-image\"><div class=\"kc l eo kd\"><div class=\"nr l\"><div class=\"dk ka gg n o gf ab by t kb\"><img class=\"gg n o gf ab kf kg\" height=\"298\" src=\"https://miro.medium.com/max/60/1*r_63luxLeVstJMzd0o-8Iw.png?q=20\" width=\"700\"/></div><img class=\"dk ka gg n o gf ab gd\" height=\"298\" width=\"700\"/><noscript><img class=\"gg n o gf ab\" height=\"298\" src=\"https://miro.medium.com/max/1400/1*r_63luxLeVstJMzd0o-8Iw.png\" width=\"700\"/></noscript></div></div><figcaption class=\"at ed ln lo hi di x y lp ao cs\"><strong class=\"ao lf\">A Block of ResNeXt with Cardinality = 32 (Left), and Its Generic Equation (Right)</strong></figcaption></figure><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"0733\">In contrast to “Network-in-Network”, it is “<strong class=\"kj lf\">Network-in-Neuron</strong>” expands along a new dimension. Instead of linear function in a simple neuron that wi times xi in each path, <strong class=\"kj lf\">a nonlinear function is performed for each path</strong>.</p><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"b715\">A new dimension <strong class=\"kj lf\"><em class=\"lg\">C</em> </strong>is introduced, called “<strong class=\"kj lf\">Cardinality</strong>”. The dimension of cardinality <strong class=\"kj lf\">controls the number of more complex transformations</strong>.</p></div></section>,\n",
       " <hr class=\"lt cs gy lu lv hi lw lx ly lz ma\"/>,\n",
       " <section class=\"hy hz ia ib ic\"><div class=\"ae fb ab di v w\"><h1 class=\"mb mc ea ap ao dy md me mf mg mh mi mj mk ml mm mn\" id=\"0a3f\"><strong class=\"bb\">2. Relationship with </strong><a class=\"cy bt lh li lj lk\" href=\"/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc\"><strong class=\"bb\">Inception-ResNet</strong></a><strong class=\"bb\"> and Grouped Convolution in </strong><a class=\"cy bt lh li lj lk\" href=\"https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160\"><strong class=\"bb\">AlexNet</strong></a></h1></div><div class=\"ae fb ab ac\"><figure class=\"ju jv jw jx jy ez ns nt nu paragraph-image\"><div class=\"kc l eo kd\"><div class=\"nv l\"><div class=\"dk ka gg n o gf ab by t kb\"><img class=\"gg n o gf ab kf kg\" height=\"276\" src=\"https://miro.medium.com/max/60/1*cIm3uy7eNvEchxRbBeBScQ.png?q=20\" width=\"1000\"/></div><img class=\"dk ka gg n o gf ab gd\" height=\"276\" width=\"1000\"/><noscript><img class=\"gg n o gf ab\" height=\"276\" src=\"https://miro.medium.com/max/2000/1*cIm3uy7eNvEchxRbBeBScQ.png\" width=\"1000\"/></noscript></div></div><figcaption class=\"at ed ln lo hi di x y lp ao cs\"><strong class=\"ao lf\">(a) ResNeXt Block, (b) Inception-ResNet Block, (c) Grouped Convolution</strong></figcaption></figure></div><div class=\"ae fb ab di v w\"><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"4bea\">To compare, <strong class=\"kj lf\">the above 3 blocks are having the SAME INTERNAL DIMENSIONS within each block.</strong></p><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"f444\"><strong class=\"kj lf\">(a) ResNeXt Block (Left)</strong></p><ul class=\"\"><li class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku np mu mv\" id=\"2537\">For each path, <strong class=\"kj lf\">Conv1×1–Conv3×3–Conv1×1 </strong>are done at each convolution path. This is the bottleneck design in <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet</a> block. The internal dimension for each path is denoted as <strong class=\"kj lf\"><em class=\"lg\">d</em> (<em class=\"lg\">d</em>=4)</strong>. The number of paths is the <strong class=\"kj lf\">cardinality <em class=\"lg\">C</em> (<em class=\"lg\">C</em>=32)</strong>. If we sum up the dimension of each Conv3×3 (i.e. <em class=\"lg\">d</em>×<em class=\"lg\">C</em>=4×32), it is also the dimensions of 128.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"f45b\">The dimension is increased directly from 4 to 256, and then added together, and also added with the skip connection path.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"ac99\">Compared with <a class=\"cy bt lh li lj lk\" href=\"/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc\">Inception-ResNet</a> that it needs to increase the dimension from 4 to 128 then to 256, <strong class=\"kj lf\">ResNeXt requires minimal extra effort designing each path.</strong></li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"ea06\">Unlike <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet</a>, in ResNeXt, the neurons at one path will not connected to the neurons at other paths.</li></ul><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"c543\"><strong class=\"kj lf\">(b) </strong><a class=\"cy bt lh li lj lk\" href=\"/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc\"><strong class=\"kj lf\">Inception-ResNet</strong></a><strong class=\"kj lf\"> Block (Middle)</strong></p><ul class=\"\"><li class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku np mu mv\" id=\"a564\">This is suggested in <a class=\"cy bt lh li lj lk\" href=\"/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc\">Inception-v4</a> to combine the <a class=\"cy bt lh li lj lk\" href=\"https://medium.com/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7\">Inception</a> module and <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet</a> block. Somehow due to the legacy problem, for each convolution path, <strong class=\"kj lf\">Conv1×1–Conv3×3 </strong>are done first. When added together (i.e. 4×32), the Conv3×3 has the dimension of 128.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"1309\">Then the outputs are concatenated together with dimension of 128. And <strong class=\"kj lf\">Conv1×1</strong> is used to restore the dimensions from 128 to 256.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"f428\">Finally the output is added with the skip connection path.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"8480\">The main difference is that they have an <strong class=\"kj lf\">early concatenation</strong>.</li></ul><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"2f32\"><strong class=\"kj lf\">(c) Grouped Convolution in </strong><a class=\"cy bt lh li lj lk\" href=\"https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160\"><strong class=\"kj lf\">AlexNet</strong></a><strong class=\"kj lf\"> (Right)</strong></p><ul class=\"\"><li class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku np mu mv\" id=\"b344\"><strong class=\"kj lf\">Conv1×1–Conv3×3–Conv1×1 </strong>are done at the convolution path, which is actually a<strong class=\"kj lf\"> bottleneck design</strong> suggested in <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet</a>. The Conv3×3 has the dimension of 128.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"9c7b\">However, <strong class=\"kj lf\">grouped convolution</strong>, suggested in <a class=\"cy bt lh li lj lk\" href=\"https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160\">AlexNet</a> is used here. Therefore this Conv3×3 is <strong class=\"kj lf\">wider but sparsely connected module. </strong>(Because the neurons at one path will not connected to the neurons at other paths, that’s why it is sparsely connected.)</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"5883\">Thus, there are <strong class=\"kj lf\">32 groups of convolutions</strong>. (2 groups only in <a class=\"cy bt lh li lj lk\" href=\"https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160\">AlexNet</a>)</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"eae5\">Then a skip connection is at parallel and added with the convolution path. Thus, the convolution path is learning the residual representation.</li></ul><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"e9f0\">Though the structures in (b) and (c) are not always the same as the general form in the equation shown in 1.2, indeed authors have tried the above three structures as shown above, and they found that the results are the same.</p><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"9345\"><strong class=\"kj lf\">Finally, authors choose to implement the structure in (c) because it is more succinct and faster than the other two forms.</strong></p></div></section>,\n",
       " <hr class=\"lt cs gy lu lv hi lw lx ly lz ma\"/>,\n",
       " <section class=\"hy hz ia ib ic\"><div class=\"ae fb ab di v w\"><h1 class=\"mb mc ea ap ao dy md me mf mg mh mi mj mk ml mm mn\" id=\"e7a5\">3. <strong class=\"bb\">Full Architecture and Ablation Study</strong></h1><h2 class=\"nb mc ea ap ao dy nc nd ne nf ng nh ni nj nk nl nm\" id=\"e30a\">3.1. Ablation Study of <em class=\"nw\">C</em> and <em class=\"nw\">d</em> Under Similar Complexity</h2></div><div class=\"ae fb ab ac\"><figure class=\"ju jv jw jx jy ez nx nt nu paragraph-image\"><div class=\"kc l eo kd\"><div class=\"ny l\"><div class=\"dk ka gg n o gf ab by t kb\"><img class=\"gg n o gf ab kf kg\" height=\"550\" src=\"https://miro.medium.com/max/60/1*UNkmAg1JVfprEfBBmqM44w.png?q=20\" width=\"1000\"/></div><img class=\"dk ka gg n o gf ab gd\" height=\"550\" width=\"1000\"/><noscript><img class=\"gg n o gf ab\" height=\"550\" src=\"https://miro.medium.com/max/2000/1*UNkmAg1JVfprEfBBmqM44w.png\" width=\"1000\"/></noscript></div></div><figcaption class=\"at ed ln lo hi di x y lp ao cs\"><strong class=\"ao lf\">Detailed Architecture (Left), Number of Parameters for Each Block (Top Right), Different Settings to Maintain Similar Complexity (Middle Right), Ablation Study for Different Settings Under Similar Complexity (Bottom Right)</strong></figcaption></figure></div><div class=\"ae fb ab di v w\"><ul class=\"\"><li class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku np mu mv\" id=\"6013\"><a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet-50</a> is a special case of ResNeXt-50 with <em class=\"lg\">C</em>=1, <em class=\"lg\">d</em>=64.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"eb48\">To have fair comparison, different ResNeXt with different <em class=\"lg\">C</em> and <em class=\"lg\">d</em> with similar complexity with <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet</a> are tried. This is shown at the middle right of the figure above.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"a697\">And it is found that ResNeXt-50 (32×4d) obtains 22.2% top-1 error for ImageNet-1K (1K means 1K classes) dataset while <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet-50</a> only obtains 23.9% top-1 error.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"6475\">And ResNeXt-101 (32×4d) obtains 21.2% top-1 error for ImageNet dataset while <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet-101</a> only obtains 22.0% top-1 error.</li></ul><h2 class=\"nb mc ea ap ao dy nc nd ne nf ng nh ni nj nk nl nm\" id=\"f880\">3.2. Importance of Cardinality</h2><figure class=\"ju jv jw jx jy ez nz x y paragraph-image\"><div class=\"kc l eo kd\"><div class=\"oa l\"><div class=\"dk ka gg n o gf ab by t kb\"><img class=\"gg n o gf ab kf kg\" height=\"344\" src=\"https://miro.medium.com/max/60/1*xxm28sWSo6i-If09kwAqPQ.png?q=20\" width=\"666\"/></div><img class=\"dk ka gg n o gf ab gd\" height=\"344\" width=\"666\"/><noscript><img class=\"gg n o gf ab\" height=\"344\" src=\"https://miro.medium.com/max/1332/1*xxm28sWSo6i-If09kwAqPQ.png\" width=\"666\"/></noscript></div></div><figcaption class=\"at ed ln lo hi di x y lp ao cs\"><strong class=\"ao lf\">Ablation Study for Different Settings of 2× Complexity Models</strong></figcaption></figure><ul class=\"\"><li class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku np mu mv\" id=\"014c\"><a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\"><strong class=\"kj lf\">ResNet-200</strong></a>: 21.7% top-1 and 5.8% top-5 error rates.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"4fd4\"><a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\"><strong class=\"kj lf\">ResNet-101, wider</strong></a>: only obtains 21.3% top-1 and 5.7% top-5 error rates, which means <strong class=\"kj lf\">only making it wider does not help much</strong>.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"4b39\"><strong class=\"kj lf\">ResNeXt-101 (2×64d)</strong>: By just making <strong class=\"kj lf\"><em class=\"lg\">C</em>=2</strong> (i.e. two convolution paths within the ResNeXt block), <strong class=\"kj lf\">an obvious improvement is already obtained</strong> with 20.7% top-1 and 5.5% top-5 error rates.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"0a63\"><strong class=\"kj lf\">ResNeXt-101 (64×4d)</strong>: By making <strong class=\"kj lf\"><em class=\"lg\">C</em>=64</strong> (i.e. two convolution paths within the ResNeXt block), <strong class=\"kj lf\">an even better improvement is already obtained</strong> with 20.4% top-1 and 5.3% top-5 error rates. This means <strong class=\"kj lf\">cardinality is essential to improve the classification accuracy</strong>.</li></ul><h2 class=\"nb mc ea ap ao dy nc nd ne nf ng nh ni nj nk nl nm\" id=\"57d7\">3.2. Importance of Residual Connections</h2><figure class=\"ju jv jw jx jy ez ob x y paragraph-image\"><div class=\"kc l eo kd\"><div class=\"oc l\"><div class=\"dk ka gg n o gf ab by t kb\"><img class=\"gg n o gf ab kf kg\" height=\"119\" src=\"https://miro.medium.com/max/60/1*EwrkhL-fqVii9MEjCHMZBA.png?q=20\" width=\"571\"/></div><img class=\"dk ka gg n o gf ab gd\" height=\"119\" width=\"571\"/><noscript><img class=\"gg n o gf ab\" height=\"119\" src=\"https://miro.medium.com/max/1142/1*EwrkhL-fqVii9MEjCHMZBA.png\" width=\"571\"/></noscript></div></div><figcaption class=\"at ed ln lo hi di x y lp ao cs\"><strong class=\"ao lf\">Importance of Residual Connections</strong></figcaption></figure><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"39d7\">Without residual connections, error rates are increased largely for both <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet-50</a> and ResNeXt-50. Residual connections are important.</p></div></section>,\n",
       " <hr class=\"lt cs gy lu lv hi lw lx ly lz ma\"/>,\n",
       " <section class=\"hy hz ia ib ic\"><div class=\"ae fb ab di v w\"><h1 class=\"mb mc ea ap ao dy md me mf mg mh mi mj mk ml mm mn\" id=\"558f\">4. Results</h1><h2 class=\"nb mc ea ap ao dy nc nd ne nf ng nh ni nj nk nl nm\" id=\"a94e\">4.1. ImageNet-1K</h2></div><div class=\"ae fb ab ac\"><figure class=\"ju jv jw jx jy ez od nt nu paragraph-image\"><div class=\"kc l eo kd\"><div class=\"oe l\"><div class=\"dk ka gg n o gf ab by t kb\"><img class=\"gg n o gf ab kf kg\" height=\"291\" src=\"https://miro.medium.com/max/60/1*oLRaAqY2cnw2E5eJ_D8jmA.png?q=20\" width=\"1000\"/></div><img class=\"dk ka gg n o gf ab gd\" height=\"291\" width=\"1000\"/><noscript><img class=\"gg n o gf ab\" height=\"291\" src=\"https://miro.medium.com/max/2000/1*oLRaAqY2cnw2E5eJ_D8jmA.png\" width=\"1000\"/></noscript></div></div><figcaption class=\"at ed ln lo hi di x y lp ao cs\"><strong class=\"ao lf\">Single Crop Testing: ResNet/ResNeXt is 224×224 and 320×320, Inception models: 299×299</strong></figcaption></figure></div><div class=\"ae fb ab di v w\"><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"d9ed\">ImageNet-1K is a subset of 22K-class ImageNet dataset, which contains 1000 classes. It is also the dataset for ILSVRC classification task.</p><ul class=\"\"><li class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku np mu mv\" id=\"752e\">With standard size image used for single crop testing, ResNeXt-101 obtains 20.4% top-1 and 5.3% top-5 error rates,</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"b264\">With larger size image used for single crop testing, ResNeXt-101 obtains 19.1% top-1 and 4.4% top-5 error rates, which has better results than all state-of-the-art approaches, <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet</a>, <a class=\"cy bt lh li lj lk\" href=\"/resnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e\">Pre-Activation ResNet</a>, <a class=\"cy bt lh li lj lk\" href=\"https://medium.com/@sh.tsang/review-inception-v3-1st-runner-up-image-classification-in-ilsvrc-2015-17915421f77c\">Inception-v3</a>, <a class=\"cy bt lh li lj lk\" href=\"/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc\">Inception-v4</a> and <a class=\"cy bt lh li lj lk\" href=\"/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc\">Inception-ResNet-v2</a>.</li></ul><h2 class=\"nb mc ea ap ao dy nc nd ne nf ng nh ni nj nk nl nm\" id=\"321e\">4.2. ImageNet-5K</h2></div><div class=\"ae fb ab ac\"><figure class=\"ju jv jw jx jy ez of nt nu paragraph-image\"><div class=\"kc l eo kd\"><div class=\"og l\"><div class=\"dk ka gg n o gf ab by t kb\"><img class=\"gg n o gf ab kf kg\" height=\"417\" src=\"https://miro.medium.com/max/60/1*V99JmLNTxlXxIPYAIFiGwA.png?q=20\" width=\"1000\"/></div><img class=\"dk ka gg n o gf ab gd\" height=\"417\" width=\"1000\"/><noscript><img class=\"gg n o gf ab\" height=\"417\" src=\"https://miro.medium.com/max/2000/1*V99JmLNTxlXxIPYAIFiGwA.png\" width=\"1000\"/></noscript></div></div><figcaption class=\"at ed ln lo hi di x y lp ao cs\"><strong class=\"ao lf\">ImageNet-5K Results (All trained from scratch)</strong></figcaption></figure></div><div class=\"ae fb ab di v w\"><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"97a7\">ImageNet-1K has been somehow saturated after so many years of development.</p><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"9240\">ImageNet-5K is a subset of 22K-class ImageNet dataset, which contains 5000 classes, which also contains ImageNet-1K classes.</p><ul class=\"\"><li class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku np mu mv\" id=\"b179\">6.8 million images, 5× of the ImageNet-1K dataset.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"73ed\">Since there is no official train/validation set, the original ImageNet-1K validation set is used for evaluation.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"c2cd\"><strong class=\"kj lf\">5K-way classification</strong> is the <strong class=\"kj lf\">softmax over 5K classes</strong>. Thus, there will be automatic errors when the network predicts the labels for the other 4K classes on the ImageNet-1K validation dataset.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"fccb\"><strong class=\"kj lf\">1K-way classification</strong> is just the <strong class=\"kj lf\">softmax over 1K classes</strong>.</li></ul><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"34a9\">ResNeXt of course got better results than <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet</a> as shown above.</p><h2 class=\"nb mc ea ap ao dy nc nd ne nf ng nh ni nj nk nl nm\" id=\"f627\">4.3. CIFAR-10 &amp; CIFAR-100</h2></div><div class=\"ae fb ab ac\"><figure class=\"ju jv jw jx jy ez oh nt nu paragraph-image\"><div class=\"kc l eo kd\"><div class=\"oi l\"><div class=\"dk ka gg n o gf ab by t kb\"><img class=\"gg n o gf ab kf kg\" height=\"435\" src=\"https://miro.medium.com/max/60/1*FqDUkl5vrLZufkQIG6xk7A.png?q=20\" width=\"1000\"/></div><img class=\"dk ka gg n o gf ab gd\" height=\"435\" width=\"1000\"/><noscript><img class=\"gg n o gf ab\" height=\"435\" src=\"https://miro.medium.com/max/2000/1*FqDUkl5vrLZufkQIG6xk7A.png\" width=\"1000\"/></noscript></div></div><figcaption class=\"at ed ln lo hi di x y lp ao cs\"><strong class=\"ao lf\">CIFAR-10 and CIFAR-100 Results</strong></figcaption></figure></div><div class=\"ae fb ab di v w\"><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"4572\">CIFAR-10 &amp; CIFAR-100, two very famous 10-class and 100-class datasets.</p><ul class=\"\"><li class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku np mu mv\" id=\"f629\">Left: Compared with <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet</a>, ResNeXt always obtains better results in CIFAR-10.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"e839\">Right: Compared with <a class=\"cy bt lh li lj lk\" href=\"/review-wrns-wide-residual-networks-image-classification-d3feb3fb2004\">Wide ResNet (WRN)</a>, ResNeXt-29 (16×64d) obtains 3.58% and 17.31% errors for CIFAR-10 and CIFAR-100 respectively. These were the best results among all state-of-the-art approaches at that moment.</li></ul><h2 class=\"nb mc ea ap ao dy nc nd ne nf ng nh ni nj nk nl nm\" id=\"39c2\">4.4. MS COCO Object Detection</h2><figure class=\"ju jv jw jx jy ez oj x y paragraph-image\"><div class=\"kc l eo kd\"><div class=\"ok l\"><div class=\"dk ka gg n o gf ab by t kb\"><img class=\"gg n o gf ab kf kg\" height=\"190\" src=\"https://miro.medium.com/max/60/1*_QhPxRX7B0jROHwhIuljCQ.png?q=20\" width=\"592\"/></div><img class=\"dk ka gg n o gf ab gd\" height=\"190\" width=\"592\"/><noscript><img class=\"gg n o gf ab\" height=\"190\" src=\"https://miro.medium.com/max/1184/1*_QhPxRX7B0jROHwhIuljCQ.png\" width=\"592\"/></noscript></div></div><figcaption class=\"at ed ln lo hi di x y lp ao cs\"><strong class=\"ao lf\">MS COCO Objection Detection Results</strong></figcaption></figure><ul class=\"\"><li class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku np mu mv\" id=\"7c48\">By plugging <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet</a>/ResNeXt into <a class=\"cy bt lh li lj lk\" href=\"/review-faster-r-cnn-object-detection-f5685cb30202\">Faster R-CNN</a>, with similar model complexity, ResNeXt always outperforms <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet</a> for both AP@0.5 (IoU&gt;0.5) and mean AP (average prediction) at all IoU levels.</li></ul></div></section>,\n",
       " <hr class=\"lt cs gy lu lv hi lw lx ly lz ma\"/>,\n",
       " <section class=\"hy hz ia ib ic\"><div class=\"ae fb ab di v w\"><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"8543\">With the success of ResNeXt, it is also utilized by Mask R-CNN for instance segmentation. Hope I can cover Mask R-CNN later on as well.</p></div></section>,\n",
       " <hr class=\"lt cs gy lu lv hi lw lx ly lz ma\"/>,\n",
       " <section class=\"hy hz ia ib ic\"><div class=\"ae fb ab di v w\"><h2 class=\"nb mc ea ap ao dy nc nd ne nf ng nh ni nj nk nl nm\" id=\"2ad8\">Reference</h2><p class=\"kh ki ea ap kj b kk mo km mp ko mq kq mr ks ms ku\" id=\"0b9a\">[2017 CVPR] [ResNeXt]<br/><a class=\"cy bt lh li lj lk\" href=\"https://arxiv.org/abs/1611.05431\">Aggregated Residual Transformations for Deep Neural Networks</a></p><h2 class=\"nb mc ea ap ao dy nc nd ne nf ng nh ni nj nk nl nm\" id=\"b06a\">My Related Reviews on Image Classification</h2><p class=\"kh ki ea ap kj b kk mo km mp ko mq kq mr ks ms ku\" id=\"79f2\">[<a class=\"cy bt lh li lj lk\" href=\"https://medium.com/@sh.tsang/paper-brief-review-of-lenet-1-lenet-4-lenet-5-boosted-lenet-4-image-classification-1f5f809dbf17\">LeNet</a>] [<a class=\"cy bt lh li lj lk\" href=\"https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160\">AlexNet</a>] [<a class=\"cy bt lh li lj lk\" href=\"https://medium.com/coinmonks/paper-review-of-zfnet-the-winner-of-ilsvlc-2013-image-classification-d1a5a0c45103\">ZFNet</a>] [<a class=\"cy bt lh li lj lk\" href=\"https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11\">VGGNet</a>] [<a class=\"cy bt lh li lj lk\" href=\"https://medium.com/coinmonks/review-sppnet-1st-runner-up-object-detection-2nd-runner-up-image-classification-in-ilsvrc-906da3753679\">SPPNet</a>] [<a class=\"cy bt lh li lj lk\" href=\"https://medium.com/coinmonks/review-prelu-net-the-first-to-surpass-human-level-performance-in-ilsvrc-2015-image-f619dddd5617\">PReLU-Net</a>] [<a class=\"cy bt lh li lj lk\" href=\"https://medium.com/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7\">GoogLeNet / Inception-v1</a>] [<a class=\"cy bt lh li lj lk\" href=\"https://medium.com/@sh.tsang/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651\">BN-Inception / Inception-v2</a>] [<a class=\"cy bt lh li lj lk\" href=\"https://medium.com/@sh.tsang/review-inception-v3-1st-runner-up-image-classification-in-ilsvrc-2015-17915421f77c\">Inception-v3</a>] [<a class=\"cy bt lh li lj lk\" href=\"/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc\">Inception-v4</a>] [<a class=\"cy bt lh li lj lk\" href=\"/review-xception-with-depthwise-separable-convolution-better-than-inception-v3-image-dc967dd42568\">Xception</a>] [<a class=\"cy bt lh li lj lk\" href=\"/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69\">MobileNetV1</a>] [<a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet</a>] [<a class=\"cy bt lh li lj lk\" href=\"/resnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e\">Pre-Activation ResNet</a>] [<a class=\"cy bt lh li lj lk\" href=\"https://medium.com/@sh.tsang/review-rir-resnet-in-resnet-image-classification-be4c79fde8ba\">RiR</a>] [<a class=\"cy bt lh li lj lk\" href=\"/review-ror-resnet-of-resnet-multilevel-resnet-image-classification-cd3b0fcc19bb\">RoR</a>] [<a class=\"cy bt lh li lj lk\" href=\"/review-stochastic-depth-image-classification-a4e225807f4a\">Stochastic Depth</a>] [<a class=\"cy bt lh li lj lk\" href=\"/review-wrns-wide-residual-networks-image-classification-d3feb3fb2004\">WRN</a>] [<a class=\"cy bt lh li lj lk\" href=\"/review-polynet-2nd-runner-up-in-ilsvrc-2016-image-classification-8a1a941ce9ea\">PolyNet</a>] [<a class=\"cy bt lh li lj lk\" href=\"/review-densenet-image-classification-b6631a8ef803\">DenseNet</a>]</p><h2 class=\"nb mc ea ap ao dy nc nd ne nf ng nh ni nj nk nl nm\" id=\"030e\">My Related Reviews on Object Detection</h2><p class=\"kh ki ea ap kj b kk mo km mp ko mq kq mr ks ms ku\" id=\"2a8b\">[<a class=\"cy bt lh li lj lk\" href=\"/review-faster-r-cnn-object-detection-f5685cb30202\">Faster R-CNN</a>]</p></div></section>]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(post_contents.div.children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__bool__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__copy__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " '__weakref__',\n",
       " '_all_strings',\n",
       " '_find_all',\n",
       " '_find_one',\n",
       " '_is_xml',\n",
       " '_lastRecursiveChild',\n",
       " '_last_descendant',\n",
       " '_should_pretty_print',\n",
       " 'append',\n",
       " 'attrs',\n",
       " 'can_be_empty_element',\n",
       " 'cdata_list_attributes',\n",
       " 'childGenerator',\n",
       " 'children',\n",
       " 'clear',\n",
       " 'contents',\n",
       " 'decode',\n",
       " 'decode_contents',\n",
       " 'decompose',\n",
       " 'descendants',\n",
       " 'encode',\n",
       " 'encode_contents',\n",
       " 'extend',\n",
       " 'extract',\n",
       " 'fetchNextSiblings',\n",
       " 'fetchParents',\n",
       " 'fetchPrevious',\n",
       " 'fetchPreviousSiblings',\n",
       " 'find',\n",
       " 'findAll',\n",
       " 'findAllNext',\n",
       " 'findAllPrevious',\n",
       " 'findChild',\n",
       " 'findChildren',\n",
       " 'findNext',\n",
       " 'findNextSibling',\n",
       " 'findNextSiblings',\n",
       " 'findParent',\n",
       " 'findParents',\n",
       " 'findPrevious',\n",
       " 'findPreviousSibling',\n",
       " 'findPreviousSiblings',\n",
       " 'find_all',\n",
       " 'find_all_next',\n",
       " 'find_all_previous',\n",
       " 'find_next',\n",
       " 'find_next_sibling',\n",
       " 'find_next_siblings',\n",
       " 'find_parent',\n",
       " 'find_parents',\n",
       " 'find_previous',\n",
       " 'find_previous_sibling',\n",
       " 'find_previous_siblings',\n",
       " 'format_string',\n",
       " 'formatter_for_name',\n",
       " 'get',\n",
       " 'getText',\n",
       " 'get_attribute_list',\n",
       " 'get_text',\n",
       " 'has_attr',\n",
       " 'has_key',\n",
       " 'hidden',\n",
       " 'index',\n",
       " 'insert',\n",
       " 'insert_after',\n",
       " 'insert_before',\n",
       " 'isSelfClosing',\n",
       " 'is_empty_element',\n",
       " 'known_xml',\n",
       " 'name',\n",
       " 'namespace',\n",
       " 'next',\n",
       " 'nextGenerator',\n",
       " 'nextSibling',\n",
       " 'nextSiblingGenerator',\n",
       " 'next_element',\n",
       " 'next_elements',\n",
       " 'next_sibling',\n",
       " 'next_siblings',\n",
       " 'parent',\n",
       " 'parentGenerator',\n",
       " 'parents',\n",
       " 'parserClass',\n",
       " 'parser_class',\n",
       " 'prefix',\n",
       " 'preserve_whitespace_tags',\n",
       " 'prettify',\n",
       " 'previous',\n",
       " 'previousGenerator',\n",
       " 'previousSibling',\n",
       " 'previousSiblingGenerator',\n",
       " 'previous_element',\n",
       " 'previous_elements',\n",
       " 'previous_sibling',\n",
       " 'previous_siblings',\n",
       " 'recursiveChildGenerator',\n",
       " 'renderContents',\n",
       " 'replaceWith',\n",
       " 'replaceWithChildren',\n",
       " 'replace_with',\n",
       " 'replace_with_children',\n",
       " 'select',\n",
       " 'select_one',\n",
       " 'setup',\n",
       " 'smooth',\n",
       " 'string',\n",
       " 'strings',\n",
       " 'stripped_strings',\n",
       " 'text',\n",
       " 'unwrap',\n",
       " 'wrap']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_contents=html_soup.find_all('article')[0]#, attrs={'id':'root'})\n",
    "dir(post_contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_containers = html_soup.find_all('img', class_ = 'sg sh gg n o gf ab gd')\n",
    "img_containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div><div class=\"gg o hv hw hx dl\"></div><section class=\"hy hz ia ib ic\"><div class=\"ae fb ab di v w\"><div><div class=\"id ie ea ap if b ig ih ii ij ik il im\" id=\"1ef0\"><h1 class=\"if b ig in ea\">Review: ResNeXt — 1st Runner Up in ILSVRC 2016 (Image Classification)</h1></div></div><div class=\"io ie at ap ao cs ip iq ir is it iu iv\" id=\"4c9a\"><h2 class=\"ao cs iw ix at\"><strong class=\"bb\">Network-in-Neuron, A New Dimensionality: Cardinality</strong></h2></div><div class=\"iy\"><div class=\"ag af\"><div><a href=\"/@sh.tsang?source=post_page---------------------------\"><img alt=\"Sik-Ho Tsang\" class=\"l fn iz ja\" height=\"48\" src=\"https://miro.medium.com/fit/c/96/96/1*OxjNUHcLFU8-pp-j8su6pg.jpeg\" width=\"48\"/></a></div><div class=\"jb ab l\"><div class=\"af\"><div style=\"flex:1\"><span class=\"ao b ap aq ar as l ea al\"><div class=\"jc af ag jd\" data-test-id=\"postByline\"><span class=\"ao cs ed aq by je ef eg jf ei ea\"><a class=\"cy cz ax ay az ba bb bc bd be jg bh bi dw dx\" href=\"/@sh.tsang?source=post_page---------------------------\">Sik-Ho Tsang</a></span><div class=\"jh l am h\"><button class=\"ji ea al bm jj jk jl jm be dw jn jo jp jq jr js bp ao b ap jt ct as bq br ae bs bt bh\">Follow</button></div></div></span></div></div><span class=\"ao b ap aq ar as l at au\"><span class=\"ao cs ed aq by je ef eg jf ei at\"><div><a class=\"cy cz ax ay az ba bb bc bd be jg bh bi dw dx\" href=\"/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac?source=post_page---------------------------\">Dec 9, 2018</a> <!-- -->·<!-- --> <!-- -->7<!-- --> min read</div></span></span></div></div></div><figure class=\"ju jv jw jx jy ez jz x y paragraph-image\"><div class=\"kc l eo kd\"><div class=\"ke l\"><div class=\"dk ka gg n o gf ab by t kb\"><img class=\"gg n o gf ab kf kg\" height=\"467\" src=\"https://miro.medium.com/max/60/0*MvA7JX1Z1rAG5byo?q=20\" width=\"700\"/></div><img class=\"dk ka gg n o gf ab gd\" height=\"467\" width=\"700\"/><noscript><img class=\"gg n o gf ab\" height=\"467\" src=\"https://miro.medium.com/max/1400/0*MvA7JX1Z1rAG5byo\" width=\"700\"/></noscript></div></div></figure><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku kv\" id=\"ecb1\"><span class=\"l kw kx ky kz la lb lc ld le eo\">In</span> this story, <strong class=\"kj lf\">ResNeXt, </strong>by <strong class=\"kj lf\">UC San Diego </strong>and <strong class=\"kj lf\">Facebook AI Research (FAIR), </strong>is reviewed. The model name, ResNeXt, contains Next. It means the <em class=\"lg\">next </em>dimension, on top of the <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet</a>. This next dimension is called the “<strong class=\"kj lf\"><em class=\"lg\">cardinality</em></strong>” <strong class=\"kj lf\">dimension</strong>. And ResNeXt becomes the <strong class=\"kj lf\">1st Runner Up of ILSVRC classification task</strong>.</p><figure class=\"ju jv jw jx jy ez ll x y paragraph-image\"><div class=\"kc l eo kd\"><div class=\"lm l\"><div class=\"dk ka gg n o gf ab by t kb\"><img class=\"gg n o gf ab kf kg\" height=\"421\" src=\"https://miro.medium.com/max/60/1*RHpn70qFNCcqyVjkPdFtGA.png?q=20\" width=\"700\"/></div><img class=\"dk ka gg n o gf ab gd\" height=\"421\" width=\"700\"/><noscript><img class=\"gg n o gf ab\" height=\"421\" src=\"https://miro.medium.com/max/1400/1*RHpn70qFNCcqyVjkPdFtGA.png\" width=\"700\"/></noscript></div></div><figcaption class=\"at ed ln lo hi di x y lp ao cs\"><strong class=\"ao lf\">ILSVRC 2016 Classification Ranking </strong><a class=\"cy bt lh li lj lk\" href=\"http://image-net.org/challenges/LSVRC/2016/results#loc\">http://image-net.org/challenges/LSVRC/2016/results#loc</a></figcaption></figure><figure class=\"ju jv jw jx jy ez lq x y paragraph-image\"><div class=\"kc l eo kd\"><div class=\"lr l\"><div class=\"dk ka gg n o gf ab by t kb\"><img class=\"gg n o gf ab kf kg\" height=\"307\" src=\"https://miro.medium.com/max/60/1*LOoc11tkDoqv0pC6OH7mwA.png?q=20\" width=\"700\"/></div><img class=\"dk ka gg n o gf ab gd\" height=\"307\" width=\"700\"/><noscript><img class=\"gg n o gf ab\" height=\"307\" src=\"https://miro.medium.com/max/1400/1*LOoc11tkDoqv0pC6OH7mwA.png\" width=\"700\"/></noscript></div></div><figcaption class=\"at ed ln lo hi di x y lp ao cs\"><strong class=\"ao lf\">Residual Block in ResNet (Left), A Block of ResNeXt with Cardinality = 32 (Right)</strong></figcaption></figure><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"f37a\">Compared with <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet</a> (The winner in ILSVRC 2015, 3.57%) and <a class=\"cy bt lh li lj lk\" href=\"/review-polynet-2nd-runner-up-in-ilsvrc-2016-image-classification-8a1a941ce9ea\">PolyNet</a> (2nd Runner Up, 3.04%, Team name CU-DeepLink), ResNeXt got 3.03% Top-5 error rate, which is a large relative improvement of about 15%!!</p><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"a174\">And it is published in <strong class=\"kj lf\">2017 CVPR</strong>, which has already got over <strong class=\"kj lf\">500 citations</strong> while I was writing this story. (<a class=\"ls av bt\" href=\"/u/aff72a0c1243?source=post_page---------------------------\">Sik-Ho Tsang</a> @ Medium)</p></div></section><hr class=\"lt cs gy lu lv hi lw lx ly lz ma\"/><section class=\"hy hz ia ib ic\"><div class=\"ae fb ab di v w\"><h1 class=\"mb mc ea ap ao dy md me mf mg mh mi mj mk ml mm mn\" id=\"f006\">What Are Covered</h1><ol class=\"\"><li class=\"kh ki ea ap kj b kk mo km mp ko mq kq mr ks ms ku mt mu mv\" id=\"1929\"><strong class=\"kj lf\">Aggregated Transformation</strong></li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku mt mu mv\" id=\"801d\"><strong class=\"kj lf\">Relationship with </strong><a class=\"cy bt lh li lj lk\" href=\"/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc\"><strong class=\"kj lf\">Inception-ResNet</strong></a><strong class=\"kj lf\">, and Grouped Convolution in </strong><a class=\"cy bt lh li lj lk\" href=\"https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160\"><strong class=\"kj lf\">AlexNet</strong></a></li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku mt mu mv\" id=\"48b6\"><strong class=\"kj lf\">Full Architecture and Ablation Study</strong></li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku mt mu mv\" id=\"0375\"><strong class=\"kj lf\">Results</strong></li></ol></div></section><hr class=\"lt cs gy lu lv hi lw lx ly lz ma\"/><section class=\"hy hz ia ib ic\"><div class=\"ae fb ab di v w\"><h1 class=\"mb mc ea ap ao dy md me mf mg mh mi mj mk ml mm mn\" id=\"ad50\">1. Aggregated Transformation</h1><h2 class=\"nb mc ea ap ao dy nc nd ne nf ng nh ni nj nk nl nm\" id=\"f984\">1.1. Revisiting Simple Neuron</h2><figure class=\"ju jv jw jx jy ez nn x y paragraph-image\"><div class=\"kc l eo kd\"><div class=\"no l\"><div class=\"dk ka gg n o gf ab by t kb\"><img class=\"gg n o gf ab kf kg\" height=\"206\" src=\"https://miro.medium.com/max/60/1*lm62iiybACCoz4KR9w5Azg.png?q=20\" width=\"651\"/></div><img class=\"dk ka gg n o gf ab gd\" height=\"206\" width=\"651\"/><noscript><img class=\"gg n o gf ab\" height=\"206\" src=\"https://miro.medium.com/max/1302/1*lm62iiybACCoz4KR9w5Azg.png\" width=\"651\"/></noscript></div></div><figcaption class=\"at ed ln lo hi di x y lp ao cs\"><strong class=\"ao lf\">A Simple Neuron (Left), and the corresponding equation (Right)</strong></figcaption></figure><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"0a28\">As we should know, <strong class=\"kj lf\">a simple neuron</strong> as above, the output is the summation of wi times xi. The above operation can be recast as <strong class=\"kj lf\">a combination of splitting, transforming, and aggregating.</strong></p><ul class=\"\"><li class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku np mu mv\" id=\"beaf\"><strong class=\"kj lf\">Splitting</strong>: the vector x is sliced as a low-dimensional embedding, and in the above, it is a single-dimension subspace xi.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"28ea\"><strong class=\"kj lf\">Transforming</strong>: the low-dimensional representation is transformed, and in the above, it is simply scaled: wixi.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"6164\"><strong class=\"kj lf\">Aggregating</strong>: the transformations in all embeddings are aggregated by summation.</li></ul><h2 class=\"nb mc ea ap ao dy nc nd ne nf ng nh ni nj nk nl nm\" id=\"e151\">1.2. Aggregated Transformations</h2><figure class=\"ju jv jw jx jy ez nq x y paragraph-image\"><div class=\"kc l eo kd\"><div class=\"nr l\"><div class=\"dk ka gg n o gf ab by t kb\"><img class=\"gg n o gf ab kf kg\" height=\"298\" src=\"https://miro.medium.com/max/60/1*r_63luxLeVstJMzd0o-8Iw.png?q=20\" width=\"700\"/></div><img class=\"dk ka gg n o gf ab gd\" height=\"298\" width=\"700\"/><noscript><img class=\"gg n o gf ab\" height=\"298\" src=\"https://miro.medium.com/max/1400/1*r_63luxLeVstJMzd0o-8Iw.png\" width=\"700\"/></noscript></div></div><figcaption class=\"at ed ln lo hi di x y lp ao cs\"><strong class=\"ao lf\">A Block of ResNeXt with Cardinality = 32 (Left), and Its Generic Equation (Right)</strong></figcaption></figure><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"0733\">In contrast to “Network-in-Network”, it is “<strong class=\"kj lf\">Network-in-Neuron</strong>” expands along a new dimension. Instead of linear function in a simple neuron that wi times xi in each path, <strong class=\"kj lf\">a nonlinear function is performed for each path</strong>.</p><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"b715\">A new dimension <strong class=\"kj lf\"><em class=\"lg\">C</em> </strong>is introduced, called “<strong class=\"kj lf\">Cardinality</strong>”. The dimension of cardinality <strong class=\"kj lf\">controls the number of more complex transformations</strong>.</p></div></section><hr class=\"lt cs gy lu lv hi lw lx ly lz ma\"/><section class=\"hy hz ia ib ic\"><div class=\"ae fb ab di v w\"><h1 class=\"mb mc ea ap ao dy md me mf mg mh mi mj mk ml mm mn\" id=\"0a3f\"><strong class=\"bb\">2. Relationship with </strong><a class=\"cy bt lh li lj lk\" href=\"/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc\"><strong class=\"bb\">Inception-ResNet</strong></a><strong class=\"bb\"> and Grouped Convolution in </strong><a class=\"cy bt lh li lj lk\" href=\"https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160\"><strong class=\"bb\">AlexNet</strong></a></h1></div><div class=\"ae fb ab ac\"><figure class=\"ju jv jw jx jy ez ns nt nu paragraph-image\"><div class=\"kc l eo kd\"><div class=\"nv l\"><div class=\"dk ka gg n o gf ab by t kb\"><img class=\"gg n o gf ab kf kg\" height=\"276\" src=\"https://miro.medium.com/max/60/1*cIm3uy7eNvEchxRbBeBScQ.png?q=20\" width=\"1000\"/></div><img class=\"dk ka gg n o gf ab gd\" height=\"276\" width=\"1000\"/><noscript><img class=\"gg n o gf ab\" height=\"276\" src=\"https://miro.medium.com/max/2000/1*cIm3uy7eNvEchxRbBeBScQ.png\" width=\"1000\"/></noscript></div></div><figcaption class=\"at ed ln lo hi di x y lp ao cs\"><strong class=\"ao lf\">(a) ResNeXt Block, (b) Inception-ResNet Block, (c) Grouped Convolution</strong></figcaption></figure></div><div class=\"ae fb ab di v w\"><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"4bea\">To compare, <strong class=\"kj lf\">the above 3 blocks are having the SAME INTERNAL DIMENSIONS within each block.</strong></p><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"f444\"><strong class=\"kj lf\">(a) ResNeXt Block (Left)</strong></p><ul class=\"\"><li class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku np mu mv\" id=\"2537\">For each path, <strong class=\"kj lf\">Conv1×1–Conv3×3–Conv1×1 </strong>are done at each convolution path. This is the bottleneck design in <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet</a> block. The internal dimension for each path is denoted as <strong class=\"kj lf\"><em class=\"lg\">d</em> (<em class=\"lg\">d</em>=4)</strong>. The number of paths is the <strong class=\"kj lf\">cardinality <em class=\"lg\">C</em> (<em class=\"lg\">C</em>=32)</strong>. If we sum up the dimension of each Conv3×3 (i.e. <em class=\"lg\">d</em>×<em class=\"lg\">C</em>=4×32), it is also the dimensions of 128.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"f45b\">The dimension is increased directly from 4 to 256, and then added together, and also added with the skip connection path.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"ac99\">Compared with <a class=\"cy bt lh li lj lk\" href=\"/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc\">Inception-ResNet</a> that it needs to increase the dimension from 4 to 128 then to 256, <strong class=\"kj lf\">ResNeXt requires minimal extra effort designing each path.</strong></li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"ea06\">Unlike <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet</a>, in ResNeXt, the neurons at one path will not connected to the neurons at other paths.</li></ul><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"c543\"><strong class=\"kj lf\">(b) </strong><a class=\"cy bt lh li lj lk\" href=\"/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc\"><strong class=\"kj lf\">Inception-ResNet</strong></a><strong class=\"kj lf\"> Block (Middle)</strong></p><ul class=\"\"><li class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku np mu mv\" id=\"a564\">This is suggested in <a class=\"cy bt lh li lj lk\" href=\"/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc\">Inception-v4</a> to combine the <a class=\"cy bt lh li lj lk\" href=\"https://medium.com/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7\">Inception</a> module and <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet</a> block. Somehow due to the legacy problem, for each convolution path, <strong class=\"kj lf\">Conv1×1–Conv3×3 </strong>are done first. When added together (i.e. 4×32), the Conv3×3 has the dimension of 128.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"1309\">Then the outputs are concatenated together with dimension of 128. And <strong class=\"kj lf\">Conv1×1</strong> is used to restore the dimensions from 128 to 256.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"f428\">Finally the output is added with the skip connection path.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"8480\">The main difference is that they have an <strong class=\"kj lf\">early concatenation</strong>.</li></ul><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"2f32\"><strong class=\"kj lf\">(c) Grouped Convolution in </strong><a class=\"cy bt lh li lj lk\" href=\"https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160\"><strong class=\"kj lf\">AlexNet</strong></a><strong class=\"kj lf\"> (Right)</strong></p><ul class=\"\"><li class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku np mu mv\" id=\"b344\"><strong class=\"kj lf\">Conv1×1–Conv3×3–Conv1×1 </strong>are done at the convolution path, which is actually a<strong class=\"kj lf\"> bottleneck design</strong> suggested in <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet</a>. The Conv3×3 has the dimension of 128.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"9c7b\">However, <strong class=\"kj lf\">grouped convolution</strong>, suggested in <a class=\"cy bt lh li lj lk\" href=\"https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160\">AlexNet</a> is used here. Therefore this Conv3×3 is <strong class=\"kj lf\">wider but sparsely connected module. </strong>(Because the neurons at one path will not connected to the neurons at other paths, that’s why it is sparsely connected.)</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"5883\">Thus, there are <strong class=\"kj lf\">32 groups of convolutions</strong>. (2 groups only in <a class=\"cy bt lh li lj lk\" href=\"https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160\">AlexNet</a>)</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"eae5\">Then a skip connection is at parallel and added with the convolution path. Thus, the convolution path is learning the residual representation.</li></ul><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"e9f0\">Though the structures in (b) and (c) are not always the same as the general form in the equation shown in 1.2, indeed authors have tried the above three structures as shown above, and they found that the results are the same.</p><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"9345\"><strong class=\"kj lf\">Finally, authors choose to implement the structure in (c) because it is more succinct and faster than the other two forms.</strong></p></div></section><hr class=\"lt cs gy lu lv hi lw lx ly lz ma\"/><section class=\"hy hz ia ib ic\"><div class=\"ae fb ab di v w\"><h1 class=\"mb mc ea ap ao dy md me mf mg mh mi mj mk ml mm mn\" id=\"e7a5\">3. <strong class=\"bb\">Full Architecture and Ablation Study</strong></h1><h2 class=\"nb mc ea ap ao dy nc nd ne nf ng nh ni nj nk nl nm\" id=\"e30a\">3.1. Ablation Study of <em class=\"nw\">C</em> and <em class=\"nw\">d</em> Under Similar Complexity</h2></div><div class=\"ae fb ab ac\"><figure class=\"ju jv jw jx jy ez nx nt nu paragraph-image\"><div class=\"kc l eo kd\"><div class=\"ny l\"><div class=\"dk ka gg n o gf ab by t kb\"><img class=\"gg n o gf ab kf kg\" height=\"550\" src=\"https://miro.medium.com/max/60/1*UNkmAg1JVfprEfBBmqM44w.png?q=20\" width=\"1000\"/></div><img class=\"dk ka gg n o gf ab gd\" height=\"550\" width=\"1000\"/><noscript><img class=\"gg n o gf ab\" height=\"550\" src=\"https://miro.medium.com/max/2000/1*UNkmAg1JVfprEfBBmqM44w.png\" width=\"1000\"/></noscript></div></div><figcaption class=\"at ed ln lo hi di x y lp ao cs\"><strong class=\"ao lf\">Detailed Architecture (Left), Number of Parameters for Each Block (Top Right), Different Settings to Maintain Similar Complexity (Middle Right), Ablation Study for Different Settings Under Similar Complexity (Bottom Right)</strong></figcaption></figure></div><div class=\"ae fb ab di v w\"><ul class=\"\"><li class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku np mu mv\" id=\"6013\"><a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet-50</a> is a special case of ResNeXt-50 with <em class=\"lg\">C</em>=1, <em class=\"lg\">d</em>=64.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"eb48\">To have fair comparison, different ResNeXt with different <em class=\"lg\">C</em> and <em class=\"lg\">d</em> with similar complexity with <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet</a> are tried. This is shown at the middle right of the figure above.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"a697\">And it is found that ResNeXt-50 (32×4d) obtains 22.2% top-1 error for ImageNet-1K (1K means 1K classes) dataset while <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet-50</a> only obtains 23.9% top-1 error.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"6475\">And ResNeXt-101 (32×4d) obtains 21.2% top-1 error for ImageNet dataset while <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet-101</a> only obtains 22.0% top-1 error.</li></ul><h2 class=\"nb mc ea ap ao dy nc nd ne nf ng nh ni nj nk nl nm\" id=\"f880\">3.2. Importance of Cardinality</h2><figure class=\"ju jv jw jx jy ez nz x y paragraph-image\"><div class=\"kc l eo kd\"><div class=\"oa l\"><div class=\"dk ka gg n o gf ab by t kb\"><img class=\"gg n o gf ab kf kg\" height=\"344\" src=\"https://miro.medium.com/max/60/1*xxm28sWSo6i-If09kwAqPQ.png?q=20\" width=\"666\"/></div><img class=\"dk ka gg n o gf ab gd\" height=\"344\" width=\"666\"/><noscript><img class=\"gg n o gf ab\" height=\"344\" src=\"https://miro.medium.com/max/1332/1*xxm28sWSo6i-If09kwAqPQ.png\" width=\"666\"/></noscript></div></div><figcaption class=\"at ed ln lo hi di x y lp ao cs\"><strong class=\"ao lf\">Ablation Study for Different Settings of 2× Complexity Models</strong></figcaption></figure><ul class=\"\"><li class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku np mu mv\" id=\"014c\"><a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\"><strong class=\"kj lf\">ResNet-200</strong></a>: 21.7% top-1 and 5.8% top-5 error rates.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"4fd4\"><a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\"><strong class=\"kj lf\">ResNet-101, wider</strong></a>: only obtains 21.3% top-1 and 5.7% top-5 error rates, which means <strong class=\"kj lf\">only making it wider does not help much</strong>.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"4b39\"><strong class=\"kj lf\">ResNeXt-101 (2×64d)</strong>: By just making <strong class=\"kj lf\"><em class=\"lg\">C</em>=2</strong> (i.e. two convolution paths within the ResNeXt block), <strong class=\"kj lf\">an obvious improvement is already obtained</strong> with 20.7% top-1 and 5.5% top-5 error rates.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"0a63\"><strong class=\"kj lf\">ResNeXt-101 (64×4d)</strong>: By making <strong class=\"kj lf\"><em class=\"lg\">C</em>=64</strong> (i.e. two convolution paths within the ResNeXt block), <strong class=\"kj lf\">an even better improvement is already obtained</strong> with 20.4% top-1 and 5.3% top-5 error rates. This means <strong class=\"kj lf\">cardinality is essential to improve the classification accuracy</strong>.</li></ul><h2 class=\"nb mc ea ap ao dy nc nd ne nf ng nh ni nj nk nl nm\" id=\"57d7\">3.2. Importance of Residual Connections</h2><figure class=\"ju jv jw jx jy ez ob x y paragraph-image\"><div class=\"kc l eo kd\"><div class=\"oc l\"><div class=\"dk ka gg n o gf ab by t kb\"><img class=\"gg n o gf ab kf kg\" height=\"119\" src=\"https://miro.medium.com/max/60/1*EwrkhL-fqVii9MEjCHMZBA.png?q=20\" width=\"571\"/></div><img class=\"dk ka gg n o gf ab gd\" height=\"119\" width=\"571\"/><noscript><img class=\"gg n o gf ab\" height=\"119\" src=\"https://miro.medium.com/max/1142/1*EwrkhL-fqVii9MEjCHMZBA.png\" width=\"571\"/></noscript></div></div><figcaption class=\"at ed ln lo hi di x y lp ao cs\"><strong class=\"ao lf\">Importance of Residual Connections</strong></figcaption></figure><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"39d7\">Without residual connections, error rates are increased largely for both <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet-50</a> and ResNeXt-50. Residual connections are important.</p></div></section><hr class=\"lt cs gy lu lv hi lw lx ly lz ma\"/><section class=\"hy hz ia ib ic\"><div class=\"ae fb ab di v w\"><h1 class=\"mb mc ea ap ao dy md me mf mg mh mi mj mk ml mm mn\" id=\"558f\">4. Results</h1><h2 class=\"nb mc ea ap ao dy nc nd ne nf ng nh ni nj nk nl nm\" id=\"a94e\">4.1. ImageNet-1K</h2></div><div class=\"ae fb ab ac\"><figure class=\"ju jv jw jx jy ez od nt nu paragraph-image\"><div class=\"kc l eo kd\"><div class=\"oe l\"><div class=\"dk ka gg n o gf ab by t kb\"><img class=\"gg n o gf ab kf kg\" height=\"291\" src=\"https://miro.medium.com/max/60/1*oLRaAqY2cnw2E5eJ_D8jmA.png?q=20\" width=\"1000\"/></div><img class=\"dk ka gg n o gf ab gd\" height=\"291\" width=\"1000\"/><noscript><img class=\"gg n o gf ab\" height=\"291\" src=\"https://miro.medium.com/max/2000/1*oLRaAqY2cnw2E5eJ_D8jmA.png\" width=\"1000\"/></noscript></div></div><figcaption class=\"at ed ln lo hi di x y lp ao cs\"><strong class=\"ao lf\">Single Crop Testing: ResNet/ResNeXt is 224×224 and 320×320, Inception models: 299×299</strong></figcaption></figure></div><div class=\"ae fb ab di v w\"><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"d9ed\">ImageNet-1K is a subset of 22K-class ImageNet dataset, which contains 1000 classes. It is also the dataset for ILSVRC classification task.</p><ul class=\"\"><li class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku np mu mv\" id=\"752e\">With standard size image used for single crop testing, ResNeXt-101 obtains 20.4% top-1 and 5.3% top-5 error rates,</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"b264\">With larger size image used for single crop testing, ResNeXt-101 obtains 19.1% top-1 and 4.4% top-5 error rates, which has better results than all state-of-the-art approaches, <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet</a>, <a class=\"cy bt lh li lj lk\" href=\"/resnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e\">Pre-Activation ResNet</a>, <a class=\"cy bt lh li lj lk\" href=\"https://medium.com/@sh.tsang/review-inception-v3-1st-runner-up-image-classification-in-ilsvrc-2015-17915421f77c\">Inception-v3</a>, <a class=\"cy bt lh li lj lk\" href=\"/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc\">Inception-v4</a> and <a class=\"cy bt lh li lj lk\" href=\"/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc\">Inception-ResNet-v2</a>.</li></ul><h2 class=\"nb mc ea ap ao dy nc nd ne nf ng nh ni nj nk nl nm\" id=\"321e\">4.2. ImageNet-5K</h2></div><div class=\"ae fb ab ac\"><figure class=\"ju jv jw jx jy ez of nt nu paragraph-image\"><div class=\"kc l eo kd\"><div class=\"og l\"><div class=\"dk ka gg n o gf ab by t kb\"><img class=\"gg n o gf ab kf kg\" height=\"417\" src=\"https://miro.medium.com/max/60/1*V99JmLNTxlXxIPYAIFiGwA.png?q=20\" width=\"1000\"/></div><img class=\"dk ka gg n o gf ab gd\" height=\"417\" width=\"1000\"/><noscript><img class=\"gg n o gf ab\" height=\"417\" src=\"https://miro.medium.com/max/2000/1*V99JmLNTxlXxIPYAIFiGwA.png\" width=\"1000\"/></noscript></div></div><figcaption class=\"at ed ln lo hi di x y lp ao cs\"><strong class=\"ao lf\">ImageNet-5K Results (All trained from scratch)</strong></figcaption></figure></div><div class=\"ae fb ab di v w\"><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"97a7\">ImageNet-1K has been somehow saturated after so many years of development.</p><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"9240\">ImageNet-5K is a subset of 22K-class ImageNet dataset, which contains 5000 classes, which also contains ImageNet-1K classes.</p><ul class=\"\"><li class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku np mu mv\" id=\"b179\">6.8 million images, 5× of the ImageNet-1K dataset.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"73ed\">Since there is no official train/validation set, the original ImageNet-1K validation set is used for evaluation.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"c2cd\"><strong class=\"kj lf\">5K-way classification</strong> is the <strong class=\"kj lf\">softmax over 5K classes</strong>. Thus, there will be automatic errors when the network predicts the labels for the other 4K classes on the ImageNet-1K validation dataset.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"fccb\"><strong class=\"kj lf\">1K-way classification</strong> is just the <strong class=\"kj lf\">softmax over 1K classes</strong>.</li></ul><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"34a9\">ResNeXt of course got better results than <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet</a> as shown above.</p><h2 class=\"nb mc ea ap ao dy nc nd ne nf ng nh ni nj nk nl nm\" id=\"f627\">4.3. CIFAR-10 &amp; CIFAR-100</h2></div><div class=\"ae fb ab ac\"><figure class=\"ju jv jw jx jy ez oh nt nu paragraph-image\"><div class=\"kc l eo kd\"><div class=\"oi l\"><div class=\"dk ka gg n o gf ab by t kb\"><img class=\"gg n o gf ab kf kg\" height=\"435\" src=\"https://miro.medium.com/max/60/1*FqDUkl5vrLZufkQIG6xk7A.png?q=20\" width=\"1000\"/></div><img class=\"dk ka gg n o gf ab gd\" height=\"435\" width=\"1000\"/><noscript><img class=\"gg n o gf ab\" height=\"435\" src=\"https://miro.medium.com/max/2000/1*FqDUkl5vrLZufkQIG6xk7A.png\" width=\"1000\"/></noscript></div></div><figcaption class=\"at ed ln lo hi di x y lp ao cs\"><strong class=\"ao lf\">CIFAR-10 and CIFAR-100 Results</strong></figcaption></figure></div><div class=\"ae fb ab di v w\"><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"4572\">CIFAR-10 &amp; CIFAR-100, two very famous 10-class and 100-class datasets.</p><ul class=\"\"><li class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku np mu mv\" id=\"f629\">Left: Compared with <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet</a>, ResNeXt always obtains better results in CIFAR-10.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"e839\">Right: Compared with <a class=\"cy bt lh li lj lk\" href=\"/review-wrns-wide-residual-networks-image-classification-d3feb3fb2004\">Wide ResNet (WRN)</a>, ResNeXt-29 (16×64d) obtains 3.58% and 17.31% errors for CIFAR-10 and CIFAR-100 respectively. These were the best results among all state-of-the-art approaches at that moment.</li></ul><h2 class=\"nb mc ea ap ao dy nc nd ne nf ng nh ni nj nk nl nm\" id=\"39c2\">4.4. MS COCO Object Detection</h2><figure class=\"ju jv jw jx jy ez oj x y paragraph-image\"><div class=\"kc l eo kd\"><div class=\"ok l\"><div class=\"dk ka gg n o gf ab by t kb\"><img class=\"gg n o gf ab kf kg\" height=\"190\" src=\"https://miro.medium.com/max/60/1*_QhPxRX7B0jROHwhIuljCQ.png?q=20\" width=\"592\"/></div><img class=\"dk ka gg n o gf ab gd\" height=\"190\" width=\"592\"/><noscript><img class=\"gg n o gf ab\" height=\"190\" src=\"https://miro.medium.com/max/1184/1*_QhPxRX7B0jROHwhIuljCQ.png\" width=\"592\"/></noscript></div></div><figcaption class=\"at ed ln lo hi di x y lp ao cs\"><strong class=\"ao lf\">MS COCO Objection Detection Results</strong></figcaption></figure><ul class=\"\"><li class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku np mu mv\" id=\"7c48\">By plugging <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet</a>/ResNeXt into <a class=\"cy bt lh li lj lk\" href=\"/review-faster-r-cnn-object-detection-f5685cb30202\">Faster R-CNN</a>, with similar model complexity, ResNeXt always outperforms <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet</a> for both AP@0.5 (IoU&gt;0.5) and mean AP (average prediction) at all IoU levels.</li></ul></div></section><hr class=\"lt cs gy lu lv hi lw lx ly lz ma\"/><section class=\"hy hz ia ib ic\"><div class=\"ae fb ab di v w\"><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"8543\">With the success of ResNeXt, it is also utilized by Mask R-CNN for instance segmentation. Hope I can cover Mask R-CNN later on as well.</p></div></section><hr class=\"lt cs gy lu lv hi lw lx ly lz ma\"/><section class=\"hy hz ia ib ic\"><div class=\"ae fb ab di v w\"><h2 class=\"nb mc ea ap ao dy nc nd ne nf ng nh ni nj nk nl nm\" id=\"2ad8\">Reference</h2><p class=\"kh ki ea ap kj b kk mo km mp ko mq kq mr ks ms ku\" id=\"0b9a\">[2017 CVPR] [ResNeXt]<br/><a class=\"cy bt lh li lj lk\" href=\"https://arxiv.org/abs/1611.05431\">Aggregated Residual Transformations for Deep Neural Networks</a></p><h2 class=\"nb mc ea ap ao dy nc nd ne nf ng nh ni nj nk nl nm\" id=\"b06a\">My Related Reviews on Image Classification</h2><p class=\"kh ki ea ap kj b kk mo km mp ko mq kq mr ks ms ku\" id=\"79f2\">[<a class=\"cy bt lh li lj lk\" href=\"https://medium.com/@sh.tsang/paper-brief-review-of-lenet-1-lenet-4-lenet-5-boosted-lenet-4-image-classification-1f5f809dbf17\">LeNet</a>] [<a class=\"cy bt lh li lj lk\" href=\"https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160\">AlexNet</a>] [<a class=\"cy bt lh li lj lk\" href=\"https://medium.com/coinmonks/paper-review-of-zfnet-the-winner-of-ilsvlc-2013-image-classification-d1a5a0c45103\">ZFNet</a>] [<a class=\"cy bt lh li lj lk\" href=\"https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11\">VGGNet</a>] [<a class=\"cy bt lh li lj lk\" href=\"https://medium.com/coinmonks/review-sppnet-1st-runner-up-object-detection-2nd-runner-up-image-classification-in-ilsvrc-906da3753679\">SPPNet</a>] [<a class=\"cy bt lh li lj lk\" href=\"https://medium.com/coinmonks/review-prelu-net-the-first-to-surpass-human-level-performance-in-ilsvrc-2015-image-f619dddd5617\">PReLU-Net</a>] [<a class=\"cy bt lh li lj lk\" href=\"https://medium.com/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7\">GoogLeNet / Inception-v1</a>] [<a class=\"cy bt lh li lj lk\" href=\"https://medium.com/@sh.tsang/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651\">BN-Inception / Inception-v2</a>] [<a class=\"cy bt lh li lj lk\" href=\"https://medium.com/@sh.tsang/review-inception-v3-1st-runner-up-image-classification-in-ilsvrc-2015-17915421f77c\">Inception-v3</a>] [<a class=\"cy bt lh li lj lk\" href=\"/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc\">Inception-v4</a>] [<a class=\"cy bt lh li lj lk\" href=\"/review-xception-with-depthwise-separable-convolution-better-than-inception-v3-image-dc967dd42568\">Xception</a>] [<a class=\"cy bt lh li lj lk\" href=\"/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69\">MobileNetV1</a>] [<a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet</a>] [<a class=\"cy bt lh li lj lk\" href=\"/resnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e\">Pre-Activation ResNet</a>] [<a class=\"cy bt lh li lj lk\" href=\"https://medium.com/@sh.tsang/review-rir-resnet-in-resnet-image-classification-be4c79fde8ba\">RiR</a>] [<a class=\"cy bt lh li lj lk\" href=\"/review-ror-resnet-of-resnet-multilevel-resnet-image-classification-cd3b0fcc19bb\">RoR</a>] [<a class=\"cy bt lh li lj lk\" href=\"/review-stochastic-depth-image-classification-a4e225807f4a\">Stochastic Depth</a>] [<a class=\"cy bt lh li lj lk\" href=\"/review-wrns-wide-residual-networks-image-classification-d3feb3fb2004\">WRN</a>] [<a class=\"cy bt lh li lj lk\" href=\"/review-polynet-2nd-runner-up-in-ilsvrc-2016-image-classification-8a1a941ce9ea\">PolyNet</a>] [<a class=\"cy bt lh li lj lk\" href=\"/review-densenet-image-classification-b6631a8ef803\">DenseNet</a>]</p><h2 class=\"nb mc ea ap ao dy nc nd ne nf ng nh ni nj nk nl nm\" id=\"030e\">My Related Reviews on Object Detection</h2><p class=\"kh ki ea ap kj b kk mo km mp ko mq kq mr ks ms ku\" id=\"2a8b\">[<a class=\"cy bt lh li lj lk\" href=\"/review-faster-r-cnn-object-detection-f5685cb30202\">Faster R-CNN</a>]</p></div></section></div>\n",
      "<div class=\"gg o hv hw hx dl\"></div>\n",
      "<div class=\"ae fb ab di v w\"><div><div class=\"id ie ea ap if b ig ih ii ij ik il im\" id=\"1ef0\"><h1 class=\"if b ig in ea\">Review: ResNeXt — 1st Runner Up in ILSVRC 2016 (Image Classification)</h1></div></div><div class=\"io ie at ap ao cs ip iq ir is it iu iv\" id=\"4c9a\"><h2 class=\"ao cs iw ix at\"><strong class=\"bb\">Network-in-Neuron, A New Dimensionality: Cardinality</strong></h2></div><div class=\"iy\"><div class=\"ag af\"><div><a href=\"/@sh.tsang?source=post_page---------------------------\"><img alt=\"Sik-Ho Tsang\" class=\"l fn iz ja\" height=\"48\" src=\"https://miro.medium.com/fit/c/96/96/1*OxjNUHcLFU8-pp-j8su6pg.jpeg\" width=\"48\"/></a></div><div class=\"jb ab l\"><div class=\"af\"><div style=\"flex:1\"><span class=\"ao b ap aq ar as l ea al\"><div class=\"jc af ag jd\" data-test-id=\"postByline\"><span class=\"ao cs ed aq by je ef eg jf ei ea\"><a class=\"cy cz ax ay az ba bb bc bd be jg bh bi dw dx\" href=\"/@sh.tsang?source=post_page---------------------------\">Sik-Ho Tsang</a></span><div class=\"jh l am h\"><button class=\"ji ea al bm jj jk jl jm be dw jn jo jp jq jr js bp ao b ap jt ct as bq br ae bs bt bh\">Follow</button></div></div></span></div></div><span class=\"ao b ap aq ar as l at au\"><span class=\"ao cs ed aq by je ef eg jf ei at\"><div><a class=\"cy cz ax ay az ba bb bc bd be jg bh bi dw dx\" href=\"/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac?source=post_page---------------------------\">Dec 9, 2018</a> <!-- -->·<!-- --> <!-- -->7<!-- --> min read</div></span></span></div></div></div><figure class=\"ju jv jw jx jy ez jz x y paragraph-image\"><div class=\"kc l eo kd\"><div class=\"ke l\"><div class=\"dk ka gg n o gf ab by t kb\"><img class=\"gg n o gf ab kf kg\" height=\"467\" src=\"https://miro.medium.com/max/60/0*MvA7JX1Z1rAG5byo?q=20\" width=\"700\"/></div><img class=\"dk ka gg n o gf ab gd\" height=\"467\" width=\"700\"/><noscript><img class=\"gg n o gf ab\" height=\"467\" src=\"https://miro.medium.com/max/1400/0*MvA7JX1Z1rAG5byo\" width=\"700\"/></noscript></div></div></figure><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku kv\" id=\"ecb1\"><span class=\"l kw kx ky kz la lb lc ld le eo\">In</span> this story, <strong class=\"kj lf\">ResNeXt, </strong>by <strong class=\"kj lf\">UC San Diego </strong>and <strong class=\"kj lf\">Facebook AI Research (FAIR), </strong>is reviewed. The model name, ResNeXt, contains Next. It means the <em class=\"lg\">next </em>dimension, on top of the <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet</a>. This next dimension is called the “<strong class=\"kj lf\"><em class=\"lg\">cardinality</em></strong>” <strong class=\"kj lf\">dimension</strong>. And ResNeXt becomes the <strong class=\"kj lf\">1st Runner Up of ILSVRC classification task</strong>.</p><figure class=\"ju jv jw jx jy ez ll x y paragraph-image\"><div class=\"kc l eo kd\"><div class=\"lm l\"><div class=\"dk ka gg n o gf ab by t kb\"><img class=\"gg n o gf ab kf kg\" height=\"421\" src=\"https://miro.medium.com/max/60/1*RHpn70qFNCcqyVjkPdFtGA.png?q=20\" width=\"700\"/></div><img class=\"dk ka gg n o gf ab gd\" height=\"421\" width=\"700\"/><noscript><img class=\"gg n o gf ab\" height=\"421\" src=\"https://miro.medium.com/max/1400/1*RHpn70qFNCcqyVjkPdFtGA.png\" width=\"700\"/></noscript></div></div><figcaption class=\"at ed ln lo hi di x y lp ao cs\"><strong class=\"ao lf\">ILSVRC 2016 Classification Ranking </strong><a class=\"cy bt lh li lj lk\" href=\"http://image-net.org/challenges/LSVRC/2016/results#loc\">http://image-net.org/challenges/LSVRC/2016/results#loc</a></figcaption></figure><figure class=\"ju jv jw jx jy ez lq x y paragraph-image\"><div class=\"kc l eo kd\"><div class=\"lr l\"><div class=\"dk ka gg n o gf ab by t kb\"><img class=\"gg n o gf ab kf kg\" height=\"307\" src=\"https://miro.medium.com/max/60/1*LOoc11tkDoqv0pC6OH7mwA.png?q=20\" width=\"700\"/></div><img class=\"dk ka gg n o gf ab gd\" height=\"307\" width=\"700\"/><noscript><img class=\"gg n o gf ab\" height=\"307\" src=\"https://miro.medium.com/max/1400/1*LOoc11tkDoqv0pC6OH7mwA.png\" width=\"700\"/></noscript></div></div><figcaption class=\"at ed ln lo hi di x y lp ao cs\"><strong class=\"ao lf\">Residual Block in ResNet (Left), A Block of ResNeXt with Cardinality = 32 (Right)</strong></figcaption></figure><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"f37a\">Compared with <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet</a> (The winner in ILSVRC 2015, 3.57%) and <a class=\"cy bt lh li lj lk\" href=\"/review-polynet-2nd-runner-up-in-ilsvrc-2016-image-classification-8a1a941ce9ea\">PolyNet</a> (2nd Runner Up, 3.04%, Team name CU-DeepLink), ResNeXt got 3.03% Top-5 error rate, which is a large relative improvement of about 15%!!</p><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"a174\">And it is published in <strong class=\"kj lf\">2017 CVPR</strong>, which has already got over <strong class=\"kj lf\">500 citations</strong> while I was writing this story. (<a class=\"ls av bt\" href=\"/u/aff72a0c1243?source=post_page---------------------------\">Sik-Ho Tsang</a> @ Medium)</p></div>\n"
     ]
    }
   ],
   "source": [
    "for i in post_contents.find_all('div')[:3]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h1 class=\"if b ig in ea\">Review: ResNeXt — 1st Runner Up in ILSVRC 2016 (Image Classification)</h1>,\n",
       " <h1 class=\"mb mc ea ap ao dy md me mf mg mh mi mj mk ml mm mn\" id=\"f006\">What Are Covered</h1>,\n",
       " <h1 class=\"mb mc ea ap ao dy md me mf mg mh mi mj mk ml mm mn\" id=\"ad50\">1. Aggregated Transformation</h1>,\n",
       " <h1 class=\"mb mc ea ap ao dy md me mf mg mh mi mj mk ml mm mn\" id=\"0a3f\"><strong class=\"bb\">2. Relationship with </strong><a class=\"cy bt lh li lj lk\" href=\"/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc\"><strong class=\"bb\">Inception-ResNet</strong></a><strong class=\"bb\"> and Grouped Convolution in </strong><a class=\"cy bt lh li lj lk\" href=\"https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160\"><strong class=\"bb\">AlexNet</strong></a></h1>,\n",
       " <h1 class=\"mb mc ea ap ao dy md me mf mg mh mi mj mk ml mm mn\" id=\"e7a5\">3. <strong class=\"bb\">Full Architecture and Ablation Study</strong></h1>,\n",
       " <h1 class=\"mb mc ea ap ao dy md me mf mg mh mi mj mk ml mm mn\" id=\"558f\">4. Results</h1>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headings= html_soup.find_all('h1')\n",
    "headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "def heading(doc,value,level=0):\n",
    "    doc.add_heading(value, level)\n",
    "\n",
    "subheading= partial(heading, level=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Doctype' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-39dd6d81f0da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhtml_soup\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\keras_flask\\lib\\site-packages\\bs4\\element.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    644\u001b[0m             raise AttributeError(\n\u001b[0;32m    645\u001b[0m                 \"'%s' object has no attribute '%s'\" % (\n\u001b[1;32m--> 646\u001b[1;33m                     self.__class__.__name__, attr))\n\u001b[0m\u001b[0;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0moutput_ready\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"minimal\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Doctype' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "for i in html_soup:\n",
    "    print(i.text)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASCII_SPACES',\n",
       " 'DEFAULT_BUILDER_FEATURES',\n",
       " 'NO_PARSER_SPECIFIED_WARNING',\n",
       " 'ROOT_TAG_NAME',\n",
       " '__bool__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__copy__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " '__weakref__',\n",
       " '_all_strings',\n",
       " '_check_markup_is_url',\n",
       " '_feed',\n",
       " '_find_all',\n",
       " '_find_one',\n",
       " '_is_xml',\n",
       " '_lastRecursiveChild',\n",
       " '_last_descendant',\n",
       " '_linkage_fixer',\n",
       " '_most_recent_element',\n",
       " '_namespaces',\n",
       " '_popToTag',\n",
       " '_should_pretty_print',\n",
       " 'append',\n",
       " 'attrs',\n",
       " 'builder',\n",
       " 'can_be_empty_element',\n",
       " 'cdata_list_attributes',\n",
       " 'childGenerator',\n",
       " 'children',\n",
       " 'clear',\n",
       " 'contains_replacement_characters',\n",
       " 'contents',\n",
       " 'currentTag',\n",
       " 'current_data',\n",
       " 'declared_html_encoding',\n",
       " 'decode',\n",
       " 'decode_contents',\n",
       " 'decompose',\n",
       " 'descendants',\n",
       " 'encode',\n",
       " 'encode_contents',\n",
       " 'endData',\n",
       " 'extend',\n",
       " 'extract',\n",
       " 'fetchNextSiblings',\n",
       " 'fetchParents',\n",
       " 'fetchPrevious',\n",
       " 'fetchPreviousSiblings',\n",
       " 'find',\n",
       " 'findAll',\n",
       " 'findAllNext',\n",
       " 'findAllPrevious',\n",
       " 'findChild',\n",
       " 'findChildren',\n",
       " 'findNext',\n",
       " 'findNextSibling',\n",
       " 'findNextSiblings',\n",
       " 'findParent',\n",
       " 'findParents',\n",
       " 'findPrevious',\n",
       " 'findPreviousSibling',\n",
       " 'findPreviousSiblings',\n",
       " 'find_all',\n",
       " 'find_all_next',\n",
       " 'find_all_previous',\n",
       " 'find_next',\n",
       " 'find_next_sibling',\n",
       " 'find_next_siblings',\n",
       " 'find_parent',\n",
       " 'find_parents',\n",
       " 'find_previous',\n",
       " 'find_previous_sibling',\n",
       " 'find_previous_siblings',\n",
       " 'format_string',\n",
       " 'formatter_for_name',\n",
       " 'get',\n",
       " 'getText',\n",
       " 'get_attribute_list',\n",
       " 'get_text',\n",
       " 'handle_data',\n",
       " 'handle_endtag',\n",
       " 'handle_starttag',\n",
       " 'has_attr',\n",
       " 'has_key',\n",
       " 'hidden',\n",
       " 'index',\n",
       " 'insert',\n",
       " 'insert_after',\n",
       " 'insert_before',\n",
       " 'isSelfClosing',\n",
       " 'is_empty_element',\n",
       " 'is_xml',\n",
       " 'known_xml',\n",
       " 'markup',\n",
       " 'name',\n",
       " 'namespace',\n",
       " 'new_string',\n",
       " 'new_tag',\n",
       " 'next',\n",
       " 'nextGenerator',\n",
       " 'nextSibling',\n",
       " 'nextSiblingGenerator',\n",
       " 'next_element',\n",
       " 'next_elements',\n",
       " 'next_sibling',\n",
       " 'next_siblings',\n",
       " 'object_was_parsed',\n",
       " 'original_encoding',\n",
       " 'parent',\n",
       " 'parentGenerator',\n",
       " 'parents',\n",
       " 'parse_only',\n",
       " 'parserClass',\n",
       " 'parser_class',\n",
       " 'popTag',\n",
       " 'prefix',\n",
       " 'preserve_whitespace_tag_stack',\n",
       " 'preserve_whitespace_tags',\n",
       " 'prettify',\n",
       " 'previous',\n",
       " 'previousGenerator',\n",
       " 'previousSibling',\n",
       " 'previousSiblingGenerator',\n",
       " 'previous_element',\n",
       " 'previous_elements',\n",
       " 'previous_sibling',\n",
       " 'previous_siblings',\n",
       " 'pushTag',\n",
       " 'recursiveChildGenerator',\n",
       " 'renderContents',\n",
       " 'replaceWith',\n",
       " 'replaceWithChildren',\n",
       " 'replace_with',\n",
       " 'replace_with_children',\n",
       " 'reset',\n",
       " 'select',\n",
       " 'select_one',\n",
       " 'setup',\n",
       " 'smooth',\n",
       " 'string',\n",
       " 'strings',\n",
       " 'stripped_strings',\n",
       " 'tagStack',\n",
       " 'text',\n",
       " 'unwrap',\n",
       " 'wrap']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(html_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<!DOCTYPE doctype html>\n",
       " <html lang=\"en\"><head><script>!function(c,f){var t,o,i,e=[],r={passive:!0,capture:!0},n=new Date,a=\"pointerup\",u=\"pointercancel\";function p(n,e){t||(t=e,o=n,i=new Date,w(f),s())}function s(){0<=o&&o<i-n&&(e.forEach(function(n){n(o,t)}),e=[])}function l(n){if(n.cancelable){var e=(1e12<n.timeStamp?new Date:performance.now())-n.timeStamp;\"pointerdown\"==n.type?function(n,e){function t(){p(n,e),i()}function o(){i()}function i(){f(a,t,r),f(u,o,r)}c(a,t,r),c(u,o,r)}(e,n):p(e,n)}}function w(e){[\"click\",\"mousedown\",\"keydown\",\"touchstart\",\"pointerdown\"].forEach(function(n){e(n,l,r)})}w(c),self.perfMetrics=self.perfMetrics||{},self.perfMetrics.onFirstInputDelay=function(n){e.push(n),s()}}(addEventListener,removeEventListener)</script><title data-rh=\"true\">Review: ResNeXt — 1st Runner Up in ILSVRC 2016 (Image Classification)</title><meta charset=\"utf-8\" data-rh=\"true\"/><meta content=\"width=device-width,minimum-scale=1,initial-scale=1\" data-rh=\"true\" name=\"viewport\"/><meta content=\"#000000\" data-rh=\"true\" name=\"theme-color\"/><meta content=\"Medium\" data-rh=\"true\" name=\"twitter:app:name:iphone\"/><meta content=\"828256236\" data-rh=\"true\" name=\"twitter:app:id:iphone\"/><meta content=\"Medium\" data-rh=\"true\" property=\"al:ios:app_name\"/><meta content=\"828256236\" data-rh=\"true\" property=\"al:ios:app_store_id\"/><meta content=\"com.medium.reader\" data-rh=\"true\" property=\"al:android:package\"/><meta content=\"542599432471018\" data-rh=\"true\" property=\"fb:app_id\"/><meta content=\"Medium\" data-rh=\"true\" property=\"og:site_name\"/><meta content=\"article\" data-rh=\"true\" property=\"og:type\"/><meta content=\"2019-03-20T16:00:42.080Z\" data-rh=\"true\" property=\"article:published_time\"/><meta content=\"Review: ResNeXt — 1st Runner Up in ILSVRC 2016 (Image Classification)\" data-rh=\"true\" name=\"title\"/><meta content=\"Review: ResNeXt — 1st Runner Up in ILSVRC 2016 (Image Classification)\" data-rh=\"true\" property=\"og:title\"/><meta content=\"Review: ResNeXt — 1st Runner Up in ILSVRC 2016 (Image Classification)\" data-rh=\"true\" property=\"twitter:title\"/><meta content=\"@TDataScience\" data-rh=\"true\" name=\"twitter:site\"/><meta content=\"medium://p/15d7f17b42ac\" data-rh=\"true\" name=\"twitter:app:url:iphone\"/><meta content=\"medium://p/15d7f17b42ac\" data-rh=\"true\" property=\"al:android:url\"/><meta content=\"medium://p/15d7f17b42ac\" data-rh=\"true\" property=\"al:ios:url\"/><meta content=\"app-id=828256236,app-argument=medium://p/15d7f17b42ac\" data-rh=\"true\" name=\"apple-itunes-app\"/><meta content=\"Medium\" data-rh=\"true\" property=\"al:android:app_name\"/><meta content=\"In this story, ResNeXt, by UC San Diego and Facebook AI Research (FAIR), is reviewed. The model name, ResNeXt, contains Next. It means the next dimension, on top of the ResNet. This next dimension…\" data-rh=\"true\" name=\"description\"/><meta content=\"Network-in-Neuron, A New Dimensionality: Cardinality\" data-rh=\"true\" property=\"og:description\"/><meta content=\"Network-in-Neuron, A New Dimensionality: Cardinality\" data-rh=\"true\" property=\"twitter:description\"/><meta content=\"https://towardsdatascience.com/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac\" data-rh=\"true\" property=\"og:url\"/><meta content=\"https://towardsdatascience.com/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac\" data-rh=\"true\" property=\"al:web:url\"/><meta content=\"https://miro.medium.com/max/1200/0*MvA7JX1Z1rAG5byo\" data-rh=\"true\" property=\"og:image\"/><meta content=\"https://miro.medium.com/max/1200/0*MvA7JX1Z1rAG5byo\" data-rh=\"true\" name=\"twitter:image:src\"/><meta content=\"summary_large_image\" data-rh=\"true\" name=\"twitter:card\"/><meta content=\"/@sh.tsang\" data-rh=\"true\" property=\"article:author\"/><meta content=\"Sik-Ho Tsang\" data-rh=\"true\" name=\"author\"/><meta content=\"index,follow\" data-rh=\"true\" name=\"robots\"/><meta content=\"unsafe-url\" data-rh=\"true\" name=\"referrer\"/><meta data-rh=\"true\" name=\"twitter:label1\" value=\"Reading time\"/><meta data-rh=\"true\" name=\"twitter:data1\" value=\"7 min read\"/><meta content=\"15d7f17b42ac\" data-rh=\"true\" name=\"parsely-post-id\"/><link data-rh=\"true\" href=\"https://plus.google.com/103654360130207659246\" rel=\"publisher\"/><link data-rh=\"true\" href=\"/osd.xml\" rel=\"search\" title=\"Medium\" type=\"application/opensearchdescription+xml\"/><link data-rh=\"true\" href=\"https://cdn-images-1.medium.com/fit/c/152/152/1*8I-HPL0bfoIzGied-dzOvA.png\" rel=\"apple-touch-icon\" sizes=\"152x152\"/><link data-rh=\"true\" href=\"https://cdn-images-1.medium.com/fit/c/120/120/1*8I-HPL0bfoIzGied-dzOvA.png\" rel=\"apple-touch-icon\" sizes=\"120x120\"/><link data-rh=\"true\" href=\"https://cdn-images-1.medium.com/fit/c/76/76/1*8I-HPL0bfoIzGied-dzOvA.png\" rel=\"apple-touch-icon\" sizes=\"76x76\"/><link data-rh=\"true\" href=\"https://cdn-images-1.medium.com/fit/c/60/60/1*8I-HPL0bfoIzGied-dzOvA.png\" rel=\"apple-touch-icon\" sizes=\"60x60\"/><link color=\"#171717\" data-rh=\"true\" href=\"https://cdn-static-1.medium.com/_/fp/icons/monogram-mask.KPLCSFEZviQN0jQ7veN2RQ.svg\" rel=\"mask-icon\"/><link data-rh=\"true\" href=\"https://cdn-static-1.medium.com/_/fp/icons/favicon-rebrand-medium.3Y6xpZ-0FSdWDnPM3hSBIA.ico\" rel=\"icon\"/><link data-rh=\"true\" href=\"https://glyph.medium.com/css/e/sr/latin/e/ssr/latin/e/ssb/latin/m2.css\" id=\"glyph_link\" rel=\"stylesheet\" type=\"text/css\"/><link data-rh=\"true\" href=\"/@sh.tsang\" rel=\"author\"/><link data-rh=\"true\" href=\"https://towardsdatascience.com/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac\" rel=\"canonical\"/><link data-rh=\"true\" href=\"android-app://com.medium.reader/https/medium.com/p/15d7f17b42ac\" rel=\"alternate\"/><script data-rh=\"true\" type=\"application/ld+json\">{\"@context\":\"http:\\u002F\\u002Fschema.org\",\"@type\":\"NewsArticle\",\"image\":{\"@type\":\"ImageObject\",\"width\":89,\"height\":60,\"url\":\"https:\\u002F\\u002Fmiro.medium.com\\u002Fmax\\u002F178\\u002F0*MvA7JX1Z1rAG5byo\"},\"thumbnailUrl\":\"https:\\u002F\\u002Fmiro.medium.com\\u002Fmax\\u002F178\\u002F0*MvA7JX1Z1rAG5byo\",\"url\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Freview-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac\",\"dateCreated\":\"2018-12-09T13:18:30.998Z\",\"datePublished\":\"2018-12-09T13:18:30.998Z\",\"dateModified\":\"2019-03-20T16:00:42.080Z\",\"headline\":\"Review: ResNeXt — 1st Runner Up in ILSVRC 2016 (Image Classification)\",\"name\":\"Review: ResNeXt — 1st Runner Up in ILSVRC 2016 (Image Classification)\",\"identifier\":\"15d7f17b42ac\",\"keywords\":[\"Lite:true\",\"Tag:Machine Learning\",\"Tag:Deep Learning\",\"Tag:Artificial Intelligence\",\"Tag:Data Science\",\"Tag:Image Classification\",\"Topic:Machine Learning\",\"Topic:Data Science\",\"Publication:towards-data-science\",\"Elevated:false\",\"LockedPostSource:LOCKED_POST_SOURCE_NONE\",\"LayerCake:3\"],\"author\":{\"@type\":\"Person\",\"name\":\"Sik-Ho Tsang\",\"url\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002F@sh.tsang\"},\"creator\":[\"Sik-Ho Tsang\"],\"publisher\":{\"@type\":\"Organization\",\"name\":\"Towards Data Science\",\"url\":\"towardsdatascience.com\",\"logo\":{\"@type\":\"ImageObject\",\"width\":161,\"height\":60,\"url\":\"https:\\u002F\\u002Fmiro.medium.com\\u002Fmax\\u002F322\\u002F1*5EUO1kUYBthpOCPzRj_l2g.png\"}},\"mainEntityOfPage\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Freview-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac\"}</script><script data-rh=\"true\">(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\n",
       " (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n",
       " m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n",
       " })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\n",
       " ga('create', 'UA-24232453-2', 'auto');\n",
       " ga('send', 'pageview');</script><script data-rh=\"true\" type=\"text/javascript\">(function(b,r,a,n,c,h,_,s,d,k){if(!b[n]||!b[n]._q){for(;s<_.length;)c(h,_[s++]);d=r.createElement(a);d.async=1;d.src=\"https://cdn.branch.io/branch-latest.min.js\";k=r.getElementsByTagName(a)[0];k.parentNode.insertBefore(d,k);b[n]=h}})(window,document,\"script\",\"branch\",function(b,r){b[r]=function(){b._q.push([r,arguments])}},{_q:[],_v:1},\"addListener applyCode autoAppIndex banner closeBanner closeJourney creditHistory credits data deepview deepviewCta first getCode init link logout redeem referrals removeListener sendSMS setBranchViewData setIdentity track validateCode trackCommerceEvent logEvent\".split(\" \"), 0);\n",
       "   branch.init('key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm', {'no_journeys': true, 'disable_exit_animation': true, 'disable_entry_animation': true, 'tracking_disabled': false}, function(err, data) {});</script><style data-fela-rehydration=\"400\" data-fela-type=\"STATIC\" type=\"text/css\">html{box-sizing:border-box}*, *:before, *:after{box-sizing:inherit}body{margin:0;padding:0;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;color:rgba(0,0,0,0.8);position:relative;min-height:100vh}h1, h2, h3, h4, h5, h6, dl, dd, ol, ul, menu, figure, blockquote, p, pre, form{margin:0}menu, ol, ul{padding:0;list-style:none;list-style-image:none}main{display:block}a{color:inherit;text-decoration:none}a, button, input{-webkit-tap-highlight-color:transparent}img, svg{vertical-align:middle}button{background:transparent;overflow:visible}button, input, optgroup, select, textarea{margin:0}</style><style data-fela-rehydration=\"400\" data-fela-type=\"KEYFRAME\" type=\"text/css\">@-webkit-keyframes k1{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}@-moz-keyframes k1{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}@keyframes k1{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}</style><style data-fela-rehydration=\"400\" data-fela-type=\"RULE\" type=\"text/css\">.a{font-family:medium-content-sans-serif-font, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, Oxygen, Ubuntu, Cantarell, \"Open Sans\", \"Helvetica Neue\", sans-serif}.b{font-weight:400}.c{background-color:rgba(255, 255, 255, 1)}.l{display:block}.m{position:fixed}.n{top:0}.o{left:0}.p{right:0}.q{z-index:500}.r{box-shadow:0 4px 12px 0 rgba(0, 0, 0, 0.05)}.s{transition:transform 300ms ease}.t{will-change:transform}.v{padding-left:24px}.w{padding-right:24px}.x{margin-left:auto}.y{margin-right:auto}.z{height:65px}.ab{width:100%}.ac{max-width:1080px}.ae{box-sizing:border-box}.af{display:flex}.ag{align-items:center}.aj{flex:1 0 auto}.ak{margin-left:-6px}.al{fill:rgba(0, 0, 0, 0.84)}.am{flex:0 0 auto}.an{margin-left:16px}.ao{font-family:medium-content-sans-serif-font, \"Lucida Grande\", \"Lucida Sans Unicode\", \"Lucida Sans\", Geneva, Arial, sans-serif}.ap{font-style:normal}.aq{line-height:20px}.ar{font-size:15.8px}.as{letter-spacing:0px}.at{color:rgba(0, 0, 0, 0.54)}.au{fill:rgba(0, 0, 0, 0.54)}.av{color:rgba(90, 118, 144, 1)}.aw{fill:rgba(102, 138, 170, 1)}.ax{font-size:inherit}.ay{border:inherit}.az{font-family:inherit}.ba{letter-spacing:inherit}.bb{font-weight:inherit}.bc{padding:0}.bd{margin:0}.be:hover{cursor:pointer}.bf:hover{color:rgba(84, 108, 131, 1)}.bg:hover{fill:rgba(90, 118, 144, 1)}.bh:focus{outline:none}.bi:disabled{cursor:default}.bj:disabled{color:rgba(3, 168, 124, 0.5)}.bk:disabled{fill:rgba(3, 168, 124, 0.5)}.bl{padding:8px 16px}.bm{background:0}.bn{border-color:rgba(102, 138, 170, 1)}.bo:hover{border-color:rgba(90, 118, 144, 1)}.bp{border-radius:4px}.bq{border-width:1px}.br{border-style:solid}.bs{display:inline-block}.bt{text-decoration:none}.bu{border-top:none}.bv{background-color:rgba(53, 88, 118, 1)}.bx{height:54px}.by{overflow:hidden}.bz{margin-right:40px}.ca{height:36px}.cb{width:97px}.cc{overflow:auto}.cd{flex:0 1 auto}.ce{list-style-type:none}.cf{line-height:40px}.cg{white-space:nowrap}.ch{overflow-x:auto}.ci{align-items:flex-start}.cj{margin-top:20px}.ck{padding-top:20px}.cl{height:80px}.cm{height:20px}.cn{margin-right:15px}.co{margin-left:15px}.cp:first-child{margin-left:0}.cq{min-width:1px}.cr{background-color:rgba(197, 210, 225, 1)}.cs{font-weight:300}.ct{font-size:15px}.cu{line-height:21px}.cv{color:rgba(197, 210, 225, 1)}.cw{text-transform:uppercase}.cx{letter-spacing:1px}.cy{color:inherit}.cz{fill:inherit}.da:hover{color:rgba(251, 255, 255, 1)}.db:hover{fill:rgba(233, 241, 250, 1)}.dc:disabled{color:rgba(150, 171, 191, 1)}.dd:disabled{fill:rgba(150, 171, 191, 1)}.de{margin-bottom:0px}.df{height:119px}.di{max-width:728px}.dj{flex-direction:column}.dk{opacity:0}.dl{pointer-events:none}.dm{will-change:opacity}.dn{transition:opacity 200ms}.do{width:131px}.dp{left:50%}.dq{transform:translateX(-516px)}.dr{top:calc(65px + 54px + 40px)}.ds{padding-bottom:28px}.dt{border-bottom:1px solid rgba(0, 0, 0, 0.1)}.du:hover{color:rgba(0, 0, 0, 0.9)}.dv:hover{fill:rgba(0, 0, 0, 0.9)}.dw:disabled{color:rgba(0, 0, 0, 0.54)}.dx:disabled{fill:rgba(0, 0, 0, 0.54)}.dy{font-weight:600}.dz{font-size:18px}.ea{color:rgba(0, 0, 0, 0.84)}.eb{padding-bottom:20px}.ec{padding-top:2px}.ed{font-size:16px}.ee{max-height:120px}.ef{text-overflow:ellipsis}.eg{display:-webkit-box}.eh{-webkit-line-clamp:6}.ei{-webkit-box-orient:vertical}.ej{padding:4px 12px}.ek{padding-top:28px}.el{margin-bottom:19px}.em{margin-left:-5px}.en{margin-right:5px}.eo{position:relative}.ep{outline:0}.eq{border:0}.er{user-select:none}.es{cursor:pointer}.et> svg{pointer-events:none}.eu:active{border-style:none}.ev:focus{fill:rgba(90, 118, 144, 1)}.ew{margin-top:5px}.ex button{text-align:left}.ey{margin-top:40px}.ez{clear:both}.fa{justify-content:center}.fb{margin:0 auto}.fc{padding:0 24px}.fd{margin-right:8px}.fe{margin-bottom:8px}.ff{border-radius:3px}.fg{padding:5px 10px}.fh{background:rgba(0, 0, 0, 0.05)}.fi{line-height:22px}.fj{margin-top:15px}.fk{justify-content:space-between}.fl{margin-right:16px}.fm{border:1px solid rgba(0, 0, 0, 0.1)}.fn{border-radius:50%}.fo{height:60px}.fp{transition:border-color 150ms ease}.fq{width:60px}.fr::before{background:\n",
       "       radial-gradient(circle, rgba(90, 118, 144, 1) 60%, transparent 70%)\n",
       "     }.fs::before{border-radius:50%}.ft::before{content:\"\"}.fu::before{display:block}.fv::before{z-index:0}.fw::before{left:0}.fx::before{height:100%}.fy::before{position:absolute}.fz::before{top:0}.ga::before{width:100%}.gb:hover::before{animation:k1 2000ms infinite cubic-bezier(.1,.12,.25,1)}.gc:active{border-style:solid}.gd{background:rgba(255, 255, 255, 1)}.ge{z-index:2}.gf{height:100%}.gg{position:absolute}.gh{padding-right:8px}.gi{display:none}.gj{padding-top:32px}.gk{border-top:1px solid rgba(0, 0, 0, 0.1)}.gl{margin-bottom:25px}.gm{margin-top:25px}.gn{margin-bottom:32px}.go{min-height:80px}.gt{width:80px}.gu{padding-left:102px}.gw{letter-spacing:0.05em}.gx{margin-bottom:6px}.gy{font-size:28px}.gz{line-height:36px}.ha{max-width:555px}.hb{max-width:450px}.hc{line-height:24px}.he{max-width:550px}.hf{padding-top:25px}.hg{padding:20px}.hh{border:1px solid rgba(102, 138, 170, 1)}.hi{text-align:center}.hj{margin-top:64px}.hk{background-color:rgba(0, 0, 0, 0.02)}.hv{top:calc(100vh + 100px)}.hw{bottom:calc(100vh + 100px)}.hx{width:10px}.hy{word-break:break-word}.hz{word-wrap:break-word}.ia:after{display:block}.ib:after{content:\"\"}.ic:after{clear:both}.id{line-height:1.23}.ie{letter-spacing:0}.if{font-family:medium-content-title-font, Georgia, Cambria, \"Times New Roman\", Times, serif}.ig{font-size:40px}.im{margin-bottom:-0.27em}.in{line-height:48px}.io{line-height:1.394}.ip{font-size:25px}.iv{margin-bottom:-0.42em}.iw{font-size:24px}.ix{line-height:32px}.iy{margin-top:32px}.iz{height:48px}.ja{width:48px}.jb{margin-left:12px}.jc{margin-bottom:2px}.je{max-height:20px}.jf{-webkit-line-clamp:1}.jg:hover{text-decoration:underline}.jh{margin-left:8px}.ji{padding:0px 8px}.jj{border-color:rgba(0, 0, 0, 0.54)}.jk:hover{color:rgba(0, 0, 0, 0.97)}.jl:hover{fill:rgba(0, 0, 0, 0.97)}.jm:hover{border-color:rgba(0, 0, 0, 0.84)}.jn:disabled{fill:rgba(0, 0, 0, 0.76)}.jo:disabled{border-color:rgba(0, 0, 0, 0.2)}.jp:disabled{cursor:inherit}.jq:disabled:hover{color:rgba(0, 0, 0, 0.54)}.jr:disabled:hover{fill:rgba(0, 0, 0, 0.76)}.js:disabled:hover{border-color:rgba(0, 0, 0, 0.2)}.jt{line-height:18px}.jz{max-width:1000px}.ka{transition:opacity 100ms 400ms}.kb{transform:translateZ(0)}.kc{margin:auto}.kd{background-color:rgba(0, 0, 0, 0.05)}.ke{padding-bottom:66.7%}.kf{filter:blur(20px)}.kg{transform:scale(1.1)}.kh{line-height:1.58}.ki{letter-spacing:-0.004em}.kj{font-family:medium-content-serif-font, Georgia, Cambria, \"Times New Roman\", Times, serif}.ku{margin-bottom:-0.46em}.kv{clear:left}.kw{float:left}.kx{font-size:66px}.ky{line-height:.83}.kz{margin-right:12px}.lf{font-weight:700}.lg{font-style:italic}.lh{background-repeat:repeat-x}.li{background-image:linear-gradient(to right,rgba(0, 0, 0, 0.84) 100%,rgba(0, 0, 0, 0.84) 0);background-image:url('data:image/svg+xml;utf8,<svg preserveAspectRatio=\"none\" viewBox=\"0 0 1 1\" xmlns=\"http://www.w3.org/2000/svg\"><line x1=\"0\" y1=\"0\" x2=\"1\" y2=\"1\" stroke=\"rgba(0, 0, 0, 0.84)\" /></svg>')}.lj{background-size:1px 1px}.lk{background-position:0 1.05em;background-position:0 calc(1em + 1px)}.ll{max-width:730px}.lm{padding-bottom:60.136986301369866%}.ln{line-height:1.4}.lo{margin-top:10px}.lq{max-width:734px}.lr{padding-bottom:43.869209809264305%}.ls{background:none}.lt{font-family:medium-content-slab-serif-font, Georgia, Cambria, \"Times New Roman\", Times, serif}.lu{border:none}.lv{margin-top:30px}.lw:before{content:\"...\"}.lx:before{letter-spacing:0.6em}.ly:before{text-indent:0.6em}.lz:before{font-style:italic}.ma:before{line-height:1.4}.mb{line-height:1.12}.mc{letter-spacing:-0.022em}.mn{margin-bottom:-0.28em}.mt{list-style-type:decimal}.mu{margin-left:30px}.mv{padding-left:0px}.nb{line-height:1.18}.nm{margin-bottom:-0.31em}.nn{max-width:651px}.no{padding-bottom:31.643625192012287%}.np{list-style-type:disc}.nq{max-width:775px}.nr{padding-bottom:42.58064516129032%}.ns{max-width:1335px}.nt{margin-left:24px}.nu{margin-right:24px}.nv{padding-bottom:27.640449438202246%}.nw{font-style:inherit}.nx{max-width:1348px}.ny{padding-bottom:55.04451038575668%}.nz{max-width:666px}.oa{padding-bottom:51.65165165165165%}.ob{max-width:571px}.oc{padding-bottom:20.84063047285464%}.od{max-width:1084px}.oe{padding-bottom:29.059040590405903%}.of{max-width:1415px}.og{padding-bottom:41.69611307420495%}.oh{max-width:1343px}.oi{padding-bottom:43.484735666418466%}.oj{max-width:592px}.ok{padding-bottom:32.0945945945946%}</style><style data-fela-rehydration=\"400\" data-fela-type=\"RULE\" media=\"all and (min-width: 1080px)\" type=\"text/css\">.d{display:none}.ht{max-width:1080px}.hu{padding:0 24px}.il{margin-top:0.78em}.iu{margin-top:0.74em}.jy{margin-top:56px}.ks{font-size:21px}.kt{margin-top:2em}.le{padding-top:7px}.ml{font-size:34px}.mm{margin-top:1.25em}.ms{margin-top:0.86em}.na{margin-top:1.05em}.nk{font-size:26px}.nl{margin-top:1.72em}</style><style data-fela-rehydration=\"400\" data-fela-type=\"RULE\" media=\"all and (max-width: 1079.98px)\" type=\"text/css\">.e{display:none}.lp{text-align:center}</style><style data-fela-rehydration=\"400\" data-fela-type=\"RULE\" media=\"all and (max-width: 903.98px)\" type=\"text/css\">.f{display:none}</style><style data-fela-rehydration=\"400\" data-fela-type=\"RULE\" media=\"all and (max-width: 727.98px)\" type=\"text/css\">.g{display:none}.ah{height:56px}.ai{display:flex}.bw{display:block}.dg{margin-bottom:0px}.dh{height:110px}.gp{margin-bottom:24px}.gq{align-items:center}.gr{width:102px}.gs{position:relative}.gv{padding-left:0}.hd{margin-top:24px}</style><style data-fela-rehydration=\"400\" data-fela-type=\"RULE\" media=\"all and (max-width: 551.98px)\" type=\"text/css\">.h{display:none}.hl{max-width:552px}.hm{padding:0 24px}.ih{margin-top:0.39em}.iq{margin-top:0.42em}.jd{margin-bottom:0px}.ju{margin-top:40px}.kk{font-size:18px}.kl{margin-top:1.56em}.la{padding-top:0}.md{font-size:30px}.me{margin-top:0.93em}.mo{margin-top:0.67em}.mw{margin-top:1.34em}.nc{font-size:24px}.nd{margin-top:1.23em}</style><style data-fela-rehydration=\"400\" data-fela-type=\"RULE\" media=\"all and (min-width: 904px) and (max-width: 1079.98px)\" type=\"text/css\">.i{display:none}.hr{max-width:1080px}.hs{padding:0 24px}.ik{margin-top:0.78em}.it{margin-top:0.74em}.jx{margin-top:56px}.kq{font-size:21px}.kr{margin-top:2em}.ld{padding-top:7px}.mj{font-size:34px}.mk{margin-top:1.25em}.mr{margin-top:0.86em}.mz{margin-top:1.05em}.ni{font-size:26px}.nj{margin-top:1.72em}</style><style data-fela-rehydration=\"400\" data-fela-type=\"RULE\" media=\"all and (min-width: 728px) and (max-width: 903.98px)\" type=\"text/css\">.j{display:none}.hp{max-width:904px}.hq{padding:0 24px}.ij{margin-top:0.78em}.is{margin-top:0.74em}.jw{margin-top:56px}.ko{font-size:21px}.kp{margin-top:2em}.lc{padding-top:7px}.mh{font-size:34px}.mi{margin-top:1.25em}.mq{margin-top:0.86em}.my{margin-top:1.05em}.ng{font-size:26px}.nh{margin-top:1.72em}</style><style data-fela-rehydration=\"400\" data-fela-type=\"RULE\" media=\"all and (min-width: 552px) and (max-width: 727.98px)\" type=\"text/css\">.k{display:none}.hn{max-width:728px}.ho{padding:0 24px}.ii{margin-top:0.39em}.ir{margin-top:0.42em}.jv{margin-top:40px}.km{font-size:18px}.kn{margin-top:1.56em}.lb{padding-top:0}.mf{font-size:30px}.mg{margin-top:0.93em}.mp{margin-top:0.67em}.mx{margin-top:1.34em}.ne{font-size:24px}.nf{margin-top:1.23em}</style><style data-fela-rehydration=\"400\" data-fela-type=\"RULE\" media=\"print\" type=\"text/css\">.u{display:none}</style></head><body><div id=\"root\"><div class=\"a b c\"><div class=\"d e f g h i j k\"></div><nav class=\"l m n o p c q r s t u\"><div class=\"branch-journeys-top\"><div class=\"l c\"><section class=\"v w x y z ab ac ae af ag ah ai\"><div class=\"af ag aj q\"><div class=\"ak l\"><a aria-label=\"Homepage\" href=\"https://medium.com/?source=post_page---------------------------\"><svg class=\"al\" height=\"45\" viewbox=\"0 0 45 45\" width=\"45\"><path d=\"M5 40V5h35v35H5zm8.56-12.63c0 .56-.03.69-.32 1.03L10.8 31.4v.4h6.97v-.4L15.3 28.4c-.29-.34-.34-.5-.34-1.03v-8.95l6.13 13.36h.71l5.26-13.36v10.64c0 .3 0 .35-.19.53l-1.85 1.8v.4h9.2v-.4l-1.83-1.8c-.18-.18-.2-.24-.2-.53V15.94c0-.3.02-.35.2-.53l1.82-1.8v-.4h-6.47l-4.62 11.55-5.2-11.54h-6.8v.4l2.15 2.63c.24.3.29.37.29.77v10.35z\"></path></svg></a></div></div><div class=\"l am q\"><div class=\"af ag\"><div class=\"af g\"><div class=\"an l\"><span class=\"ao b ap aq ar as l at au\"><a class=\"av aw ax ay az ba bb bc bd be bf bg bh bi bj bk\" href=\"https://medium.com/m/signin?operation=login&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Freview-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac&amp;source=post_page--------------------------nav_reg-\">Sign in</a></span></div></div><div class=\"an l\"><button class=\"bl bm av aw bn bf bg bo be bp ao b ap aq ar as bq br ae bs bt bh\">Get started</button></div></div></div></section></div><div class=\"bu l bv bw\"><section class=\"v w x y bx ab ac ae by af ag\"><div class=\"bz l am\"><a href=\"https://towardsdatascience.com/?source=post_page---------------------------\"><div class=\"ca cb l\"><img alt=\"Towards Data Science\" class=\"\" height=\"36\" src=\"https://miro.medium.com/max/194/1*5EUO1kUYBthpOCPzRj_l2g.png\" width=\"97\"/></div></a></div><div class=\"cc l cd\"><ul class=\"ce bd cf cg ch af ci g cj ck cl\"><li class=\"af ag cm cn co cp\"><span class=\"ao cs ct cu cv cw cx\"><a class=\"cy cz ax ay az ba bb bc bd be da db bh bi dc dd\" href=\"https://towardsdatascience.com/data-science/home?source=post_page---------------------------\">Data Science</a></span></li><li class=\"af ag cm cn co cp\"><span class=\"ao cs ct cu cv cw cx\"><a class=\"cy cz ax ay az ba bb bc bd be da db bh bi dc dd\" href=\"https://towardsdatascience.com/machine-learning/home?source=post_page---------------------------\">Machine Learning</a></span></li><li class=\"af ag cm cn co cp\"><span class=\"ao cs ct cu cv cw cx\"><a class=\"cy cz ax ay az ba bb bc bd be da db bh bi dc dd\" href=\"https://towardsdatascience.com/programming/home?source=post_page---------------------------\">Programming</a></span></li><li class=\"af ag cm cn co cp\"><span class=\"ao cs ct cu cv cw cx\"><a class=\"cy cz ax ay az ba bb bc bd be da db bh bi dc dd\" href=\"https://towardsdatascience.com/data-visualization/home?source=post_page---------------------------\">Visualization</a></span></li><li class=\"af ag cm cn co cp\"><span class=\"ao cs ct cu cv cw cx\"><a class=\"cy cz ax ay az ba bb bc bd be da db bh bi dc dd\" href=\"https://towardsdatascience.com/artificial-intelligence/home?source=post_page---------------------------\">AI</a></span></li><li class=\"af ag cm cn co cp\"><span class=\"ao cs ct cu cv cw cx\"><a class=\"cy cz ax ay az ba bb bc bd be da db bh bi dc dd\" href=\"https://towardsdatascience.com/data-journalism/home?source=post_page---------------------------\">Journalism</a></span></li><li class=\"af ag cm cn co cp\"><span class=\"ao cs ct cu cv cw cx\"><a class=\"cy cz ax ay az ba bb bc bd be da db bh bi dc dd\" href=\"/toronto-machine-learning-summit-8bae371d4bb1?source=post_page---------------------------\">Events</a></span></li><span class=\"cm cq cr\"></span><li class=\"af ag cm cn co cp\"><span class=\"ao cs ct cu cv cw cx\"><a class=\"cy cz ax ay az ba bb bc bd be da db bh bi dc dd\" href=\"https://towardsdatascience.com/contribute/home?source=post_page---------------------------\">Submit</a></span></li></ul></div></section></div></div></nav><div class=\"de df l dg dh\"></div><article><section class=\"v w x y ab di ae af dj\"></section><span class=\"l\"></span><div><div class=\"gg o hv hw hx dl\"></div><section class=\"hy hz ia ib ic\"><div class=\"ae fb ab di v w\"><div><div class=\"id ie ea ap if b ig ih ii ij ik il im\" id=\"1ef0\"><h1 class=\"if b ig in ea\">Review: ResNeXt — 1st Runner Up in ILSVRC 2016 (Image Classification)</h1></div></div><div class=\"io ie at ap ao cs ip iq ir is it iu iv\" id=\"4c9a\"><h2 class=\"ao cs iw ix at\"><strong class=\"bb\">Network-in-Neuron, A New Dimensionality: Cardinality</strong></h2></div><div class=\"iy\"><div class=\"ag af\"><div><a href=\"/@sh.tsang?source=post_page---------------------------\"><img alt=\"Sik-Ho Tsang\" class=\"l fn iz ja\" height=\"48\" src=\"https://miro.medium.com/fit/c/96/96/1*OxjNUHcLFU8-pp-j8su6pg.jpeg\" width=\"48\"/></a></div><div class=\"jb ab l\"><div class=\"af\"><div style=\"flex:1\"><span class=\"ao b ap aq ar as l ea al\"><div class=\"jc af ag jd\" data-test-id=\"postByline\"><span class=\"ao cs ed aq by je ef eg jf ei ea\"><a class=\"cy cz ax ay az ba bb bc bd be jg bh bi dw dx\" href=\"/@sh.tsang?source=post_page---------------------------\">Sik-Ho Tsang</a></span><div class=\"jh l am h\"><button class=\"ji ea al bm jj jk jl jm be dw jn jo jp jq jr js bp ao b ap jt ct as bq br ae bs bt bh\">Follow</button></div></div></span></div></div><span class=\"ao b ap aq ar as l at au\"><span class=\"ao cs ed aq by je ef eg jf ei at\"><div><a class=\"cy cz ax ay az ba bb bc bd be jg bh bi dw dx\" href=\"/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac?source=post_page---------------------------\">Dec 9, 2018</a> <!-- -->·<!-- --> <!-- -->7<!-- --> min read</div></span></span></div></div></div><figure class=\"ju jv jw jx jy ez jz x y paragraph-image\"><div class=\"kc l eo kd\"><div class=\"ke l\"><div class=\"dk ka gg n o gf ab by t kb\"><img class=\"gg n o gf ab kf kg\" height=\"467\" src=\"https://miro.medium.com/max/60/0*MvA7JX1Z1rAG5byo?q=20\" width=\"700\"/></div><img class=\"dk ka gg n o gf ab gd\" height=\"467\" width=\"700\"/><noscript><img class=\"gg n o gf ab\" height=\"467\" src=\"https://miro.medium.com/max/1400/0*MvA7JX1Z1rAG5byo\" width=\"700\"/></noscript></div></div></figure><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku kv\" id=\"ecb1\"><span class=\"l kw kx ky kz la lb lc ld le eo\">In</span> this story, <strong class=\"kj lf\">ResNeXt, </strong>by <strong class=\"kj lf\">UC San Diego </strong>and <strong class=\"kj lf\">Facebook AI Research (FAIR), </strong>is reviewed. The model name, ResNeXt, contains Next. It means the <em class=\"lg\">next </em>dimension, on top of the <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet</a>. This next dimension is called the “<strong class=\"kj lf\"><em class=\"lg\">cardinality</em></strong>” <strong class=\"kj lf\">dimension</strong>. And ResNeXt becomes the <strong class=\"kj lf\">1st Runner Up of ILSVRC classification task</strong>.</p><figure class=\"ju jv jw jx jy ez ll x y paragraph-image\"><div class=\"kc l eo kd\"><div class=\"lm l\"><div class=\"dk ka gg n o gf ab by t kb\"><img class=\"gg n o gf ab kf kg\" height=\"421\" src=\"https://miro.medium.com/max/60/1*RHpn70qFNCcqyVjkPdFtGA.png?q=20\" width=\"700\"/></div><img class=\"dk ka gg n o gf ab gd\" height=\"421\" width=\"700\"/><noscript><img class=\"gg n o gf ab\" height=\"421\" src=\"https://miro.medium.com/max/1400/1*RHpn70qFNCcqyVjkPdFtGA.png\" width=\"700\"/></noscript></div></div><figcaption class=\"at ed ln lo hi di x y lp ao cs\"><strong class=\"ao lf\">ILSVRC 2016 Classification Ranking </strong><a class=\"cy bt lh li lj lk\" href=\"http://image-net.org/challenges/LSVRC/2016/results#loc\">http://image-net.org/challenges/LSVRC/2016/results#loc</a></figcaption></figure><figure class=\"ju jv jw jx jy ez lq x y paragraph-image\"><div class=\"kc l eo kd\"><div class=\"lr l\"><div class=\"dk ka gg n o gf ab by t kb\"><img class=\"gg n o gf ab kf kg\" height=\"307\" src=\"https://miro.medium.com/max/60/1*LOoc11tkDoqv0pC6OH7mwA.png?q=20\" width=\"700\"/></div><img class=\"dk ka gg n o gf ab gd\" height=\"307\" width=\"700\"/><noscript><img class=\"gg n o gf ab\" height=\"307\" src=\"https://miro.medium.com/max/1400/1*LOoc11tkDoqv0pC6OH7mwA.png\" width=\"700\"/></noscript></div></div><figcaption class=\"at ed ln lo hi di x y lp ao cs\"><strong class=\"ao lf\">Residual Block in ResNet (Left), A Block of ResNeXt with Cardinality = 32 (Right)</strong></figcaption></figure><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"f37a\">Compared with <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet</a> (The winner in ILSVRC 2015, 3.57%) and <a class=\"cy bt lh li lj lk\" href=\"/review-polynet-2nd-runner-up-in-ilsvrc-2016-image-classification-8a1a941ce9ea\">PolyNet</a> (2nd Runner Up, 3.04%, Team name CU-DeepLink), ResNeXt got 3.03% Top-5 error rate, which is a large relative improvement of about 15%!!</p><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"a174\">And it is published in <strong class=\"kj lf\">2017 CVPR</strong>, which has already got over <strong class=\"kj lf\">500 citations</strong> while I was writing this story. (<a class=\"ls av bt\" href=\"/u/aff72a0c1243?source=post_page---------------------------\">Sik-Ho Tsang</a> @ Medium)</p></div></section><hr class=\"lt cs gy lu lv hi lw lx ly lz ma\"/><section class=\"hy hz ia ib ic\"><div class=\"ae fb ab di v w\"><h1 class=\"mb mc ea ap ao dy md me mf mg mh mi mj mk ml mm mn\" id=\"f006\">What Are Covered</h1><ol class=\"\"><li class=\"kh ki ea ap kj b kk mo km mp ko mq kq mr ks ms ku mt mu mv\" id=\"1929\"><strong class=\"kj lf\">Aggregated Transformation</strong></li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku mt mu mv\" id=\"801d\"><strong class=\"kj lf\">Relationship with </strong><a class=\"cy bt lh li lj lk\" href=\"/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc\"><strong class=\"kj lf\">Inception-ResNet</strong></a><strong class=\"kj lf\">, and Grouped Convolution in </strong><a class=\"cy bt lh li lj lk\" href=\"https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160\"><strong class=\"kj lf\">AlexNet</strong></a></li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku mt mu mv\" id=\"48b6\"><strong class=\"kj lf\">Full Architecture and Ablation Study</strong></li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku mt mu mv\" id=\"0375\"><strong class=\"kj lf\">Results</strong></li></ol></div></section><hr class=\"lt cs gy lu lv hi lw lx ly lz ma\"/><section class=\"hy hz ia ib ic\"><div class=\"ae fb ab di v w\"><h1 class=\"mb mc ea ap ao dy md me mf mg mh mi mj mk ml mm mn\" id=\"ad50\">1. Aggregated Transformation</h1><h2 class=\"nb mc ea ap ao dy nc nd ne nf ng nh ni nj nk nl nm\" id=\"f984\">1.1. Revisiting Simple Neuron</h2><figure class=\"ju jv jw jx jy ez nn x y paragraph-image\"><div class=\"kc l eo kd\"><div class=\"no l\"><div class=\"dk ka gg n o gf ab by t kb\"><img class=\"gg n o gf ab kf kg\" height=\"206\" src=\"https://miro.medium.com/max/60/1*lm62iiybACCoz4KR9w5Azg.png?q=20\" width=\"651\"/></div><img class=\"dk ka gg n o gf ab gd\" height=\"206\" width=\"651\"/><noscript><img class=\"gg n o gf ab\" height=\"206\" src=\"https://miro.medium.com/max/1302/1*lm62iiybACCoz4KR9w5Azg.png\" width=\"651\"/></noscript></div></div><figcaption class=\"at ed ln lo hi di x y lp ao cs\"><strong class=\"ao lf\">A Simple Neuron (Left), and the corresponding equation (Right)</strong></figcaption></figure><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"0a28\">As we should know, <strong class=\"kj lf\">a simple neuron</strong> as above, the output is the summation of wi times xi. The above operation can be recast as <strong class=\"kj lf\">a combination of splitting, transforming, and aggregating.</strong></p><ul class=\"\"><li class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku np mu mv\" id=\"beaf\"><strong class=\"kj lf\">Splitting</strong>: the vector x is sliced as a low-dimensional embedding, and in the above, it is a single-dimension subspace xi.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"28ea\"><strong class=\"kj lf\">Transforming</strong>: the low-dimensional representation is transformed, and in the above, it is simply scaled: wixi.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"6164\"><strong class=\"kj lf\">Aggregating</strong>: the transformations in all embeddings are aggregated by summation.</li></ul><h2 class=\"nb mc ea ap ao dy nc nd ne nf ng nh ni nj nk nl nm\" id=\"e151\">1.2. Aggregated Transformations</h2><figure class=\"ju jv jw jx jy ez nq x y paragraph-image\"><div class=\"kc l eo kd\"><div class=\"nr l\"><div class=\"dk ka gg n o gf ab by t kb\"><img class=\"gg n o gf ab kf kg\" height=\"298\" src=\"https://miro.medium.com/max/60/1*r_63luxLeVstJMzd0o-8Iw.png?q=20\" width=\"700\"/></div><img class=\"dk ka gg n o gf ab gd\" height=\"298\" width=\"700\"/><noscript><img class=\"gg n o gf ab\" height=\"298\" src=\"https://miro.medium.com/max/1400/1*r_63luxLeVstJMzd0o-8Iw.png\" width=\"700\"/></noscript></div></div><figcaption class=\"at ed ln lo hi di x y lp ao cs\"><strong class=\"ao lf\">A Block of ResNeXt with Cardinality = 32 (Left), and Its Generic Equation (Right)</strong></figcaption></figure><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"0733\">In contrast to “Network-in-Network”, it is “<strong class=\"kj lf\">Network-in-Neuron</strong>” expands along a new dimension. Instead of linear function in a simple neuron that wi times xi in each path, <strong class=\"kj lf\">a nonlinear function is performed for each path</strong>.</p><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"b715\">A new dimension <strong class=\"kj lf\"><em class=\"lg\">C</em> </strong>is introduced, called “<strong class=\"kj lf\">Cardinality</strong>”. The dimension of cardinality <strong class=\"kj lf\">controls the number of more complex transformations</strong>.</p></div></section><hr class=\"lt cs gy lu lv hi lw lx ly lz ma\"/><section class=\"hy hz ia ib ic\"><div class=\"ae fb ab di v w\"><h1 class=\"mb mc ea ap ao dy md me mf mg mh mi mj mk ml mm mn\" id=\"0a3f\"><strong class=\"bb\">2. Relationship with </strong><a class=\"cy bt lh li lj lk\" href=\"/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc\"><strong class=\"bb\">Inception-ResNet</strong></a><strong class=\"bb\"> and Grouped Convolution in </strong><a class=\"cy bt lh li lj lk\" href=\"https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160\"><strong class=\"bb\">AlexNet</strong></a></h1></div><div class=\"ae fb ab ac\"><figure class=\"ju jv jw jx jy ez ns nt nu paragraph-image\"><div class=\"kc l eo kd\"><div class=\"nv l\"><div class=\"dk ka gg n o gf ab by t kb\"><img class=\"gg n o gf ab kf kg\" height=\"276\" src=\"https://miro.medium.com/max/60/1*cIm3uy7eNvEchxRbBeBScQ.png?q=20\" width=\"1000\"/></div><img class=\"dk ka gg n o gf ab gd\" height=\"276\" width=\"1000\"/><noscript><img class=\"gg n o gf ab\" height=\"276\" src=\"https://miro.medium.com/max/2000/1*cIm3uy7eNvEchxRbBeBScQ.png\" width=\"1000\"/></noscript></div></div><figcaption class=\"at ed ln lo hi di x y lp ao cs\"><strong class=\"ao lf\">(a) ResNeXt Block, (b) Inception-ResNet Block, (c) Grouped Convolution</strong></figcaption></figure></div><div class=\"ae fb ab di v w\"><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"4bea\">To compare, <strong class=\"kj lf\">the above 3 blocks are having the SAME INTERNAL DIMENSIONS within each block.</strong></p><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"f444\"><strong class=\"kj lf\">(a) ResNeXt Block (Left)</strong></p><ul class=\"\"><li class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku np mu mv\" id=\"2537\">For each path, <strong class=\"kj lf\">Conv1×1–Conv3×3–Conv1×1 </strong>are done at each convolution path. This is the bottleneck design in <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet</a> block. The internal dimension for each path is denoted as <strong class=\"kj lf\"><em class=\"lg\">d</em> (<em class=\"lg\">d</em>=4)</strong>. The number of paths is the <strong class=\"kj lf\">cardinality <em class=\"lg\">C</em> (<em class=\"lg\">C</em>=32)</strong>. If we sum up the dimension of each Conv3×3 (i.e. <em class=\"lg\">d</em>×<em class=\"lg\">C</em>=4×32), it is also the dimensions of 128.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"f45b\">The dimension is increased directly from 4 to 256, and then added together, and also added with the skip connection path.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"ac99\">Compared with <a class=\"cy bt lh li lj lk\" href=\"/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc\">Inception-ResNet</a> that it needs to increase the dimension from 4 to 128 then to 256, <strong class=\"kj lf\">ResNeXt requires minimal extra effort designing each path.</strong></li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"ea06\">Unlike <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet</a>, in ResNeXt, the neurons at one path will not connected to the neurons at other paths.</li></ul><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"c543\"><strong class=\"kj lf\">(b) </strong><a class=\"cy bt lh li lj lk\" href=\"/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc\"><strong class=\"kj lf\">Inception-ResNet</strong></a><strong class=\"kj lf\"> Block (Middle)</strong></p><ul class=\"\"><li class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku np mu mv\" id=\"a564\">This is suggested in <a class=\"cy bt lh li lj lk\" href=\"/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc\">Inception-v4</a> to combine the <a class=\"cy bt lh li lj lk\" href=\"https://medium.com/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7\">Inception</a> module and <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet</a> block. Somehow due to the legacy problem, for each convolution path, <strong class=\"kj lf\">Conv1×1–Conv3×3 </strong>are done first. When added together (i.e. 4×32), the Conv3×3 has the dimension of 128.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"1309\">Then the outputs are concatenated together with dimension of 128. And <strong class=\"kj lf\">Conv1×1</strong> is used to restore the dimensions from 128 to 256.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"f428\">Finally the output is added with the skip connection path.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"8480\">The main difference is that they have an <strong class=\"kj lf\">early concatenation</strong>.</li></ul><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"2f32\"><strong class=\"kj lf\">(c) Grouped Convolution in </strong><a class=\"cy bt lh li lj lk\" href=\"https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160\"><strong class=\"kj lf\">AlexNet</strong></a><strong class=\"kj lf\"> (Right)</strong></p><ul class=\"\"><li class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku np mu mv\" id=\"b344\"><strong class=\"kj lf\">Conv1×1–Conv3×3–Conv1×1 </strong>are done at the convolution path, which is actually a<strong class=\"kj lf\"> bottleneck design</strong> suggested in <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet</a>. The Conv3×3 has the dimension of 128.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"9c7b\">However, <strong class=\"kj lf\">grouped convolution</strong>, suggested in <a class=\"cy bt lh li lj lk\" href=\"https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160\">AlexNet</a> is used here. Therefore this Conv3×3 is <strong class=\"kj lf\">wider but sparsely connected module. </strong>(Because the neurons at one path will not connected to the neurons at other paths, that’s why it is sparsely connected.)</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"5883\">Thus, there are <strong class=\"kj lf\">32 groups of convolutions</strong>. (2 groups only in <a class=\"cy bt lh li lj lk\" href=\"https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160\">AlexNet</a>)</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"eae5\">Then a skip connection is at parallel and added with the convolution path. Thus, the convolution path is learning the residual representation.</li></ul><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"e9f0\">Though the structures in (b) and (c) are not always the same as the general form in the equation shown in 1.2, indeed authors have tried the above three structures as shown above, and they found that the results are the same.</p><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"9345\"><strong class=\"kj lf\">Finally, authors choose to implement the structure in (c) because it is more succinct and faster than the other two forms.</strong></p></div></section><hr class=\"lt cs gy lu lv hi lw lx ly lz ma\"/><section class=\"hy hz ia ib ic\"><div class=\"ae fb ab di v w\"><h1 class=\"mb mc ea ap ao dy md me mf mg mh mi mj mk ml mm mn\" id=\"e7a5\">3. <strong class=\"bb\">Full Architecture and Ablation Study</strong></h1><h2 class=\"nb mc ea ap ao dy nc nd ne nf ng nh ni nj nk nl nm\" id=\"e30a\">3.1. Ablation Study of <em class=\"nw\">C</em> and <em class=\"nw\">d</em> Under Similar Complexity</h2></div><div class=\"ae fb ab ac\"><figure class=\"ju jv jw jx jy ez nx nt nu paragraph-image\"><div class=\"kc l eo kd\"><div class=\"ny l\"><div class=\"dk ka gg n o gf ab by t kb\"><img class=\"gg n o gf ab kf kg\" height=\"550\" src=\"https://miro.medium.com/max/60/1*UNkmAg1JVfprEfBBmqM44w.png?q=20\" width=\"1000\"/></div><img class=\"dk ka gg n o gf ab gd\" height=\"550\" width=\"1000\"/><noscript><img class=\"gg n o gf ab\" height=\"550\" src=\"https://miro.medium.com/max/2000/1*UNkmAg1JVfprEfBBmqM44w.png\" width=\"1000\"/></noscript></div></div><figcaption class=\"at ed ln lo hi di x y lp ao cs\"><strong class=\"ao lf\">Detailed Architecture (Left), Number of Parameters for Each Block (Top Right), Different Settings to Maintain Similar Complexity (Middle Right), Ablation Study for Different Settings Under Similar Complexity (Bottom Right)</strong></figcaption></figure></div><div class=\"ae fb ab di v w\"><ul class=\"\"><li class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku np mu mv\" id=\"6013\"><a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet-50</a> is a special case of ResNeXt-50 with <em class=\"lg\">C</em>=1, <em class=\"lg\">d</em>=64.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"eb48\">To have fair comparison, different ResNeXt with different <em class=\"lg\">C</em> and <em class=\"lg\">d</em> with similar complexity with <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet</a> are tried. This is shown at the middle right of the figure above.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"a697\">And it is found that ResNeXt-50 (32×4d) obtains 22.2% top-1 error for ImageNet-1K (1K means 1K classes) dataset while <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet-50</a> only obtains 23.9% top-1 error.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"6475\">And ResNeXt-101 (32×4d) obtains 21.2% top-1 error for ImageNet dataset while <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet-101</a> only obtains 22.0% top-1 error.</li></ul><h2 class=\"nb mc ea ap ao dy nc nd ne nf ng nh ni nj nk nl nm\" id=\"f880\">3.2. Importance of Cardinality</h2><figure class=\"ju jv jw jx jy ez nz x y paragraph-image\"><div class=\"kc l eo kd\"><div class=\"oa l\"><div class=\"dk ka gg n o gf ab by t kb\"><img class=\"gg n o gf ab kf kg\" height=\"344\" src=\"https://miro.medium.com/max/60/1*xxm28sWSo6i-If09kwAqPQ.png?q=20\" width=\"666\"/></div><img class=\"dk ka gg n o gf ab gd\" height=\"344\" width=\"666\"/><noscript><img class=\"gg n o gf ab\" height=\"344\" src=\"https://miro.medium.com/max/1332/1*xxm28sWSo6i-If09kwAqPQ.png\" width=\"666\"/></noscript></div></div><figcaption class=\"at ed ln lo hi di x y lp ao cs\"><strong class=\"ao lf\">Ablation Study for Different Settings of 2× Complexity Models</strong></figcaption></figure><ul class=\"\"><li class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku np mu mv\" id=\"014c\"><a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\"><strong class=\"kj lf\">ResNet-200</strong></a>: 21.7% top-1 and 5.8% top-5 error rates.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"4fd4\"><a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\"><strong class=\"kj lf\">ResNet-101, wider</strong></a>: only obtains 21.3% top-1 and 5.7% top-5 error rates, which means <strong class=\"kj lf\">only making it wider does not help much</strong>.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"4b39\"><strong class=\"kj lf\">ResNeXt-101 (2×64d)</strong>: By just making <strong class=\"kj lf\"><em class=\"lg\">C</em>=2</strong> (i.e. two convolution paths within the ResNeXt block), <strong class=\"kj lf\">an obvious improvement is already obtained</strong> with 20.7% top-1 and 5.5% top-5 error rates.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"0a63\"><strong class=\"kj lf\">ResNeXt-101 (64×4d)</strong>: By making <strong class=\"kj lf\"><em class=\"lg\">C</em>=64</strong> (i.e. two convolution paths within the ResNeXt block), <strong class=\"kj lf\">an even better improvement is already obtained</strong> with 20.4% top-1 and 5.3% top-5 error rates. This means <strong class=\"kj lf\">cardinality is essential to improve the classification accuracy</strong>.</li></ul><h2 class=\"nb mc ea ap ao dy nc nd ne nf ng nh ni nj nk nl nm\" id=\"57d7\">3.2. Importance of Residual Connections</h2><figure class=\"ju jv jw jx jy ez ob x y paragraph-image\"><div class=\"kc l eo kd\"><div class=\"oc l\"><div class=\"dk ka gg n o gf ab by t kb\"><img class=\"gg n o gf ab kf kg\" height=\"119\" src=\"https://miro.medium.com/max/60/1*EwrkhL-fqVii9MEjCHMZBA.png?q=20\" width=\"571\"/></div><img class=\"dk ka gg n o gf ab gd\" height=\"119\" width=\"571\"/><noscript><img class=\"gg n o gf ab\" height=\"119\" src=\"https://miro.medium.com/max/1142/1*EwrkhL-fqVii9MEjCHMZBA.png\" width=\"571\"/></noscript></div></div><figcaption class=\"at ed ln lo hi di x y lp ao cs\"><strong class=\"ao lf\">Importance of Residual Connections</strong></figcaption></figure><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"39d7\">Without residual connections, error rates are increased largely for both <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet-50</a> and ResNeXt-50. Residual connections are important.</p></div></section><hr class=\"lt cs gy lu lv hi lw lx ly lz ma\"/><section class=\"hy hz ia ib ic\"><div class=\"ae fb ab di v w\"><h1 class=\"mb mc ea ap ao dy md me mf mg mh mi mj mk ml mm mn\" id=\"558f\">4. Results</h1><h2 class=\"nb mc ea ap ao dy nc nd ne nf ng nh ni nj nk nl nm\" id=\"a94e\">4.1. ImageNet-1K</h2></div><div class=\"ae fb ab ac\"><figure class=\"ju jv jw jx jy ez od nt nu paragraph-image\"><div class=\"kc l eo kd\"><div class=\"oe l\"><div class=\"dk ka gg n o gf ab by t kb\"><img class=\"gg n o gf ab kf kg\" height=\"291\" src=\"https://miro.medium.com/max/60/1*oLRaAqY2cnw2E5eJ_D8jmA.png?q=20\" width=\"1000\"/></div><img class=\"dk ka gg n o gf ab gd\" height=\"291\" width=\"1000\"/><noscript><img class=\"gg n o gf ab\" height=\"291\" src=\"https://miro.medium.com/max/2000/1*oLRaAqY2cnw2E5eJ_D8jmA.png\" width=\"1000\"/></noscript></div></div><figcaption class=\"at ed ln lo hi di x y lp ao cs\"><strong class=\"ao lf\">Single Crop Testing: ResNet/ResNeXt is 224×224 and 320×320, Inception models: 299×299</strong></figcaption></figure></div><div class=\"ae fb ab di v w\"><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"d9ed\">ImageNet-1K is a subset of 22K-class ImageNet dataset, which contains 1000 classes. It is also the dataset for ILSVRC classification task.</p><ul class=\"\"><li class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku np mu mv\" id=\"752e\">With standard size image used for single crop testing, ResNeXt-101 obtains 20.4% top-1 and 5.3% top-5 error rates,</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"b264\">With larger size image used for single crop testing, ResNeXt-101 obtains 19.1% top-1 and 4.4% top-5 error rates, which has better results than all state-of-the-art approaches, <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet</a>, <a class=\"cy bt lh li lj lk\" href=\"/resnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e\">Pre-Activation ResNet</a>, <a class=\"cy bt lh li lj lk\" href=\"https://medium.com/@sh.tsang/review-inception-v3-1st-runner-up-image-classification-in-ilsvrc-2015-17915421f77c\">Inception-v3</a>, <a class=\"cy bt lh li lj lk\" href=\"/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc\">Inception-v4</a> and <a class=\"cy bt lh li lj lk\" href=\"/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc\">Inception-ResNet-v2</a>.</li></ul><h2 class=\"nb mc ea ap ao dy nc nd ne nf ng nh ni nj nk nl nm\" id=\"321e\">4.2. ImageNet-5K</h2></div><div class=\"ae fb ab ac\"><figure class=\"ju jv jw jx jy ez of nt nu paragraph-image\"><div class=\"kc l eo kd\"><div class=\"og l\"><div class=\"dk ka gg n o gf ab by t kb\"><img class=\"gg n o gf ab kf kg\" height=\"417\" src=\"https://miro.medium.com/max/60/1*V99JmLNTxlXxIPYAIFiGwA.png?q=20\" width=\"1000\"/></div><img class=\"dk ka gg n o gf ab gd\" height=\"417\" width=\"1000\"/><noscript><img class=\"gg n o gf ab\" height=\"417\" src=\"https://miro.medium.com/max/2000/1*V99JmLNTxlXxIPYAIFiGwA.png\" width=\"1000\"/></noscript></div></div><figcaption class=\"at ed ln lo hi di x y lp ao cs\"><strong class=\"ao lf\">ImageNet-5K Results (All trained from scratch)</strong></figcaption></figure></div><div class=\"ae fb ab di v w\"><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"97a7\">ImageNet-1K has been somehow saturated after so many years of development.</p><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"9240\">ImageNet-5K is a subset of 22K-class ImageNet dataset, which contains 5000 classes, which also contains ImageNet-1K classes.</p><ul class=\"\"><li class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku np mu mv\" id=\"b179\">6.8 million images, 5× of the ImageNet-1K dataset.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"73ed\">Since there is no official train/validation set, the original ImageNet-1K validation set is used for evaluation.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"c2cd\"><strong class=\"kj lf\">5K-way classification</strong> is the <strong class=\"kj lf\">softmax over 5K classes</strong>. Thus, there will be automatic errors when the network predicts the labels for the other 4K classes on the ImageNet-1K validation dataset.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"fccb\"><strong class=\"kj lf\">1K-way classification</strong> is just the <strong class=\"kj lf\">softmax over 1K classes</strong>.</li></ul><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"34a9\">ResNeXt of course got better results than <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet</a> as shown above.</p><h2 class=\"nb mc ea ap ao dy nc nd ne nf ng nh ni nj nk nl nm\" id=\"f627\">4.3. CIFAR-10 &amp; CIFAR-100</h2></div><div class=\"ae fb ab ac\"><figure class=\"ju jv jw jx jy ez oh nt nu paragraph-image\"><div class=\"kc l eo kd\"><div class=\"oi l\"><div class=\"dk ka gg n o gf ab by t kb\"><img class=\"gg n o gf ab kf kg\" height=\"435\" src=\"https://miro.medium.com/max/60/1*FqDUkl5vrLZufkQIG6xk7A.png?q=20\" width=\"1000\"/></div><img class=\"dk ka gg n o gf ab gd\" height=\"435\" width=\"1000\"/><noscript><img class=\"gg n o gf ab\" height=\"435\" src=\"https://miro.medium.com/max/2000/1*FqDUkl5vrLZufkQIG6xk7A.png\" width=\"1000\"/></noscript></div></div><figcaption class=\"at ed ln lo hi di x y lp ao cs\"><strong class=\"ao lf\">CIFAR-10 and CIFAR-100 Results</strong></figcaption></figure></div><div class=\"ae fb ab di v w\"><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"4572\">CIFAR-10 &amp; CIFAR-100, two very famous 10-class and 100-class datasets.</p><ul class=\"\"><li class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku np mu mv\" id=\"f629\">Left: Compared with <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet</a>, ResNeXt always obtains better results in CIFAR-10.</li><li class=\"kh ki ea ap kj b kk mw km mx ko my kq mz ks na ku np mu mv\" id=\"e839\">Right: Compared with <a class=\"cy bt lh li lj lk\" href=\"/review-wrns-wide-residual-networks-image-classification-d3feb3fb2004\">Wide ResNet (WRN)</a>, ResNeXt-29 (16×64d) obtains 3.58% and 17.31% errors for CIFAR-10 and CIFAR-100 respectively. These were the best results among all state-of-the-art approaches at that moment.</li></ul><h2 class=\"nb mc ea ap ao dy nc nd ne nf ng nh ni nj nk nl nm\" id=\"39c2\">4.4. MS COCO Object Detection</h2><figure class=\"ju jv jw jx jy ez oj x y paragraph-image\"><div class=\"kc l eo kd\"><div class=\"ok l\"><div class=\"dk ka gg n o gf ab by t kb\"><img class=\"gg n o gf ab kf kg\" height=\"190\" src=\"https://miro.medium.com/max/60/1*_QhPxRX7B0jROHwhIuljCQ.png?q=20\" width=\"592\"/></div><img class=\"dk ka gg n o gf ab gd\" height=\"190\" width=\"592\"/><noscript><img class=\"gg n o gf ab\" height=\"190\" src=\"https://miro.medium.com/max/1184/1*_QhPxRX7B0jROHwhIuljCQ.png\" width=\"592\"/></noscript></div></div><figcaption class=\"at ed ln lo hi di x y lp ao cs\"><strong class=\"ao lf\">MS COCO Objection Detection Results</strong></figcaption></figure><ul class=\"\"><li class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku np mu mv\" id=\"7c48\">By plugging <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet</a>/ResNeXt into <a class=\"cy bt lh li lj lk\" href=\"/review-faster-r-cnn-object-detection-f5685cb30202\">Faster R-CNN</a>, with similar model complexity, ResNeXt always outperforms <a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet</a> for both AP@0.5 (IoU&gt;0.5) and mean AP (average prediction) at all IoU levels.</li></ul></div></section><hr class=\"lt cs gy lu lv hi lw lx ly lz ma\"/><section class=\"hy hz ia ib ic\"><div class=\"ae fb ab di v w\"><p class=\"kh ki ea ap kj b kk kl km kn ko kp kq kr ks kt ku\" id=\"8543\">With the success of ResNeXt, it is also utilized by Mask R-CNN for instance segmentation. Hope I can cover Mask R-CNN later on as well.</p></div></section><hr class=\"lt cs gy lu lv hi lw lx ly lz ma\"/><section class=\"hy hz ia ib ic\"><div class=\"ae fb ab di v w\"><h2 class=\"nb mc ea ap ao dy nc nd ne nf ng nh ni nj nk nl nm\" id=\"2ad8\">Reference</h2><p class=\"kh ki ea ap kj b kk mo km mp ko mq kq mr ks ms ku\" id=\"0b9a\">[2017 CVPR] [ResNeXt]<br/><a class=\"cy bt lh li lj lk\" href=\"https://arxiv.org/abs/1611.05431\">Aggregated Residual Transformations for Deep Neural Networks</a></p><h2 class=\"nb mc ea ap ao dy nc nd ne nf ng nh ni nj nk nl nm\" id=\"b06a\">My Related Reviews on Image Classification</h2><p class=\"kh ki ea ap kj b kk mo km mp ko mq kq mr ks ms ku\" id=\"79f2\">[<a class=\"cy bt lh li lj lk\" href=\"https://medium.com/@sh.tsang/paper-brief-review-of-lenet-1-lenet-4-lenet-5-boosted-lenet-4-image-classification-1f5f809dbf17\">LeNet</a>] [<a class=\"cy bt lh li lj lk\" href=\"https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160\">AlexNet</a>] [<a class=\"cy bt lh li lj lk\" href=\"https://medium.com/coinmonks/paper-review-of-zfnet-the-winner-of-ilsvlc-2013-image-classification-d1a5a0c45103\">ZFNet</a>] [<a class=\"cy bt lh li lj lk\" href=\"https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11\">VGGNet</a>] [<a class=\"cy bt lh li lj lk\" href=\"https://medium.com/coinmonks/review-sppnet-1st-runner-up-object-detection-2nd-runner-up-image-classification-in-ilsvrc-906da3753679\">SPPNet</a>] [<a class=\"cy bt lh li lj lk\" href=\"https://medium.com/coinmonks/review-prelu-net-the-first-to-surpass-human-level-performance-in-ilsvrc-2015-image-f619dddd5617\">PReLU-Net</a>] [<a class=\"cy bt lh li lj lk\" href=\"https://medium.com/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7\">GoogLeNet / Inception-v1</a>] [<a class=\"cy bt lh li lj lk\" href=\"https://medium.com/@sh.tsang/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651\">BN-Inception / Inception-v2</a>] [<a class=\"cy bt lh li lj lk\" href=\"https://medium.com/@sh.tsang/review-inception-v3-1st-runner-up-image-classification-in-ilsvrc-2015-17915421f77c\">Inception-v3</a>] [<a class=\"cy bt lh li lj lk\" href=\"/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc\">Inception-v4</a>] [<a class=\"cy bt lh li lj lk\" href=\"/review-xception-with-depthwise-separable-convolution-better-than-inception-v3-image-dc967dd42568\">Xception</a>] [<a class=\"cy bt lh li lj lk\" href=\"/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69\">MobileNetV1</a>] [<a class=\"cy bt lh li lj lk\" href=\"/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\">ResNet</a>] [<a class=\"cy bt lh li lj lk\" href=\"/resnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e\">Pre-Activation ResNet</a>] [<a class=\"cy bt lh li lj lk\" href=\"https://medium.com/@sh.tsang/review-rir-resnet-in-resnet-image-classification-be4c79fde8ba\">RiR</a>] [<a class=\"cy bt lh li lj lk\" href=\"/review-ror-resnet-of-resnet-multilevel-resnet-image-classification-cd3b0fcc19bb\">RoR</a>] [<a class=\"cy bt lh li lj lk\" href=\"/review-stochastic-depth-image-classification-a4e225807f4a\">Stochastic Depth</a>] [<a class=\"cy bt lh li lj lk\" href=\"/review-wrns-wide-residual-networks-image-classification-d3feb3fb2004\">WRN</a>] [<a class=\"cy bt lh li lj lk\" href=\"/review-polynet-2nd-runner-up-in-ilsvrc-2016-image-classification-8a1a941ce9ea\">PolyNet</a>] [<a class=\"cy bt lh li lj lk\" href=\"/review-densenet-image-classification-b6631a8ef803\">DenseNet</a>]</p><h2 class=\"nb mc ea ap ao dy nc nd ne nf ng nh ni nj nk nl nm\" id=\"030e\">My Related Reviews on Object Detection</h2><p class=\"kh ki ea ap kj b kk mo km mp ko mq kq mr ks ms ku\" id=\"2a8b\">[<a class=\"cy bt lh li lj lk\" href=\"/review-faster-r-cnn-object-detection-f5685cb30202\">Faster R-CNN</a>]</p></div></section></div></article><div class=\"dk dl dm m dn do dp dq dr e\" data-test-id=\"post-sidebar\"><div class=\"af dj\"><div class=\"ds dt l\"><a class=\"cy cz ax ay az ba bb bc bd be du dv bh bi dw dx\" href=\"https://towardsdatascience.com/?source=post_sidebar--------------------------post_sidebar-\"><h2 class=\"ao dy dz aq ea\">Towards Data Science</h2></a><div class=\"eb ec l\"><h4 class=\"ao cs ed aq by ee ef eg eh ei at\">Sharing concepts, ideas, and codes.</h4></div><div class=\"bs\"><button class=\"ej bm av aw bn bf bg bo be bp ao b ap aq ar as bq br ae bs bt bh\">Follow</button></div></div><div class=\"ek el em af\"><div class=\"af ag\"><div class=\"en l eo\"><a class=\"cy cz ax ay az ba bb bc bd be du dv bh bi dw dx\" href=\"https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Freview-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac&amp;source=post_sidebar-----15d7f17b42ac---------------------clap_sidebar-\"><div class=\"bc ep eq er es et eu aw ev\"><svg height=\"29\" width=\"29\"><g fill-rule=\"evenodd\"><path d=\"M13.74 1l.76 2.97.76-2.97zM16.82 4.78l1.84-2.56-1.43-.47zM10.38 2.22l1.84 2.56-.41-3.03zM22.38 22.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M9.1 22.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L6.1 15.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L6.4 11.26l-1.18-1.18a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L11.96 14a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L8.43 9.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L20.63 15c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM13 6.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 23 23.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z\"></path></g></svg></div></a></div><div class=\"ew l\"><div class=\"ex\"><h4 class=\"ao cs ed aq at\"><button class=\"cy cz ax ay az ba bb bc bd be du dv bh bi dw dx\">192 </button></h4></div></div></div></div><div><div class=\"bs\"><a class=\"cy cz ax ay az ba bb bc bd be du dv bh bi dw dx\" href=\"https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Freview-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac&amp;source=post_sidebar--------------------------bookmark_sidebar-\"><svg height=\"25\" viewbox=\"0 0 25 25\" width=\"25\"><path d=\"M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z\" fill-rule=\"evenodd\"></path></svg></a></div></div></div></div><div><div class=\"ey ez af dj fa\"><div class=\"fb di fc ab\"><ul class=\"bc bd\"><li class=\"bs ce fd fe\"><a class=\"ff fg bt at l fh fi a b ct\" href=\"/tag/machine-learning\">Machine Learning</a></li><li class=\"bs ce fd fe\"><a class=\"ff fg bt at l fh fi a b ct\" href=\"/tag/deep-learning\">Deep Learning</a></li><li class=\"bs ce fd fe\"><a class=\"ff fg bt at l fh fi a b ct\" href=\"/tag/artificial-intelligence\">Artificial Intelligence</a></li><li class=\"bs ce fd fe\"><a class=\"ff fg bt at l fh fi a b ct\" href=\"/tag/data-science\">Data Science</a></li><li class=\"bs ce fd fe\"><a class=\"ff fg bt at l fh fi a b ct\" href=\"/tag/image-classification\">Image Classification</a></li></ul><div class=\"fj af fk u\"><div class=\"af ag\"><div class=\"fl l eo\"><a class=\"cy cz ax ay az ba bb bc bd be du dv bh bi dw dx\" href=\"https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Freview-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac&amp;source=post_actions_footer-----15d7f17b42ac---------------------clap_footer-\"><div class=\"c fm fn af ag fo eo fp fq bo fr fs ft fu fv fw fx fy fz ga gb\"><div class=\"bc ep eq er es et gc ag gd fn af aw fa ge o gf gg n ab ev\"><svg height=\"33\" viewbox=\"0 0 33 33\" width=\"33\"><path d=\"M28.86 17.34l-3.64-6.4c-.3-.43-.71-.73-1.16-.8a1.12 1.12 0 0 0-.9.21c-.62.5-.73 1.18-.32 2.06l1.22 2.6 1.4 2.45c2.23 4.09 1.51 8-2.15 11.66a9.6 9.6 0 0 1-.8.71 6.53 6.53 0 0 0 4.3-2.1c3.82-3.82 3.57-7.87 2.05-10.39zm-6.25 11.08c3.35-3.35 4-6.78 1.98-10.47L21.2 12c-.3-.43-.71-.72-1.16-.8a1.12 1.12 0 0 0-.9.22c-.62.49-.74 1.18-.32 2.06l1.72 3.63a.5.5 0 0 1-.81.57l-8.91-8.9a1.33 1.33 0 0 0-1.89 1.88l5.3 5.3a.5.5 0 0 1-.71.7l-5.3-5.3-1.49-1.49c-.5-.5-1.38-.5-1.88 0a1.34 1.34 0 0 0 0 1.89l1.49 1.5 5.3 5.28a.5.5 0 0 1-.36.86.5.5 0 0 1-.36-.15l-5.29-5.29a1.34 1.34 0 0 0-1.88 0 1.34 1.34 0 0 0 0 1.89l2.23 2.23L9.3 21.4a.5.5 0 0 1-.36.85.5.5 0 0 1-.35-.14l-3.32-3.33a1.33 1.33 0 0 0-1.89 0 1.32 1.32 0 0 0-.39.95c0 .35.14.69.4.94l6.39 6.4c3.53 3.53 8.86 5.3 12.82 1.35zM12.73 9.26l5.68 5.68-.49-1.04c-.52-1.1-.43-2.13.22-2.89l-3.3-3.3a1.34 1.34 0 0 0-1.88 0 1.33 1.33 0 0 0-.4.94c0 .22.07.42.17.61zm14.79 19.18a7.46 7.46 0 0 1-6.41 2.31 7.92 7.92 0 0 1-3.67.9c-3.05 0-6.12-1.63-8.36-3.88l-6.4-6.4A2.31 2.31 0 0 1 2 19.72a2.33 2.33 0 0 1 1.92-2.3l-.87-.87a2.34 2.34 0 0 1 0-3.3 2.33 2.33 0 0 1 1.24-.64l-.14-.14a2.34 2.34 0 0 1 0-3.3 2.39 2.39 0 0 1 3.3 0l.14.14a2.33 2.33 0 0 1 3.95-1.24l.09.09c.09-.42.29-.83.62-1.16a2.34 2.34 0 0 1 3.3 0l3.38 3.39a2.17 2.17 0 0 1 1.27-.17c.54.08 1.03.35 1.45.76.1-.55.41-1.03.9-1.42a2.12 2.12 0 0 1 1.67-.4 2.8 2.8 0 0 1 1.85 1.25l3.65 6.43c1.7 2.83 2.03 7.37-2.2 11.6zM13.22.48l-1.92.89 2.37 2.83-.45-3.72zm8.48.88L19.78.5l-.44 3.7 2.36-2.84zM16.5 3.3L15.48 0h2.04L16.5 3.3z\" fill-rule=\"evenodd\"></path></svg></div></div></a></div><div class=\"ew l\"><div class=\"ex\"><h4 class=\"ao cs ed aq ea\"><button class=\"cy cz ax ay az ba bb bc bd be du dv bh bi dw dx\">192 claps</button></h4></div></div></div><div class=\"af ag\"><div class=\"gh l am g\"><a class=\"cy cz ax ay az ba bb bc bd be du dv bh bi dw dx\" href=\"//medium.com/p/15d7f17b42ac/share/twitter?source=follow_footer--------------------------follow_footer-\"><svg class=\"al\" height=\"29\" width=\"29\"><path d=\"M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z\"></path></svg></a></div><div class=\"gh l am g\"><a class=\"cy cz ax ay az ba bb bc bd be du dv bh bi dw dx\" href=\"//medium.com/p/15d7f17b42ac/share/facebook?source=follow_footer--------------------------follow_footer-\"><svg class=\"al\" height=\"29\" width=\"29\"><path d=\"M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79\"></path></svg></a></div><div class=\"gh gi bw\"><div class=\"bs\"><button class=\"cy cz ax ay az ba bb bc bd be du dv bh bi dw dx\"><svg class=\"al\" height=\"25\" width=\"25\"><g fill-rule=\"evenodd\"><path d=\"M15.6 5a.42.42 0 0 0 .17-.3.42.42 0 0 0-.12-.33l-2.8-2.79a.5.5 0 0 0-.7 0l-2.8 2.8a.4.4 0 0 0-.1.32c0 .12.07.23.16.3h.02a.45.45 0 0 0 .57-.04l2-2V10c0 .28.23.5.5.5s.5-.22.5-.5V2.93l2.02 2.02c.08.07.18.12.3.13.11.01.21-.02.3-.08v.01\"></path><path d=\"M18 7h-1.5a.5.5 0 0 0 0 1h1.6c.5 0 .9.4.9.9v10.2c0 .5-.4.9-.9.9H6.9a.9.9 0 0 1-.9-.9V8.9c0-.5.4-.9.9-.9h1.6a.5.5 0 0 0 .35-.15A.5.5 0 0 0 9 7.5a.5.5 0 0 0-.15-.35A.5.5 0 0 0 8.5 7H7a2 2 0 0 0-2 2v10c0 1.1.9 2 2 2h11a2 2 0 0 0 2-2V9a2 2 0 0 0-2-2\"></path></g></svg></button></div></div><div class=\"gh l am\"><div><div class=\"bs\"><a class=\"cy cz ax ay az ba bb bc bd be du dv bh bi dw dx\" href=\"https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Freview-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac&amp;source=post_actions_footer--------------------------bookmark_sidebar-\"><svg height=\"25\" viewbox=\"0 0 25 25\" width=\"25\"><path d=\"M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z\" fill-rule=\"evenodd\"></path></svg></a></div></div></div><div class=\"bs\"><div class=\"bs\"><div class=\"l am\"><button class=\"cy cz ax ay az ba bb bc bd be du dv bh bi dw dx\"><svg class=\"al\" height=\"25\" viewbox=\"-480.5 272.5 21 21\" width=\"25\"><path d=\"M-463 284.6c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5z\"></path></svg></button></div></div></div></div></div><div class=\"gj gk gl gm l u\"><div class=\"gn go l eo\"><span class=\"l gp ai gq\"><div class=\"l gg gr gs\"><a href=\"/@sh.tsang?source=follow_footer--------------------------follow_footer-\"><img alt=\"Sik-Ho Tsang\" class=\"l fn cl gt\" height=\"80\" src=\"https://miro.medium.com/fit/c/160/160/1*OxjNUHcLFU8-pp-j8su6pg.jpeg\" width=\"80\"/></a></div><span class=\"l\"><div class=\"gu l gv\"><p class=\"ao cs ct aq at cw gw\">Written by</p></div><div class=\"gu gx af gv\"><div class=\"ab af ag fk\"><h2 class=\"ao dy gy gz ea\"><a class=\"cy cz ax ay az ba bb bc bd be du dv bh bi dw dx\" href=\"/@sh.tsang?source=follow_footer--------------------------follow_footer-\">Sik-Ho Tsang</a></h2><div class=\"l g\"><button class=\"ej bm av aw bn bf bg bo be bp ao b ap aq ar as bq br ae bs bt bh\">Follow</button></div></div></div></span></span><div class=\"gu ha l gv bw\"><div class=\"hb l\"><h4 class=\"ao cs dz hc at\">PhD, Researcher. I share what I've learnt and done. :) My LinkedIn: https://www.linkedin.com/in/sh-tsang/</h4></div><div class=\"gi hd bw\"><button class=\"ej bm av aw bn bf bg bo be bp ao b ap aq ar as bq br ae bs bt bh\">Follow</button></div></div></div><div class=\"gj l\"></div><div class=\"gn go l eo\"><span class=\"l gp ai gq\"><div class=\"l gg gr gs\"><a href=\"https://towardsdatascience.com/?source=follow_footer--------------------------follow_footer-\"><img alt=\"Towards Data Science\" class=\"bp gt cl\" height=\"80\" src=\"https://miro.medium.com/fit/c/160/160/1*F0LADxTtsKOgmPa-_7iUEQ.jpeg\" width=\"80\"/></a></div><span class=\"l\"><div class=\"gu gx af gv\"><div class=\"ab af ag fk\"><h2 class=\"ao dy gy gz ea\"><a class=\"cy cz ax ay az ba bb bc bd be du dv bh bi dw dx\" href=\"https://towardsdatascience.com/?source=follow_footer--------------------------follow_footer-\">Towards Data Science</a></h2><div class=\"l g\"><div class=\"bs\"><button class=\"ej bm av aw bn bf bg bo be bp ao b ap aq ar as bq br ae bs bt bh\">Follow</button></div></div></div></div></span></span><div class=\"gu he l gv bw\"><div class=\"hb l\"><h4 class=\"ao cs dz hc at\">Sharing concepts, ideas, and codes.</h4></div><div class=\"gi hd bw\"><div class=\"bs\"><button class=\"ej bm av aw bn bf bg bo be bp ao b ap aq ar as bq br ae bs bt bh\">Follow</button></div></div></div></div></div><div class=\"hf gk l u\"><a class=\"cy cz ax ay az ba bb bc bd be du dv bh bi dw dx\" href=\"https://medium.com/p/15d7f17b42ac/responses/show?source=follow_footer--------------------------follow_footer-\"><div class=\"hg hh ff l hi g\"><span class=\"av\">Write the first response</span></div></a></div></div><div class=\"hj l hk u\"><div class=\"fb hl hm hn ho hp hq hr hs ht hu ab\"></div></div></div></div><script>window.PARSELY = window.PARSELY || {autotrack: false}</script></div></div><script>window.__BUILD_ID__ = \"development\"</script><script>window.__GRAPHQL_URI__ = \"https://towardsdatascience.com/_/graphql\"</script><script>window.__PRELOADED_STATE__ = {\"config\":{\"nodeEnv\":\"production\",\"version\":\"master-20190801-221217-bcc23d8e97\",\"productName\":\"Medium\",\"publicUrl\":\"https:\\u002F\\u002Fcdn-client.medium.com\\u002Flite\",\"authDomain\":\"medium.com\",\"authGoogleClientId\":\"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com\",\"favicon\":\"production\",\"glyphUrl\":\"https:\\u002F\\u002Fglyph.medium.com\",\"iTunesAppId\":\"828256236\",\"branchKey\":\"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm\",\"lightStep\":{\"name\":\"lite-web\",\"host\":\"collector-medium.lightstep.com\",\"token\":\"ce5be895bef60919541332990ac9fef2\",\"appVersion\":\"master-20190801-221217-bcc23d8e97\"},\"algolia\":{\"appId\":\"MQ57UUUQZ2\",\"apiKeySearch\":\"394474ced050e3911ae2249ecc774921\",\"indexPrefix\":\"medium_\",\"host\":\"-dsn.algolia.net\"},\"recaptchaKey\":\"6LdAokEUAAAAAC7seICd4vtC8chDb3jIXDQulyUJ\",\"sentry\":{\"dsn\":\"https:\\u002F\\u002F589e367c28ca47b195ce200d1507d18b@sentry.io\\u002F1423575\",\"environment\":\"production\"},\"isAmp\":false,\"googleAnalyticsCode\":\"UA-24232453-2\",\"signInWallCustomDomainCollectionIds\":[\"3a8144eabfe3\",\"336d898217ee\",\"61061eb0c96b\",\"138adf9c44c\",\"819cc2aaeee0\"]},\"debug\":{\"requestId\":\"7d6b8300-b1c9-407e-942f-91694e288ada\",\"originalSpanCarrier\":{\"ot-tracer-spanid\":\"6bd4e49e0947b0eb\",\"ot-tracer-traceid\":\"6ef58dd866c0c4b3\",\"ot-tracer-sampled\":\"true\"}},\"session\":{\"user\":{\"id\":\"lo_1L3tnD46Guse\"},\"xsrf\":\"\"},\"stats\":{\"itemCount\":0,\"sending\":false,\"timeout\":null,\"backup\":{}},\"navigation\":{\"showBranchBanner\":null,\"hideGoogleOneTap\":false,\"currentLocation\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Freview-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac\",\"host\":\"towardsdatascience.com\",\"hostname\":\"towardsdatascience.com\",\"currentHash\":\"\"},\"client\":{\"isBot\":false,\"isDnt\":false,\"isEu\":false,\"isNativeMedium\":false,\"isCustomDomain\":true},\"multiVote\":{\"clapsPerPost\":{}},\"metadata\":{\"faviconImageId\":null}}</script><script>window.__APOLLO_STATE__ = {\"ROOT_QUERY.variantFlags.0\":{\"name\":\"allow_access\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.0.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.0.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.1\":{\"name\":\"allow_signup\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.1.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.1.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.2\":{\"name\":\"allow_test_auth\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.2.valueType\",\"typename\":\"VariantFlagString\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.2.valueType\":{\"__typename\":\"VariantFlagString\",\"value\":\"disallow\"},\"ROOT_QUERY.variantFlags.3\":{\"name\":\"signin_services\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.3.valueType\",\"typename\":\"VariantFlagString\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.3.valueType\":{\"__typename\":\"VariantFlagString\",\"value\":\"twitter,facebook,google,email,google-fastidv,google-one-tap\"},\"ROOT_QUERY.variantFlags.4\":{\"name\":\"signup_services\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.4.valueType\",\"typename\":\"VariantFlagString\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.4.valueType\":{\"__typename\":\"VariantFlagString\",\"value\":\"twitter,facebook,google,email,google-fastidv,google-one-tap\"},\"ROOT_QUERY.variantFlags.5\":{\"name\":\"google_sign_in_android\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.5.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.5.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.6\":{\"name\":\"browsable_stream_config_bucket\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.6.valueType\",\"typename\":\"VariantFlagString\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.6.valueType\":{\"__typename\":\"VariantFlagString\",\"value\":\"curated-topics\"},\"ROOT_QUERY.variantFlags.7\":{\"name\":\"enable_dedicated_series_tab_api_ios\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.7.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.7.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.8\":{\"name\":\"enable_post_import\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.8.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.8.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.9\":{\"name\":\"available_monthly_plan\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.9.valueType\",\"typename\":\"VariantFlagString\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.9.valueType\":{\"__typename\":\"VariantFlagString\",\"value\":\"60e220181034\"},\"ROOT_QUERY.variantFlags.10\":{\"name\":\"available_annual_plan\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.10.valueType\",\"typename\":\"VariantFlagString\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.10.valueType\":{\"__typename\":\"VariantFlagString\",\"value\":\"2c754bcc2995\"},\"ROOT_QUERY.variantFlags.11\":{\"name\":\"disable_ios_resume_reading_toast\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.11.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.11.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.12\":{\"name\":\"is_not_medium_subscriber\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.12.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.12.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.13\":{\"name\":\"glyph_font_set\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.13.valueType\",\"typename\":\"VariantFlagString\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.13.valueType\":{\"__typename\":\"VariantFlagString\",\"value\":\"m2\"},\"ROOT_QUERY.variantFlags.14\":{\"name\":\"enable_branding\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.14.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.14.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.15\":{\"name\":\"enable_branding_fonts\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.15.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.15.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.16\":{\"name\":\"enable_automated_mission_control_triggers\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.16.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.16.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.17\":{\"name\":\"enable_lite_profile\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.17.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.17.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.18\":{\"name\":\"enable_marketing_emails\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.18.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.18.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.19\":{\"name\":\"enable_parsely\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.19.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.19.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.20\":{\"name\":\"enable_branch_io\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.20.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.20.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.21\":{\"name\":\"enable_ios_post_stats\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.21.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.21.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.22\":{\"name\":\"enable_lite_topics\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.22.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.22.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.23\":{\"name\":\"enable_lite_stories\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.23.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.23.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.24\":{\"name\":\"redis_read_write_splitting\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.24.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.24.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.25\":{\"name\":\"enable_tipalti_onboarding\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.25.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.25.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.26\":{\"name\":\"enable_annual_renewal_reminder_email\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.26.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.26.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.27\":{\"name\":\"enable_janky_spam_rules\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.27.valueType\",\"typename\":\"VariantFlagString\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.27.valueType\":{\"__typename\":\"VariantFlagString\",\"value\":\"users,posts\"},\"ROOT_QUERY.variantFlags.28\":{\"name\":\"enable_new_collaborative_filtering_data\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.28.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.28.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.29\":{\"name\":\"enable_google_one_tap\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.29.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.29.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.30\":{\"name\":\"enable_email_sign_in_captcha\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.30.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.30.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.31\":{\"name\":\"enable_primary_topic_for_mobile\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.31.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.31.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.32\":{\"name\":\"enable_lite_post\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.32.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.32.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.33\":{\"name\":\"enable_logged_out_homepage_signup\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.33.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.33.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.34\":{\"name\":\"use_new_admin_topic_backend\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.34.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.34.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.35\":{\"name\":\"enable_quarantine_rules\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.35.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.35.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.36\":{\"name\":\"enable_patronus_on_kubernetes\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.36.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.36.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.37\":{\"name\":\"pub_sidebar\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.37.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.37.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.38\":{\"name\":\"disable_mobile_featured_chunk\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.38.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.38.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.39\":{\"name\":\"enable_embedding_based_diversification\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.39.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.39.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.40\":{\"name\":\"enable_lite_pub_header_menu\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.40.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.40.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.41\":{\"name\":\"enable_lite_claps\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.41.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.41.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.42\":{\"name\":\"enable_live_user_post_scoring\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.42.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.42.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.43\":{\"name\":\"enable_lite_post_highlights\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.43.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.43.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.44\":{\"name\":\"enable_lite_post_highlights_view_only\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.44.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.44.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.45\":{\"name\":\"enable_tick_landing_page\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.45.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.45.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.46\":{\"name\":\"enable_lite_post_cd\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.46.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.46.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.47\":{\"name\":\"enable_trumpland_landing_page\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.47.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.47.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.48\":{\"name\":\"enable_lite_email_sign_in_flow\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.48.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.48.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.49\":{\"name\":\"enable_daily_read_digest_promo\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.49.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.49.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.50\":{\"name\":\"enable_lite_paywall_alert\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.50.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.50.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.51\":{\"name\":\"enable_edit_alt_text\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.51.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.51.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.52\":{\"name\":\"enable_serve_recs_from_ml_rank_homepage\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.52.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.52.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.53\":{\"name\":\"enable_serve_recs_from_ml_rank_digest\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.53.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.53.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.54\":{\"name\":\"enable_serve_recs_from_ml_rank_app_highlights\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.54.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.54.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.55\":{\"name\":\"enable_lite_thanks_to\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.55.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.55.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.56\":{\"name\":\"enable_lite_google_captcha\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.56.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.56.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.57\":{\"name\":\"enable_lite_branch_io\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.57.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.57.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.58\":{\"name\":\"enable_lite_notifications\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.58.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.58.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.59\":{\"name\":\"enable_lite_audio_upsells\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.59.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.59.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.60\":{\"name\":\"enable_ticks_digest_promo\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.60.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.60.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.61\":{\"name\":\"enable_lite_verify_email_butter_bar\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.61.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.61.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.62\":{\"name\":\"remove_social_proof_on_digest\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.62.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.62.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.63\":{\"name\":\"enable_lite_unread_notification_count_mutation\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.63.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.63.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY.variantFlags.64\":{\"name\":\"rank_model\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.64.valueType\",\"typename\":\"VariantFlagString\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.64.valueType\":{\"__typename\":\"VariantFlagString\",\"value\":\"default\"},\"ROOT_QUERY.variantFlags.65\":{\"name\":\"disable_gosocial_followers_that_you_follow\",\"valueType\":{\"type\":\"id\",\"generated\":true,\"id\":\"$ROOT_QUERY.variantFlags.65.valueType\",\"typename\":\"VariantFlagBoolean\"},\"__typename\":\"VariantFlag\"},\"$ROOT_QUERY.variantFlags.65.valueType\":{\"__typename\":\"VariantFlagBoolean\",\"value\":true},\"ROOT_QUERY\":{\"variantFlags\":[{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.0\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.1\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.2\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.3\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.4\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.5\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.6\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.7\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.8\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.9\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.10\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.11\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.12\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.13\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.14\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.15\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.16\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.17\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.18\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.19\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.20\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.21\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.22\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.23\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.24\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.25\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.26\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.27\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.28\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.29\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.30\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.31\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.32\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.33\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.34\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.35\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.36\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.37\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.38\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.39\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.40\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.41\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.42\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.43\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.44\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.45\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.46\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.47\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.48\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.49\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.50\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.51\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.52\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.53\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.54\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.55\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.56\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.57\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.58\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.59\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.60\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.61\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.62\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.63\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.64\",\"typename\":\"VariantFlag\"},{\"type\":\"id\",\"generated\":true,\"id\":\"ROOT_QUERY.variantFlags.65\",\"typename\":\"VariantFlag\"}],\"viewer\":null,\"meterPost({\\\"postId\\\":\\\"15d7f17b42ac\\\",\\\"postMeteringOptions\\\":{}})\":{\"type\":\"id\",\"generated\":false,\"id\":\"MeteringInfo:singleton\",\"typename\":\"MeteringInfo\"},\"postResult({\\\"id\\\":\\\"15d7f17b42ac\\\"})\":{\"type\":\"id\",\"generated\":false,\"id\":\"Post:15d7f17b42ac\",\"typename\":\"Post\"}},\"MeteringInfo:singleton\":{\"__typename\":\"MeteringInfo\",\"postIds\":{\"type\":\"json\",\"json\":[]},\"maxUnlockCount\":3,\"unlocksRemaining\":3},\"Post:15d7f17b42ac\":{\"__typename\":\"Post\",\"visibility\":\"PUBLIC\",\"latestPublishedVersion\":\"45b93b66c1d8\",\"collection\":{\"type\":\"id\",\"generated\":false,\"id\":\"Collection:7f60cf5620c9\",\"typename\":\"Collection\"},\"id\":\"15d7f17b42ac\",\"creator\":{\"type\":\"id\",\"generated\":false,\"id\":\"User:aff72a0c1243\",\"typename\":\"User\"},\"isLocked\":false,\"lockedSource\":\"LOCKED_POST_SOURCE_NONE\",\"sequence\":null,\"mediumUrl\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Freview-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac\",\"canonicalUrl\":\"\",\"content({\\\"postMeteringOptions\\\":{}})\":{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}})\",\"typename\":\"PostContent\"},\"firstPublishedAt\":1544361510998,\"isPublished\":true,\"layerCake\":3,\"primaryTopic\":{\"type\":\"id\",\"generated\":false,\"id\":\"data-science\",\"typename\":\"Topic\"},\"title\":\"Review: ResNeXt — 1st Runner Up in ILSVRC 2016 (Image Classification)\",\"highlights\":[{\"type\":\"id\",\"generated\":false,\"id\":\"Quote:anon_65ad4bde583f\",\"typename\":\"Quote\"}],\"readingTime\":6.437735849056605,\"statusForCollection\":\"APPROVED\",\"allowResponses\":true,\"tags\":[{\"type\":\"id\",\"generated\":false,\"id\":\"Tag:machine-learning\",\"typename\":\"Tag\"},{\"type\":\"id\",\"generated\":false,\"id\":\"Tag:deep-learning\",\"typename\":\"Tag\"},{\"type\":\"id\",\"generated\":false,\"id\":\"Tag:artificial-intelligence\",\"typename\":\"Tag\"},{\"type\":\"id\",\"generated\":false,\"id\":\"Tag:data-science\",\"typename\":\"Tag\"},{\"type\":\"id\",\"generated\":false,\"id\":\"Tag:image-classification\",\"typename\":\"Tag\"}],\"viewerClapCount\":null,\"readingList\":\"READING_LIST_NONE\",\"clapCount\":192,\"voterCount\":69,\"recommenders\":[],\"pendingCollection\":null,\"responsesCount\":0,\"collaborators\":[],\"inResponseToPostResult\":null,\"inResponseToMediaResource\":null,\"curationEligibleAt\":0,\"audioVersionUrl\":null,\"socialTitle\":\"\",\"socialDek\":\"\",\"metaDescription\":\"\",\"latestPublishedAt\":1553097642080,\"previewContent\":{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.previewContent\",\"typename\":\"PreviewContent\"},\"previewImage\":{\"type\":\"id\",\"generated\":false,\"id\":\"ImageMetadata:0*MvA7JX1Z1rAG5byo\",\"typename\":\"ImageMetadata\"},\"updatedAt\":1553097642080,\"topics\":[{\"type\":\"id\",\"generated\":false,\"id\":\"machine-learning\",\"typename\":\"Topic\"},{\"type\":\"id\",\"generated\":false,\"id\":\"data-science\",\"typename\":\"Topic\"}],\"isSuspended\":false},\"Collection:7f60cf5620c9\":{\"id\":\"7f60cf5620c9\",\"domain\":\"towardsdatascience.com\",\"slug\":\"towards-data-science\",\"__typename\":\"Collection\",\"googleAnalyticsId\":\"UA-19707169-24\",\"colorBehavior\":\"ACCENT_COLOR_AND_FILL_BACKGROUND\",\"name\":\"Towards Data Science\",\"logo\":{\"type\":\"id\",\"generated\":false,\"id\":\"ImageMetadata:1*5EUO1kUYBthpOCPzRj_l2g.png\",\"typename\":\"ImageMetadata\"},\"creator\":{\"type\":\"id\",\"generated\":false,\"id\":\"User:895063a310f4\",\"typename\":\"User\"},\"viewerCanManage\":false,\"avatar\":{\"type\":\"id\",\"generated\":false,\"id\":\"ImageMetadata:1*F0LADxTtsKOgmPa-_7iUEQ.jpeg\",\"typename\":\"ImageMetadata\"},\"isEnrolledInHightower\":false,\"navItems\":[{\"type\":\"id\",\"generated\":true,\"id\":\"Collection:7f60cf5620c9.navItems.0\",\"typename\":\"NavItem\"},{\"type\":\"id\",\"generated\":true,\"id\":\"Collection:7f60cf5620c9.navItems.1\",\"typename\":\"NavItem\"},{\"type\":\"id\",\"generated\":true,\"id\":\"Collection:7f60cf5620c9.navItems.2\",\"typename\":\"NavItem\"},{\"type\":\"id\",\"generated\":true,\"id\":\"Collection:7f60cf5620c9.navItems.3\",\"typename\":\"NavItem\"},{\"type\":\"id\",\"generated\":true,\"id\":\"Collection:7f60cf5620c9.navItems.4\",\"typename\":\"NavItem\"},{\"type\":\"id\",\"generated\":true,\"id\":\"Collection:7f60cf5620c9.navItems.5\",\"typename\":\"NavItem\"},{\"type\":\"id\",\"generated\":true,\"id\":\"Collection:7f60cf5620c9.navItems.6\",\"typename\":\"NavItem\"},{\"type\":\"id\",\"generated\":true,\"id\":\"Collection:7f60cf5620c9.navItems.7\",\"typename\":\"NavItem\"}],\"colorPalette\":{\"type\":\"id\",\"generated\":true,\"id\":\"$Collection:7f60cf5620c9.colorPalette\",\"typename\":\"ColorPalette\"},\"viewerCanEditOwnPosts\":false,\"viewerCanEditPosts\":false,\"description\":\"Sharing concepts, ideas, and codes.\",\"viewerIsFollowing\":false,\"viewerIsSubscribedToLetters\":false,\"mediumNewsletterId\":\"\",\"isUserSubscribedToMediumNewsletter\":false,\"ampEnabled\":false,\"twitterUsername\":\"TDataScience\",\"facebookPageId\":null,\"favicon\":{\"type\":\"id\",\"generated\":false,\"id\":\"ImageMetadata:1*F0LADxTtsKOgmPa-_7iUEQ.jpeg\",\"typename\":\"ImageMetadata\"}},\"User:aff72a0c1243\":{\"id\":\"aff72a0c1243\",\"__typename\":\"User\",\"isSuspended\":false,\"allowNotes\":true,\"name\":\"Sik-Ho Tsang\",\"isFollowing\":false,\"username\":\"sh.tsang\",\"bio\":\"PhD, Researcher. I share what I've learnt and done. :) My LinkedIn: https:\\u002F\\u002Fwww.linkedin.com\\u002Fin\\u002Fsh-tsang\\u002F\",\"imageId\":\"1*OxjNUHcLFU8-pp-j8su6pg.jpeg\",\"mediumMemberAt\":0,\"isBlocking\":false,\"isPartnerProgramEnrolled\":false,\"twitterScreenName\":\"\"},\"ImageMetadata:1*5EUO1kUYBthpOCPzRj_l2g.png\":{\"id\":\"1*5EUO1kUYBthpOCPzRj_l2g.png\",\"originalWidth\":1010,\"originalHeight\":376,\"__typename\":\"ImageMetadata\"},\"User:895063a310f4\":{\"id\":\"895063a310f4\",\"__typename\":\"User\"},\"ImageMetadata:1*F0LADxTtsKOgmPa-_7iUEQ.jpeg\":{\"id\":\"1*F0LADxTtsKOgmPa-_7iUEQ.jpeg\",\"__typename\":\"ImageMetadata\"},\"Collection:7f60cf5620c9.navItems.0\":{\"title\":\"Data Science\",\"url\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Fdata-science\\u002Fhome\",\"type\":\"TOPIC_PAGE\",\"__typename\":\"NavItem\"},\"Collection:7f60cf5620c9.navItems.1\":{\"title\":\"Machine Learning\",\"url\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Fmachine-learning\\u002Fhome\",\"type\":\"TOPIC_PAGE\",\"__typename\":\"NavItem\"},\"Collection:7f60cf5620c9.navItems.2\":{\"title\":\"Programming\",\"url\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Fprogramming\\u002Fhome\",\"type\":\"TOPIC_PAGE\",\"__typename\":\"NavItem\"},\"Collection:7f60cf5620c9.navItems.3\":{\"title\":\"Visualization\",\"url\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Fdata-visualization\\u002Fhome\",\"type\":\"TOPIC_PAGE\",\"__typename\":\"NavItem\"},\"Collection:7f60cf5620c9.navItems.4\":{\"title\":\"AI\",\"url\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Fartificial-intelligence\\u002Fhome\",\"type\":\"TOPIC_PAGE\",\"__typename\":\"NavItem\"},\"Collection:7f60cf5620c9.navItems.5\":{\"title\":\"Journalism\",\"url\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Fdata-journalism\\u002Fhome\",\"type\":\"TOPIC_PAGE\",\"__typename\":\"NavItem\"},\"Collection:7f60cf5620c9.navItems.6\":{\"title\":\"Events\",\"url\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Ftoronto-machine-learning-summit-8bae371d4bb1\",\"type\":\"POST_NAV_ITEM\",\"__typename\":\"NavItem\"},\"Collection:7f60cf5620c9.navItems.7\":{\"title\":\"Submit\",\"url\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Fcontribute\\u002Fhome\",\"type\":\"EXTERNAL_LINK_NAV_ITEM\",\"__typename\":\"NavItem\"},\"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum\":{\"backgroundColor\":\"#FF355876\",\"colorPoints\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.0\",\"typename\":\"ColorPoint\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.1\",\"typename\":\"ColorPoint\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.2\",\"typename\":\"ColorPoint\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.3\",\"typename\":\"ColorPoint\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.4\",\"typename\":\"ColorPoint\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.5\",\"typename\":\"ColorPoint\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.6\",\"typename\":\"ColorPoint\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.7\",\"typename\":\"ColorPoint\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.8\",\"typename\":\"ColorPoint\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.9\",\"typename\":\"ColorPoint\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.10\",\"typename\":\"ColorPoint\"}],\"__typename\":\"ColorSpectrum\"},\"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.0\":{\"color\":\"#FF355876\",\"point\":0,\"__typename\":\"ColorPoint\"},\"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.1\":{\"color\":\"#FF4D6C88\",\"point\":0.1,\"__typename\":\"ColorPoint\"},\"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.2\":{\"color\":\"#FF637F99\",\"point\":0.2,\"__typename\":\"ColorPoint\"},\"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.3\":{\"color\":\"#FF7791A8\",\"point\":0.3,\"__typename\":\"ColorPoint\"},\"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.4\":{\"color\":\"#FF8CA2B7\",\"point\":0.4,\"__typename\":\"ColorPoint\"},\"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.5\":{\"color\":\"#FF9FB3C6\",\"point\":0.5,\"__typename\":\"ColorPoint\"},\"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.6\":{\"color\":\"#FFB2C3D4\",\"point\":0.6,\"__typename\":\"ColorPoint\"},\"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.7\":{\"color\":\"#FFC5D2E1\",\"point\":0.7,\"__typename\":\"ColorPoint\"},\"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.8\":{\"color\":\"#FFD7E2EE\",\"point\":0.8,\"__typename\":\"ColorPoint\"},\"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.9\":{\"color\":\"#FFE9F1FA\",\"point\":0.9,\"__typename\":\"ColorPoint\"},\"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.10\":{\"color\":\"#FFFBFFFF\",\"point\":1,\"__typename\":\"ColorPoint\"},\"$Collection:7f60cf5620c9.colorPalette\":{\"tintBackgroundSpectrum\":{\"type\":\"id\",\"generated\":true,\"id\":\"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum\",\"typename\":\"ColorSpectrum\"},\"__typename\":\"ColorPalette\",\"defaultBackgroundSpectrum\":{\"type\":\"id\",\"generated\":true,\"id\":\"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum\",\"typename\":\"ColorSpectrum\"},\"highlightSpectrum\":{\"type\":\"id\",\"generated\":true,\"id\":\"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum\",\"typename\":\"ColorSpectrum\"}},\"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum\":{\"backgroundColor\":\"#FFFFFFFF\",\"colorPoints\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.0\",\"typename\":\"ColorPoint\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.1\",\"typename\":\"ColorPoint\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.2\",\"typename\":\"ColorPoint\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.3\",\"typename\":\"ColorPoint\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.4\",\"typename\":\"ColorPoint\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.5\",\"typename\":\"ColorPoint\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.6\",\"typename\":\"ColorPoint\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.7\",\"typename\":\"ColorPoint\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.8\",\"typename\":\"ColorPoint\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.9\",\"typename\":\"ColorPoint\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.10\",\"typename\":\"ColorPoint\"}],\"__typename\":\"ColorSpectrum\"},\"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.0\":{\"color\":\"#FF668AAA\",\"point\":0,\"__typename\":\"ColorPoint\"},\"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.1\":{\"color\":\"#FF61809D\",\"point\":0.1,\"__typename\":\"ColorPoint\"},\"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.2\":{\"color\":\"#FF5A7690\",\"point\":0.2,\"__typename\":\"ColorPoint\"},\"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.3\":{\"color\":\"#FF546C83\",\"point\":0.3,\"__typename\":\"ColorPoint\"},\"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.4\":{\"color\":\"#FF4D6275\",\"point\":0.4,\"__typename\":\"ColorPoint\"},\"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.5\":{\"color\":\"#FF455768\",\"point\":0.5,\"__typename\":\"ColorPoint\"},\"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.6\":{\"color\":\"#FF3D4C5A\",\"point\":0.6,\"__typename\":\"ColorPoint\"},\"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.7\":{\"color\":\"#FF34414C\",\"point\":0.7,\"__typename\":\"ColorPoint\"},\"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.8\":{\"color\":\"#FF2B353E\",\"point\":0.8,\"__typename\":\"ColorPoint\"},\"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.9\":{\"color\":\"#FF21282F\",\"point\":0.9,\"__typename\":\"ColorPoint\"},\"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.10\":{\"color\":\"#FF161B1F\",\"point\":1,\"__typename\":\"ColorPoint\"},\"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum\":{\"backgroundColor\":\"#FFFFFFFF\",\"colorPoints\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.0\",\"typename\":\"ColorPoint\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.1\",\"typename\":\"ColorPoint\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.2\",\"typename\":\"ColorPoint\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.3\",\"typename\":\"ColorPoint\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.4\",\"typename\":\"ColorPoint\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.5\",\"typename\":\"ColorPoint\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.6\",\"typename\":\"ColorPoint\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.7\",\"typename\":\"ColorPoint\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.8\",\"typename\":\"ColorPoint\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.9\",\"typename\":\"ColorPoint\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.10\",\"typename\":\"ColorPoint\"}],\"__typename\":\"ColorSpectrum\"},\"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.0\":{\"color\":\"#FFEDF4FC\",\"point\":0,\"__typename\":\"ColorPoint\"},\"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.1\":{\"color\":\"#FFE9F2FD\",\"point\":0.1,\"__typename\":\"ColorPoint\"},\"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.2\":{\"color\":\"#FFE6F1FD\",\"point\":0.2,\"__typename\":\"ColorPoint\"},\"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.3\":{\"color\":\"#FFE2EFFD\",\"point\":0.3,\"__typename\":\"ColorPoint\"},\"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.4\":{\"color\":\"#FFDFEEFD\",\"point\":0.4,\"__typename\":\"ColorPoint\"},\"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.5\":{\"color\":\"#FFDBECFE\",\"point\":0.5,\"__typename\":\"ColorPoint\"},\"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.6\":{\"color\":\"#FFD7EBFE\",\"point\":0.6,\"__typename\":\"ColorPoint\"},\"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.7\":{\"color\":\"#FFD4E9FE\",\"point\":0.7,\"__typename\":\"ColorPoint\"},\"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.8\":{\"color\":\"#FFD0E7FF\",\"point\":0.8,\"__typename\":\"ColorPoint\"},\"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.9\":{\"color\":\"#FFCCE6FF\",\"point\":0.9,\"__typename\":\"ColorPoint\"},\"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.10\":{\"color\":\"#FFC8E4FF\",\"point\":1,\"__typename\":\"ColorPoint\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}})\":{\"isLockedPreviewOnly\":false,\"__typename\":\"PostContent\",\"bodyModel\":{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel\",\"typename\":\"RichText\"}},\"data-science\":{\"name\":\"Data Science\",\"slug\":\"data-science\",\"__typename\":\"Topic\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.sections.0\":{\"name\":\"7076\",\"startIndex\":0,\"textLayout\":null,\"imageLayout\":null,\"backgroundImage\":null,\"videoLayout\":null,\"backgroundVideo\":null,\"__typename\":\"Section\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.sections.1\":{\"name\":\"9cdf\",\"startIndex\":8,\"textLayout\":null,\"imageLayout\":null,\"backgroundImage\":null,\"videoLayout\":null,\"backgroundVideo\":null,\"__typename\":\"Section\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.sections.2\":{\"name\":\"14b3\",\"startIndex\":13,\"textLayout\":null,\"imageLayout\":null,\"backgroundImage\":null,\"videoLayout\":null,\"backgroundVideo\":null,\"__typename\":\"Section\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.sections.3\":{\"name\":\"0aae\",\"startIndex\":24,\"textLayout\":null,\"imageLayout\":null,\"backgroundImage\":null,\"videoLayout\":null,\"backgroundVideo\":null,\"__typename\":\"Section\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.sections.4\":{\"name\":\"2e16\",\"startIndex\":44,\"textLayout\":null,\"imageLayout\":null,\"backgroundImage\":null,\"videoLayout\":null,\"backgroundVideo\":null,\"__typename\":\"Section\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.sections.5\":{\"name\":\"98c7\",\"startIndex\":60,\"textLayout\":null,\"imageLayout\":null,\"backgroundImage\":null,\"videoLayout\":null,\"backgroundVideo\":null,\"__typename\":\"Section\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.sections.6\":{\"name\":\"69fb\",\"startIndex\":83,\"textLayout\":null,\"imageLayout\":null,\"backgroundImage\":null,\"videoLayout\":null,\"backgroundVideo\":null,\"__typename\":\"Section\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.sections.7\":{\"name\":\"57b1\",\"startIndex\":84,\"textLayout\":null,\"imageLayout\":null,\"backgroundImage\":null,\"videoLayout\":null,\"backgroundVideo\":null,\"__typename\":\"Section\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel\":{\"sections\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.sections.0\",\"typename\":\"Section\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.sections.1\",\"typename\":\"Section\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.sections.2\",\"typename\":\"Section\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.sections.3\",\"typename\":\"Section\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.sections.4\",\"typename\":\"Section\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.sections.5\",\"typename\":\"Section\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.sections.6\",\"typename\":\"Section\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.sections.7\",\"typename\":\"Section\"}],\"paragraphs\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.0\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.1\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.2\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.3\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.4\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.5\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.6\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.7\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.8\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.9\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.10\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.11\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.12\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.13\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.14\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.15\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.16\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.17\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.18\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.19\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.20\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.21\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.22\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.23\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.24\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.25\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.26\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.27\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.28\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.29\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.30\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.31\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.32\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.33\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.34\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.35\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.36\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.37\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.38\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.39\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.40\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.41\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.42\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.43\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.44\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.45\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.46\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.47\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.48\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.49\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.50\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.51\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.52\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.53\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.54\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.55\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.56\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.57\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.58\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.59\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.60\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.61\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.62\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.63\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.64\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.65\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.66\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.67\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.68\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.69\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.70\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.71\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.72\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.73\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.74\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.75\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.76\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.77\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.78\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.79\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.80\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.81\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.82\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.83\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.84\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.85\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.86\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.87\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.88\",\"typename\":\"Paragraph\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.89\",\"typename\":\"Paragraph\"}],\"__typename\":\"RichText\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.0\":{\"name\":\"1ef0\",\"__typename\":\"Paragraph\",\"type\":\"H3\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Review: ResNeXt — 1st Runner Up in ILSVRC 2016 (Image Classification)\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.1\":{\"name\":\"4c9a\",\"__typename\":\"Paragraph\",\"type\":\"H4\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Network-in-Neuron, A New Dimensionality: Cardinality\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.1.markups.0\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.1.markups.0\":{\"type\":\"STRONG\",\"start\":0,\"end\":52,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.2\":{\"name\":\"fffc\",\"__typename\":\"Paragraph\",\"type\":\"IMG\",\"href\":null,\"layout\":\"INSET_CENTER\",\"metadata\":{\"type\":\"id\",\"generated\":false,\"id\":\"ImageMetadata:0*MvA7JX1Z1rAG5byo\",\"typename\":\"ImageMetadata\"},\"text\":\"\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"iframe\":null,\"mixtapeMetadata\":null},\"ImageMetadata:0*MvA7JX1Z1rAG5byo\":{\"id\":\"0*MvA7JX1Z1rAG5byo\",\"originalHeight\":667,\"originalWidth\":1000,\"focusPercentX\":null,\"focusPercentY\":null,\"alt\":null,\"__typename\":\"ImageMetadata\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.3\":{\"name\":\"ecb1\",\"__typename\":\"Paragraph\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"In this story, ResNeXt, by UC San Diego and Facebook AI Research (FAIR), is reviewed. The model name, ResNeXt, contains Next. It means the next dimension, on top of the ResNet. This next dimension is called the “cardinality” dimension. And ResNeXt becomes the 1st Runner Up of ILSVRC classification task.\",\"hasDropCap\":true,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.3.markups.0\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.3.markups.1\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.3.markups.2\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.3.markups.3\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.3.markups.4\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.3.markups.5\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.3.markups.6\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.3.markups.7\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.3.markups.8\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.3.markups.0\":{\"type\":\"A\",\"start\":169,\"end\":175,\"href\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Freview-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.3.markups.1\":{\"type\":\"STRONG\",\"start\":15,\"end\":24,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.3.markups.2\":{\"type\":\"STRONG\",\"start\":27,\"end\":40,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.3.markups.3\":{\"type\":\"STRONG\",\"start\":44,\"end\":73,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.3.markups.4\":{\"type\":\"STRONG\",\"start\":212,\"end\":223,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.3.markups.5\":{\"type\":\"STRONG\",\"start\":225,\"end\":234,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.3.markups.6\":{\"type\":\"STRONG\",\"start\":260,\"end\":303,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.3.markups.7\":{\"type\":\"EM\",\"start\":139,\"end\":144,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.3.markups.8\":{\"type\":\"EM\",\"start\":212,\"end\":223,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.4\":{\"name\":\"d931\",\"__typename\":\"Paragraph\",\"type\":\"IMG\",\"href\":null,\"layout\":\"INSET_CENTER\",\"metadata\":{\"type\":\"id\",\"generated\":false,\"id\":\"ImageMetadata:1*RHpn70qFNCcqyVjkPdFtGA.png\",\"typename\":\"ImageMetadata\"},\"text\":\"ILSVRC 2016 Classification Ranking http:\\u002F\\u002Fimage-net.org\\u002Fchallenges\\u002FLSVRC\\u002F2016\\u002Fresults#loc\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.4.markups.0\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.4.markups.1\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"ImageMetadata:1*RHpn70qFNCcqyVjkPdFtGA.png\":{\"id\":\"1*RHpn70qFNCcqyVjkPdFtGA.png\",\"originalHeight\":439,\"originalWidth\":730,\"focusPercentX\":null,\"focusPercentY\":null,\"alt\":null,\"__typename\":\"ImageMetadata\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.4.markups.0\":{\"type\":\"A\",\"start\":35,\"end\":89,\"href\":\"http:\\u002F\\u002Fimage-net.org\\u002Fchallenges\\u002FLSVRC\\u002F2016\\u002Fresults#loc\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.4.markups.1\":{\"type\":\"STRONG\",\"start\":0,\"end\":35,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.5\":{\"name\":\"1fc3\",\"__typename\":\"Paragraph\",\"type\":\"IMG\",\"href\":null,\"layout\":\"INSET_CENTER\",\"metadata\":{\"type\":\"id\",\"generated\":false,\"id\":\"ImageMetadata:1*LOoc11tkDoqv0pC6OH7mwA.png\",\"typename\":\"ImageMetadata\"},\"text\":\"Residual Block in ResNet (Left), A Block of ResNeXt with Cardinality = 32 (Right)\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.5.markups.0\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"ImageMetadata:1*LOoc11tkDoqv0pC6OH7mwA.png\":{\"id\":\"1*LOoc11tkDoqv0pC6OH7mwA.png\",\"originalHeight\":322,\"originalWidth\":734,\"focusPercentX\":null,\"focusPercentY\":null,\"alt\":null,\"__typename\":\"ImageMetadata\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.5.markups.0\":{\"type\":\"STRONG\",\"start\":0,\"end\":81,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.6\":{\"name\":\"f37a\",\"__typename\":\"Paragraph\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Compared with ResNet (The winner in ILSVRC 2015, 3.57%) and PolyNet (2nd Runner Up, 3.04%, Team name CU-DeepLink), ResNeXt got 3.03% Top-5 error rate, which is a large relative improvement of about 15%!!\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.6.markups.0\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.6.markups.1\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.6.markups.0\":{\"type\":\"A\",\"start\":14,\"end\":20,\"href\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Freview-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.6.markups.1\":{\"type\":\"A\",\"start\":60,\"end\":67,\"href\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Freview-polynet-2nd-runner-up-in-ilsvrc-2016-image-classification-8a1a941ce9ea\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.7\":{\"name\":\"a174\",\"__typename\":\"Paragraph\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"And it is published in 2017 CVPR, which has already got over 500 citations while I was writing this story. (Sik-Ho Tsang @ Medium)\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.7.markups.0\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.7.markups.1\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.7.markups.2\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.7.markups.0\":{\"type\":\"A\",\"start\":108,\"end\":120,\"href\":null,\"anchorType\":\"USER\",\"userId\":\"aff72a0c1243\",\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.7.markups.1\":{\"type\":\"STRONG\",\"start\":23,\"end\":32,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.7.markups.2\":{\"type\":\"STRONG\",\"start\":61,\"end\":74,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.8\":{\"name\":\"f006\",\"__typename\":\"Paragraph\",\"type\":\"H3\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"What Are Covered\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.9\":{\"name\":\"1929\",\"__typename\":\"Paragraph\",\"type\":\"OLI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Aggregated Transformation\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.9.markups.0\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.9.markups.0\":{\"type\":\"STRONG\",\"start\":0,\"end\":25,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.10\":{\"name\":\"801d\",\"__typename\":\"Paragraph\",\"type\":\"OLI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Relationship with Inception-ResNet, and Grouped Convolution in AlexNet\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.10.markups.0\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.10.markups.1\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.10.markups.2\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.10.markups.0\":{\"type\":\"A\",\"start\":18,\"end\":34,\"href\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Freview-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.10.markups.1\":{\"type\":\"A\",\"start\":63,\"end\":70,\"href\":\"https:\\u002F\\u002Fmedium.com\\u002Fcoinmonks\\u002Fpaper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.10.markups.2\":{\"type\":\"STRONG\",\"start\":0,\"end\":70,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.11\":{\"name\":\"48b6\",\"__typename\":\"Paragraph\",\"type\":\"OLI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Full Architecture and Ablation Study\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.11.markups.0\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.11.markups.0\":{\"type\":\"STRONG\",\"start\":0,\"end\":36,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.12\":{\"name\":\"0375\",\"__typename\":\"Paragraph\",\"type\":\"OLI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Results\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.12.markups.0\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.12.markups.0\":{\"type\":\"STRONG\",\"start\":0,\"end\":7,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.13\":{\"name\":\"ad50\",\"__typename\":\"Paragraph\",\"type\":\"H3\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"1. Aggregated Transformation\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.14\":{\"name\":\"f984\",\"__typename\":\"Paragraph\",\"type\":\"H4\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"1.1. Revisiting Simple Neuron\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.15\":{\"name\":\"8008\",\"__typename\":\"Paragraph\",\"type\":\"IMG\",\"href\":null,\"layout\":\"INSET_CENTER\",\"metadata\":{\"type\":\"id\",\"generated\":false,\"id\":\"ImageMetadata:1*lm62iiybACCoz4KR9w5Azg.png\",\"typename\":\"ImageMetadata\"},\"text\":\"A Simple Neuron (Left), and the corresponding equation (Right)\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.15.markups.0\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"ImageMetadata:1*lm62iiybACCoz4KR9w5Azg.png\":{\"id\":\"1*lm62iiybACCoz4KR9w5Azg.png\",\"originalHeight\":206,\"originalWidth\":651,\"focusPercentX\":null,\"focusPercentY\":null,\"alt\":null,\"__typename\":\"ImageMetadata\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.15.markups.0\":{\"type\":\"STRONG\",\"start\":0,\"end\":62,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.16\":{\"name\":\"0a28\",\"__typename\":\"Paragraph\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"As we should know, a simple neuron as above, the output is the summation of wi times xi. The above operation can be recast as a combination of splitting, transforming, and aggregating.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.16.markups.0\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.16.markups.1\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.16.markups.0\":{\"type\":\"STRONG\",\"start\":19,\"end\":34,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.16.markups.1\":{\"type\":\"STRONG\",\"start\":126,\"end\":184,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.17\":{\"name\":\"beaf\",\"__typename\":\"Paragraph\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Splitting: the vector x is sliced as a low-dimensional embedding, and in the above, it is a single-dimension subspace xi.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.17.markups.0\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.17.markups.0\":{\"type\":\"STRONG\",\"start\":0,\"end\":9,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.18\":{\"name\":\"28ea\",\"__typename\":\"Paragraph\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Transforming: the low-dimensional representation is transformed, and in the above, it is simply scaled: wixi.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.18.markups.0\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.18.markups.0\":{\"type\":\"STRONG\",\"start\":0,\"end\":12,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.19\":{\"name\":\"6164\",\"__typename\":\"Paragraph\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Aggregating: the transformations in all embeddings are aggregated by summation.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.19.markups.0\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.19.markups.0\":{\"type\":\"STRONG\",\"start\":0,\"end\":11,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.20\":{\"name\":\"e151\",\"__typename\":\"Paragraph\",\"type\":\"H4\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"1.2. Aggregated Transformations\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.21\":{\"name\":\"22fb\",\"__typename\":\"Paragraph\",\"type\":\"IMG\",\"href\":null,\"layout\":\"INSET_CENTER\",\"metadata\":{\"type\":\"id\",\"generated\":false,\"id\":\"ImageMetadata:1*r_63luxLeVstJMzd0o-8Iw.png\",\"typename\":\"ImageMetadata\"},\"text\":\"A Block of ResNeXt with Cardinality = 32 (Left), and Its Generic Equation (Right)\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.21.markups.0\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"ImageMetadata:1*r_63luxLeVstJMzd0o-8Iw.png\":{\"id\":\"1*r_63luxLeVstJMzd0o-8Iw.png\",\"originalHeight\":330,\"originalWidth\":775,\"focusPercentX\":null,\"focusPercentY\":null,\"alt\":null,\"__typename\":\"ImageMetadata\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.21.markups.0\":{\"type\":\"STRONG\",\"start\":0,\"end\":81,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.22\":{\"name\":\"0733\",\"__typename\":\"Paragraph\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"In contrast to “Network-in-Network”, it is “Network-in-Neuron” expands along a new dimension. Instead of linear function in a simple neuron that wi times xi in each path, a nonlinear function is performed for each path.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.22.markups.0\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.22.markups.1\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.22.markups.0\":{\"type\":\"STRONG\",\"start\":44,\"end\":61,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.22.markups.1\":{\"type\":\"STRONG\",\"start\":171,\"end\":218,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.23\":{\"name\":\"b715\",\"__typename\":\"Paragraph\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"A new dimension C is introduced, called “Cardinality”. The dimension of cardinality controls the number of more complex transformations.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.23.markups.0\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.23.markups.1\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.23.markups.2\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.23.markups.3\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.23.markups.0\":{\"type\":\"STRONG\",\"start\":16,\"end\":18,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.23.markups.1\":{\"type\":\"STRONG\",\"start\":41,\"end\":52,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.23.markups.2\":{\"type\":\"STRONG\",\"start\":84,\"end\":135,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.23.markups.3\":{\"type\":\"EM\",\"start\":16,\"end\":17,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.24\":{\"name\":\"0a3f\",\"__typename\":\"Paragraph\",\"type\":\"H3\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"2. Relationship with Inception-ResNet and Grouped Convolution in AlexNet\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.24.markups.0\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.24.markups.1\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.24.markups.2\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.24.markups.0\":{\"type\":\"A\",\"start\":21,\"end\":37,\"href\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Freview-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.24.markups.1\":{\"type\":\"A\",\"start\":65,\"end\":72,\"href\":\"https:\\u002F\\u002Fmedium.com\\u002Fcoinmonks\\u002Fpaper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.24.markups.2\":{\"type\":\"STRONG\",\"start\":0,\"end\":72,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.25\":{\"name\":\"febc\",\"__typename\":\"Paragraph\",\"type\":\"IMG\",\"href\":null,\"layout\":\"OUTSET_CENTER\",\"metadata\":{\"type\":\"id\",\"generated\":false,\"id\":\"ImageMetadata:1*cIm3uy7eNvEchxRbBeBScQ.png\",\"typename\":\"ImageMetadata\"},\"text\":\"(a) ResNeXt Block, (b) Inception-ResNet Block, (c) Grouped Convolution\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.25.markups.0\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"ImageMetadata:1*cIm3uy7eNvEchxRbBeBScQ.png\":{\"id\":\"1*cIm3uy7eNvEchxRbBeBScQ.png\",\"originalHeight\":369,\"originalWidth\":1335,\"focusPercentX\":null,\"focusPercentY\":null,\"alt\":null,\"__typename\":\"ImageMetadata\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.25.markups.0\":{\"type\":\"STRONG\",\"start\":0,\"end\":70,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.26\":{\"name\":\"4bea\",\"__typename\":\"Paragraph\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"To compare, the above 3 blocks are having the SAME INTERNAL DIMENSIONS within each block.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.26.markups.0\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.26.markups.0\":{\"type\":\"STRONG\",\"start\":12,\"end\":89,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.27\":{\"name\":\"f444\",\"__typename\":\"Paragraph\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"(a) ResNeXt Block (Left)\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.27.markups.0\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.27.markups.0\":{\"type\":\"STRONG\",\"start\":0,\"end\":24,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.28\":{\"name\":\"2537\",\"__typename\":\"Paragraph\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"For each path, Conv1×1–Conv3×3–Conv1×1 are done at each convolution path. This is the bottleneck design in ResNet block. The internal dimension for each path is denoted as d (d=4). The number of paths is the cardinality C (C=32). If we sum up the dimension of each Conv3×3 (i.e. d×C=4×32), it is also the dimensions of 128.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.28.markups.0\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.28.markups.1\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.28.markups.2\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.28.markups.3\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.28.markups.4\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.28.markups.5\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.28.markups.6\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.28.markups.7\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.28.markups.8\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.28.markups.9\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.28.markups.0\":{\"type\":\"A\",\"start\":107,\"end\":113,\"href\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Freview-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.28.markups.1\":{\"type\":\"STRONG\",\"start\":15,\"end\":39,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.28.markups.2\":{\"type\":\"STRONG\",\"start\":172,\"end\":179,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.28.markups.3\":{\"type\":\"STRONG\",\"start\":208,\"end\":228,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.28.markups.4\":{\"type\":\"EM\",\"start\":172,\"end\":173,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.28.markups.5\":{\"type\":\"EM\",\"start\":175,\"end\":176,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.28.markups.6\":{\"type\":\"EM\",\"start\":220,\"end\":221,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.28.markups.7\":{\"type\":\"EM\",\"start\":223,\"end\":224,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.28.markups.8\":{\"type\":\"EM\",\"start\":279,\"end\":280,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.28.markups.9\":{\"type\":\"EM\",\"start\":281,\"end\":282,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.29\":{\"name\":\"f45b\",\"__typename\":\"Paragraph\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"The dimension is increased directly from 4 to 256, and then added together, and also added with the skip connection path.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.30\":{\"name\":\"ac99\",\"__typename\":\"Paragraph\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Compared with Inception-ResNet that it needs to increase the dimension from 4 to 128 then to 256, ResNeXt requires minimal extra effort designing each path.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.30.markups.0\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.30.markups.1\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.30.markups.0\":{\"type\":\"A\",\"start\":14,\"end\":30,\"href\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Freview-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.30.markups.1\":{\"type\":\"STRONG\",\"start\":98,\"end\":156,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.31\":{\"name\":\"ea06\",\"__typename\":\"Paragraph\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Unlike ResNet, in ResNeXt, the neurons at one path will not connected to the neurons at other paths.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.31.markups.0\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.31.markups.0\":{\"type\":\"A\",\"start\":7,\"end\":13,\"href\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Freview-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.32\":{\"name\":\"c543\",\"__typename\":\"Paragraph\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"(b) Inception-ResNet Block (Middle)\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.32.markups.0\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.32.markups.1\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.32.markups.0\":{\"type\":\"A\",\"start\":4,\"end\":20,\"href\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Freview-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.32.markups.1\":{\"type\":\"STRONG\",\"start\":0,\"end\":35,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.33\":{\"name\":\"a564\",\"__typename\":\"Paragraph\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"This is suggested in Inception-v4 to combine the Inception module and ResNet block. Somehow due to the legacy problem, for each convolution path, Conv1×1–Conv3×3 are done first. When added together (i.e. 4×32), the Conv3×3 has the dimension of 128.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.33.markups.0\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.33.markups.1\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.33.markups.2\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.33.markups.3\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.33.markups.0\":{\"type\":\"A\",\"start\":21,\"end\":33,\"href\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Freview-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.33.markups.1\":{\"type\":\"A\",\"start\":49,\"end\":58,\"href\":\"https:\\u002F\\u002Fmedium.com\\u002Fcoinmonks\\u002Fpaper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.33.markups.2\":{\"type\":\"A\",\"start\":70,\"end\":76,\"href\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Freview-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.33.markups.3\":{\"type\":\"STRONG\",\"start\":146,\"end\":162,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.34\":{\"name\":\"1309\",\"__typename\":\"Paragraph\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Then the outputs are concatenated together with dimension of 128. And Conv1×1 is used to restore the dimensions from 128 to 256.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.34.markups.0\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.34.markups.0\":{\"type\":\"STRONG\",\"start\":70,\"end\":77,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.35\":{\"name\":\"f428\",\"__typename\":\"Paragraph\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Finally the output is added with the skip connection path.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.36\":{\"name\":\"8480\",\"__typename\":\"Paragraph\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"The main difference is that they have an early concatenation.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.36.markups.0\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.36.markups.0\":{\"type\":\"STRONG\",\"start\":41,\"end\":60,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.37\":{\"name\":\"2f32\",\"__typename\":\"Paragraph\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"(c) Grouped Convolution in AlexNet (Right)\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.37.markups.0\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.37.markups.1\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.37.markups.0\":{\"type\":\"A\",\"start\":27,\"end\":34,\"href\":\"https:\\u002F\\u002Fmedium.com\\u002Fcoinmonks\\u002Fpaper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.37.markups.1\":{\"type\":\"STRONG\",\"start\":0,\"end\":42,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.38\":{\"name\":\"b344\",\"__typename\":\"Paragraph\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Conv1×1–Conv3×3–Conv1×1 are done at the convolution path, which is actually a bottleneck design suggested in ResNet. The Conv3×3 has the dimension of 128.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.38.markups.0\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.38.markups.1\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.38.markups.2\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.38.markups.0\":{\"type\":\"A\",\"start\":109,\"end\":115,\"href\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Freview-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.38.markups.1\":{\"type\":\"STRONG\",\"start\":0,\"end\":24,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.38.markups.2\":{\"type\":\"STRONG\",\"start\":77,\"end\":95,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.39\":{\"name\":\"9c7b\",\"__typename\":\"Paragraph\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"However, grouped convolution, suggested in AlexNet is used here. Therefore this Conv3×3 is wider but sparsely connected module. (Because the neurons at one path will not connected to the neurons at other paths, that’s why it is sparsely connected.)\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.39.markups.0\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.39.markups.1\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.39.markups.2\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.39.markups.0\":{\"type\":\"A\",\"start\":43,\"end\":50,\"href\":\"https:\\u002F\\u002Fmedium.com\\u002Fcoinmonks\\u002Fpaper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.39.markups.1\":{\"type\":\"STRONG\",\"start\":9,\"end\":28,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.39.markups.2\":{\"type\":\"STRONG\",\"start\":91,\"end\":128,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.40\":{\"name\":\"5883\",\"__typename\":\"Paragraph\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Thus, there are 32 groups of convolutions. (2 groups only in AlexNet)\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.40.markups.0\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.40.markups.1\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.40.markups.0\":{\"type\":\"A\",\"start\":61,\"end\":68,\"href\":\"https:\\u002F\\u002Fmedium.com\\u002Fcoinmonks\\u002Fpaper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.40.markups.1\":{\"type\":\"STRONG\",\"start\":16,\"end\":41,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.41\":{\"name\":\"eae5\",\"__typename\":\"Paragraph\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Then a skip connection is at parallel and added with the convolution path. Thus, the convolution path is learning the residual representation.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.42\":{\"name\":\"e9f0\",\"__typename\":\"Paragraph\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Though the structures in (b) and (c) are not always the same as the general form in the equation shown in 1.2, indeed authors have tried the above three structures as shown above, and they found that the results are the same.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.43\":{\"name\":\"9345\",\"__typename\":\"Paragraph\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Finally, authors choose to implement the structure in (c) because it is more succinct and faster than the other two forms.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.43.markups.0\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.43.markups.0\":{\"type\":\"STRONG\",\"start\":0,\"end\":122,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.44\":{\"name\":\"e7a5\",\"__typename\":\"Paragraph\",\"type\":\"H3\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"3. Full Architecture and Ablation Study\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.44.markups.0\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.44.markups.0\":{\"type\":\"STRONG\",\"start\":3,\"end\":39,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.45\":{\"name\":\"e30a\",\"__typename\":\"Paragraph\",\"type\":\"H4\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"3.1. Ablation Study of C and d Under Similar Complexity\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.45.markups.0\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.45.markups.1\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.45.markups.0\":{\"type\":\"EM\",\"start\":23,\"end\":24,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.45.markups.1\":{\"type\":\"EM\",\"start\":29,\"end\":30,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.46\":{\"name\":\"4fb4\",\"__typename\":\"Paragraph\",\"type\":\"IMG\",\"href\":null,\"layout\":\"OUTSET_CENTER\",\"metadata\":{\"type\":\"id\",\"generated\":false,\"id\":\"ImageMetadata:1*UNkmAg1JVfprEfBBmqM44w.png\",\"typename\":\"ImageMetadata\"},\"text\":\"Detailed Architecture (Left), Number of Parameters for Each Block (Top Right), Different Settings to Maintain Similar Complexity (Middle Right), Ablation Study for Different Settings Under Similar Complexity (Bottom Right)\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.46.markups.0\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"ImageMetadata:1*UNkmAg1JVfprEfBBmqM44w.png\":{\"id\":\"1*UNkmAg1JVfprEfBBmqM44w.png\",\"originalHeight\":742,\"originalWidth\":1348,\"focusPercentX\":null,\"focusPercentY\":null,\"alt\":null,\"__typename\":\"ImageMetadata\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.46.markups.0\":{\"type\":\"STRONG\",\"start\":0,\"end\":222,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.47\":{\"name\":\"6013\",\"__typename\":\"Paragraph\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"ResNet-50 is a special case of ResNeXt-50 with C=1, d=64.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.47.markups.0\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.47.markups.1\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.47.markups.2\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.47.markups.0\":{\"type\":\"A\",\"start\":0,\"end\":9,\"href\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Freview-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.47.markups.1\":{\"type\":\"EM\",\"start\":47,\"end\":48,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.47.markups.2\":{\"type\":\"EM\",\"start\":52,\"end\":53,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.48\":{\"name\":\"eb48\",\"__typename\":\"Paragraph\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"To have fair comparison, different ResNeXt with different C and d with similar complexity with ResNet are tried. This is shown at the middle right of the figure above.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.48.markups.0\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.48.markups.1\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.48.markups.2\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.48.markups.0\":{\"type\":\"A\",\"start\":95,\"end\":101,\"href\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Freview-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.48.markups.1\":{\"type\":\"EM\",\"start\":58,\"end\":59,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.48.markups.2\":{\"type\":\"EM\",\"start\":64,\"end\":65,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.49\":{\"name\":\"a697\",\"__typename\":\"Paragraph\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"And it is found that ResNeXt-50 (32×4d) obtains 22.2% top-1 error for ImageNet-1K (1K means 1K classes) dataset while ResNet-50 only obtains 23.9% top-1 error.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.49.markups.0\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.49.markups.0\":{\"type\":\"A\",\"start\":118,\"end\":127,\"href\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Freview-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.50\":{\"name\":\"6475\",\"__typename\":\"Paragraph\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"And ResNeXt-101 (32×4d) obtains 21.2% top-1 error for ImageNet dataset while ResNet-101 only obtains 22.0% top-1 error.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.50.markups.0\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.50.markups.0\":{\"type\":\"A\",\"start\":77,\"end\":87,\"href\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Freview-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.51\":{\"name\":\"f880\",\"__typename\":\"Paragraph\",\"type\":\"H4\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"3.2. Importance of Cardinality\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.52\":{\"name\":\"53b3\",\"__typename\":\"Paragraph\",\"type\":\"IMG\",\"href\":null,\"layout\":\"INSET_CENTER\",\"metadata\":{\"type\":\"id\",\"generated\":false,\"id\":\"ImageMetadata:1*xxm28sWSo6i-If09kwAqPQ.png\",\"typename\":\"ImageMetadata\"},\"text\":\"Ablation Study for Different Settings of 2× Complexity Models\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.52.markups.0\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"ImageMetadata:1*xxm28sWSo6i-If09kwAqPQ.png\":{\"id\":\"1*xxm28sWSo6i-If09kwAqPQ.png\",\"originalHeight\":344,\"originalWidth\":666,\"focusPercentX\":null,\"focusPercentY\":null,\"alt\":null,\"__typename\":\"ImageMetadata\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.52.markups.0\":{\"type\":\"STRONG\",\"start\":0,\"end\":61,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.53\":{\"name\":\"014c\",\"__typename\":\"Paragraph\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"ResNet-200: 21.7% top-1 and 5.8% top-5 error rates.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.53.markups.0\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.53.markups.1\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.53.markups.0\":{\"type\":\"A\",\"start\":0,\"end\":10,\"href\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Freview-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.53.markups.1\":{\"type\":\"STRONG\",\"start\":0,\"end\":10,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.54\":{\"name\":\"4fd4\",\"__typename\":\"Paragraph\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"ResNet-101, wider: only obtains 21.3% top-1 and 5.7% top-5 error rates, which means only making it wider does not help much.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.54.markups.0\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.54.markups.1\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.54.markups.2\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.54.markups.0\":{\"type\":\"A\",\"start\":0,\"end\":17,\"href\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Freview-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.54.markups.1\":{\"type\":\"STRONG\",\"start\":0,\"end\":17,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.54.markups.2\":{\"type\":\"STRONG\",\"start\":84,\"end\":123,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.55\":{\"name\":\"4b39\",\"__typename\":\"Paragraph\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"ResNeXt-101 (2×64d): By just making C=2 (i.e. two convolution paths within the ResNeXt block), an obvious improvement is already obtained with 20.7% top-1 and 5.5% top-5 error rates.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.55.markups.0\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.55.markups.1\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.55.markups.2\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.55.markups.3\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.55.markups.0\":{\"type\":\"STRONG\",\"start\":0,\"end\":19,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.55.markups.1\":{\"type\":\"STRONG\",\"start\":36,\"end\":39,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.55.markups.2\":{\"type\":\"STRONG\",\"start\":95,\"end\":137,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.55.markups.3\":{\"type\":\"EM\",\"start\":36,\"end\":37,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.56\":{\"name\":\"0a63\",\"__typename\":\"Paragraph\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"ResNeXt-101 (64×4d): By making C=64 (i.e. two convolution paths within the ResNeXt block), an even better improvement is already obtained with 20.4% top-1 and 5.3% top-5 error rates. This means cardinality is essential to improve the classification accuracy.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.56.markups.0\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.56.markups.1\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.56.markups.2\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.56.markups.3\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.56.markups.4\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.56.markups.0\":{\"type\":\"STRONG\",\"start\":0,\"end\":19,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.56.markups.1\":{\"type\":\"STRONG\",\"start\":31,\"end\":35,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.56.markups.2\":{\"type\":\"STRONG\",\"start\":91,\"end\":137,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.56.markups.3\":{\"type\":\"STRONG\",\"start\":194,\"end\":257,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.56.markups.4\":{\"type\":\"EM\",\"start\":31,\"end\":32,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.57\":{\"name\":\"57d7\",\"__typename\":\"Paragraph\",\"type\":\"H4\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"3.2. Importance of Residual Connections\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.58\":{\"name\":\"ea41\",\"__typename\":\"Paragraph\",\"type\":\"IMG\",\"href\":null,\"layout\":\"INSET_CENTER\",\"metadata\":{\"type\":\"id\",\"generated\":false,\"id\":\"ImageMetadata:1*EwrkhL-fqVii9MEjCHMZBA.png\",\"typename\":\"ImageMetadata\"},\"text\":\"Importance of Residual Connections\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.58.markups.0\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"ImageMetadata:1*EwrkhL-fqVii9MEjCHMZBA.png\":{\"id\":\"1*EwrkhL-fqVii9MEjCHMZBA.png\",\"originalHeight\":119,\"originalWidth\":571,\"focusPercentX\":null,\"focusPercentY\":null,\"alt\":null,\"__typename\":\"ImageMetadata\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.58.markups.0\":{\"type\":\"STRONG\",\"start\":0,\"end\":34,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.59\":{\"name\":\"39d7\",\"__typename\":\"Paragraph\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Without residual connections, error rates are increased largely for both ResNet-50 and ResNeXt-50. Residual connections are important.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.59.markups.0\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.59.markups.0\":{\"type\":\"A\",\"start\":73,\"end\":82,\"href\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Freview-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.60\":{\"name\":\"558f\",\"__typename\":\"Paragraph\",\"type\":\"H3\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"4. Results\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.61\":{\"name\":\"a94e\",\"__typename\":\"Paragraph\",\"type\":\"H4\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"4.1. ImageNet-1K\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.62\":{\"name\":\"882e\",\"__typename\":\"Paragraph\",\"type\":\"IMG\",\"href\":null,\"layout\":\"OUTSET_CENTER\",\"metadata\":{\"type\":\"id\",\"generated\":false,\"id\":\"ImageMetadata:1*oLRaAqY2cnw2E5eJ_D8jmA.png\",\"typename\":\"ImageMetadata\"},\"text\":\"Single Crop Testing: ResNet\\u002FResNeXt is 224×224 and 320×320, Inception models: 299×299\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.62.markups.0\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"ImageMetadata:1*oLRaAqY2cnw2E5eJ_D8jmA.png\":{\"id\":\"1*oLRaAqY2cnw2E5eJ_D8jmA.png\",\"originalHeight\":315,\"originalWidth\":1084,\"focusPercentX\":null,\"focusPercentY\":null,\"alt\":null,\"__typename\":\"ImageMetadata\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.62.markups.0\":{\"type\":\"STRONG\",\"start\":0,\"end\":85,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.63\":{\"name\":\"d9ed\",\"__typename\":\"Paragraph\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"ImageNet-1K is a subset of 22K-class ImageNet dataset, which contains 1000 classes. It is also the dataset for ILSVRC classification task.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.64\":{\"name\":\"752e\",\"__typename\":\"Paragraph\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"With standard size image used for single crop testing, ResNeXt-101 obtains 20.4% top-1 and 5.3% top-5 error rates,\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.65\":{\"name\":\"b264\",\"__typename\":\"Paragraph\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"With larger size image used for single crop testing, ResNeXt-101 obtains 19.1% top-1 and 4.4% top-5 error rates, which has better results than all state-of-the-art approaches, ResNet, Pre-Activation ResNet, Inception-v3, Inception-v4 and Inception-ResNet-v2.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.65.markups.0\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.65.markups.1\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.65.markups.2\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.65.markups.3\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.65.markups.4\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.65.markups.0\":{\"type\":\"A\",\"start\":176,\"end\":182,\"href\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Freview-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.65.markups.1\":{\"type\":\"A\",\"start\":184,\"end\":205,\"href\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Fresnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.65.markups.2\":{\"type\":\"A\",\"start\":207,\"end\":219,\"href\":\"https:\\u002F\\u002Fmedium.com\\u002F@sh.tsang\\u002Freview-inception-v3-1st-runner-up-image-classification-in-ilsvrc-2015-17915421f77c\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.65.markups.3\":{\"type\":\"A\",\"start\":221,\"end\":233,\"href\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Freview-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.65.markups.4\":{\"type\":\"A\",\"start\":238,\"end\":257,\"href\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Freview-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.66\":{\"name\":\"321e\",\"__typename\":\"Paragraph\",\"type\":\"H4\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"4.2. ImageNet-5K\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.67\":{\"name\":\"3341\",\"__typename\":\"Paragraph\",\"type\":\"IMG\",\"href\":null,\"layout\":\"OUTSET_CENTER\",\"metadata\":{\"type\":\"id\",\"generated\":false,\"id\":\"ImageMetadata:1*V99JmLNTxlXxIPYAIFiGwA.png\",\"typename\":\"ImageMetadata\"},\"text\":\"ImageNet-5K Results (All trained from scratch)\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.67.markups.0\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"ImageMetadata:1*V99JmLNTxlXxIPYAIFiGwA.png\":{\"id\":\"1*V99JmLNTxlXxIPYAIFiGwA.png\",\"originalHeight\":590,\"originalWidth\":1415,\"focusPercentX\":null,\"focusPercentY\":null,\"alt\":null,\"__typename\":\"ImageMetadata\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.67.markups.0\":{\"type\":\"STRONG\",\"start\":0,\"end\":46,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.68\":{\"name\":\"97a7\",\"__typename\":\"Paragraph\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"ImageNet-1K has been somehow saturated after so many years of development.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.69\":{\"name\":\"9240\",\"__typename\":\"Paragraph\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"ImageNet-5K is a subset of 22K-class ImageNet dataset, which contains 5000 classes, which also contains ImageNet-1K classes.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.70\":{\"name\":\"b179\",\"__typename\":\"Paragraph\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"6.8 million images, 5× of the ImageNet-1K dataset.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.71\":{\"name\":\"73ed\",\"__typename\":\"Paragraph\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Since there is no official train\\u002Fvalidation set, the original ImageNet-1K validation set is used for evaluation.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.72\":{\"name\":\"c2cd\",\"__typename\":\"Paragraph\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"5K-way classification is the softmax over 5K classes. Thus, there will be automatic errors when the network predicts the labels for the other 4K classes on the ImageNet-1K validation dataset.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.72.markups.0\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.72.markups.1\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.72.markups.0\":{\"type\":\"STRONG\",\"start\":0,\"end\":21,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.72.markups.1\":{\"type\":\"STRONG\",\"start\":29,\"end\":52,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.73\":{\"name\":\"fccb\",\"__typename\":\"Paragraph\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"1K-way classification is just the softmax over 1K classes.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.73.markups.0\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.73.markups.1\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.73.markups.0\":{\"type\":\"STRONG\",\"start\":0,\"end\":21,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.73.markups.1\":{\"type\":\"STRONG\",\"start\":34,\"end\":57,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.74\":{\"name\":\"34a9\",\"__typename\":\"Paragraph\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"ResNeXt of course got better results than ResNet as shown above.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.74.markups.0\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.74.markups.0\":{\"type\":\"A\",\"start\":42,\"end\":48,\"href\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Freview-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.75\":{\"name\":\"f627\",\"__typename\":\"Paragraph\",\"type\":\"H4\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"4.3. CIFAR-10 & CIFAR-100\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.76\":{\"name\":\"adf6\",\"__typename\":\"Paragraph\",\"type\":\"IMG\",\"href\":null,\"layout\":\"OUTSET_CENTER\",\"metadata\":{\"type\":\"id\",\"generated\":false,\"id\":\"ImageMetadata:1*FqDUkl5vrLZufkQIG6xk7A.png\",\"typename\":\"ImageMetadata\"},\"text\":\"CIFAR-10 and CIFAR-100 Results\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.76.markups.0\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"ImageMetadata:1*FqDUkl5vrLZufkQIG6xk7A.png\":{\"id\":\"1*FqDUkl5vrLZufkQIG6xk7A.png\",\"originalHeight\":584,\"originalWidth\":1343,\"focusPercentX\":null,\"focusPercentY\":null,\"alt\":null,\"__typename\":\"ImageMetadata\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.76.markups.0\":{\"type\":\"STRONG\",\"start\":0,\"end\":30,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.77\":{\"name\":\"4572\",\"__typename\":\"Paragraph\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"CIFAR-10 & CIFAR-100, two very famous 10-class and 100-class datasets.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.78\":{\"name\":\"f629\",\"__typename\":\"Paragraph\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Left: Compared with ResNet, ResNeXt always obtains better results in CIFAR-10.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.78.markups.0\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.78.markups.0\":{\"type\":\"A\",\"start\":20,\"end\":26,\"href\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Freview-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.79\":{\"name\":\"e839\",\"__typename\":\"Paragraph\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Right: Compared with Wide ResNet (WRN), ResNeXt-29 (16×64d) obtains 3.58% and 17.31% errors for CIFAR-10 and CIFAR-100 respectively. These were the best results among all state-of-the-art approaches at that moment.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.79.markups.0\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.79.markups.0\":{\"type\":\"A\",\"start\":21,\"end\":38,\"href\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Freview-wrns-wide-residual-networks-image-classification-d3feb3fb2004\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.80\":{\"name\":\"39c2\",\"__typename\":\"Paragraph\",\"type\":\"H4\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"4.4. MS COCO Object Detection\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.81\":{\"name\":\"534b\",\"__typename\":\"Paragraph\",\"type\":\"IMG\",\"href\":null,\"layout\":\"INSET_CENTER\",\"metadata\":{\"type\":\"id\",\"generated\":false,\"id\":\"ImageMetadata:1*_QhPxRX7B0jROHwhIuljCQ.png\",\"typename\":\"ImageMetadata\"},\"text\":\"MS COCO Objection Detection Results\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.81.markups.0\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"ImageMetadata:1*_QhPxRX7B0jROHwhIuljCQ.png\":{\"id\":\"1*_QhPxRX7B0jROHwhIuljCQ.png\",\"originalHeight\":190,\"originalWidth\":592,\"focusPercentX\":null,\"focusPercentY\":null,\"alt\":null,\"__typename\":\"ImageMetadata\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.81.markups.0\":{\"type\":\"STRONG\",\"start\":0,\"end\":35,\"href\":null,\"anchorType\":null,\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.82\":{\"name\":\"7c48\",\"__typename\":\"Paragraph\",\"type\":\"ULI\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"By plugging ResNet\\u002FResNeXt into Faster R-CNN, with similar model complexity, ResNeXt always outperforms ResNet for both AP@0.5 (IoU\\u003E0.5) and mean AP (average prediction) at all IoU levels.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.82.markups.0\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.82.markups.1\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.82.markups.2\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.82.markups.0\":{\"type\":\"A\",\"start\":12,\"end\":18,\"href\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Freview-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.82.markups.1\":{\"type\":\"A\",\"start\":32,\"end\":44,\"href\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Freview-faster-r-cnn-object-detection-f5685cb30202\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.82.markups.2\":{\"type\":\"A\",\"start\":104,\"end\":110,\"href\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Freview-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.83\":{\"name\":\"8543\",\"__typename\":\"Paragraph\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"With the success of ResNeXt, it is also utilized by Mask R-CNN for instance segmentation. Hope I can cover Mask R-CNN later on as well.\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.84\":{\"name\":\"2ad8\",\"__typename\":\"Paragraph\",\"type\":\"H4\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"Reference\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.85\":{\"name\":\"0b9a\",\"__typename\":\"Paragraph\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"[2017 CVPR] [ResNeXt]\\nAggregated Residual Transformations for Deep Neural Networks\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.85.markups.0\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.85.markups.0\":{\"type\":\"A\",\"start\":22,\"end\":82,\"href\":\"https:\\u002F\\u002Farxiv.org\\u002Fabs\\u002F1611.05431\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.86\":{\"name\":\"b06a\",\"__typename\":\"Paragraph\",\"type\":\"H4\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"My Related Reviews on Image Classification\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.87\":{\"name\":\"79f2\",\"__typename\":\"Paragraph\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"[LeNet] [AlexNet] [ZFNet] [VGGNet] [SPPNet] [PReLU-Net] [GoogLeNet \\u002F Inception-v1] [BN-Inception \\u002F Inception-v2] [Inception-v3] [Inception-v4] [Xception] [MobileNetV1] [ResNet] [Pre-Activation ResNet] [RiR] [RoR] [Stochastic Depth] [WRN] [PolyNet] [DenseNet]\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.87.markups.0\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.87.markups.1\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.87.markups.2\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.87.markups.3\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.87.markups.4\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.87.markups.5\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.87.markups.6\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.87.markups.7\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.87.markups.8\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.87.markups.9\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.87.markups.10\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.87.markups.11\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.87.markups.12\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.87.markups.13\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.87.markups.14\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.87.markups.15\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.87.markups.16\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.87.markups.17\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.87.markups.18\",\"typename\":\"Markup\"},{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.87.markups.19\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.87.markups.0\":{\"type\":\"A\",\"start\":1,\"end\":6,\"href\":\"https:\\u002F\\u002Fmedium.com\\u002F@sh.tsang\\u002Fpaper-brief-review-of-lenet-1-lenet-4-lenet-5-boosted-lenet-4-image-classification-1f5f809dbf17\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.87.markups.1\":{\"type\":\"A\",\"start\":9,\"end\":16,\"href\":\"https:\\u002F\\u002Fmedium.com\\u002Fcoinmonks\\u002Fpaper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.87.markups.2\":{\"type\":\"A\",\"start\":19,\"end\":24,\"href\":\"https:\\u002F\\u002Fmedium.com\\u002Fcoinmonks\\u002Fpaper-review-of-zfnet-the-winner-of-ilsvlc-2013-image-classification-d1a5a0c45103\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.87.markups.3\":{\"type\":\"A\",\"start\":27,\"end\":33,\"href\":\"https:\\u002F\\u002Fmedium.com\\u002Fcoinmonks\\u002Fpaper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.87.markups.4\":{\"type\":\"A\",\"start\":36,\"end\":42,\"href\":\"https:\\u002F\\u002Fmedium.com\\u002Fcoinmonks\\u002Freview-sppnet-1st-runner-up-object-detection-2nd-runner-up-image-classification-in-ilsvrc-906da3753679\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.87.markups.5\":{\"type\":\"A\",\"start\":45,\"end\":54,\"href\":\"https:\\u002F\\u002Fmedium.com\\u002Fcoinmonks\\u002Freview-prelu-net-the-first-to-surpass-human-level-performance-in-ilsvrc-2015-image-f619dddd5617\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.87.markups.6\":{\"type\":\"A\",\"start\":57,\"end\":81,\"href\":\"https:\\u002F\\u002Fmedium.com\\u002Fcoinmonks\\u002Fpaper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.87.markups.7\":{\"type\":\"A\",\"start\":84,\"end\":111,\"href\":\"https:\\u002F\\u002Fmedium.com\\u002F@sh.tsang\\u002Freview-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.87.markups.8\":{\"type\":\"A\",\"start\":114,\"end\":126,\"href\":\"https:\\u002F\\u002Fmedium.com\\u002F@sh.tsang\\u002Freview-inception-v3-1st-runner-up-image-classification-in-ilsvrc-2015-17915421f77c\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.87.markups.9\":{\"type\":\"A\",\"start\":129,\"end\":141,\"href\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Freview-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.87.markups.10\":{\"type\":\"A\",\"start\":144,\"end\":152,\"href\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Freview-xception-with-depthwise-separable-convolution-better-than-inception-v3-image-dc967dd42568\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.87.markups.11\":{\"type\":\"A\",\"start\":155,\"end\":166,\"href\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Freview-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.87.markups.12\":{\"type\":\"A\",\"start\":169,\"end\":175,\"href\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Freview-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.87.markups.13\":{\"type\":\"A\",\"start\":178,\"end\":199,\"href\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Fresnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.87.markups.14\":{\"type\":\"A\",\"start\":202,\"end\":205,\"href\":\"https:\\u002F\\u002Fmedium.com\\u002F@sh.tsang\\u002Freview-rir-resnet-in-resnet-image-classification-be4c79fde8ba\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.87.markups.15\":{\"type\":\"A\",\"start\":208,\"end\":211,\"href\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Freview-ror-resnet-of-resnet-multilevel-resnet-image-classification-cd3b0fcc19bb\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.87.markups.16\":{\"type\":\"A\",\"start\":214,\"end\":230,\"href\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Freview-stochastic-depth-image-classification-a4e225807f4a\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.87.markups.17\":{\"type\":\"A\",\"start\":233,\"end\":236,\"href\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Freview-wrns-wide-residual-networks-image-classification-d3feb3fb2004\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.87.markups.18\":{\"type\":\"A\",\"start\":239,\"end\":246,\"href\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Freview-polynet-2nd-runner-up-in-ilsvrc-2016-image-classification-8a1a941ce9ea\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.87.markups.19\":{\"type\":\"A\",\"start\":249,\"end\":257,\"href\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Freview-densenet-image-classification-b6631a8ef803\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.88\":{\"name\":\"030e\",\"__typename\":\"Paragraph\",\"type\":\"H4\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"My Related Reviews on Object Detection\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.89\":{\"name\":\"2a8b\",\"__typename\":\"Paragraph\",\"type\":\"P\",\"href\":null,\"layout\":null,\"metadata\":null,\"text\":\"[Faster R-CNN]\",\"hasDropCap\":null,\"dropCapImage\":null,\"markups\":[{\"type\":\"id\",\"generated\":true,\"id\":\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.89.markups.0\",\"typename\":\"Markup\"}],\"iframe\":null,\"mixtapeMetadata\":null},\"$Post:15d7f17b42ac.content({\\\"postMeteringOptions\\\":{}}).bodyModel.paragraphs.89.markups.0\":{\"type\":\"A\",\"start\":1,\"end\":13,\"href\":\"https:\\u002F\\u002Ftowardsdatascience.com\\u002Freview-faster-r-cnn-object-detection-f5685cb30202\",\"anchorType\":\"LINK\",\"userId\":null,\"linkMetadata\":null,\"__typename\":\"Markup\"},\"Quote:anon_65ad4bde583f\":{\"id\":\"anon_65ad4bde583f\",\"paragraphs\":[{\"type\":\"id\",\"generated\":true,\"id\":\"Quote:anon_65ad4bde583f.paragraphs.0\",\"typename\":\"Paragraph\"}],\"__typename\":\"Quote\",\"userId\":\"anon\",\"startOffset\":0,\"endOffset\":122,\"user\":null},\"Quote:anon_65ad4bde583f.paragraphs.0\":{\"name\":\"9345\",\"__typename\":\"Paragraph\"},\"Tag:machine-learning\":{\"id\":\"machine-learning\",\"displayTitle\":\"Machine Learning\",\"__typename\":\"Tag\"},\"Tag:deep-learning\":{\"id\":\"deep-learning\",\"displayTitle\":\"Deep Learning\",\"__typename\":\"Tag\"},\"Tag:artificial-intelligence\":{\"id\":\"artificial-intelligence\",\"displayTitle\":\"Artificial Intelligence\",\"__typename\":\"Tag\"},\"Tag:data-science\":{\"id\":\"data-science\",\"displayTitle\":\"Data Science\",\"__typename\":\"Tag\"},\"Tag:image-classification\":{\"id\":\"image-classification\",\"displayTitle\":\"Image Classification\",\"__typename\":\"Tag\"},\"$Post:15d7f17b42ac.previewContent\":{\"subtitle\":\"Network-in-Neuron, A New Dimensionality: Cardinality\",\"__typename\":\"PreviewContent\"},\"machine-learning\":{\"name\":\"Machine Learning\",\"slug\":\"machine-learning\",\"__typename\":\"Topic\"}}</script><script src=\"https://cdn-client.medium.com/lite/static/js/manifest.a8b9f1a7.js\"></script><script src=\"https://cdn-client.medium.com/lite/static/js/vendors~main.4648f493.chunk.js\"></script><script src=\"https://cdn-client.medium.com/lite/static/js/main.a268dda2.chunk.js\"></script><script src=\"https://cdn-client.medium.com/lite/static/js/vendors~screen.landingpages.trumpland~screen.post~screen.post.amp~screen.post.series~screen.profile~~b319665e.f2be28a6.chunk.js\"></script>\n",
       " <script src=\"https://cdn-client.medium.com/lite/static/js/screen.post~screen.post.amp~screen.post.series~screen.profile~screen.sequence.library~screen.sequenc~036c6b37.27089621.chunk.js\"></script>\n",
       " <script src=\"https://cdn-client.medium.com/lite/static/js/screen.landingpages.trumpland~screen.post~screen.post.amp~screen.post.series~screen.profile~screen.s~5e114ebe.de03a970.chunk.js\"></script>\n",
       " <script src=\"https://cdn-client.medium.com/lite/static/js/screen.post~screen.post.amp~screen.sequence.post.e4320c00.chunk.js\"></script>\n",
       " <script src=\"https://cdn-client.medium.com/lite/static/js/screen.post.86da995d.chunk.js\"></script><script>window.main();</script></body></html>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_soup.tagStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_flask",
   "language": "python",
   "name": "keras_flask"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
