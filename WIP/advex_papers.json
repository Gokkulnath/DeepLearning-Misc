[["2019-10-17", "http://arxiv.org/abs/1910.08051", "Instance adaptive adversarial training: Improved accuracy tradeoffs in neural nets. (82%)", ["Yogesh Balaji", " Tom Goldstein", " Judy Hoffman"], "  Adversarial training is by far the most successful strategy for improving\nrobustness of neural networks to adversarial attacks. Despite its success as a\ndefense mechanism, adversarial training fails to generalize well to unperturbed\ntest set. We hypothesize that this poor generalization is a consequence of\nadversarial training with uniform perturbation radius around every training\nsample. Samples close to decision boundary can be morphed into a different\nclass under a small perturbation budget, and enforcing large margins around\nthese samples produce poor decision boundaries that generalize poorly.\nMotivated by this hypothesis, we propose instance adaptive adversarial training\n-- a technique that enforces sample-specific perturbation margins around every\ntraining sample. We show that using our approach, test accuracy on unperturbed\nsamples improve with a marginal drop in robustness. Extensive experiments on\nCIFAR-10, CIFAR-100 and Imagenet datasets demonstrate the effectiveness of our\nproposed approach.\n"], ["2019-10-16", "http://arxiv.org/abs/1910.07629", "A New Defense Against Adversarial Images: Turning a Weakness into a Strength. (98%)", ["Tao Yu", " Shengyuan Hu", " Chuan Guo", " Wei-Lun Chao", " Kilian Q. Weinberger"], "  Natural images are virtually surrounded by low-density misclassified regions\nthat can be efficiently discovered by gradient-guided search --- enabling the\ngeneration of adversarial images. While many techniques for detecting these\nattacks have been proposed, they are easily bypassed when the adversary has\nfull knowledge of the detection mechanism and adapts the attack strategy\naccordingly. In this paper, we adopt a novel perspective and regard the\nomnipresence of adversarial perturbations as a strength rather than a weakness.\nWe postulate that if an image has been tampered with, these adversarial\ndirections either become harder to find with gradient methods or have\nsubstantially higher density than for natural images. We develop a practical\ntest for this signature characteristic to successfully detect adversarial\nattacks, achieving unprecedented accuracy under the white-box setting where the\nadversary is given full knowledge of our detection mechanism.\n"], ["2019-10-15", "http://arxiv.org/abs/1910.06813", "Improving Robustness of time series classifier with Neural ODE guided gradient based data augmentation. (99%)", ["Anindya Sarkar", " Anirudh Sunder Raj", " Raghu Sesha Iyengar"], "  Exploring adversarial attack vectors and studying their effects on machine\nlearning algorithms has been of interest to researchers. Deep neural networks\nworking with time series data have received lesser interest compared to their\nimage counterparts in this context. In a recent finding, it has been revealed\nthat current state-of-the-art deep learning time series classifiers are\nvulnerable to adversarial attacks. In this paper, we introduce two local\ngradient based and one spectral density based time series data augmentation\ntechniques. We show that a model trained with data obtained using our\ntechniques obtains state-of-the-art classification accuracy on various time\nseries benchmarks. In addition, it improves the robustness of the model against\nsome of the most common corruption techniques,such as Fast Gradient Sign Method\n(FGSM) and Basic Iterative Method (BIM).\n"], ["2019-10-15", "http://arxiv.org/abs/1910.07416", "Understanding Misclassifications by Attributes. (99%)", ["Sadaf Gulshad", " Zeynep Akata", " Jan Hendrik Metzen", " Arnold Smeulders"], "  In this paper, we aim to understand and explain the decisions of deep neural\nnetworks by studying the behavior of predicted attributes when adversarial\nexamples are introduced. We study the changes in attributes for clean as well\nas adversarial images in both standard and adversarially robust networks. We\npropose a metric to quantify the robustness of an adversarially robust network\nagainst adversarial attacks. In a standard network, attributes predicted for\nadversarial images are consistent with the wrong class, while attributes\npredicted for the clean images are consistent with the true class. In an\nadversarially robust network, the attributes predicted for adversarial images\nclassified correctly are consistent with the true class. Finally, we show that\nthe ability to robustify a network varies for different datasets. For the fine\ngrained dataset, it is higher as compared to the coarse-grained dataset.\nAdditionally, the ability to robustify a network increases with the increase in\nadversarial noise.\n"], ["2019-10-15", "http://arxiv.org/abs/1910.07517", "Adversarial Examples for Models of Code. (99%)", ["Noam Yefet", " Uri Alon", " Eran Yahav"], "  We introduce a novel approach for attacking trained models of code with\nadversarial examples. The main idea is to force a given trained model to make a\nprediction of the adversary's choice by introducing small perturbations that do\nnot change program semantics. We find these perturbations by deriving the\ndesired prediction with respect to the model's inputs while holding the model\nweights constant and following the gradients to slightly modify the input.\n  To defend a model against such attacks, we propose placing a defensive model\nin front of the downstream model. The defensive model detects unlikely\nmutations and masks them before feeding the input to the downstream model.\n  We show that our attack succeeds in changing a prediction to the adversary's\ndesire (\"targeted attack\") up to 89% of the times, and succeeds in changing a\ngiven prediction to any incorrect prediction (\"non-targeted attack\") 94% of the\ntimes. By using our proposed defense, the success rate of the attack drops\ndrastically for both targeted and non-targeted attacks, with a minor penalty of\n2% relative degradation in accuracy while not performing under attack.\n"], ["2019-10-15", "http://arxiv.org/abs/1910.07067", "On adversarial patches: real-world attack on ArcFace-100 face recognition system. (98%)", ["Mikhail Pautov", " Grigorii Melnikov", " Edgar Kaziakhmedov", " Klim Kireev", " Aleksandr Petiushko"], "  Recent works showed the vulnerability of image classifiers to adversarial\nattacks in the digital domain. However, the majority of attacks involve adding\nsmall perturbation to an image to fool the classifier. Unfortunately, such\nprocedures can not be used to conduct a real-world attack, where adding an\nadversarial attribute to the photo is a more practical approach. In this paper,\nwe study the problem of real-world attacks on face recognition systems. We\nexamine security of one of the best public face recognition systems,\nLResNet100E-IR with ArcFace loss, and propose a simple method to attack it in\nthe physical world. The method suggests creating an adversarial patch that can\nbe printed, added as a face attribute and photographed; the photo of a person\nwith such attribute is then passed to the classifier such that the classifier's\nrecognized class changes from correct to the desired one. Proposed generating\nprocedure allows projecting adversarial patches not only on different areas of\nthe face, such as nose or forehead but also on some wearable accessory, such as\neyeglasses.\n"], ["2019-10-15", "http://arxiv.org/abs/1910.07042", "MUTE: Data-Similarity Driven Multi-hot Target Encoding for Neural Network Design. (2%)", ["Mayoore S. Jaiswal", " Bumboo Kang", " Jinho Lee", " Minsik Cho"], "  Target encoding is an effective technique to deliver better performance for\nconventional machine learning methods, and recently, for deep neural networks\nas well. However, the existing target encoding approaches require significant\nincrease in the learning capacity, thus demand higher computation power and\nmore training data. In this paper, we present a novel and efficient target\nencoding scheme, MUTE to improve both generalizability and robustness of a\ntarget model by understanding the inter-class characteristics of a target\ndataset. By extracting the confusion level between the target classes in a\ndataset, MUTE strategically optimizes the Hamming distances among target\nencoding. Such optimized target encoding offers higher classification strength\nfor neural network models with negligible computation overhead and without\nincreasing the model size. When MUTE is applied to the popular image\nclassification networks and datasets, our experimental results show that MUTE\noffers better generalization and defense against the noises and adversarial\nattacks over the existing solutions.\n"], ["2019-10-14", "http://arxiv.org/abs/1910.06261", "Real-world attack on MTCNN face detection system.", ["Edgar Kaziakhmedov", " Klim Kireev", " Grigorii Melnikov", " Mikhail Pautov", " Aleksandr Petiushko"], "  Recent studies proved that deep learning approaches achieve remarkable\nresults on face detection task. On the other hand, the advances gave rise to a\nnew problem associated with the security of the deep convolutional neural\nnetwork models unveiling potential risks of DCNNs based applications. Even\nminor input changes in the digital domain can result in the network being\nfooled. It was shown then that some deep learning-based face detectors are\nprone to adversarial attacks not only in a digital domain but also in the real\nworld. In the paper, we investigate the security of the well-known cascade CNN\nface detection system - MTCNN and introduce an easily reproducible and a robust\nway to attack it. We propose different face attributes printed on an ordinary\nwhite and black printer and attached either to the medical face mask or to the\nface directly. Our approach is capable of breaking the MTCNN detector in a\nreal-world scenario.\n"], ["2019-10-14", "http://arxiv.org/abs/1910.06296", "DeepSearch: Simple and Effective Blackbox Fuzzing of Deep Neural Networks.", ["Fuyuan Zhang", " Sankalan Pal Chowdhury", " Maria Christakis"], "  Although deep neural networks have been successful in image classification,\nthey are prone to adversarial attacks. To generate misclassified inputs, there\nhas emerged a wide variety of techniques, such as black- and whitebox testing\nof neural networks. In this paper, we present DeepSearch, a novel\nblackbox-fuzzing technique for image classifiers. Despite its simplicity,\nDeepSearch is shown to be more effective in finding adversarial examples than\nclosely related black- and whitebox approaches. DeepSearch is additionally able\nto generate the most subtle adversarial examples in comparison to these\napproaches.\n"], ["2019-10-14", "http://arxiv.org/abs/1910.06259", "Confidence-Calibrated Adversarial Training: Towards Robust Models Generalizing Beyond the Attack Used During Training.", ["David Stutz", " Matthias Hein", " Bernt Schiele"], "  Adversarial training is the standard to train models robust against\nadversarial examples. However, especially for complex datasets, adversarial\ntraining incurs a significant loss in accuracy and is known to generalize\npoorly to stronger attacks, e.g., larger perturbations or other threat models.\nIn this paper, we introduce confidence-calibrated adversarial training (CCAT)\nwhere the key idea is to enforce that the confidence on adversarial examples\ndecays with their distance to the attacked examples. We show that CCAT\npreserves better the accuracy of normal training while robustness against\nadversarial examples is achieved via confidence thresholding. Most importantly,\nin strong contrast to adversarial training, the robustness of CCAT generalizes\nto larger perturbations and other threat models, not encountered during\ntraining. We also discuss our extensive work to design strong adaptive attacks\nagainst CCAT and standard adversarial training which is of independent\ninterest. We present experimental results on MNIST, SVHN and Cifar10.\n"], ["2019-10-14", "http://arxiv.org/abs/1910.06838", "Man-in-the-Middle Attacks against Machine Learning Classifiers via Malicious Generative Models. (99%)", ["Derek Derui", " Wang", " Chaoran Li", " Sheng Wen", " Surya Nepal", " Yang Xiang"], "  Deep Neural Networks (DNNs) are vulnerable to deliberately crafted\nadversarial examples. In the past few years, many efforts have been spent on\nexploring query-optimisation attacks to find adversarial examples of either\nblack-box or white-box DNN models, as well as the defending countermeasures\nagainst those attacks. In this work, we explore vulnerabilities of DNN models\nunder the umbrella of Man-in-the-Middle (MitM) attacks, which has not been\ninvestigated before. From the perspective of an MitM adversary, the\naforementioned adversarial example attacks are not viable anymore. First, such\nattacks must acquire the outputs from the models by multiple times before\nactually launching attacks, which is difficult for the MitM adversary in\npractice. Second, such attacks are one-off and cannot be directly generalised\nonto new data examples, which decreases the rate of return for the attacker. In\ncontrast, using generative models to craft adversarial examples on the fly can\nmitigate the drawbacks. However, the adversarial capability of the generative\nmodels, such as Variational Auto-Encoder (VAE), has not been extensively\nstudied. Therefore, given a classifier, we investigate using a VAE decoder to\neither transform benign inputs to their adversarial counterparts or decode\noutputs from benign VAE encoders to be adversarial examples. The proposed\nmethod can endue more capability to MitM attackers. Based on our evaluation,\nthe proposed attack can achieve above 95% success rate on both MNIST and\nCIFAR10 datasets, which is better or comparable with state-of-the-art\nquery-optimisation attacks. At the meantime, the attack is 104 times faster\nthan the query-optimisation attacks.\n"], ["2019-10-14", "http://arxiv.org/abs/1910.06513", "ZO-AdaMM: Zeroth-Order Adaptive Momentum Method for Black-Box Optimization. (47%)", ["Xiangyi Chen", " Sijia Liu", " Kaidi Xu", " Xingguo Li", " Xue Lin", " Mingyi Hong", " David Cox"], "  The adaptive momentum method (AdaMM), which uses past gradients to update\ndescent directions and learning rates simultaneously, has become one of the\nmost popular first-order optimization methods for solving machine learning\nproblems. However, AdaMM is not suited for solving black-box optimization\nproblems, where explicit gradient forms are difficult or infeasible to obtain.\nIn this paper, we propose a zeroth-order AdaMM (ZO-AdaMM) algorithm, that\ngeneralizes AdaMM to the gradient-free regime. We show that the convergence\nrate of ZO-AdaMM for both convex and nonconvex optimization is roughly a factor\nof $O(\\sqrt{d})$ worse than that of the first-order AdaMM algorithm, where $d$\nis problem size. In particular, we provide a deep understanding on why\nMahalanobis distance matters in convergence of ZO-AdaMM and other AdaMM-type\nmethods. As a byproduct, our analysis makes the first step toward understanding\nadaptive learning rate methods for nonconvex constrained optimization.\nFurthermore, we demonstrate two applications, designing per-image and universal\nadversarial attacks from black-box neural networks, respectively. We perform\nextensive experiments on ImageNet and empirically show that ZO-AdaMM converges\nmuch faster to a solution of high accuracy compared with $6$ state-of-the-art\nZO optimization methods.\n"], ["2019-10-14", "http://arxiv.org/abs/1910.06461", "Learning-based Intelligent Attack against Mobile Robots with Obstacle-avoidance. (1%)", ["Yushan Li", " Jianping He", " Cailian Chen", " Xinping Guan"], "  The security issue of mobile robots have attracted considerable attention in\nrecent years. Most existing works focus on detection and countermeasures for\nsome classic attacks from cyberspace. Nevertheless, those work are generally\nbased on some prior assumptions for the attacker (e.g., the system dynamics is\nknown, or internal access is compromised). A few work are delicated to physical\nattacks, however, there still lacks certain intelligence and advanced control\ndesign. In this paper, we propose a physical-based and intelligent attack\nframework against the obstacle-avoidance of mobile robots. The novelty of our\nwork lies in the following: i) Without any prior information of the system\ndynamics, the attacker can learn the detection area and goal position of a\nmobile robot by trial and observation, and the obstacle-avoidance mechanism is\nlearned by support vector regression (SVR) method; ii) Considering different\nattack requirements, different attack strategies are proposed to implement the\nattack efficiently; iii) The framework is suitable for holonomic and\nnon-holonomic mobile robots, and the algorithm performance analysis about time\ncomplexity and optimality is provided. Furthermore, the condition is obtained\nto guarantee the success of the attack. Simulations illustrate the\neffectiveness of the proposed framework.\n"], ["2019-10-12", "http://arxiv.org/abs/1910.05513", "On Robustness of Neural Ordinary Differential Equations.", ["Hanshu Yan", " Jiawei Du", " Vincent Y. F. Tan", " Jiashi Feng"], "  Neural ordinary differential equations (ODEs) have been attracting increasing\nattention in various research domains recently. There have been some works\nstudying optimization issues and approximation capabilities of neural ODEs, but\ntheir robustness is still yet unclear. In this work, we fill this important gap\nby exploring robustness properties of neural ODEs both empirically and\ntheoretically. We first present an empirical study on the robustness of the\nneural ODE-based networks (ODENets) by exposing them to inputs with various\ntypes of perturbations and subsequently investigating the changes of the\ncorresponding outputs. In contrast to conventional convolutional neural\nnetworks (CNNs), we find that the ODENets are more robust against both random\nGaussian perturbations and adversarial attack examples. We then provide an\ninsightful understanding of this phenomenon by exploiting a certain desirable\nproperty of the flow of a continuous-time ODE, namely that integral curves are\nnon-intersecting. Our work suggests that, due to their intrinsic robustness, it\nis promising to use neural ODEs as a basic block for building robust deep\nnetwork models. To further enhance the robustness of vanilla neural ODEs, we\npropose the time-invariant steady neural ODE (TisODE), which regularizes the\nflow on perturbed data via the time-invariant property and the imposition of a\nsteady-state constraint. We show that the TisODE method outperforms vanilla\nneural ODEs and also can work in conjunction with other state-of-the-art\narchitectural methods to build more robust deep networks.\n"], ["2019-10-11", "http://arxiv.org/abs/1910.05262", "Hear \"No Evil\", See \"Kenansville\": Efficient and Transferable Black-Box Attacks on Speech Recognition and Voice Identification Systems.", ["Hadi Abdullah", " Muhammad Sajidur Rahman", " Washington Garcia", " Logan Blue", " Kevin Warren", " Anurag Swarnim Yadav", " Tom Shrimpton", " Patrick Traynor"], "  Automatic speech recognition and voice identification systems are being\ndeployed in a wide array of applications, from providing control mechanisms to\ndevices lacking traditional interfaces, to the automatic transcription of\nconversations and authentication of users. Many of these applications have\nsignificant security and privacy considerations. We develop attacks that force\nmistranscription and misidentification in state of the art systems, with\nminimal impact on human comprehension. Processing pipelines for modern systems\nare comprised of signal preprocessing and feature extraction steps, whose\noutput is fed to a machine-learned model. Prior work has focused on the models,\nusing white-box knowledge to tailor model-specific attacks. We focus on the\npipeline stages before the models, which (unlike the models) are quite similar\nacross systems. As such, our attacks are black-box and transferable, and\ndemonstrably achieve mistranscription and misidentification rates as high as\n100% by modifying only a few frames of audio. We perform a study via Amazon\nMechanical Turk demonstrating that there is no statistically significant\ndifference between human perception of regular and perturbed audio. Our\nfindings suggest that models may learn aspects of speech that are generally not\nperceived by human subjects, but that are crucial for model accuracy. We also\nfind that certain English language phonemes (in particular, vowels) are\nsignificantly more susceptible to our attack. We show that the attacks are\neffective when mounted over cellular networks, where signals are subject to\ndegradation due to transcoding, jitter, and packet loss.\n"], ["2019-10-11", "http://arxiv.org/abs/1910.05018", "Verification of Neural Networks: Specifying Global Robustness using Generative Models.", ["Nathana\u00ebl Fijalkow", " Mohit Kumar Gupta"], "  The success of neural networks across most machine learning tasks and the\npersistence of adversarial examples have made the verification of such models\nan important quest. Several techniques have been successfully developed to\nverify robustness, and are now able to evaluate neural networks with thousands\nof nodes. The main weakness of this approach is in the specification:\nrobustness is asserted on a validation set consisting of a finite set of\nexamples, i.e. locally.\n  We propose a notion of global robustness based on generative models, which\nasserts the robustness on a very large and representative set of examples. We\nshow how this can be used for verifying neural networks. In this paper we\nexperimentally explore the merits of this approach, and show how it can be used\nto construct realistic adversarial examples.\n"], ["2019-10-10", "http://arxiv.org/abs/1910.04819", "Information Robust Dirichlet Networks for Predictive Uncertainty Estimation.", ["Theodoros Tsiligkaridis"], "  Precise estimation of uncertainty in predictions for AI systems is a critical\nfactor in ensuring trust and safety. Conventional neural networks tend to be\noverconfident as they do not account for uncertainty during training. In\ncontrast to Bayesian neural networks that learn approximate distributions on\nweights to infer prediction confidence, we propose a novel method, Information\nRobust Dirichlet networks, that learns the Dirichlet distribution on prediction\nprobabilities by minimizing the expected $L_p$ norm of the prediction error and\nan information divergence loss that penalizes information flow towards\nincorrect classes, while simultaneously maximizing differential entropy of\nsmall adversarial perturbations to provide accurate uncertainty estimates.\nProperties of the new cost function are derived to indicate how improved\nuncertainty estimation is achieved. Experiments using real datasets show that\nour technique outperforms state-of-the-art neural networks, by a large margin,\nfor estimating in-distribution and out-of-distribution uncertainty, and\ndetecting adversarial examples.\n"], ["2019-10-10", "http://arxiv.org/abs/1910.04618", "Universal Adversarial Perturbation for Text Classification.", ["Hang Gao", " Tim Oates"], "  Given a state-of-the-art deep neural network text classifier, we show the\nexistence of a universal and very small perturbation vector (in the embedding\nspace) that causes natural text to be misclassified with high probability.\nUnlike images on which a single fixed-size adversarial perturbation can be\nfound, text is of variable length, so we define the \"universality\" as\n\"token-agnostic\", where a single perturbation is applied to each token,\nresulting in different perturbations of flexible sizes at the sequence level.\nWe propose an algorithm to compute universal adversarial perturbations, and\nshow that the state-of-the-art deep neural networks are highly vulnerable to\nthem, even though they keep the neighborhood of tokens mostly preserved. We\nalso show how to use these adversarial perturbations to generate adversarial\ntext samples. The surprising existence of universal \"token-agnostic\"\nadversarial perturbations may reveal important properties of a text classifier.\n"], ["2019-10-09", "http://arxiv.org/abs/1910.03850", "Learning deep forest with multi-scale Local Binary Pattern features for face anti-spoofing.", ["Rizhao Cai", " Changsheng Chen"], "  Face Anti-Spoofing (FAS) is significant for the security of face recognition\nsystems. Convolutional Neural Networks (CNNs) have been introduced to the field\nof the FAS and have achieved competitive performance. However, CNN-based\nmethods are vulnerable to the adversarial attack. Attackers could generate\nadversarial-spoofing examples to circumvent a CNN-based face liveness detector.\nStudies about the transferability of the adversarial attack reveal that\nutilizing handcrafted feature-based methods could improve security in a\nsystem-level. Therefore, handcrafted feature-based methods are worth our\nexploration. In this paper, we introduce the deep forest, which is proposed as\nan alternative towards CNNs by Zhou et al., in the problem of the FAS. To the\nbest of our knowledge, this is the first attempt at exploiting the deep forest\nin the problem of FAS. Moreover, we propose to re-devise the representation\nconstructing by using LBP descriptors rather than the Grained-Scanning\nMechanism in the original scheme. Our method achieves competitive results. On\nthe benchmark database IDIAP REPLAY-ATTACK, 0\\% Equal Error Rate (EER) is\nachieved. This work provides a competitive option in a fusing scheme for\nimproving system-level security and offers important ideas to those who want to\nexplore methods besides CNNs.\n"], ["2019-10-09", "http://arxiv.org/abs/1910.03810", "Adversarial Learning of Deepfakes in Accounting.", ["Marco Schreyer", " Timur Sattarov", " Bernd Reimer", " Damian Borth"], "  Nowadays, organizations collect vast quantities of accounting relevant\ntransactions, referred to as 'journal entries', in 'Enterprise Resource\nPlanning' (ERP) systems. The aggregation of those entries ultimately defines an\norganization's financial statement. To detect potential misstatements and\nfraud, international audit standards demand auditors to directly assess journal\nentries using 'Computer Assisted AuditTechniques' (CAATs). At the same time,\ndiscoveries in deep learning research revealed that machine learning models are\nvulnerable to 'adversarial attacks'. It also became evident that such attack\ntechniques can be misused to generate 'Deepfakes' designed to directly attack\nthe perception of humans by creating convincingly altered media content. The\nresearch of such developments and their potential impact on the finance and\naccounting domain is still in its early stage. We believe that it is of vital\nrelevance to investigate how such techniques could be maliciously misused in\nthis sphere. In this work, we show an adversarial attack against CAATs using\ndeep neural networks. We first introduce a real-world 'thread model' designed\nto camouflage accounting anomalies such as fraudulent journal entries. Second,\nwe show that adversarial autoencoder neural networks are capable of learning a\nhuman interpretable model of journal entries that disentangles the entries\nlatent generative factors. Finally, we demonstrate how such a model can be\nmaliciously misused by a perpetrator to generate robust 'adversarial' journal\nentries that mislead CAATs.\n"], ["2019-10-09", "http://arxiv.org/abs/1910.03916", "Deep Latent Defence.", ["Giulio Zizzo", " Chris Hankin", " Sergio Maffeis", " Kevin Jones"], "  Deep learning methods have shown state of the art performance in a range of\ntasks from computer vision to natural language processing. However, it is well\nknown that such systems are vulnerable to attackers who craft inputs in order\nto cause misclassification. The level of perturbation an attacker needs to\nintroduce in order to cause such a misclassification can be extremely small,\nand often imperceptible. This is of significant security concern, particularly\nwhere misclassification can cause harm to humans.\n  We thus propose Deep Latent Defence, an architecture which seeks to combine\nadversarial training with a detection system. At its core Deep Latent Defence\nhas a adversarially trained neural network. A series of encoders take the\nintermediate layer representation of data as it passes though the network and\nproject it to a latent space which we use for detecting adversarial samples via\na $k$-nn classifier. We present results using both grey and white box\nattackers, as well as an adaptive $L_{\\infty}$ bounded attack which was\nconstructed specifically to try and evade our defence. We find that even under\nthe strongest attacker model that we have investigated our defence is able to\noffer significant defensive benefits.\n"], ["2019-10-09", "http://arxiv.org/abs/1910.04279", "Adversarial Training: embedding adversarial perturbations into the parameter space of a neural network to build a robust system.", ["Shixian Wen", " Laurent Itti"], "  Adversarial training, in which a network is trained on both adversarial and\nclean examples, is one of the most trusted defense methods against adversarial\nattacks. However, there are three major practical difficulties in implementing\nand deploying this method - expensive in terms of extra memory and computation\ncosts; accuracy trade-off between clean and adversarial examples; and lack of\ndiversity of adversarial perturbations. Classical adversarial training uses\nfixed, precomputed perturbations in adversarial examples (input space). In\ncontrast, we introduce dynamic adversarial perturbations into the parameter\nspace of the network, by adding perturbation biases to the fully connected\nlayers of deep convolutional neural network. During training, using only clean\nimages, the perturbation biases are updated in the Fast Gradient Sign Direction\nto automatically create and store adversarial perturbations by recycling the\ngradient information computed. The network learns and adjusts itself\nautomatically to these learned adversarial perturbations. Thus, we can achieve\nadversarial training with negligible cost compared to requiring a training set\nof adversarial example images. In addition, if combined with classical\nadversarial training, our perturbation biases can alleviate accuracy trade-off\ndifficulties, and diversify adversarial perturbations.\n"], ["2019-10-08", "http://arxiv.org/abs/1910.03468", "Directional Adversarial Training for Cost Sensitive Deep Learning Classification Applications.", ["Matteo Terzi", " Gian Antonio Susto", " Pratik Chaudhari"], "  In many real-world applications of Machine Learning it is of paramount\nimportance not only to provide accurate predictions, but also to ensure certain\nlevels of robustness. Adversarial Training is a training procedure aiming at\nproviding models that are robust to worst-case perturbations around predefined\npoints. Unfortunately, one of the main issues in adversarial training is that\nrobustness w.r.t. gradient-based attackers is always achieved at the cost of\nprediction accuracy. In this paper, a new algorithm, called Wasserstein\nProjected Gradient Descent (WPGD), for adversarial training is proposed. WPGD\nprovides a simple way to obtain cost-sensitive robustness, resulting in a finer\ncontrol of the robustness-accuracy trade-off. Moreover, WPGD solves an optimal\ntransport problem on the output space of the network and it can efficiently\ndiscover directions where robustness is required, allowing to control the\ndirectional trade-off between accuracy and robustness. The proposed WPGD is\nvalidated in this work on image recognition tasks with different benchmark\ndatasets and architectures. Moreover, real world-like datasets are often\nunbalanced: this paper shows that when dealing with such type of datasets, the\nperformance of adversarial training are mainly affected in term of standard\naccuracy.\n"], ["2019-10-08", "http://arxiv.org/abs/1910.03624", "SmoothFool: An Efficient Framework for Computing Smooth Adversarial Perturbations.", ["Ali Dabouei", " Sobhan Soleymani", " Fariborz Taherkhani", " Jeremy Dawson", " Nasser M. Nasrabadi"], "  Deep neural networks are susceptible to adversarial manipulations in the\ninput domain. The extent of vulnerability has been explored intensively in\ncases of $\\ell_p$-bounded and $\\ell_p$-minimal adversarial perturbations.\nHowever, the vulnerability of DNNs to adversarial perturbations with specific\nstatistical properties or frequency-domain characteristics has not been\nsufficiently explored. In this paper, we study the smoothness of perturbations\nand propose SmoothFool, a general and computationally efficient framework for\ncomputing smooth adversarial perturbations. Through extensive experiments, we\nvalidate the efficacy of the proposed method for both the white-box and\nblack-box attack scenarios. In particular, we demonstrate that: (i) there exist\nextremely smooth adversarial perturbations for well-established and widely used\nnetwork architectures, (ii) smoothness significantly enhances the robustness of\nperturbations against state-of-the-art defense mechanisms, (iii) smoothness\nimproves the transferability of adversarial perturbations across both data\npoints and network architectures, and (iv) class categories exhibit a variable\nrange of susceptibility to smooth perturbations. Our results suggest that\nsmooth APs can play a significant role in exploring the vulnerability extent of\nDNNs to adversarial examples.\n"], ["2019-10-07", "http://arxiv.org/abs/1910.02673", "Interpretable Disentanglement of Neural Networks by Extracting Class-Specific Subnetwork.", ["Yulong Wang", " Xiaolin Hu", " Hang Su"], "  We propose a novel perspective to understand deep neural networks in an\ninterpretable disentanglement form. For each semantic class, we extract a\nclass-specific functional subnetwork from the original full model, with\ncompressed structure while maintaining comparable prediction performance. The\nstructure representations of extracted subnetworks display a resemblance to\ntheir corresponding class semantic similarities. We also apply extracted\nsubnetworks in visual explanation and adversarial example detection tasks by\nmerely replacing the original full model with class-specific subnetworks.\nExperiments demonstrate that this intuitive operation can effectively improve\nexplanation saliency accuracy for gradient-based explanation methods, and\nincrease the detection rate for confidence score-based adversarial example\ndetection methods.\n"], ["2019-10-05", "http://arxiv.org/abs/1910.02244", "Yet another but more efficient black-box adversarial attack: tiling and evolution strategies.", ["Laurent Meunier", " Jamal Atif", " Olivier Teytaud"], "  We introduce a new black-box attack achieving state of the art performances.\nOur approach is based on a new objective function, borrowing ideas from\n$\\ell_\\infty$-white box attacks, and particularly designed to fit\nderivative-free optimization requirements. It only requires to have access to\nthe logits of the classifier without any other information which is a more\nrealistic scenario. Not only we introduce a new objective function, we extend\nprevious works on black box adversarial attacks to a larger spectrum of\nevolution strategies and other derivative-free optimization methods. We also\nhighlight a new intriguing property that deep neural networks are not robust to\nsingle shot tiled attacks. Our models achieve, with a budget limited to\n$10,000$ queries, results up to $99.2\\%$ of success rate against InceptionV3\nclassifier with $630$ queries to the network on average in the untargeted\nattacks setting, which is an improvement by $90$ queries of the current state\nof the art. In the targeted setting, we are able to reach, with a limited\nbudget of $100,000$, $100\\%$ of success rate with a budget of $6,662$ queries\non average, i.e. we need $800$ queries less than the current state of the art.\n"], ["2019-10-05", "http://arxiv.org/abs/1910.02354", "Unrestricted Adversarial Attacks for Semantic Segmentation.", ["Guangyu Shen", " Chengzhi Mao", " Junfeng Yang", " Baishakhi Ray"], "  Semantic segmentation is one of the most impactful applications of machine\nlearning; however, their robustness under adversarial attack is not well\nstudied. In this paper, we focus on generating unrestricted adversarial\nexamples for semantic segmentation models. We demonstrate a simple yet\neffective method to generate unrestricted adversarial examples using\nconditional generative adversarial networks (CGAN) without any hand-crafted\nmetric. The na\\\"ive implementation of CGAN, however, yields inferior image\nquality and low attack success rate. Instead, we leverage the SPADE\n(Spatially-adaptive denormalization) structure with an additional loss item,\nwhich is able to generate effective adversarial attacks in a single step. We\nvalidate our approach on the well studied Cityscapes and ADE20K datasets, and\ndemonstrate that our synthetic adversarial examples are not only realistic, but\nalso improve the attack success rate by up to 41.0\\% compared with the state of\nthe art adversarial attack methods including PGD attack.\n"], ["2019-10-04", "http://arxiv.org/abs/1910.02125", "Requirements for Developing Robust Neural Networks.", ["John S. Hyatt", " Michael S. Lee"], "  Validation accuracy is a necessary, but not sufficient, measure of a neural\nnetwork classifier's quality. High validation accuracy during development does\nnot guarantee that a model is free of serious flaws, such as vulnerability to\nadversarial attacks or a tendency to misclassify (with high confidence) data it\nwas not trained on. The model may also be incomprehensible to a human or base\nits decisions on unreasonable criteria. These problems, which are not unique to\nclassifiers, have been the focus of a substantial amount of recent research.\nHowever, they are not prioritized during model development, which almost always\noptimizes on validation accuracy to the exclusion of everything else. The\nproduct of this approach is likely to fail in unexpected ways outside of the\ntraining environment. We believe that, in addition to validation accuracy, the\nmodel development process must give added weight to other performance metrics\nsuch as explainability, resistance to adversarial attacks, and overconfidence\non out-of-distribution data.\n"], ["2019-10-04", "http://arxiv.org/abs/1910.02095", "Adversarial Examples for Cost-Sensitive Classifiers.", ["Gavin S. Hartnett", " Andrew J. Lohn", " Alexander P. Sedlack"], "  Motivated by safety-critical classification problems, we investigate\nadversarial attacks against cost-sensitive classifiers. We use current\nstate-of-the-art adversarially-resistant neural network classifiers [1] as the\nunderlying models. Cost-sensitive predictions are then achieved via a final\nprocessing step in the feed-forward evaluation of the network. We evaluate the\neffectiveness of cost-sensitive classifiers against a variety of attacks and we\nintroduce a new cost-sensitive attack which performs better than targeted\nattacks in some cases. We also explored the measures a defender can take in\norder to limit their vulnerability to these attacks. This attacker/defender\nscenario is naturally framed as a two-player zero-sum finite game which we\nanalyze using game theory.\n"], ["2019-10-03", "http://arxiv.org/abs/1910.01624", "Verification of Neural Network Behaviour: Formal Guarantees for Power System Applications.", ["Andreas Venzke", " Spyros Chatzivasileiadis"], "  This paper presents for the first time, to our knowledge, a framework for\nverifying neural network behavior in power systems applications. Up to this\nmoment, neural networks have been applied in power systems as a black-box; this\nhas presented a major barrier for their adoption in practice. Developing a\nrigorous framework based on mixed integer linear programming, our methods can\ndetermine the range of inputs that neural networks classify as safe or unsafe,\nand are able to identify adversarial examples. Such methods have the potential\nto build the missing trust of power system operators on neural networks, and\nunlock a series of new applications in power systems. This paper presents the\nmain theoretical framework and addresses concerns related to scalability and\naccuracy. We demonstrate our methods on the IEEE 9-bus, 14-bus, and 162-bus\nsystems, treating both N-1 security and small-signal stability.\n"], ["2019-10-03", "http://arxiv.org/abs/1910.01329", "Perturbations are not Enough: Generating Adversarial Examples with Spatial Distortions.", ["He Zhao", " Trung Le", " Paul Montague", " Vel Olivier De", " Tamas Abraham", " Dinh Phung"], "  Deep neural network image classifiers are reported to be susceptible to\nadversarial evasion attacks, which use carefully crafted images created to\nmislead a classifier. Recently, various kinds of adversarial attack methods\nhave been proposed, most of which focus on adding small perturbations to input\nimages. Despite the success of existing approaches, the way to generate\nrealistic adversarial images with small perturbations remains a challenging\nproblem. In this paper, we aim to address this problem by proposing a novel\nadversarial method, which generates adversarial examples by imposing not only\nperturbations but also spatial distortions on input images, including scaling,\nrotation, shear, and translation. As humans are less susceptible to small\nspatial distortions, the proposed approach can produce visually more realistic\nattacks with smaller perturbations, able to deceive classifiers without\naffecting human predictions. We learn our method by amortized techniques with\nneural networks and generate adversarial examples efficiently by a forward pass\nof the networks. Extensive experiments on attacking different types of\nnon-robustified classifiers and robust classifiers with defence show that our\nmethod has state-of-the-art performance in comparison with advanced attack\nparallels.\n"], ["2019-10-03", "http://arxiv.org/abs/1910.02785", "BUZz: BUffer Zones for defending adversarial examples in image classification.", ["Phuong Ha Nguyen", " Kaleel Mahmood", " Lam M. Nguyen", " Thanh Nguyen", " Dijk Marten van"], "  We propose a novel defense against all existing gradient based adversarial\nattacks on deep neural networks for image classification problems. Our defense\nis based on a combination of deep neural networks and simple image\ntransformations. While straight forward in implementation, this defense yields\na unique security property which we term buffer zones. In this paper, we\nformalize the concept of buffer zones. We argue that our defense based on\nbuffer zones is secure against state-of-the-art black box attacks. We are able\nto achieve this security even when the adversary has access to the {\\em entire}\noriginal training data set and unlimited query access to the defense. We verify\nour security claims through experimentation using FashionMNIST, CIFAR-10 and\nCIFAR-100. We demonstrate $<10\\%$ attack success rate -- significantly lower\nthan what other well-known defenses offer -- at only a price of a 15-20\\% drop\nin clean accuracy. By using a new intuitive metric we explain why this\ntrade-off offers a significant improvement over prior work.\n"], ["2019-10-02", "http://arxiv.org/abs/1910.00982", "Robust Few-Shot Learning with Adversarially Queried Meta-Learners.", ["Micah Goldblum", " Liam Fowl", " Tom Goldstein"], "  Previous work on adversarially robust neural networks requires large training\nsets and computationally expensive training procedures. On the other hand,\nfew-shot learning methods are highly vulnerable to adversarial examples. The\ngoal of our work is to produce networks which both perform well at few-shot\ntasks and are simultaneously robust to adversarial examples. We adapt\nadversarial training for meta-learning, we adapt robust architectural features\nto small networks for meta-learning, we test pre-processing defenses as an\nalternative to adversarial training for meta-learning, and we investigate the\nadvantages of robust meta-learning over robust transfer-learning for few-shot\ntasks. This work provides a thorough analysis of adversarially robust methods\nin the context of meta-learning, and we lay the foundation for future work on\ndefenses for few-shot tasks.\n"], ["2019-10-02", "http://arxiv.org/abs/1910.01907", "Attacking Vision-based Perception in End-to-End Autonomous Driving Models.", ["Adith Boloor", " Karthik Garimella", " Xin He", " Christopher Gill", " Yevgeniy Vorobeychik", " Xuan Zhang"], "  Recent advances in machine learning, especially techniques such as deep\nneural networks, are enabling a range of emerging applications. One such\nexample is autonomous driving, which often relies on deep learning for\nperception. However, deep learning-based perception has been shown to be\nvulnerable to a host of subtle adversarial manipulations of images.\nNevertheless, the vast majority of such demonstrations focus on perception that\nis disembodied from end-to-end control. We present novel end-to-end attacks on\nautonomous driving in simulation, using simple physically realizable attacks:\nthe painting of black lines on the road. These attacks target deep neural\nnetwork models for end-to-end autonomous driving control. A systematic\ninvestigation shows that such attacks are easy to engineer, and we describe\nscenarios (e.g., right turns) in which they are highly effective. We define\nseveral objective functions that quantify the success of an attack and develop\ntechniques based on Bayesian Optimization to efficiently traverse the search\nspace of higher dimensional attacks. Additionally, we define a novel class of\nhijacking attacks, where painted lines on the road cause the driver-less car to\nfollow a target path. Through the use of network deconvolution, we provide\ninsights into the successful attacks, which appear to work by mimicking\nactivations of entirely different scenarios. Our code is available at\nhttps://github.com/xz-group/AdverseDrive\n"], ["2019-10-01", "http://arxiv.org/abs/1910.00736", "Boosting Image Recognition with Non-differentiable Constraints.", ["Xuan Li", " Yuchen Lu", " Peng Xu", " Jizong Peng", " Christian Desrosiers", " Xue Liu"], "  In this paper, we study the problem of image recognition with\nnon-differentiable constraints. A lot of real-life recognition applications\nrequire a rich output structure with deterministic constraints that are\ndiscrete or modeled by a non-differentiable function. A prime example is\nrecognizing digit sequences, which are restricted by such rules (e.g.,\n\\textit{container code detection}, \\textit{social insurance number\nrecognition}, etc.). We investigate the usefulness of adding non-differentiable\nconstraints in learning for the task of digit sequence recognition. Toward this\ngoal, we synthesize six different datasets from MNIST and Cropped SVHN, with\nthree discrete rules inspired by real-life protocols. To deal with the\nnon-differentiability of these rules, we propose a reinforcement learning\napproach based on the policy gradient method. We find that incorporating this\nrule-based reinforcement can effectively increase the accuracy for all datasets\nand provide a good inductive bias which improves the model even with limited\ndata. On one of the datasets, MNIST\\_Rule2, models trained with rule-based\nreinforcement increase the accuracy by 4.7\\% for 2000 samples and 23.6\\% for\n500 samples. We further test our model against synthesized adversarial\nexamples, e.g., blocking out digits, and observe that adding our rule-based\nreinforcement increases the model robustness with a relatively smaller\nperformance drop.\n"], ["2019-10-01", "http://arxiv.org/abs/1910.00727", "Generating Semantic Adversarial Examples with Differentiable Rendering.", ["Lakshya Jain", " Wilson Wu", " Steven Chen", " Uyeong Jang", " Varun Chandrasekaran", " Sanjit Seshia", " Somesh Jha"], "  Machine learning (ML) algorithms, especially deep neural networks, have\ndemonstrated success in several domains. However, several types of attacks have\nraised concerns about deploying ML in safety-critical domains, such as\nautonomous driving and security. An attacker perturbs a data point slightly in\nthe concrete feature space (e.g., pixel space) and causes the ML algorithm to\nproduce incorrect output (e.g. a perturbed stop sign is classified as a yield\nsign). These perturbed data points are called adversarial examples, and there\nare numerous algorithms in the literature for constructing adversarial examples\nand defending against them. In this paper we explore semantic adversarial\nexamples (SAEs) where an attacker creates perturbations in the semantic space\nrepresenting the environment that produces input for the ML model. For example,\nan attacker can change the background of the image to be cloudier to cause\nmisclassification. We present an algorithm for constructing SAEs that uses\nrecent advances in differential rendering and inverse graphics.\n"], ["2019-10-01", "http://arxiv.org/abs/1910.00327", "Attacking CNN-based anti-spoofing face authentication in the physical domain.", ["Bowen Zhang", " Benedetta Tondi", " Mauro Barni"], "  In this paper, we study the vulnerability of anti-spoofing methods based on\ndeep learning against adversarial perturbations. We first show that attacking a\nCNN-based anti-spoofing face authentication system turns out to be a difficult\ntask. When a spoofed face image is attacked in the physical world, in fact, the\nattack has not only to remove the rebroadcast artefacts present in the image,\nbut it has also to take into account that the attacked image will be recaptured\nagain and then compensate for the distortions that will be re-introduced after\nthe attack by the subsequent rebroadcast process. Subsequently, we propose a\nmethod to craft robust physical domain adversarial images against anti-spoofing\nCNN-based face authentication. The attack built in this way can successfully\npass all the steps in the authentication chain (that is, face detection, face\nrecognition and spoofing detection), by achieving simultaneously the following\ngoals: i) make the spoofing detection fail; ii) let the facial region be\ndetected as a face and iii) recognized as belonging to the victim of the\nattack. The effectiveness of the proposed attack is validated experimentally\nwithin a realistic setting, by considering the REPLAY-MOBILE database, and by\nfeeding the adversarial images to a real face authentication system capturing\nthe input images through a mobile phone camera.\n"], ["2019-10-01", "http://arxiv.org/abs/1910.00511", "An Efficient and Margin-Approaching Zero-Confidence Adversarial Attack.", ["Yang Zhang", " Shiyu Chang", " Mo Yu", " Kaizhi Qian"], "  There are two major paradigms of white-box adversarial attacks that attempt\nto impose input perturbations. The first paradigm, called the fix-perturbation\nattack, crafts adversarial samples within a given perturbation level. The\nsecond paradigm, called the zero-confidence attack, finds the smallest\nperturbation needed to cause mis-classification, also known as the margin of an\ninput feature. While the former paradigm is well-resolved, the latter is not.\nExisting zero-confidence attacks either introduce significant ap-proximation\nerrors, or are too time-consuming. We therefore propose MARGINATTACK, a\nzero-confidence attack framework that is able to compute the margin with\nimproved accuracy and efficiency. Our experiments show that MARGINATTACK is\nable to compute a smaller margin than the state-of-the-art zero-confidence\nattacks, and matches the state-of-the-art fix-perturbation at-tacks. In\naddition, it runs significantly faster than the Carlini-Wagner attack,\ncurrently the most ac-curate zero-confidence attack algorithm.\n"], ["2019-10-01", "http://arxiv.org/abs/1910.00470", "Deep Neural Rejection against Adversarial Examples.", ["Angelo Sotgiu", " Ambra Demontis", " Marco Melis", " Battista Biggio", " Giorgio Fumera", " Xiaoyi Feng", " Fabio Roli"], "  Despite the impressive performances reported by deep neural networks in\ndifferent application domains, they remain largely vulnerable to adversarial\nexamples, i.e., input samples that are carefully perturbed to cause\nmisclassification at test time. In this work, we propose a deep neural\nrejection mechanism to detect adversarial examples, based on the idea of\nrejecting samples that exhibit anomalous feature representations at different\nnetwork layers. With respect to competing approaches, our method does not\nrequire generating adversarial examples at training time, and it is less\ncomputationally demanding. To properly evaluate our method, we define an\nadaptive white-box attack that is aware of the defense mechanism and aims to\nbypass it. Under this worst-case setting, we empirically show that our approach\noutperforms previously-proposed methods that detect adversarial examples by\nonly analyzing the feature representation provided by the output network layer.\n"], ["2019-10-01", "http://arxiv.org/abs/1910.01742", "Cross-Layer Strategic Ensemble Defense Against Adversarial Examples.", ["Wenqi Wei", " Ling Liu", " Margaret Loper", " Ka-Ho Chow", " Emre Gursoy", " Stacey Truex", " Yanzhao Wu"], "  Deep neural network (DNN) has demonstrated its success in multiple domains.\nHowever, DNN models are inherently vulnerable to adversarial examples, which\nare generated by adding adversarial perturbations to benign inputs to fool the\nDNN model to misclassify. In this paper, we present a cross-layer strategic\nensemble framework and a suite of robust defense algorithms, which are\nattack-independent, and capable of auto-repairing and auto-verifying the target\nmodel being attacked. Our strategic ensemble approach makes three original\ncontributions. First, we employ input-transformation diversity to design the\ninput-layer strategic transformation ensemble algorithms. Second, we utilize\nmodel-disagreement diversity to develop the output-layer strategic model\nensemble algorithms. Finally, we create an input-output cross-layer strategic\nensemble defense that strengthens the defensibility by combining diverse input\ntransformation based model ensembles with diverse output verification model\nensembles. Evaluated over 10 attacks on ImageNet dataset, we show that our\nstrategic ensemble defense algorithms can achieve high defense success rates\nand are more robust with high attack prevention success rates and low benign\nfalse negative rates, compared to existing representative defense methods.\n"], ["2019-09-30", "http://arxiv.org/abs/1909.13806", "Min-Max Optimization without Gradients: Convergence and Applications to Adversarial ML.", ["Sijia Liu", " Songtao Lu", " Xiangyi Chen", " Yao Feng", " Kaidi Xu", " Abdullah Al-Dujaili", " Minyi Hong", " Una-May Obelilly"], "  In this paper, we study the problem of constrained robust (min-max)\noptimization ina black-box setting, where the desired optimizer cannot access\nthe gradients of the objective function but may query its values. We present a\nprincipled optimization framework, integrating a zeroth-order (ZO) gradient\nestimator with an alternating projected stochastic gradient descent-ascent\nmethod, where the former only requires a small number of function queries and\nthe later needs just one-step descent/ascent update. We show that the proposed\nframework, referred to as ZO-Min-Max, has a sub-linear convergence rate under\nmild conditions and scales gracefully with problem size. From an application\nside, we explore a promising connection between black-box min-max optimization\nand black-box evasion and poisoning attacks in adversarial machine learning\n(ML). Our empirical evaluations on these use cases demonstrate the\neffectiveness of our approach and its scalability to dimensions that prohibit\nusing recent black-box solvers.\n"], ["2019-09-30", "http://arxiv.org/abs/1910.00068", "Adversarial Patches Exploiting Contextual Reasoning in Object Detection.", ["Aniruddha Saha", " Akshayvarun Subramanya", " Koninika Patil", " Hamed Pirsiavash"], "  The usefulness of spatial context in most fast object detection algorithms\nthat do a single forward pass per image is well known where they utilize\ncontext to improve their accuracy. In fact, they must do it to increase the\ninference speed by processing the image just once. We show that an adversary\ncan attack the model by exploiting contextual reasoning. We develop adversarial\nattack algorithms that make an object detector blind to a particular category\nchosen by the adversary even though the patch does not overlap with the missed\ndetections. We also show that limiting the use of contextual reasoning in\nlearning the object detector acts as a form of defense that improves the\naccuracy of the detector after an attack. We believe defending against our\npractical adversarial attack algorithms is not easy and needs attention from\nthe research community.\n"], ["2019-09-30", "http://arxiv.org/abs/1909.13857", "Black-box Adversarial Attacks with Bayesian Optimization.", ["Satya Narayan Shukla", " Anit Kumar Sahu", " Devin Willmott", " J. Zico Kolter"], "  We focus on the problem of black-box adversarial attacks, where the aim is to\ngenerate adversarial examples using information limited to loss function\nevaluations of input-output pairs. We use Bayesian optimization~(BO) to\nspecifically cater to scenarios involving low query budgets to develop query\nefficient adversarial attacks. We alleviate the issues surrounding BO in\nregards to optimizing high dimensional deep learning models by effective\ndimension upsampling techniques. Our proposed approach achieves performance\ncomparable to the state of the art black-box adversarial attacks albeit with a\nmuch lower average query count. In particular, in low query budget regimes, our\nproposed method reduces the query count up to $80\\%$ with respect to the state\nof the art methods.\n"], ["2019-09-29", "http://arxiv.org/abs/1910.06907", "Techniques for Adversarial Examples Threatening the Safety of Artificial Intelligence Based Systems. (81%)", ["Utku Kose"], "  Artificial intelligence is known as the most effective technological field\nfor rapid developments shaping the future of the world. Even today, it is\npossible to see intense use of intelligence systems in all fields of the life.\nAlthough advantages of the Artificial Intelligence are widely observed, there\nis also a dark side employing efforts to design hacking oriented techniques\nagainst Artificial Intelligence. Thanks to such techniques, it is possible to\ntrick intelligent systems causing directed results for unsuccessful outputs.\nThat is critical for also cyber wars of the future as it is predicted that the\nwars will be done unmanned, autonomous intelligent systems. Moving from the\nexplanations, objective of this study is to provide information regarding\nadversarial examples threatening the Artificial Intelligence and focus on\ndetails of some techniques, which are used for creating adversarial examples.\nAdversarial examples are known as training data, which can trick a Machine\nLearning technique to learn incorrectly about the target problem and cause an\nunsuccessful or maliciously directed intelligent system at the end. The study\nenables the readers to learn enough about details of recent techniques for\ncreating adversarial examples.\n"], ["2019-09-27", "http://arxiv.org/abs/1909.12734", "Maximal adversarial perturbations for obfuscation: Hiding certain attributes while preserving rest.", ["Indu Ilanchezian", " Praneeth Vepakomma", " Abhishek Singh", " Otkrist Gupta", " G. N. Srinivasa Prasanna", " Ramesh Raskar"], "  In this paper we investigate the usage of adversarial perturbations for the\npurpose of privacy from human perception and model (machine) based detection.\nWe employ adversarial perturbations for obfuscating certain variables in raw\ndata while preserving the rest. Current adversarial perturbation methods are\nused for data poisoning with minimal perturbations of the raw data such that\nthe machine learning model's performance is adversely impacted while the human\nvision cannot perceive the difference in the poisoned dataset due to minimal\nnature of perturbations. We instead apply relatively maximal perturbations of\nraw data to conditionally damage model's classification of one attribute while\npreserving the model performance over another attribute. In addition, the\nmaximal nature of perturbation helps adversely impact human perception in\nclassifying hidden attribute apart from impacting model performance. We\nvalidate our result qualitatively by showing the obfuscated dataset and\nquantitatively by showing the inability of models trained on clean data to\npredict the hidden attribute from the perturbed dataset while being able to\npredict the rest of attributes.\n"], ["2019-09-27", "http://arxiv.org/abs/1909.12741", "Impact of Low-bitwidth Quantization on the Adversarial Robustness for Embedded Neural Networks.", ["R\u00e9mi Bernhard", " Pierre-Alain Moellic", " Jean-Max Dutertre"], "  As the will to deploy neural networks models on embedded systems grows, and\nconsidering the related memory footprint and energy consumption issues, finding\nlighter solutions to store neural networks such as weight quantization and more\nefficient inference methods become major research topics. Parallel to that,\nadversarial machine learning has risen recently with an impressive and\nsignificant attention, unveiling some critical flaws of machine learning\nmodels, especially neural networks. In particular, perturbed inputs called\nadversarial examples have been shown to fool a model into making incorrect\npredictions. In this article, we investigate the adversarial robustness of\nquantized neural networks under different threat models for a classical\nsupervised image classification task. We show that quantization does not offer\nany robust protection, results in severe form of gradient masking and advance\nsome hypotheses to explain it. However, we experimentally observe poor\ntransferability capacities which we explain by quantization value shift\nphenomenon and gradient misalignment and explore how these results can be\nexploited with an ensemble-based defense.\n"], ["2019-09-26", "http://arxiv.org/abs/1909.12031", "Towards Understanding the Transferability of Deep Representations.", ["Hong Liu", " Mingsheng Long", " Jianmin Wang", " Michael I. Jordan"], "  Deep neural networks trained on a wide range of datasets demonstrate\nimpressive transferability. Deep features appear general in that they are\napplicable to many datasets and tasks. Such property is in prevalent use in\nreal-world applications. A neural network pretrained on large datasets, such as\nImageNet, can significantly boost generalization and accelerate training if\nfine-tuned to a smaller target dataset. Despite its pervasiveness, few effort\nhas been devoted to uncovering the reason of transferability in deep feature\nrepresentations. This paper tries to understand transferability from the\nperspectives of improved generalization, optimization and the feasibility of\ntransferability. We demonstrate that 1) Transferred models tend to find flatter\nminima, since their weight matrices stay close to the original flat region of\npretrained parameters when transferred to a similar target dataset; 2)\nTransferred representations make the loss landscape more favorable with\nimproved Lipschitzness, which accelerates and stabilizes training\nsubstantially. The improvement largely attributes to the fact that the\nprincipal component of gradient is suppressed in the pretrained parameters,\nthus stabilizing the magnitude of gradient in back-propagation. 3) The\nfeasibility of transferability is related to the similarity of both input and\nlabel. And a surprising discovery is that the feasibility is also impacted by\nthe training stages in that the transferability first increases during\ntraining, and then declines. We further provide a theoretical analysis to\nverify our observations.\n"], ["2019-09-26", "http://arxiv.org/abs/1909.12180", "Towards neural networks that provably know when they don't know.", ["Alexander Meinke", " Matthias Hein"], "  It has recently been shown that ReLU networks produce arbitrarily\nover-confident predictions far away from the training data. Thus, ReLU networks\ndo not know when they don't know. However, this is a highly important property\nin safety critical applications. In the context of out-of-distribution\ndetection (OOD) there have been a number of proposals to mitigate this problem\nbut none of them are able to make any mathematical guarantees. In this paper we\npropose a new approach to OOD which overcomes both problems. Our approach can\nbe used with ReLU networks and provides provably low confidence predictions far\naway from the training data as well as the first certificates for low\nconfidence predictions in a neighborhood of an out-distribution point. In the\nexperiments we show that state-of-the-art methods fail in this worst-case\nsetting whereas our model can guarantee its performance while retaining\nstate-of-the-art OOD performance.\n"], ["2019-09-26", "http://arxiv.org/abs/1909.12272", "Lower Bounds on Adversarial Robustness from Optimal Transport.", ["Arjun Nitin Bhagoji", " Daniel Cullina", " Prateek Mittal"], "  While progress has been made in understanding the robustness of machine\nlearning classifiers to test-time adversaries (evasion attacks), fundamental\nquestions remain unresolved. In this paper, we use optimal transport to\ncharacterize the minimum possible loss in an adversarial classification\nscenario. In this setting, an adversary receives a random labeled example from\none of two classes, perturbs the example subject to a neighborhood constraint,\nand presents the modified example to the classifier. We define an appropriate\ncost function such that the minimum transportation cost between the\ndistributions of the two classes determines the minimum $0-1$ loss for any\nclassifier. When the classifier comes from a restricted hypothesis class, the\noptimal transportation cost provides a lower bound. We apply our framework to\nthe case of Gaussian data with norm-bounded adversaries and explicitly show\nmatching bounds for the classification and transport problems as well as the\noptimality of linear classifiers. We also characterize the sample complexity of\nlearning in this setting, deriving and extending previously known results as a\nspecial case. Finally, we use our framework to study the gap between the\noptimal classification performance possible and that currently achieved by\nstate-of-the-art robustly trained neural networks for datasets of interest,\nnamely, MNIST, Fashion MNIST and CIFAR-10.\n"], ["2019-09-26", "http://arxiv.org/abs/1909.12167", "Adversarial Machine Learning Attack on Modulation Classification.", ["Muhammad Usama", " Muhammad Asim", " Junaid Qadir", " Ala Al-Fuqaha", " Muhammad Ali Imran"], "  Modulation classification is an important component of cognitive self-driving\nnetworks. Recently many ML-based modulation classification methods have been\nproposed. We have evaluated the robustness of 9 ML-based modulation classifiers\nagainst the powerful Carlini \\& Wagner (C-W) attack and showed that the current\nML-based modulation classifiers do not provide any deterrence against\nadversarial ML examples. To the best of our knowledge, we are the first to\nreport the results of the application of the C-W attack for creating\nadversarial examples against various ML models for modulation classification.\n"], ["2019-09-26", "http://arxiv.org/abs/1909.12161", "Adversarial ML Attack on Self Organizing Cellular Networks.", ["Salah-ud-din Farooq", " Muhammad Usama", " Junaid Qadir", " Muhammad Ali Imran"], "  Deep Neural Networks (DNN) have been widely adopted in self-organizing\nnetworks (SON) for automating different networking tasks. Recently, it has been\nshown that DNN lack robustness against adversarial examples where an adversary\ncan fool the DNN model into incorrect classification by introducing a small\nimperceptible perturbation to the original example. SON is expected to use DNN\nfor multiple fundamental cellular tasks and many DNN-based solutions for\nperforming SON tasks have been proposed in the literature have not been tested\nagainst adversarial examples. In this paper, we have tested and explained the\nrobustness of SON against adversarial example and investigated the performance\nof an important SON use case in the face of adversarial attacks. We have also\ngenerated explanations of incorrect classifications by utilizing an explainable\nartificial intelligence (AI) technique.\n"], ["2019-09-25", "http://arxiv.org/abs/1909.11786", "Probabilistic Modeling of Deep Features for Out-of-Distribution and Adversarial Detection.", ["Nilesh A. Ahuja", " Ibrahima Ndiour", " Trushant Kalyanpur", " Omesh Tickoo"], "  We present a principled approach for detecting out-of-distribution (OOD) and\nadversarial samples in deep neural networks. Our approach consists in modeling\nthe outputs of the various layers (deep features) with parametric probability\ndistributions once training is completed. At inference, the likelihoods of the\ndeep features w.r.t the previously learnt distributions are calculated and used\nto derive uncertainty estimates that can discriminate in-distribution samples\nfrom OOD samples. We explore the use of two classes of multivariate\ndistributions for modeling the deep features - Gaussian and Gaussian mixture -\nand study the trade-off between accuracy and computational complexity. We\ndemonstrate benefits of our approach on image features by detecting OOD images\nand adversarially-generated images, using popular DNN architectures on MNIST\nand CIFAR10 datasets. We show that more precise modeling of the feature\ndistributions result in significantly improved detection of OOD and adversarial\nsamples; up to 12 percentage points in AUPR and AUROC metrics. We further show\nthat our approach remains extremely effective when applied to video data and\nassociated spatio-temporal features by detecting adversarial samples on\nactivity classification tasks using UCF101 dataset, and the C3D network. To our\nknowledge, our methodology is the first one reported for reliably detecting\nwhite-box adversarial framing, a state-of-the-art adversarial attack for video\nclassifiers.\n"], ["2019-09-25", "http://arxiv.org/abs/1909.11515", "Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks.", ["Tianyu Pang", " Kun Xu", " Jun Zhu"], "  It has been widely recognized that adversarial examples can be easily crafted\nto fool deep networks, which mainly root from the locally non-linear behavior\nnearby input examples. Applying mixup in training provides an effective\nmechanism to improve generalization performance and model robustness against\nadversarial perturbations, which introduces the globally linear behavior\nin-between training examples. However, in previous work, the mixup-trained\nmodels only passively defend adversarial attacks in inference by directly\nclassifying the inputs, where the induced global linearity is not well\nexploited. Namely, since the locality of the adversarial perturbations, it\nwould be more efficient to actively break the locality via the globality of the\nmodel predictions. Inspired by simple geometric intuition, we develop an\ninference principle, named mixup inference (MI), for mixup-trained models. MI\nmixups the input with other random clean samples, which can shrink and transfer\nthe equivalent perturbation if the input is adversarial. Our experiments on\nCIFAR-10 and CIFAR-100 demonstrate that MI can further improve the adversarial\nrobustness for the models trained by mixup and its variants.\n"], ["2019-09-25", "http://arxiv.org/abs/1909.11764", "FreeLB: Enhanced Adversarial Training for Language Understanding.", ["Chen Zhu", " Yu Cheng", " Zhe Gan", " Siqi Sun", " Tom Goldstein", " Jingjing Liu"], "  Adversarial training, which minimizes the maximal risk for label-preserving\ninput perturbations, has proved to be effective for improving the\ngeneralization of language models. In this work, we propose a novel adversarial\ntraining algorithm - FreeLB, that promotes higher robustness and invariance in\nthe embedding space, by adding adversarial perturbations to word embeddings and\nminimizing the resultant adversarial risk inside different regions around input\nsamples. To validate the effectiveness of the proposed approach, we apply it to\nTransformer-based models for natural language understanding and commonsense\nreasoning tasks. Experiments on the GLUE benchmark show that when applied only\nto the finetuning stage, it is able to improve the overall test scores of\nBERT-based model from 78.3 to 79.4, and RoBERTa-large model from 88.5 to 88.8.\nIn addition, the proposed approach achieves state-of-the-art single-model test\naccuracies of 85.44% and 67.75% on ARC-Easy and ARC-Challenge. Experiments on\nCommonsenseQA benchmark further demonstrate that FreeLB can be generalized and\nboost the performance of RoBERTa-large model on other tasks as well.\n"], ["2019-09-24", "http://arxiv.org/abs/1909.11202", "A Visual Analytics Framework for Adversarial Text Generation.", ["Brandon Laughlin", " Christopher Collins", " Karthik Sankaranarayanan", " Khalil El-Khatib"], "  This paper presents a framework which enables a user to more easily make\ncorrections to adversarial texts. While attack algorithms have been\ndemonstrated to automatically build adversaries, changes made by the algorithms\ncan often have poor semantics or syntax. Our framework is designed to\nfacilitate human intervention by aiding users in making corrections. The\nframework extends existing attack algorithms to work within an evolutionary\nattack process paired with a visual analytics loop. Using an interactive\ndashboard a user is able to review the generation process in real time and\nreceive suggestions from the system for edits to be made. The adversaries can\nbe used to both diagnose robustness issues within a single classifier or to\ncompare various classifier options. With the weaknesses identified, the\nframework can also be used as a first step in mitigating adversarial threats.\nThe framework can be used as part of further research into defense methods in\nwhich the adversarial examples are used to evaluate new countermeasures. We\ndemonstrate the framework with a word swapping attack for the task of sentiment\nclassification.\n"], ["2019-09-24", "http://arxiv.org/abs/1909.11167", "Intelligent image synthesis to attack a segmentation CNN using adversarial learning.", ["Liang Chen", " Paul Bentley", " Kensaku Mori", " Kazunari Misawa", " Michitaka Fujiwara", " Daniel Rueckert"], "  Deep learning approaches based on convolutional neural networks (CNNs) have\nbeen successful in solving a number of problems in medical imaging, including\nimage segmentation. In recent years, it has been shown that CNNs are vulnerable\nto attacks in which the input image is perturbed by relatively small amounts of\nnoise so that the CNN is no longer able to perform a segmentation of the\nperturbed image with sufficient accuracy. Therefore, exploring methods on how\nto attack CNN-based models as well as how to defend models against attacks have\nbecome a popular topic as this also provides insights into the performance and\ngeneralization abilities of CNNs. However, most of the existing work assumes\nunrealistic attack models, i.e. the resulting attacks were specified in\nadvance. In this paper, we propose a novel approach for generating adversarial\nexamples to attack CNN-based segmentation models for medical images. Our\napproach has three key features: 1) The generated adversarial examples exhibit\nanatomical variations (in form of deformations) as well as appearance\nperturbations; 2) The adversarial examples attack segmentation models so that\nthe Dice scores decrease by a pre-specified amount; 3) The attack is not\nrequired to be specified beforehand. We have evaluated our approach on\nCNN-based approaches for the multi-organ segmentation problem in 2D CT images.\nWe show that the proposed approach can be used to attack different CNN-based\nsegmentation models.\n"], ["2019-09-24", "http://arxiv.org/abs/1909.10773", "Sign-OPT: A Query-Efficient Hard-label Adversarial Attack.", ["Minhao Cheng", " Simranjit Singh", " Patrick Chen", " Pin-Yu Chen", " Sijia Liu", " Cho-Jui Hsieh"], "  We study the most practical problem setup for evaluating adversarial\nrobustness of a machine learning system with limited access: the hard-label\nblack-box attack setting for generating adversarial examples, where limited\nmodel queries are allowed and only the decision is provided to a queried data\ninput. Several algorithms have been proposed for this problem but they\ntypically require huge amount (>20,000) of queries for attacking one example.\nAmong them, one of the state-of-the-art approaches (Cheng et al., 2019) showed\nthat hard-label attack can be modeled as an optimization problem where the\nobjective function can be evaluated by binary search with additional model\nqueries, thereby a zeroth order optimization algorithm can be applied. In this\npaper, we adopt the same optimization formulation but propose to directly\nestimate the sign of gradient at any direction instead of the gradient itself,\nwhich enjoys the benefit of single query. Using this single query oracle for\nretrieving sign of directional derivative, we develop a novel query-efficient\nSign-OPT approach for hard-label black-box attack. We provide a convergence\nanalysis of the new algorithm and conduct experiments on several models on\nMNIST, CIFAR-10 and ImageNet. We find that Sign-OPT attack consistently\nrequires 5X to 10X fewer queries when compared to the current state-of-the-art\napproaches, and usually converges to an adversarial example with smaller\nperturbation.\n"], ["2019-09-23", "http://arxiv.org/abs/1909.10480", "Adversarial Examples for Deep Learning Cyber Security Analytics.", ["Alesia Chernikova", " Alina Oprea"], "  As advances in Deep Neural Networks demonstrate unprecedented levels of\nperformance in many critical applications, their vulnerability to attacks is\nstill an open question. Adversarial examples are small modifications of\nlegitimate data points, resulting in mis-classification at testing time. As\nDeep Neural Networks found a wide range of applications to cyber security\nanalytics, it becomes important to study the robustness of these models in this\nsetting. We consider adversarial testing-time attacks against Deep Learning\nmodels designed for cyber security applications. In security applications,\nmachine learning models are not typically trained directly on the raw network\ntraffic or security logs, but on intermediate features defined by domain\nexperts. Existing attacks applied directly to the intermediate feature\nrepresentation result in violation of feature constraints, leading to invalid\nadversarial examples. We propose a general framework for crafting adversarial\nattacks that takes into consideration the mathematical dependencies between\nintermediate features in model input vector, as well as physical constraints\nimposed by the applications. We apply our methods on two security applications,\na malicious connection and a malicious domain classifier, to generate feasible\nadversarial examples in these domains. We show that with minimal effort (e.g.,\ngenerating 12 network connections), an attacker can change the prediction of a\nmodel from Malicious to Benign. We extensively evaluate the success of our\nattacks, and how they depend on several optimization objectives and imbalance\nratios in the training data.\n"], ["2019-09-23", "http://arxiv.org/abs/1909.10147", "Robust Local Features for Improving the Generalization of Adversarial Training.", ["Chubiao Song", " Kun He", " Jiadong Lin", " Liwei Wang", " John E. Hopcroft"], "  Adversarial training has been demonstrated as one of the most effective\nmethods for training robust models so as to defend against adversarial\nexamples. However, adversarial training often lacks adversarially robust\ngeneralization on unseen data. Recent works show that adversarially trained\nmodels may be more biased towards global structure features. Instead, in this\nwork, we would like to investigate the relationship between the generalization\nof adversarial training and the robust local features, as the local features\ngeneralize well for unseen shape variation. To learn the robust local features,\nwe develop a Random Block Shuffle (RBS) transformation to break up the global\nstructure features on normal adversarial examples. We continue to propose a new\napproach called Robust Local Features for Adversarial Training (RLFAT), which\nfirst learns the robust local features by adversarial training on the\nRBS-transformed adversarial examples, and then transfers the robust local\nfeatures into the training of normal adversarial examples. Finally, we\nimplement RLFAT in two currently state-of-the-art adversarial training\nframeworks. Extensive experiments on STL-10, CIFAR-10, CIFAR-100 datasets show\nthat RLFAT improves the adversarially robust generalization as well as the\nstandard generalization of adversarial training. Additionally, we demonstrate\nthat our method captures more local features of the object, aligning better\nwith human perception.\n"], ["2019-09-23", "http://arxiv.org/abs/1909.10594", "MemGuard: Defending against Black-Box Membership Inference Attacks via Adversarial Examples.", ["Jinyuan Jia", " Ahmed Salem", " Michael Backes", " Yang Zhang", " Neil Zhenqiang Gong"], "  In a membership inference attack, an attacker aims to infer whether a data\nsample is in a target classifier's training dataset or not. Specifically, given\na black-box access to the target classifier, the attacker trains a binary\nclassifier, which takes a data sample's confidence score vector predicted by\nthe target classifier as an input and predicts the data sample to be a member\nor non-member of the target classifier's training dataset. Membership inference\nattacks pose severe privacy and security threats to the training dataset. Most\nexisting defenses leverage differential privacy when training the target\nclassifier or regularize the training process of the target classifier. These\ndefenses suffer from two key limitations: 1) they do not have formal\nutility-loss guarantees of the confidence score vectors, and 2) they achieve\nsuboptimal privacy-utility tradeoffs.\n  In this work, we propose MemGuard, the first defense with formal utility-loss\nguarantees against black-box membership inference attacks. Instead of tampering\nthe training process of the target classifier, MemGuard adds noise to each\nconfidence score vector predicted by the target classifier. Our key observation\nis that attacker uses a classifier to predict member or non-member and\nclassifier is vulnerable to adversarial examples. Based on the observation, we\npropose to add a carefully crafted noise vector to a confidence score vector to\nturn it into an adversarial example that misleads the attacker's classifier.\nOur experimental results on three datasets show that MemGuard can effectively\ndefend against membership inference attacks and achieve better privacy-utility\ntradeoffs than existing defenses. Our work is the first one to show that\nadversarial examples can be used as defensive mechanisms to defend against\nmembership inference attacks.\n"], ["2019-09-22", "http://arxiv.org/abs/1909.09938", "HAWKEYE: Adversarial Example Detector for Deep Neural Networks.", ["Jinkyu Koo", " Michael Roth", " Saurabh Bagchi"], "  Adversarial examples (AEs) are images that can mislead deep neural network\n(DNN) classifiers via introducing slight perturbations into original images.\nRecent work has shown that detecting AEs can be more effective against AEs than\npreventing them from being generated. However, the state-of-the-art AE\ndetection still shows a high false positive rate, thereby rejecting a\nconsiderable amount of normal images. To address this issue, we propose\nHAWKEYE, which is a separate neural network that analyzes the output layer of\nthe DNN, and detects AEs. HAWKEYE's AE detector utilizes a quantized version of\nan input image as a reference, and is trained to distinguish the variation\ncharacteristics of the DNN output on an input image from the DNN output on its\nreference image. We also show that cascading our AE detectors that are trained\nfor different quantization step sizes can drastically reduce a false positive\nrate, while keeping a detection rate high.\n"], ["2019-09-20", "http://arxiv.org/abs/1909.09481", "Adversarial Learning with Margin-based Triplet Embedding Regularization.", ["Yaoyao Zhong", " Weihong Deng"], "  The Deep neural networks (DNNs) have achieved great success on a variety of\ncomputer vision tasks, however, they are highly vulnerable to adversarial\nattacks. To address this problem, we propose to improve the local smoothness of\nthe representation space, by integrating a margin-based triplet embedding\nregularization term into the classification objective, so that the obtained\nmodel learns to resist adversarial examples. The regularization term consists\nof two steps optimizations which find potential perturbations and punish them\nby a large margin in an iterative way. Experimental results on MNIST,\nCASIA-WebFace, VGGFace2 and MS-Celeb-1M reveal that our approach increases the\nrobustness of the network against both feature and label adversarial attacks in\nsimple object classification and deep face recognition.\n"], ["2019-09-20", "http://arxiv.org/abs/1909.09552", "Defending Against Physically Realizable Attacks on Image Classification.", ["Tong Wu", " Liang Tong", " Yevgeniy Vorobeychik"], "  We study the problem of defending deep neural network approaches for image\nclassification from physically realizable attacks. First, we demonstrate that\nthe two most scalable and effective methods for learning robust models,\nadversarial training with PGD attacks and randomized smoothing, exhibit very\nlimited effectiveness against three of the highest profile physical attacks.\nNext, we propose a new abstract adversarial model, rectangular occlusion\nattacks, in which an adversary places a small adversarially crafted rectangle\nin an image, and develop two approaches for efficiently computing the resulting\nadversarial examples. Finally, we demonstrate that adversarial training using\nour new attack yields image classification models that exhibit high robustness\nagainst the physically realizable attacks we study, offering the first\neffective generic defense against such attacks.\n"], ["2019-09-20", "http://arxiv.org/abs/1909.09735", "COPYCAT: Practical Adversarial Attacks on Visualization-Based Malware Detection.", ["Aminollah Khormali", " Ahmed Abusnaina", " Songqing Chen", " DaeHun Nyang", " Aziz Mohaisen"], "  Despite many attempts, the state-of-the-art of adversarial machine learning\non malware detection systems generally yield unexecutable samples. In this\nwork, we set out to examine the robustness of visualization-based malware\ndetection system against adversarial examples (AEs) that not only are able to\nfool the model, but also maintain the executability of the original input. As\nsuch, we first investigate the application of existing off-the-shelf\nadversarial attack approaches on malware detection systems through which we\nfound that those approaches do not necessarily maintain the functionality of\nthe original inputs. Therefore, we proposed an approach to generate adversarial\nexamples, COPYCAT, which is specifically designed for malware detection systems\nconsidering two main goals; achieving a high misclassification rate and\nmaintaining the executability and functionality of the original input. We\ndesigned two main configurations for COPYCAT, namely AE padding and sample\ninjection. While the first configuration results in untargeted\nmisclassification attacks, the sample injection configuration is able to force\nthe model to generate a targeted output, which is highly desirable in the\nmalware attribution setting. We evaluate the performance of COPYCAT through an\nextensive set of experiments on two malware datasets, and report that we were\nable to generate adversarial samples that are misclassified at a rate of 98.9%\nand 96.5% with Windows and IoT binary datasets, respectively, outperforming the\nmisclassification rates in the literature. Most importantly, we report that\nthose AEs were executable unlike AEs generated by off-the-shelf approaches. Our\ntransferability study demonstrates that the generated AEs through our proposed\nmethod can be generalized to other models.\n"], ["2019-09-19", "http://arxiv.org/abs/1909.09263", "Propagated Perturbation of Adversarial Attack for well-known CNNs: Empirical Study and its Explanation.", ["Jihyeun Yoon", " Kyungyul Kim", " Jongseong Jang"], "  Deep Neural Network based classifiers are known to be vulnerable to\nperturbations of inputs constructed by an adversarial attack to force\nmisclassification. Most studies have focused on how to make vulnerable noise by\ngradient based attack methods or to defense model from adversarial attack. The\nuse of the denoiser model is one of a well-known solution to reduce the\nadversarial noise although classification performance had not significantly\nimproved. In this study, we aim to analyze the propagation of adversarial\nattack as an explainable AI(XAI) point of view. Specifically, we examine the\ntrend of adversarial perturbations through the CNN architectures. To analyze\nthe propagated perturbation, we measured normalized Euclidean Distance and\ncosine distance in each CNN layer between the feature map of the perturbed\nimage passed through denoiser and the non-perturbed original image. We used\nfive well-known CNN based classifiers and three gradient-based adversarial\nattacks. From the experimental results, we observed that in most cases,\nEuclidean Distance explosively increases in the final fully connected layer\nwhile cosine distance fluctuated and disappeared at the last layer. This means\nthat the use of denoiser can decrease the amount of noise. However, it failed\nto defense accuracy degradation.\n"], ["2019-09-19", "http://arxiv.org/abs/1909.08864", "Adversarial Vulnerability Bounds for Gaussian Process Classification.", ["Michael Thomas Smith", " Kathrin Grosse", " Michael Backes", " Mauricio A Alvarez"], "  Machine learning (ML) classification is increasingly used in safety-critical\nsystems. Protecting ML classifiers from adversarial examples is crucial. We\npropose that the main threat is that of an attacker perturbing a confidently\nclassified input to produce a confident misclassification. To protect against\nthis we devise an adversarial bound (AB) for a Gaussian process classifier,\nthat holds for the entire input domain, bounding the potential for any future\nadversarial method to cause such misclassification. This is a formal guarantee\nof robustness, not just an empirically derived result. We investigate how to\nconfigure the classifier to maximise the bound, including the use of a sparse\napproximation, leading to the method producing a practical, useful and provably\nrobust classifier, which we test using a variety of datasets.\n"], ["2019-09-19", "http://arxiv.org/abs/1909.08830", "Absum: Simple Regularization Method for Reducing Structural Sensitivity of Convolutional Neural Networks.", ["Sekitoshi Kanai", " Yasutoshi Ida", " Yasuhiro Fujiwara", " Masanori Yamada", " Shuichi Adachi"], "  We propose Absum, which is a regularization method for improving adversarial\nrobustness of convolutional neural networks (CNNs). Although CNNs can\naccurately recognize images, recent studies have shown that the convolution\noperations in CNNs commonly have structural sensitivity to specific noise\ncomposed of Fourier basis functions. By exploiting this sensitivity, they\nproposed a simple black-box adversarial attack: Single Fourier attack. To\nreduce structural sensitivity, we can use regularization of convolution filter\nweights since the sensitivity of linear transform can be assessed by the norm\nof the weights. However, standard regularization methods can prevent\nminimization of the loss function because they impose a tight constraint for\nobtaining high robustness. To solve this problem, Absum imposes a loose\nconstraint; it penalizes the absolute values of the summation of the parameters\nin the convolution layers. Absum can improve robustness against single Fourier\nattack while being as simple and efficient as standard regularization methods\n(e.g., weight decay and L1 regularization). Our experiments demonstrate that\nAbsum improves robustness against single Fourier attack more than standard\nregularization methods. Furthermore, we reveal that robust CNNs with Absum are\nmore robust against transferred attacks due to decreasing the common\nsensitivity and against high-frequency noise than standard regularization\nmethods. We also reveal that Absum can improve robustness against\ngradient-based attacks (projected gradient descent) when used with adversarial\ntraining.\n"], ["2019-09-19", "http://arxiv.org/abs/1909.09034", "Training Robust Deep Neural Networks via Adversarial Noise Propagation.", ["Aishan Liu", " Xianglong Liu", " Chongzhi Zhang", " Hang Yu", " Qiang Liu", " Junfeng He"], "  Deep neural networks have been found vulnerable to noises like adversarial\nexamples and corruption in practice. A number of adversarial defense methods\nhave been developed, which indeed improve the model robustness towards\nadversarial examples in practice. However, only relying on training with the\ndata mixed with noises, most of them still fail to defend the generalized types\nof noises. Motivated by the fact that hidden layers play a very important role\nin maintaining a robust model, this paper comes up with a simple yet powerful\ntraining algorithm named Adversarial Noise Propagation (ANP) that injects\ndiversified noises into the hidden layers in a layer-wise manner. We show that\nANP can be efficiently implemented by exploiting the nature of the popular\nbackward-forward training style for deep models. To comprehensively understand\nthe behaviors and contributions of hidden layers, we further explore the\ninsights from hidden representation insensitivity and human vision perception\nalignment. Extensive experiments on MNIST, CIFAR-10, CIFAR-10-C, CIFAR-10-P and\nImageNet demonstrate that ANP enables the strong robustness for deep models\nagainst the generalized noises including both adversarial and corrupted ones,\nand significantly outperforms various adversarial defense methods.\n"], ["2019-09-19", "http://arxiv.org/abs/1909.12927", "Toward Robust Image Classification.", ["Basemah Alshemali", " Alta Graham", " Jugal Kalita"], "  Neural networks are frequently used for image classification, but can be\nvulnerable to misclassification caused by adversarial images. Attempts to make\nneural network image classification more robust have included variations on\npreprocessing (cropping, applying noise, blurring), adversarial training, and\ndropout randomization. In this paper, we implemented a model for adversarial\ndetection based on a combination of two of these techniques: dropout\nrandomization with preprocessing applied to images within a given Bayesian\nuncertainty. We evaluated our model on the MNIST dataset, using adversarial\nimages generated using Fast Gradient Sign Method (FGSM), Jacobian-based\nSaliency Map Attack (JSMA) and Basic Iterative Method (BIM) attacks. Our model\nachieved an average adversarial image detection accuracy of 97%, with an\naverage image classification accuracy, after discarding images flagged as\nadversarial, of 99%. Our average detection accuracy exceeded that of recent\npapers using similar techniques.\n"], ["2019-09-17", "http://arxiv.org/abs/1909.08072", "Adversarial Attacks and Defenses in Images, Graphs and Text: A Review.", ["Han Xu", " Yao Ma", " Haochen Liu", " Debayan Deb", " Hui Liu", " Jiliang Tang", " Anil Jain"], "  Deep neural networks (DNN) have achieved unprecedented success in numerous\nmachine learning tasks in various domains. However, the existence of\nadversarial examples raises our concerns in adopting deep learning to\nsafety-critical applications. As a result, we have witnessed increasing\ninterests in studying attack and defense mechanisms for DNN models on different\ndata types, such as images, graphs and text. Thus, it is necessary to provide a\nsystematic and comprehensive overview of the main threats of attacks and the\nsuccess of corresponding countermeasures. In this survey, we review the state\nof the art algorithms for generating adversarial examples and the\ncountermeasures against adversarial examples, for three most popular data\ntypes, including images, graphs and text.\n"], ["2019-09-17", "http://arxiv.org/abs/1909.07873", "Generating Black-Box Adversarial Examples for Text Classifiers Using a Deep Reinforced Model.", ["Prashanth Vijayaraghavan", " Deb Roy"], "  Recently, generating adversarial examples has become an important means of\nmeasuring robustness of a deep learning model. Adversarial examples help us\nidentify the susceptibilities of the model and further counter those\nvulnerabilities by applying adversarial training techniques. In natural\nlanguage domain, small perturbations in the form of misspellings or paraphrases\ncan drastically change the semantics of the text. We propose a reinforcement\nlearning based approach towards generating adversarial examples in black-box\nsettings. We demonstrate that our method is able to fool well-trained models\nfor (a) IMDB sentiment classification task and (b) AG's news corpus news\ncategorization task with significantly high success rates. We find that the\nadversarial examples generated are semantics-preserving perturbations to the\noriginal text.\n"], ["2019-09-17", "http://arxiv.org/abs/1909.08526", "Defending against Machine Learning based Inference Attacks via Adversarial Examples: Opportunities and Challenges.", ["Jinyuan Jia", " Neil Zhenqiang Gong"], "  As machine learning (ML) becomes more and more powerful and easily\naccessible, attackers increasingly leverage ML to perform automated large-scale\ninference attacks in various domains. In such an ML-equipped inference attack,\nan attacker has access to some data (called public data) of an individual, a\nsoftware, or a system; and the attacker uses an ML classifier to automatically\ninfer their private data. Inference attacks pose severe privacy and security\nthreats to individuals and systems. Inference attacks are successful because\nprivate data are statistically correlated with public data, and ML classifiers\ncan capture such statistical correlations. In this chapter, we discuss the\nopportunities and challenges of defending against ML-equipped inference attacks\nvia adversarial examples. Our key observation is that attackers rely on ML\nclassifiers in inference attacks. The adversarial machine learning community\nhas demonstrated that ML classifiers have various vulnerabilities. Therefore,\nwe can turn the vulnerabilities of ML into defenses against inference attacks.\nFor example, ML classifiers are vulnerable to adversarial examples, which add\ncarefully crafted noise to normal examples such that an ML classifier makes\npredictions for the examples as we desire. To defend against inference attacks,\nwe can add carefully crafted noise into the public data to turn them into\nadversarial examples, such that attackers' classifiers make incorrect\npredictions for the private data. However, existing methods to construct\nadversarial examples are insufficient because they did not consider the unique\nchallenges and requirements for the crafted noise at defending against\ninference attacks. In this chapter, we take defending against inference attacks\nin online social networks as an example to illustrate the opportunities and\nchallenges.\n"], ["2019-09-16", "http://arxiv.org/abs/1909.07558", "HAD-GAN: A Human-perception Auxiliary Defense GAN model to Defend Adversarial Examples.", ["Wanting Yu", " Hongyi Yu", " Lingyun Jiang", " Mengli Zhang", " Kai Qiao", " Linyuan Wang", " Bin Yan"], "  Adversarial examples reveal the vulnerability and unexplained nature of\nneural networks. It is of great practical significance to study the defense of\nadversarial examples. In fact, most adversarial examples that misclassify\nnetworks are often undetectable by humans. In this paper, we propose a defense\nmodel to train the classifier into a human-perception classification model with\nshape preference. The proposed model consisting of a TTN (Texture Transfer\nNetwork) and an auxiliary defense GAN (Generative Adversarial Networks) is\ncalled HAD-GAN (Human-perception Auxiliary Defense GAN). The TTN is used to\nextend the texture samples of a clean image and makes classifiers more focused\non its shape. And GAN is utilized to form a training framework for the model\nand generate the images we need. A series of experiments conducted on MNIST,\nFashion-MNIST and CIFAR10 show that the proposed model outperforms the\nstate-of-the-art defense methods for network robust, and have a significant\nimprovement on defense ability of adversarial examples.\n"], ["2019-09-16", "http://arxiv.org/abs/1909.07490", "They Might NOT Be Giants: Crafting Black-Box Adversarial Examples with Fewer Queries Using Particle Swarm Optimization.", ["Rayan Mosli", " Matthew Wright", " Bo Yuan", " Yin Pan"], "  Machine learning models have been found to be susceptible to adversarial\nexamples that are often indistinguishable from the original inputs. These\nadversarial examples are created by applying adversarial perturbations to input\nsamples, which would cause them to be misclassified by the target models.\nAttacks that search and apply the perturbations to create adversarial examples\nare performed in both white-box and black-box settings, depending on the\ninformation available to the attacker about the target. For black-box attacks,\nthe only capability available to the attacker is the ability to query the\ntarget with specially crafted inputs and observing the labels returned by the\nmodel. Current black-box attacks either have low success rates, requires a high\nnumber of queries, or produce adversarial examples that are easily\ndistinguishable from their sources. In this paper, we present AdversarialPSO, a\nblack-box attack that uses fewer queries to create adversarial examples with\nhigh success rates. AdversarialPSO is based on the evolutionary search\nalgorithm Particle Swarm Optimization, a populationbased gradient-free\noptimization algorithm. It is flexible in balancing the number of queries\nsubmitted to the target vs the quality of imperceptible adversarial examples.\nThe attack has been evaluated using the image classification benchmark datasets\nCIFAR-10, MNIST, and Imagenet, achieving success rates of 99.6%, 96.3%, and\n82.0%, respectively, while submitting substantially fewer queries than the\nstate-of-the-art. We also present a black-box method for isolating salient\nfeatures used by models when making classifications. This method, called Swarms\nwith Individual Search Spaces or SWISS, creates adversarial examples by finding\nand modifying the most important features in the input.\n"], ["2019-09-16", "http://arxiv.org/abs/1909.07283", "Towards Quality Assurance of Software Product Lines with Adversarial Configurations.", ["Paul Temple", " Mathieu Acher", " Gilles Perrouin", " Battista Biggio", " Jean-marc Jezequel", " Fabio Roli"], "  Software product line (SPL) engineers put a lot of effort to ensure that,\nthrough the setting of a large number of possible configuration options,\nproducts are acceptable and well-tailored to customers' needs. Unfortunately,\noptions and their mutual interactions create a huge configuration space which\nis intractable to exhaustively explore. Instead of testing all products,\nmachine learning techniques are increasingly employed to approximate the set of\nacceptable products out of a small training sample of configurations. Machine\nlearning (ML) techniques can refine a software product line through learned\nconstraints and a priori prevent non-acceptable products to be derived. In this\npaper, we use adversarial ML techniques to generate adversarial configurations\nfooling ML classifiers and pinpoint incorrect classifications of products\n(videos) derived from an industrial video generator. Our attacks yield (up to)\na 100% misclassification rate and a drop in accuracy of 5%. We discuss the\nimplications these results have on SPL quality assurance.\n"], ["2019-09-16", "http://arxiv.org/abs/1909.06978", "Interpreting and Improving Adversarial Robustness with Neuron Sensitivity.", ["Chongzhi Zhang", " Aishan Liu", " Xianglong Liu", " Yitao Xu", " Hang Yu", " Yuqing Ma", " Tianlin Li"], "  Deep neural networks (DNNs) are vulnerable to adversarial examples where\ninputs with imperceptible perturbations mislead DNNs to incorrect results.\nDespite the potential risk they bring, adversarial examples are also valuable\nfor providing insights into the weakness and blind-spots of DNNs. Thus, the\ninterpretability of a DNN in adversarial setting aims to explain the rationale\nbehind its decision-making process and makes deeper understanding which results\nin better practical applications. To address this issue, we try to explain\nadversarial robustness for deep models from a new perspective of neuron\nsensitivity which is measured by neuron behavior variation intensity against\nbenign and adversarial examples. In this paper, we first draw the close\nconnection between adversarial robustness and neuron sensitivities, as\nsensitive neurons make the most non-trivial contributions to model predictions\nin adversarial setting. Based on that, we further propose to improve\nadversarial robustness by constraining the similarities of sensitive neurons\nbetween benign and adversarial examples which stabilizes the behaviors of\nsensitive neurons in adversarial setting. Moreover, we demonstrate that\nstate-of-the-art adversarial training methods improve model robustness by\nreducing neuron sensitivities which in turn confirms the strong connections\nbetween adversarial robustness and neuron sensitivity as well as the\neffectiveness of using sensitive neurons to build robust models. Extensive\nexperiments on various datasets demonstrate that our algorithm effectively\nachieve excellent results.\n"], ["2019-09-15", "http://arxiv.org/abs/1909.06727", "An Empirical Study towards Characterizing Deep Learning Development and Deployment across Different Frameworks and Platforms.", ["Qianyu Guo", " Sen Chen", " Xiaofei Xie", " Lei Ma", " Qiang Hu", " Hongtao Liu", " Yang Liu", " Jianjun Zhao", " Xiaohong Li"], "  Deep Learning (DL) has recently achieved tremendous success. A variety of DL\nframeworks and platforms play a key role to catalyze such progress. However,\nthe differences in architecture designs and implementations of existing\nframeworks and platforms bring new challenges for DL software development and\ndeployment. Till now, there is no study on how various mainstream frameworks\nand platforms influence both DL software development and deployment in\npractice. To fill this gap, we take the first step towards understanding how\nthe most widely-used DL frameworks and platforms support the DL software\ndevelopment and deployment. We conduct a systematic study on these frameworks\nand platforms by using two types of DNN architectures and three popular\ndatasets. (1) For development process, we investigate the prediction accuracy\nunder the same runtime training configuration or same model weights/biases. We\nalso study the adversarial robustness of trained models by leveraging the\nexisting adversarial attack techniques. The experimental results show that the\ncomputing differences across frameworks could result in an obvious prediction\naccuracy decline, which should draw the attention of DL developers. (2) For\ndeployment process, we investigate the prediction accuracy and performance\n(refers to time cost and memory consumption) when the trained models are\nmigrated/quantized from PC to real mobile devices and web browsers. The DL\nplatform study unveils that the migration and quantization still suffer from\ncompatibility and reliability issues. Meanwhile, we find several DL software\nbugs by using the results as a benchmark. We further validate the results\nthrough bug confirmation from stakeholders and industrial positive feedback to\nhighlight the implications of our study. Through our study, we summarize\npractical guidelines, identify challenges and pinpoint new research directions.\n"], ["2019-09-15", "http://arxiv.org/abs/1909.06872", "Detecting Adversarial Samples Using Influence Functions and Nearest Neighbors.", ["Gilad Cohen", " Guillermo Sapiro", " Raja Giryes"], "  Deep neural networks (DNNs) are notorious for their vulnerability to\nadversarial attacks, which are small perturbations added to their input images\nto mislead their prediction. Detection of adversarial examples is, therefore, a\nfundamental requirement for robust classification frameworks. In this work, we\npresent a method for detecting such adversarial attacks, which is suitable for\nany pre-trained neural network classifier. We use influence functions to\nmeasure the impact of every training sample on the validation set data. From\nthe influence scores, we find the most supportive training samples for any\ngiven validation example. A k-nearest neighbor (k-NN) model fitted on the DNN's\nactivation layers is employed to search for the ranking of these supporting\ntraining samples. We observe that these samples are highly correlated with the\nnearest neighbors of the normal inputs, while this correlation is much weaker\nfor adversarial inputs. We train an adversarial detector using the k-NN ranks\nand distances and show that it successfully distinguishes adversarial examples,\ngetting state-of-the-art results on four attack methods with three datasets.\n"], ["2019-09-14", "http://arxiv.org/abs/1909.06723", "Natural Language Adversarial Attacks and Defenses in Word Level.", ["Xiaosen Wang", " Hao Jin", " Kun He"], "  Up until recent two years, inspired by the big amount of research about\nadversarial example in the field of computer vision, there has been a growing\ninterest in adversarial attacks for Natural Language Processing (NLP). What\nfollowed was a very few works of adversarial defense for NLP. However, there\nexists no defense method against the successful synonyms substitution based\nattacks that aim to satisfy all the lexical, grammatical, semantic constraints\nand thus are hard to perceived by humans. To fill this gap, we postulate the\ngeneralization of the model leads to the existence of adversarial examples, and\npropose an adversarial defense method called Synonyms Encoding Method (SEM),\nwhich inserts an encoder before the input layer of the model and then trains\nthe model to eliminate adversarial perturbations. Extensive experiments\ndemonstrate that SEM can efficiently defend current best synonym substitution\nbased adversarial attacks with almost no decay on the accuracy for benign\nexamples. Besides, to better evaluate SEM, we also propose a strong attack\nmethod called Improved Genetic Algorithm (IGA) that adopts the genetic\nmetaheuristic against synonyms substitution based attacks. Compared with\nexisting genetic based adversarial attack, the proposed IGA can achieve higher\nattack success rate at the same time maintain the transferability of\nadversarial examples.\n"], ["2019-09-13", "http://arxiv.org/abs/1909.06500", "Adversarial Attack on Skeleton-based Human Action Recognition.", ["Jian Liu", " Naveed Akhtar", " Ajmal Mian"], "  Deep learning models achieve impressive performance for skeleton-based human\naction recognition. However, the robustness of these models to adversarial\nattacks remains largely unexplored due to their complex spatio-temporal nature\nthat must represent sparse and discrete skeleton joints. This work presents the\nfirst adversarial attack on skeleton-based action recognition with graph\nconvolutional networks. The proposed targeted attack, termed Constrained\nIterative Attack for Skeleton Actions (CIASA), perturbs joint locations in an\naction sequence such that the resulting adversarial sequence preserves the\ntemporal coherence, spatial integrity, and the anthropomorphic plausibility of\nthe skeletons. CIASA achieves this feat by satisfying multiple physical\nconstraints, and employing spatial skeleton realignments for the perturbed\nskeletons along with regularization of the adversarial skeletons with\nGenerative networks. We also explore the possibility of semantically\nimperceptible localized attacks with CIASA, and succeed in fooling the\nstate-of-the-art skeleton action recognition models with high confidence. CIASA\nperturbations show high transferability for black-box attacks. We also show\nthat the perturbed skeleton sequences are able to induce adversarial behavior\nin the RGB videos created with computer graphics. A comprehensive evaluation\nwith NTU and Kinetics datasets ascertains the effectiveness of CIASA for\ngraph-based skeleton action recognition and reveals the imminent threat to the\nspatio-temporal deep learning tasks in general.\n"], ["2019-09-13", "http://arxiv.org/abs/1909.06044", "Say What I Want: Towards the Dark Side of Neural Dialogue Models.", ["Haochen Liu", " Tyler Derr", " Zitao Liu", " Jiliang Tang"], "  Neural dialogue models have been widely adopted in various chatbot\napplications because of their good performance in simulating and generalizing\nhuman conversations. However, there exists a dark side of these models -- due\nto the vulnerability of neural networks, a neural dialogue model can be\nmanipulated by users to say what they want, which brings in concerns about the\nsecurity of practical chatbot services. In this work, we investigate whether we\ncan craft inputs that lead a well-trained black-box neural dialogue model to\ngenerate targeted outputs. We formulate this as a reinforcement learning (RL)\nproblem and train a Reverse Dialogue Generator which efficiently finds such\ninputs for targeted outputs. Experiments conducted on a representative neural\ndialogue model show that our proposed model is able to discover such desired\ninputs in a considerable portion of cases. Overall, our work reveals this\nweakness of neural dialogue models and may prompt further researches of\ndeveloping corresponding solutions to avoid it.\n"], ["2019-09-13", "http://arxiv.org/abs/1909.06271", "White-Box Adversarial Defense via Self-Supervised Data Estimation.", ["Zudi Lin", " Hanspeter Pfister", " Ziming Zhang"], "  In this paper, we study the problem of how to defend classifiers against\nadversarial attacks that fool the classifiers using subtly modified input data.\nIn contrast to previous works, here we focus on the white-box adversarial\ndefense where the attackers are granted full access to not only the classifiers\nbut also defenders to produce as strong attacks as possible. In such a context\nwe propose viewing a defender as a functional, a higher-order function that\ntakes functions as its argument to represent a function space, rather than\nfixed functions conventionally. From this perspective, a defender should be\nrealized and optimized individually for each adversarial input. To this end, we\npropose RIDE, an efficient and provably convergent self-supervised learning\nalgorithm for individual data estimation to protect the predictions from\nadversarial attacks. We demonstrate the significant improvement of adversarial\ndefense performance on image recognition, eg, 98%, 76%, 43% test accuracy on\nMNIST, CIFAR-10, and ImageNet datasets respectively under the state-of-the-art\nBPDA attacker.\n"], ["2019-09-13", "http://arxiv.org/abs/1909.06137", "Defending Against Adversarial Attacks by Suppressing the Largest Eigenvalue of Fisher Information Matrix.", ["Chaomin Shen", " Yaxin Peng", " Guixu Zhang", " Jinsong Fan"], "  We propose a scheme for defending against adversarial attacks by suppressing\nthe largest eigenvalue of the Fisher information matrix (FIM). Our starting\npoint is one explanation on the rationale of adversarial examples. Based on the\nidea of the difference between a benign sample and its adversarial example is\nmeasured by the Euclidean norm, while the difference between their\nclassification probability densities at the last (softmax) layer of the network\ncould be measured by the Kullback-Leibler (KL) divergence, the explanation\nshows that the output difference is a quadratic form of the input difference.\nIf the eigenvalue of this quadratic form (a.k.a. FIM) is large, the output\ndifference becomes large even when the input difference is small, which\nexplains the adversarial phenomenon. This makes the adversarial defense\npossible by controlling the eigenvalues of the FIM. Our solution is adding one\nterm representing the trace of the FIM to the loss function of the original\nnetwork, as the largest eigenvalue is bounded by the trace. Our defensive\nscheme is verified by experiments using a variety of common attacking methods\non typical deep neural networks, e.g. LeNet, VGG and ResNet, with datasets\nMNIST, CIFAR-10, and German Traffic Sign Recognition Benchmark (GTSRB). Our new\nnetwork, after adopting the novel loss function and retraining, has an\neffective and robust defensive capability, as it decreases the fooling ratio of\nthe generated adversarial examples, and remains the classification accuracy of\nthe original network.\n"], ["2019-09-12", "http://arxiv.org/abs/1909.05527", "Inspecting adversarial examples using the Fisher information.", ["J\u00f6rg Martin", " Clemens Elster"], "  Adversarial examples are slight perturbations that are designed to fool\nartificial neural networks when fed as an input. In this work the usability of\nthe Fisher information for the detection of such adversarial attacks is\nstudied. We discuss various quantities whose computation scales well with the\nnetwork size, study their behavior on adversarial examples and show how they\ncan highlight the importance of single input neurons, thereby providing a\nvisual tool for further analyzing (un-)reasonable behavior of a neural network.\nThe potential of our methods is demonstrated by applications to the MNIST,\nCIFAR10 and Fruits-360 datasets.\n"], ["2019-09-12", "http://arxiv.org/abs/1909.05580", "An Empirical Investigation of Randomized Defenses against Adversarial Attacks.", ["Yannik Potdevin", " Dirk Nowotka", " Vijay Ganesh"], "  In recent years, Deep Neural Networks (DNNs) have had a dramatic impact on a\nvariety of problems that were long considered very difficult, e. g., image\nclassification and automatic language translation to name just a few. The\naccuracy of modern DNNs in classification tasks is remarkable indeed. At the\nsame time, attackers have devised powerful methods to construct\nspecially-crafted malicious inputs (often referred to as adversarial examples)\nthat can trick DNNs into mis-classifying them. What is worse is that despite\nthe many defense mechanisms proposed to protect DNNs against adversarial\nattacks, attackers are often able to circumvent these defenses, rendering them\nuseless. This state of affairs is extremely worrying, especially since machine\nlearning systems get adopted at scale.\n  In this paper, we propose a scientific evaluation methodology aimed at\nassessing the quality, efficacy, robustness and efficiency of randomized\ndefenses to protect DNNs against adversarial examples. Using this methodology,\nwe evaluate a variety of defense mechanisms. In addition, we also propose a\ndefense mechanism we call Randomly Perturbed Ensemble Neural Networks (RPENNs).\nWe provide a thorough and comprehensive evaluation of the considered defense\nmechanisms against a white-box attacker model, six different adversarial attack\nmethods and using the ILSVRC2012 validation data set.\n"], ["2019-09-12", "http://arxiv.org/abs/1909.05921", "Transferable Adversarial Robustness using Adversarially Trained Autoencoders.", ["Pratik Vaishnavi", " Kevin Eykholt", " Atul Prakash", " Amir Rahmati"], "  Machine learning has proven to be an extremely useful tool for solving\ncomplex problems in many application domains. This prevalence makes it an\nattractive target for malicious actors. Adversarial machine learning is a\nwell-studied field of research in which an adversary seeks to cause predicable\nerrors in a machine learning algorithm through careful manipulation of the\ninput. In response, numerous techniques have been proposed to harden machine\nlearning algorithms and mitigate the effect of adversarial attacks. Of these\ntechniques, adversarial training, which augments the training data with\nadversarial inputs, has proven to be an effective defensive technique. However,\nadversarial training is computationally expensive and the improvements in\nadversarial performance are limited to a single model. In this paper, we\npropose Adversarially-Trained Autoencoder Augmentation, the first transferable\nadversarial defense that is robust to certain adaptive adversaries. We\ndisentangle adversarial robustness from the classification pipeline by\nadversarially training an autoencoder with respect to the classification loss.\nWe show that our approach achieves comparable results to state-of-the-art\nadversarially trained models on the MNIST, Fashion-MNIST, and CIFAR-10\ndatasets. Furthermore, we can transfer our approach to other vulnerable models\nand improve their adversarial performance without additional training. Finally,\nwe combine our defense with ensemble methods and parallelize adversarial\ntraining across multiple vulnerable pre-trained models. In a single adversarial\ntraining session, the autoencoder can achieve adversarial performance on the\nvulnerable models that is comparable or better than standard adversarial\ntraining.\n"], ["2019-09-11", "http://arxiv.org/abs/1909.05443", "Feedback Learning for Improving the Robustness of Neural Networks.", ["Chang Song", " Zuoguan Wang", " Hai Li"], "  Recent research studies revealed that neural networks are vulnerable to\nadversarial attacks. State-of-the-art defensive techniques add various\nadversarial examples in training to improve models' adversarial robustness.\nHowever, these methods are not universal and can't defend unknown or\nnon-adversarial evasion attacks. In this paper, we analyze the model robustness\nin the decision space. A feedback learning method is then proposed, to\nunderstand how well a model learns and to facilitate the retraining process of\nremedying the defects. The evaluations according to a set of distance-based\ncriteria show that our method can significantly improve models' accuracy and\nrobustness against different types of evasion attacks. Moreover, we observe the\nexistence of inter-class inequality and propose to compensate it by changing\nthe proportions of examples generated in different classes.\n"], ["2019-09-11", "http://arxiv.org/abs/1909.05040", "Sparse and Imperceivable Adversarial Attacks.", ["Francesco Croce", " Matthias Hein"], "  Neural networks have been proven to be vulnerable to a variety of adversarial\nattacks. From a safety perspective, highly sparse adversarial attacks are\nparticularly dangerous. On the other hand the pixelwise perturbations of sparse\nattacks are typically large and thus can be potentially detected. We propose a\nnew black-box technique to craft adversarial examples aiming at minimizing\n$l_0$-distance to the original image. Extensive experiments show that our\nattack is better or competitive to the state of the art. Moreover, we can\nintegrate additional bounds on the componentwise perturbation. Allowing pixels\nto change only in region of high variation and avoiding changes along\naxis-aligned edges makes our adversarial examples almost non-perceivable.\nMoreover, we adapt the Projected Gradient Descent attack to the $l_0$-norm\nintegrating componentwise constraints. This allows us to do adversarial\ntraining to enhance the robustness of classifiers against sparse and\nimperceivable adversarial manipulations.\n"], ["2019-09-10", "http://arxiv.org/abs/1909.04779", "Localized Adversarial Training for Increased Accuracy and Robustness in Image Classification.", ["Eitan Rothberg", " Tingting Chen", " Luo Jie", " Hao Ji"], "  Today's state-of-the-art image classifiers fail to correctly classify\ncarefully manipulated adversarial images. In this work, we develop a new,\nlocalized adversarial attack that generates adversarial examples by\nimperceptibly altering the backgrounds of normal images. We first use this\nattack to highlight the unnecessary sensitivity of neural networks to changes\nin the background of an image, then use it as part of a new training technique:\nlocalized adversarial training. By including locally adversarial images in the\ntraining set, we are able to create a classifier that suffers less loss than a\nnon-adversarially trained counterpart model on both natural and adversarial\ninputs. The evaluation of our localized adversarial training algorithm on MNIST\nand CIFAR-10 datasets shows decreased accuracy loss on natural images, and\nincreased robustness against adversarial inputs.\n"], ["2019-09-10", "http://arxiv.org/abs/1909.04837", "Identifying and Resisting Adversarial Videos Using Temporal Consistency.", ["Xiaojun Jia", " Xingxing Wei", " Xiaochun Cao"], "  Video classification is a challenging task in computer vision. Although Deep\nNeural Networks (DNNs) have achieved excellent performance in video\nclassification, recent research shows adding imperceptible perturbations to\nclean videos can make the well-trained models output wrong labels with high\nconfidence. In this paper, we propose an effective defense framework to\ncharacterize and defend adversarial videos. The proposed method contains two\nphases: (1) adversarial video detection using temporal consistency between\nadjacent frames, and (2) adversarial perturbation reduction via denoisers in\nthe spatial and temporal domains respectively. Specifically, because of the\nlinear nature of DNNs, the imperceptible perturbations will enlarge with the\nincreasing of DNNs depth, which leads to the inconsistency of DNNs output\nbetween adjacent frames. However, the benign video frames often have the same\noutputs with their neighbor frames owing to the slight changes. Based on this\nobservation, we can distinguish between adversarial videos and benign videos.\nAfter that, we utilize different defense strategies against different attacks.\nWe propose the temporal defense, which reconstructs the polluted frames with\ntheir temporally neighbor clean frames, to deal with the adversarial videos\nwith sparse polluted frames. For the videos with dense polluted frames, we use\nan efficient adversarial denoiser to process each frame in the spatial domain,\nand thus purify the perturbations (we call it as spatial defense). A series of\nexperiments conducted on the UCF-101 dataset demonstrate that the proposed\nmethod significantly improves the robustness of video classifiers against\nadversarial attacks.\n"], ["2019-09-10", "http://arxiv.org/abs/1909.04778", "Effectiveness of Adversarial Examples and Defenses for Malware Classification.", ["Robert Podschwadt", " Hassan Takabi"], "  Artificial neural networks have been successfully used for many different\nclassification tasks including malware detection and distinguishing between\nmalicious and non-malicious programs. Although artificial neural networks\nperform very well on these tasks, they are also vulnerable to adversarial\nexamples. An adversarial example is a sample that has minor modifications made\nto it so that the neural network misclassifies it. Many techniques have been\nproposed, both for crafting adversarial examples and for hardening neural\nnetworks against them. Most previous work has been done in the image domain.\nSome of the attacks have been adopted to work in the malware domain which\ntypically deals with binary feature vectors. In order to better understand the\nspace of adversarial examples in malware classification, we study different\napproaches of crafting adversarial examples and defense techniques in the\nmalware domain and compare their effectiveness on multiple datasets.\n"], ["2019-09-10", "http://arxiv.org/abs/1909.04839", "Towards Noise-Robust Neural Networks via Progressive Adversarial Training.", ["Hang Yu", " Aishan Liu", " Xianglong Liu", " Jichen Yang", " Chongzhi Zhang"], "  Adversarial examples, intentionally designed inputs tending to mislead deep\nneural networks, have attracted great attention in the past few years. Although\na series of defense strategies have been developed and achieved encouraging\nmodel robustness, most of them are still vulnerable to the more commonly\nwitnessed corruptions, e.g., Gaussian noise, blur, etc., in the real world. In\nthis paper, we theoretically and empirically discover the fact that there\nexists an inherent connection between adversarial robustness and corruption\nrobustness. Based on the fundamental discovery, this paper further proposes a\nmore powerful training method named Progressive Adversarial Training (PAT) that\nadds diversified adversarial noises progressively during training, and thus\nobtains robust model against both adversarial examples and corruptions through\nhigher training data complexity. Meanwhile, we also theoretically find that PAT\ncan promise better generalization ability. Experimental evaluation on MNIST,\nCIFAR-10 and SVHN show that PAT is able to enhance the robustness and\ngeneralization of the state-of-the-art network structures, performing\ncomprehensively well compared to various augmentation methods. Moreover, we\nalso propose Mixed Test to evaluate model generalization ability more fairly.\n"], ["2019-09-10", "http://arxiv.org/abs/1909.04326", "UPC: Learning Universal Physical Camouflage Attacks on Object Detectors.", ["Lifeng Huang", " Chengying Gao", " Yuyin Zhou", " Changqing Zou", " Cihang Xie", " Alan Yuille", " Ning Liu"], "  In this paper, we study physical adversarial attacks on object detectors in\nthe wild. Prior arts on this matter mostly craft instance-dependent\nperturbations only for rigid and planar objects. To this end, we propose to\nlearn an adversarial pattern to effectively attack all instances belonging to\nthe same object category (e.g., person, car), referred to as Universal Physical\nCamouflage Attack (UPC). Concretely, UPC crafts camouflage by jointly fooling\nthe region proposal network, as well as misleading the classifier and the\nregressor to output errors. In order to make UPC effective for articulated\nnon-rigid or non-planar objects, we introduce a set of transformations for the\ngenerated camouflage patterns to mimic their deformable properties. We\nadditionally impose optimization constraint to make generated patterns look\nnatural for human observers. To fairly evaluate the effectiveness of different\nphysical-world attacks on object detectors, we present the first standardized\nvirtual database, AttackScenes, which simulates the real 3D world in a\ncontrollable and reproducible environment. Extensive experiments suggest the\nsuperiority of our proposed UPC compared with existing physical adversarial\nattackers not only in virtual environments (AttackScenes), but also in\nreal-world physical environments. Codes, models, and demos are publicly\navailable at https://mesunhlf.github.io/index_physical.html.\n"], ["2019-09-10", "http://arxiv.org/abs/1909.04385", "FDA: Feature Disruptive Attack.", ["Aditya Ganeshan", " B. S. Vivek", " R. Venkatesh Babu"], "  Though Deep Neural Networks (DNN) show excellent performance across various\ncomputer vision tasks, several works show their vulnerability to adversarial\nsamples, i.e., image samples with imperceptible noise engineered to manipulate\nthe network's prediction. Adversarial sample generation methods range from\nsimple to complex optimization techniques. Majority of these methods generate\nadversaries through optimization objectives that are tied to the pre-softmax or\nsoftmax output of the network. In this work we, (i) show the drawbacks of such\nattacks, (ii) propose two new evaluation metrics: Old Label New Rank (OLNR) and\nNew Label Old Rank (NLOR) in order to quantify the extent of damage made by an\nattack, and (iii) propose a new adversarial attack FDA: Feature Disruptive\nAttack, to address the drawbacks of existing attacks. FDA works by generating\nimage perturbation that disrupt features at each layer of the network and\ncauses deep-features to be highly corrupt. This allows FDA adversaries to\nseverely reduce the performance of deep networks. We experimentally validate\nthat FDA generates stronger adversaries than other state-of-the-art methods for\nimage classification, even in the presence of various defense measures. More\nimportantly, we show that FDA disrupts feature-representation based tasks even\nwithout access to the task-specific network or methodology. Code available at:\nhttps://github.com/BardOfCodes/fda\n"], ["2019-09-10", "http://arxiv.org/abs/1909.04311", "Learning to Disentangle Robust and Vulnerable Features for Adversarial Detection.", ["Byunggill Joe", " Sung Ju Hwang", " Insik Shin"], "  Although deep neural networks have shown promising performances on various\ntasks, even achieving human-level performance on some, they are shown to be\nsusceptible to incorrect predictions even with imperceptibly small\nperturbations to an input. There exists a large number of previous works which\nproposed to defend against such adversarial attacks either by robust inference\nor detection of adversarial inputs. Yet, most of them cannot effectively defend\nagainst whitebox attacks where an adversary has a knowledge of the model and\ndefense. More importantly, they do not provide a convincing reason why the\ngenerated adversarial inputs successfully fool the target models. To address\nthese shortcomings of the existing approaches, we hypothesize that the\nadversarial inputs are tied to latent features that are susceptible to\nadversarial perturbation, which we call vulnerable features. Then based on this\nintuition, we propose a minimax game formulation to disentangle the latent\nfeatures of each instance into robust and vulnerable ones, using variational\nautoencoders with two latent spaces. We thoroughly validate our model for both\nblackbox and whitebox attacks on MNIST, Fashion MNIST5, and Cat & Dog datasets,\nwhose results show that the adversarial inputs cannot bypass our detector\nwithout changing its semantics, in which case the attack has failed.\n"], ["2019-09-10", "http://arxiv.org/abs/1909.04288", "Toward Finding The Global Optimal of Adversarial Examples.", ["Zhenxin Xiao", " Kai-Wei Chang", " Cho-Jui Hsieh"], "  Current machine learning models are vulnerable to adversarial examples\n(Goodfellow et al., 2014), we noticed that current state-of-the-art methods\n(Kurakin et al., 2016; Cheng et al., 2018) to attack a well-trained model often\nstuck in local optimal values. We conduct series of experiments on both\nwhite-box and black-box settings, and find out that by different\ninitialization, the attack algorithm will finally converge to very different\nlocal optimals, suggesting the importance of careful and thorough search in the\nattack space. In this paper, we propose a general boosting algorithm that can\nhelp current attack to find a more global optimal example. Specifically, we\nsearch for the adversarial examples by starting from different\npoints/directions, and in certain interval we adopt successive halving\n(Jamieson & Talwalkar, 2016) to cut down the searching directions that are not\npromising, and use Bayesian Optimization (Pelikan et al., 1999; Bergstra et\nal., 2011) to resample from the search space based on the knowledge obtained\nfrom past searches. We demonstrate that by applying our methods to\nstate-of-the-art attack algorithms in both black-and white box setting, we can\nfurther reduce the distortion between the original image and adversarial sample\nabout 10%-20%. By adopting dynamic successive halving, we can reduce the\ncomputation cost 5-10 times without harming the final result. We conduct\nexperiments in models trained on MNIST or ImageNet and also try on decision\ntree models, these experiments suggest that our method is a general way to\nboost the performance of current adversarial attack methods.\n"], ["2019-09-09", "http://arxiv.org/abs/1909.04068", "Adversarial Robustness Against the Union of Multiple Perturbation Models.", ["Pratyush Maini", " Eric Wong", " J. Zico Kolter"], "  Owing to the susceptibility of deep learning systems to adversarial attacks,\nthere has been a great deal of work in developing (both empirically and\ncertifiably) robust classifiers, but the vast majority has defended against\nsingle types of attacks. Recent work has looked at defending against multiple\nattacks, specifically on the MNIST dataset, yet this approach used a relatively\ncomplex architecture, claiming that standard adversarial training can not apply\nbecause it \"overfits\" to a particular norm. In this work, we show that it is\nindeed possible to adversarially train a robust model against a union of\nnorm-bounded attacks, by using a natural generalization of the standard\nPGD-based procedure for adversarial training to multiple threat models. With\nthis approach, we are able to train standard architectures which are robust\nagainst $\\ell_\\infty$, $\\ell_2$, and $\\ell_1$ attacks, outperforming past\napproaches on the MNIST dataset and providing the first CIFAR10 network trained\nto be simultaneously robust against $(\\ell_{\\infty}, \\ell_{2},\\ell_{1})$ threat\nmodels, which achieves adversarial accuracy rates of $(47.6\\%, 64.8\\%, 53.4\\%)$\nfor $(\\ell_{\\infty}, \\ell_{2},\\ell_{1})$ perturbations with radius $\\epsilon =\n(0.03,0.5,12)$.\n"], ["2019-09-08", "http://arxiv.org/abs/1909.03413", "STA: Adversarial Attacks on Siamese Trackers.", ["Xugang Wu", " Xiaoping Wang", " Xu Zhou", " Songlei Jian"], "  Recently, the majority of visual trackers adopt Convolutional Neural Network\n(CNN) as their backbone to achieve high tracking accuracy. However, less\nattention has been paid to the potential adversarial threats brought by CNN,\nincluding Siamese network.\n  In this paper, we first analyze the existing vulnerabilities in Siamese\ntrackers and propose the requirements for a successful adversarial attack. On\nthis basis, we formulate the adversarial generation problem and propose an\nend-to-end pipeline to generate a perturbed texture map for the 3D object that\ncauses the trackers to fail. Finally, we conduct thorough experiments to verify\nthe effectiveness of our algorithm. Experiment results show that adversarial\nexamples generated by our algorithm can successfully lower the tracking\naccuracy of victim trackers and even make them drift off. To the best of our\nknowledge, this is the first work to generate 3D adversarial examples on visual\ntrackers.\n"], ["2019-09-08", "http://arxiv.org/abs/1909.03418", "When Explainability Meets Adversarial Learning: Detecting Adversarial Examples using SHAP Signatures.", ["Gil Fidel", " Ron Bitton", " Asaf Shabtai"], "  State-of-the-art deep neural networks (DNNs) are highly effective in solving\nmany complex real-world problems. However, these models are vulnerable to\nadversarial perturbation attacks, and despite the plethora of research in this\ndomain, to this day, adversaries still have the upper hand in the cat and mouse\ngame of adversarial example generation methods vs. detection and prevention\nmethods. In this research, we present a novel detection method that uses\nShapley Additive Explanations (SHAP) values computed for the internal layers of\na DNN classifier to discriminate between normal and adversarial inputs. We\nevaluate our method by building an extensive dataset of adversarial examples\nover the popular CIFAR-10 and MNIST datasets, and training a neural\nnetwork-based detector to distinguish between normal and adversarial inputs. We\nevaluate our detector against adversarial examples generated by diverse\nstate-of-the-art attacks and demonstrate its high detection accuracy and strong\ngeneralization ability to adversarial inputs generated with different attack\nmethods.\n"], ["2019-09-06", "http://arxiv.org/abs/1909.03084", "Learning to Discriminate Perturbations for Blocking Adversarial Attacks in Text Classification.", ["Yichao Zhou", " Jyun-Yu Jiang", " Kai-Wei Chang", " Wei Wang"], "  Adversarial attacks against machine learning models have threatened various\nreal-world applications such as spam filtering and sentiment analysis. In this\npaper, we propose a novel framework, learning to DIScriminate Perturbations\n(DISP), to identify and adjust malicious perturbations, thereby blocking\nadversarial attacks for text classification models. To identify adversarial\nattacks, a perturbation discriminator validates how likely a token in the text\nis perturbed and provides a set of potential perturbations. For each potential\nperturbation, an embedding estimator learns to restore the embedding of the\noriginal word based on the context and a replacement token is chosen based on\napproximate kNN search. DISP can block adversarial attacks for any NLP model\nwithout modifying the model structure or training procedure. Extensive\nexperiments on two benchmark datasets demonstrate that DISP significantly\noutperforms baseline methods in blocking adversarial attacks for text\nclassification. In addition, in-depth analysis shows the robustness of DISP\nacross different situations.\n"], ["2019-09-06", "http://arxiv.org/abs/1909.04495", "Natural Adversarial Sentence Generation with Gradient-based Perturbation.", ["Yu-Lun Hsieh", " Minhao Cheng", " Da-Cheng Juan", " Wei Wei", " Wen-Lian Hsu", " Cho-Jui Hsieh"], "  This work proposes a novel algorithm to generate natural language adversarial\ninput for text classification models, in order to investigate the robustness of\nthese models. It involves applying gradient-based perturbation on the sentence\nembeddings that are used as the features for the classifier, and learning a\ndecoder for generation. We employ this method to a sentiment analysis model and\nverify its effectiveness in inducing incorrect predictions by the model. We\nalso conduct quantitative and qualitative analysis on these examples and\ndemonstrate that our approach can generate more natural adversaries. In\naddition, it can be used to successfully perform black-box attacks, which\ninvolves attacking other existing models whose parameters are not known. On a\npublic sentiment analysis API, the proposed method introduces a 20% relative\ndecrease in average accuracy and 74% relative increase in absolute error.\n"], ["2019-09-06", "http://arxiv.org/abs/1909.02918", "Blackbox Attacks on Reinforcement Learning Agents Using Approximated Temporal Information.", ["Yiren Zhao", " Ilia Shumailov", " Han Cui", " Xitong Gao", " Robert Mullins", " Ross Anderson"], "  Recent research on reinforcement learning has shown that trained agents are\nvulnerable to maliciously crafted adversarial samples. In this work, we show\nhow adversarial samples against RL agents can be generalised from White-box and\nGrey-box attacks to a strong Black-box case, namely where the attacker has no\nknowledge of the agents and their training methods. We use sequence-to-sequence\nmodels to predict a single action or a sequence of future actions that a\ntrained agent will make. Our approximation model, based on time-series\ninformation from the agent, successfully predicts agents' future actions with\nconsistently above 80% accuracy on a wide range of games and training methods.\nSecond, we find that although such adversarial samples are transferable, they\ndo not outperform random Gaussian noise as a means of reducing the game scores\nof trained RL agents. This highlights a serious methodological deficiency in\nprevious work on such agents; random jamming should have been taken as the\nbaseline for evaluation. Third, we do find a novel use for adversarial samples\nin this context: they can be used to trigger a trained agent to misbehave after\na specific delay. This appears to be a genuinely new type of attack; it\npotentially enables an attacker to use devices controlled by RL agents as time\nbombs.\n"], ["2019-09-05", "http://arxiv.org/abs/1909.02583", "Spatiotemporally Constrained Action Space Attacks on Deep Reinforcement Learning Agents.", ["Xian Yeow Lee", " Sambit Ghadai", " Kai Liang Tan", " Chinmay Hegde", " Soumik Sarkar"], "  Robustness of Deep Reinforcement Learning (DRL) algorithms towards\nadversarial attacks in real world applications such as those deployed in\ncyber-physical systems (CPS) are of increasing concern. Numerous studies have\ninvestigated the mechanisms of attacks on the RL agent's state space.\nNonetheless, attacks on the RL agent's action space (AS) (corresponding to\nactuators in engineering systems) are equally perverse; such attacks are\nrelatively less studied in the ML literature. In this work, we first frame the\nproblem as an optimization problem of minimizing the cumulative reward of an RL\nagent with decoupled constraints as the budget of attack. We propose a\nwhite-box Myopic Action Space (MAS) attack algorithm that distributes the\nattacks across the action space dimensions. Next, we reformulate the\noptimization problem above with the same objective function, but with a\ntemporally coupled constraint on the attack budget to take into account the\napproximated dynamics of the agent. This leads to the white-box Look-ahead\nAction Space (LAS) attack algorithm that distributes the attacks across the\naction and temporal dimensions. Our results shows that using the same amount of\nresources, the LAS attack deteriorates the agent's performance significantly\nmore than the MAS attack. This reveals the possibility that with limited\nresource, an adversary can utilize the agent's dynamics to malevolently craft\nattacks that causes the agent to fail. Additionally, we leverage these attack\nstrategies as a possible tool to gain insights on the potential vulnerabilities\nof DRL agents.\n"], ["2019-09-05", "http://arxiv.org/abs/1909.02560", "Adversarial Examples with Difficult Common Words for Paraphrase Identification.", ["Zhouxing Shi", " Minlie Huang", " Ting Yao", " Jingfang Xu"], "  Despite the success of deep models for paraphrase identification on benchmark\ndatasets, these models are still vulnerable to adversarial examples. In this\npaper, we propose a novel algorithm to generate a new type of adversarial\nexamples to study the robustness of deep paraphrase identification models. We\nfirst sample an original sentence pair from the corpus and then adversarially\nreplace some word pairs with difficult common words. We take multiple steps and\nuse beam search to find a modification solution that makes the target model\nfail, and thereby obtain an adversarial example. The word replacement is also\nconstrained by heuristic rules and a language model, to preserve the label and\ngrammaticality of the example during modification. Experiments show that our\nalgorithm can generate adversarial examples on which the performance of the\ntarget model drops dramatically. Meanwhile, human annotators are much less\naffected, and the generated sentences retain a good grammaticality. We also\nshow that adversarial training with generated adversarial examples can improve\nmodel robustness.\n"], ["2019-09-04", "http://arxiv.org/abs/1909.02436", "Are Adversarial Robustness and Common Perturbation Robustness Independent Attributes ?.", ["Alfred Laugros", " Alice Caplier", " Matthieu Ospici"], "  Neural Networks have been shown to be sensitive to common perturbations such\nas blur, Gaussian noise, rotations, etc. They are also vulnerable to some\nartificial malicious corruptions called adversarial examples. The adversarial\nexamples study has recently become very popular and it sometimes even reduces\nthe term \"adversarial robustness\" to the term \"robustness\". Yet, we do not know\nto what extent the adversarial robustness is related to the global robustness.\nSimilarly, we do not know if a robustness to various common perturbations such\nas translations or contrast losses for instance, could help with adversarial\ncorruptions. We intend to study the links between the robustnesses of neural\nnetworks to both perturbations. With our experiments, we provide one of the\nfirst benchmark designed to estimate the robustness of neural networks to\ncommon perturbations. We show that increasing the robustness to carefully\nselected common perturbations, can make neural networks more robust to unseen\ncommon perturbations. We also prove that adversarial robustness and robustness\nto common perturbations are independent. Our results make us believe that\nneural network robustness should be addressed in a broader sense.\n"], ["2019-09-03", "http://arxiv.org/abs/1909.00986", "Certified Robustness to Adversarial Word Substitutions.", ["Robin Jia", " Aditi Raghunathan", " Kerem G\u00f6ksel", " Percy Liang"], "  State-of-the-art NLP models can often be fooled by adversaries that apply\nseemingly innocuous label-preserving transformations (e.g., paraphrasing) to\ninput text. The number of possible transformations scales exponentially with\ntext length, so data augmentation cannot cover all transformations of an input.\nThis paper considers one exponentially large family of label-preserving\ntransformations, in which every word in the input can be replaced with a\nsimilar word. We train the first models that are provably robust to all word\nsubstitutions in this family. Our training procedure uses Interval Bound\nPropagation (IBP) to minimize an upper bound on the worst-case loss that any\ncombination of word substitutions can induce. To evaluate models' robustness to\nthese transformations, we measure accuracy on adversarially chosen word\nsubstitutions applied to test examples. Our IBP-trained models attain $75\\%$\nadversarial accuracy on both sentiment analysis on IMDB and natural language\ninference on SNLI. In comparison, on IMDB, models trained normally and ones\ntrained with data augmentation achieve adversarial accuracy of only $8\\%$ and\n$35\\%$, respectively.\n"], ["2019-09-03", "http://arxiv.org/abs/1909.01492", "Achieving Verified Robustness to Symbol Substitutions via Interval Bound Propagation.", ["Po-Sen Huang", " Robert Stanforth", " Johannes Welbl", " Chris Dyer", " Dani Yogatama", " Sven Gowal", " Krishnamurthy Dvijotham", " Pushmeet Kohli"], "  Neural networks are part of many contemporary NLP systems, yet their\nempirical successes come at the price of vulnerability to adversarial attacks.\nPrevious work has used adversarial training and data augmentation to partially\nmitigate such brittleness, but these are unlikely to find worst-case\nadversaries due to the complexity of the search space arising from discrete\ntext perturbations. In this work, we approach the problem from the opposite\ndirection: to formally verify a system's robustness against a predefined class\nof adversarial attacks. We study text classification under synonym replacements\nor character flip perturbations. We propose modeling these input perturbations\nas a simplex and then using Interval Bound Propagation -- a formal model\nverification method. We modify the conventional log-likelihood training\nobjective to train models that can be efficiently verified, which would\notherwise come with exponential search complexity. The resulting models show\nonly little difference in terms of nominal accuracy, but have much improved\nverified accuracy under perturbations and come with an efficiently computable\nformal guarantee on worst case adversaries.\n"], ["2019-09-02", "http://arxiv.org/abs/1909.00900", "Metric Learning for Adversarial Robustness.", ["Chengzhi Mao", " Ziyuan Zhong", " Junfeng Yang", " Carl Vondrick", " Baishakhi Ray"], "  Deep networks are well-known to be fragile to adversarial attacks. Using\nseveral standard image datasets and established attack mechanisms, we conduct\nan empirical analysis of deep representations under attack, and find that the\nattack causes the internal representation to shift closer to the \"false\" class.\nMotivated by this observation, we propose to regularize the representation\nspace under attack with metric learning in order to produce more robust\nclassifiers. By carefully sampling examples for metric learning, our learned\nrepresentation not only increases robustness, but also can detect previously\nunseen adversarial samples. Quantitative experiments show improvement of\nrobustness accuracy by up to 4\\% and detection efficiency by up to 6\\%\naccording to Area Under Curve (AUC) score over baselines.\n"], ["2019-08-29", "http://arxiv.org/abs/1908.11514", "Adversarial Training Methods for Network Embedding.", ["Quanyu Dai", " Xiao Shen", " Liang Zhang", " Qiang Li", " Dan Wang"], "  Network Embedding is the task of learning continuous node representations for\nnetworks, which has been shown effective in a variety of tasks such as link\nprediction and node classification. Most of existing works aim to preserve\ndifferent network structures and properties in low-dimensional embedding\nvectors, while neglecting the existence of noisy information in many real-world\nnetworks and the overfitting issue in the embedding learning process. Most\nrecently, generative adversarial networks (GANs) based regularization methods\nare exploited to regularize embedding learning process, which can encourage a\nglobal smoothness of embedding vectors. These methods have very complicated\narchitecture and suffer from the well-recognized non-convergence problem of\nGANs. In this paper, we aim to introduce a more succinct and effective local\nregularization method, namely adversarial training, to network embedding so as\nto achieve model robustness and better generalization performance. Firstly, the\nadversarial training method is applied by defining adversarial perturbations in\nthe embedding space with an adaptive $L_2$ norm constraint that depends on the\nconnectivity pattern of node pairs. Though effective as a regularizer, it\nsuffers from the interpretability issue which may hinder its application in\ncertain real-world scenarios. To improve this strategy, we further propose an\ninterpretable adversarial training method by enforcing the reconstruction of\nthe adversarial examples in the discrete graph domain. These two regularization\nmethods can be applied to many existing embedding models, and we take DeepWalk\nas the base model for illustration in the paper. Empirical evaluations in both\nlink prediction and node classification demonstrate the effectiveness of the\nproposed methods.\n"], ["2019-08-29", "http://arxiv.org/abs/1908.11091", "Deep Neural Network Ensembles against Deception: Ensemble Diversity, Accuracy and Robustness.", ["Ling Liu", " Wenqi Wei", " Ka-Ho Chow", " Margaret Loper", " Emre Gursoy", " Stacey Truex", " Yanzhao Wu"], "  Ensemble learning is a methodology that integrates multiple DNN learners for\nimproving prediction performance of individual learners. Diversity is greater\nwhen the errors of the ensemble prediction is more uniformly distributed.\nGreater diversity is highly correlated with the increase in ensemble accuracy.\nAnother attractive property of diversity optimized ensemble learning is its\nrobustness against deception: an adversarial perturbation attack can mislead\none DNN model to misclassify but may not fool other ensemble DNN members\nconsistently. In this paper we first give an overview of the concept of\nensemble diversity and examine the three types of ensemble diversity in the\ncontext of DNN classifiers. We then describe a set of ensemble diversity\nmeasures, a suite of algorithms for creating diversity ensembles and for\nperforming ensemble consensus (voted or learned) for generating high accuracy\nensemble output by strategically combining outputs of individual members. This\npaper concludes with a discussion on a set of open issues in quantifying\nensemble diversity for robust deep learning.\n"], ["2019-08-29", "http://arxiv.org/abs/1908.11230", "Defending Against Misclassification Attacks in Transfer Learning.", ["Bang Wu", " Xiangwen Yang", " Shuo Wang", " Xingliang Yuan", " Cong Wang", " Carsten Rudolph"], "  Transfer learning accelerates the development of new models (Student Models).\nIt applies relevant knowledge from a pre-trained model (Teacher Model) to the\nnew ones with a small amount of training data, yet without affecting the model\naccuracy. However, these Teacher Models are normally open in order to\nfacilitate sharing and reuse, which creates an attack plane in transfer\nlearning systems. Among others, recent emerging attacks demonstrate that\nadversarial inputs can be built with negligible perturbations to the normal\ninputs. Such inputs can mimic the internal features of the student models\ndirectly based on the knowledge of the Teacher Models and cause\nmisclassification in final predictions.\n  In this paper, we propose an effective defence against the above\nmisclassification attacks in transfer learning. First, we propose a distilled\ndifferentiator that can address the targeted attacks, where adversarial inputs\nare misclassified to a specific class. Specifically, this dedicated\ndifferentiator is designed with network activation pruning and retraining in a\nfine-tuned manner, so as to reach high defence rates and high model accuracy.\nTo address the non-targeted attacks that misclassify adversarial inputs to\nrandomly selected classes, we further employ an ensemble structure from the\ndifferentiators to cover all possible misclassification. Our evaluations over\ncommon image recognition tasks confirm that the student models applying our\ndefence can reject most of the adversarial inputs with a marginal accuracy\nloss. We also show that our defence outperforms prior approaches in both\ntargeted and non-targeted attacks.\n"], ["2019-08-29", "http://arxiv.org/abs/1908.11332", "Universal, transferable and targeted adversarial attacks.", ["Junde Wu", " Rao Fu"], "  Deep Neural Network has been found vulnerable recently. A kind of\nwell-designed inputs, which called adversarial examples, can lead the networks\nto make incorrect predictions. Depending on the different scenarios, goals and\ncapabilities, the difficulty to generate the attack is different. For example,\ngenerating a targeted attack is more difficult than a non-targeted attack, a\nuniversal attack is more difficult than a non-universal attack, a transferable\nattack is more difficult than a nontransferable one. The question is: Is there\nexist an attack that can survival in the most harsh adversity to meet all these\nrequirements. Although many cheap and effective attacks have been proposed,\nthis question is still not completely solved over large models and large scale\ndataset. In this paper, we learn a universal mapping from the sources to the\nadversarial examples. These examples can fool classification networks into\nclassifying all of them to one targeted class. Besides, they are also\ntransferable between different models.\n"], ["2019-08-26", "http://arxiv.org/abs/1908.09705", "A Statistical Defense Approach for Detecting Adversarial Examples.", ["Alessandro Cennamo", " Ido Freeman", " Anton Kummert"], "  Adversarial examples are maliciously modified inputs created to fool deep\nneural networks (DNN). The discovery of such inputs presents a major issue to\nthe expansion of DNN-based solutions. Many researchers have already contributed\nto the topic, providing both cutting edge-attack techniques and various\ndefensive strategies. In this work, we focus on the development of a system\ncapable of detecting adversarial samples by exploiting statistical information\nfrom the training-set. Our detector computes several distorted replicas of the\ntest input, then collects the classifier's prediction vectors to build a\nmeaningful signature for the detection task. Then, the signature is projected\nonto the class-specific statistic vector to infer the input's nature. The\nclassification output of the original input is used to select the\nclass-statistic vector. We show that our method reliably detects malicious\ninputs, outperforming state-of-the-art approaches in various settings, while\nbeing complementary to other defensive solutions.\n"], ["2019-08-25", "http://arxiv.org/abs/1908.09364", "Adversarial Edit Attacks for Tree Data.", ["Benjamin Paa\u00dfen"], "  Many machine learning models can be attacked with adversarial examples, i.e.\ninputs close to correctly classified examples that are classified incorrectly.\nHowever, most research on adversarial attacks to date is limited to vectorial\ndata, in particular image data. In this contribution, we extend the field by\nintroducing adversarial edit attacks for tree-structured data with potential\napplications in medicine and automated program analysis. Our approach solely\nrelies on the tree edit distance and a logarithmic number of black-box queries\nto the attacked classifier without any need for gradient information. We\nevaluate our approach on two programming and two biomedical data sets and show\nthat many established tree classifiers, like tree-kernel-SVMs and recursive\nneural networks, can be attacked effectively.\n"], ["2019-08-25", "http://arxiv.org/abs/1908.09327", "advPattern: Physical-World Attacks on Deep Person Re-Identification via Adversarially Transformable Patterns.", ["Zhibo Wang", " Siyan Zheng", " Mengkai Song", " Qian Wang", " Alireza Rahimpourz", " Hairong Qi"], "  Person re-identification (re-ID) is the task of matching person images across\ncamera views, which plays an important role in surveillance and security\napplications. Inspired by great progress of deep learning, deep re-ID models\nbegan to be popular and gained state-of-the-art performance. However, recent\nworks found that deep neural networks (DNNs) are vulnerable to adversarial\nexamples, posing potential threats to DNNs based applications. This phenomenon\nthrows a serious question about whether deep re-ID based systems are vulnerable\nto adversarial attacks. In this paper, we take the first attempt to implement\nrobust physical-world attacks against deep re-ID. We propose a novel attack\nalgorithm, called advPattern, for generating adversarial patterns on clothes,\nwhich learns the variations of image pairs across cameras to pull closer the\nimage features from the same camera, while pushing features from different\ncameras farther. By wearing our crafted \"invisible cloak\", an adversary can\nevade person search, or impersonate a target person to fool deep re-ID models\nin physical world. We evaluate the effectiveness of our transformable patterns\non adversaries'clothes with Market1501 and our established PRCS dataset. The\nexperimental results show that the rank-1 accuracy of re-ID models for matching\nthe adversary decreases from 87.9% to 27.1% under Evading Attack. Furthermore,\nthe adversary can impersonate a target person with 47.1% rank-1 accuracy and\n67.9% mAP under Impersonation Attack. The results demonstrate that deep re-ID\nsystems are vulnerable to our physical attacks.\n"], ["2019-08-24", "http://arxiv.org/abs/1908.09163", "Targeted Mismatch Adversarial Attack: Query with a Flower to Retrieve the Tower.", ["Giorgos Tolias", " Filip Radenovic", " Ond{\u0159}ej Chum"], "  Access to online visual search engines implies sharing of private user\ncontent - the query images. We introduce the concept of targeted mismatch\nattack for deep learning based retrieval systems to generate an adversarial\nimage to conceal the query image. The generated image looks nothing like the\nuser intended query, but leads to identical or very similar retrieval results.\nTransferring attacks to fully unseen networks is challenging. We show\nsuccessful attacks to partially unknown systems, by designing various loss\nfunctions for the adversarial image construction. These include loss functions,\nfor example, for unknown global pooling operation or unknown input resolution\nby the retrieval system. We evaluate the attacks on standard retrieval\nbenchmarks and compare the results retrieved with the original and adversarial\nimage.\n"], ["2019-08-23", "http://arxiv.org/abs/1908.11435", "Improving Adversarial Robustness via Attention and Adversarial Logit Pairing.", ["Dou Goodman", " Xingjian Li", " Jun Huan", " Tao Wei"], "  Though deep neural networks have achieved the state of the art performance in\nvisual classification, recent studies have shown that they are all vulnerable\nto the attack of adversarial examples. In this paper, we develop improved\ntechniques for defending against adversarial examples.First, we introduce\nenhanced defense using a technique we call \\textbf{Attention and Adversarial\nLogit Pairing(AT+ALP)}, a method that encourages both attention map and logit\nfor pairs of examples to be similar. When applied to clean examples and their\nadversarial counterparts, \\textbf{AT+ALP} improves accuracy on adversarial\nexamples over adversarial training.Next,We show that our \\textbf{AT+ALP} can\neffectively increase the average activations of adversarial examples in the key\narea and demonstrate that it focuse on more discriminate features to improve\nthe robustness of the model.Finally,we conducte extensive experiments using a\nwide range of datasets and the experiment results show that our \\textbf{AT+ALP}\nachieves \\textbf{the state of the art} defense.For example,on \\textbf{17 Flower\nCategory Database}, under strong 200-iteration \\textbf{PGD} gray-box and\nblack-box attacks where prior art has 34\\% and 39\\% accuracy, our method\nachieves \\textbf{50\\%} and \\textbf{51\\%}.Compared with previous work,our work\nis evaluated under highly challenging PGD attack:the maximum perturbation\n$\\epsilon \\in \\{0.25,0.5\\}$ i.e. $L_\\infty \\in \\{0.25,0.5\\}$ with 10 to 200\nattack iterations.To our knowledge, such a strong attack has not been\npreviously explored on a wide range of datasets.\n"], ["2019-08-23", "http://arxiv.org/abs/1908.08705", "AdvHat: Real-world adversarial attack on ArcFace Face ID system.", ["Stepan Komkov", " Aleksandr Petiushko"], "  In this paper we propose a novel easily reproducible technique to attack the\nbest public Face ID system ArcFace in different shooting conditions. To create\nan attack, we print the rectangular paper sticker on a common color printer and\nput it on the hat. The adversarial sticker is prepared with a novel algorithm\nfor off-plane transformations of the image which imitates sticker location on\nthe hat. Such an approach confuses the state-of-the-art public Face ID model\nLResNet100E-IR, ArcFace@ms1m-refine-v2 and is transferable to other Face ID\nmodels.\n"], ["2019-08-22", "http://arxiv.org/abs/1908.08413", "Saliency Methods for Explaining Adversarial Attacks.", ["Jindong Gu", " Volker Tresp"], "  The classification decisions of neural networks can be misled by small\nimperceptible perturbations. This work aims to explain the misled\nclassifications using saliency methods. The idea behind saliency methods is to\nexplain the classification decisions of neural networks by creating so-called\nsaliency maps. Unfortunately, a number of recent publications have shown that\nmany of the proposed saliency methods do not provide insightful explanations. A\nprominent example is Guided Backpropagation (GuidedBP), which simply performs\n(partial) image recovery. However, our numerical analysis shows the saliency\nmaps created by GuidedBP do indeed contain class-discriminative information. We\npropose a simple and efficient way to enhance the saliency maps. The proposed\nenhanced GuidedBP shows the state-of-the-art performance to explain adversary\nclassifications.\n"], ["2019-08-21", "http://arxiv.org/abs/1908.08016", "Testing Robustness Against Unforeseen Adversaries.", ["Daniel Kang", " Yi Sun", " Dan Hendrycks", " Tom Brown", " Jacob Steinhardt"], "  Considerable work on adversarial defense has studied robustness to a fixed,\nknown family of adversarial distortions, most frequently L_p-bounded\ndistortions. In reality, the specific form of attack will rarely be known and\nadversaries are free to employ distortions outside of any fixed set. The\npresent work advocates measuring robustness against this much broader range of\nunforeseen attacks---attacks whose precise form is not known when designing a\ndefense.\n  We propose a methodology for evaluating a defense against a diverse range of\ndistortion types together with a summary metric UAR that measures the\nUnforeseen Attack Robustness against a distortion. We construct novel JPEG,\nFog, Gabor, and Snow adversarial attacks to simulate unforeseen adversaries and\nperform a careful study of adversarial robustness against these and existing\ndistortion types. We find that evaluation against existing L_p attacks yields\nhighly correlated information that may not generalize to other attacks and\nidentify a set of 4 attacks that yields more diverse information. We further\nfind that adversarial training against either one or multiple distortions,\nincluding our novel ones, does not confer robustness to unforeseen distortions.\nThese results underscore the need to study robustness against unforeseen\ndistortions and provide a starting point for doing so.\n"], ["2019-08-21", "http://arxiv.org/abs/1908.07899", "Evaluating Defensive Distillation For Defending Text Processing Neural Networks Against Adversarial Examples.", ["Marcus Soll", " Tobias Hinz", " Sven Magg", " Stefan Wermter"], "  Adversarial examples are artificially modified input samples which lead to\nmisclassifications, while not being detectable by humans. These adversarial\nexamples are a challenge for many tasks such as image and text classification,\nespecially as research shows that many adversarial examples are transferable\nbetween different classifiers. In this work, we evaluate the performance of a\npopular defensive strategy for adversarial examples called defensive\ndistillation, which can be successful in hardening neural networks against\nadversarial examples in the image domain. However, instead of applying\ndefensive distillation to networks for image classification, we examine, for\nthe first time, its performance on text classification tasks and also evaluate\nits effect on the transferability of adversarial text examples. Our results\nindicate that defensive distillation only has a minimal impact on text\nclassifying neural networks and does neither help with increasing their\nrobustness against adversarial examples nor prevent the transferability of\nadversarial examples between neural networks.\n"], ["2019-08-20", "http://arxiv.org/abs/1908.07558", "Robust Graph Neural Network Against Poisoning Attacks via Transfer Learning.", ["Xianfeng Tang", " Yandong Li", " Yiwei Sun", " Huaxiu Yao", " Prasenjit Mitra", " Suhang Wang"], "  Graph neural networks (GNNs) are widely used in many applications. However,\ntheir robustness against adversarial attacks is criticized. Prior studies show\nthat using unnoticeable modifications on graph topology or nodal features can\nsignificantly reduce the performances of GNNs. It is very challenging to design\nrobust graph neural networks against poisoning attack and several efforts have\nbeen taken. Existing work aims at reducing the negative impact from adversarial\nedges only with the poisoned graph, which is sub-optimal since they fail to\ndiscriminate adversarial edges from normal ones. On the other hand, clean\ngraphs from similar domains as the target poisoned graph are usually available\nin the real world. By perturbing these clean graphs, we create supervised\nknowledge to train the ability to detect adversarial edges so that the\nrobustness of GNNs is elevated. However, such potential for clean graphs is\nneglected by existing work. To this end, we investigate a novel problem of\nimproving the robustness of GNNs against poisoning attacks by exploring clean\ngraphs. Specifically, we propose PA-GNN, which relies on a penalized\naggregation mechanism that directly restrict the negative impact of adversarial\nedges by assigning them lower attention coefficients. To optimize PA-GNN for a\npoisoned graph, we design a meta-optimization algorithm that trains PA-GNN to\npenalize perturbations using clean graphs and their adversarial counterparts,\nand transfers such ability to improve the robustness of PA-GNN on the poisoned\ngraph. Experimental results on four real-world datasets demonstrate the\nrobustness of PA-GNN against poisoning attacks on graphs.\n"], ["2019-08-20", "http://arxiv.org/abs/1908.07667", "Denoising and Verification Cross-Layer Ensemble Against Black-box Adversarial Attacks.", ["Ka-Ho Chow", " Wenqi Wei", " Yanzhao Wu", " Ling Liu"], "  Deep neural networks (DNNs) have demonstrated impressive performance on many\nchallenging machine learning tasks. However, DNNs are vulnerable to adversarial\ninputs generated by adding maliciously crafted perturbations to the benign\ninputs. As a growing number of attacks have been reported to generate\nadversarial inputs of varying sophistication, the defense-attack arms race has\nbeen accelerated. In this paper, we present MODEF, a cross-layer model\ndiversity ensemble framework. MODEF intelligently combines unsupervised model\ndenoising ensemble with supervised model verification ensemble by quantifying\nmodel diversity, aiming to boost the robustness of the target model against\nadversarial examples. Evaluated using eleven representative attacks on popular\nbenchmark datasets, we show that MODEF achieves remarkable defense success\nrates, compared with existing defense methods, and provides a superior\ncapability of repairing adversarial inputs and making correct predictions with\nhigh accuracy in the presence of black-box attacks.\n"], ["2019-08-19", "http://arxiv.org/abs/1908.07125", "Universal Adversarial Triggers for NLP.", ["Eric Wallace", " Shi Feng", " Nikhil Kandpal", " Matt Gardner", " Sameer Singh"], "  Adversarial examples highlight model vulnerabilities and are useful for\nevaluation and interpretation. We define universal adversarial triggers:\ninput-agnostic sequences of tokens that trigger a model to produce a specific\nprediction when concatenated to any input from a dataset. We propose a\ngradient-guided search over tokens which finds short trigger sequences (e.g.,\none word for classification and four words for language modeling) that\nsuccessfully trigger the target prediction. For example, triggers cause SNLI\nentailment accuracy to drop from 89.94% to 0.55%, 72% of \"why\" questions in\nSQuAD to be answered \"to kill american people\", and the GPT-2 language model to\nspew racist output even when conditioned on non-racial contexts. Furthermore,\nalthough the triggers are optimized using white-box access to a specific model,\nthey transfer to other models for all tasks we consider. Finally, since\ntriggers are input-agnostic, they provide an analysis of global model behavior.\nFor instance, they confirm that SNLI models exploit dataset biases and help to\ndiagnose heuristics learned by reading comprehension models.\n"], ["2019-08-19", "http://arxiv.org/abs/1908.07000", "Hybrid Batch Attacks: Finding Black-box Adversarial Examples with Limited Queries.", ["Fnu Suya", " Jianfeng Chi", " David Evans", " Yuan Tian"], "  In a black-box setting, the adversary only has API access to the target model\nand each query is expensive. Prior work on black-box adversarial examples\nfollows one of two main strategies: (1) transfer attacks use white-box attacks\non local models to find candidate adversarial examples that transfer to the\ntarget model, and (2) optimization-based attacks use queries to the target\nmodel and apply optimization techniques to search for adversarial examples. We\npropose hybrid attacks that combine both strategies, using candidate\nadversarial examples from local models as starting points for\noptimization-based attacks and using labels learned in optimization-based\nattacks to tune local models for finding transfer candidates. We empirically\ndemonstrate on the MNIST, CIFAR10, and ImageNet datasets that our hybrid attack\nstrategy reduces cost and improves success rates, and in combination with our\nseed prioritization strategy, enables batch attacks that can efficiently find\nadversarial examples with only a handful of queries.\n"], ["2019-08-19", "http://arxiv.org/abs/1908.07116", "Protecting Neural Networks with Hierarchical Random Switching: Towards Better Robustness-Accuracy Trade-off for Stochastic Defenses.", ["Xiao Wang", " Siyue Wang", " Pin-Yu Chen", " Yanzhi Wang", " Brian Kulis", " Xue Lin", " Peter Chin"], "  Despite achieving remarkable success in various domains, recent studies have\nuncovered the vulnerability of deep neural networks to adversarial\nperturbations, creating concerns on model generalizability and new threats such\nas prediction-evasive misclassification or stealthy reprogramming. Among\ndifferent defense proposals, stochastic network defenses such as random neuron\nactivation pruning or random perturbation to layer inputs are shown to be\npromising for attack mitigation. However, one critical drawback of current\ndefenses is that the robustness enhancement is at the cost of noticeable\nperformance degradation on legitimate data, e.g., large drop in test accuracy.\nThis paper is motivated by pursuing for a better trade-off between adversarial\nrobustness and test accuracy for stochastic network defenses. We propose\nDefense Efficiency Score (DES), a comprehensive metric that measures the gain\nin unsuccessful attack attempts at the cost of drop in test accuracy of any\ndefense. To achieve a better DES, we propose hierarchical random switching\n(HRS), which protects neural networks through a novel randomization scheme. A\nHRS-protected model contains several blocks of randomly switching channels to\nprevent adversaries from exploiting fixed model structures and parameters for\ntheir malicious purposes. Extensive experiments show that HRS is superior in\ndefending against state-of-the-art white-box and adaptive adversarial\nmisclassification attacks. We also demonstrate the effectiveness of HRS in\ndefending adversarial reprogramming, which is the first defense against\nadversarial programs. Moreover, in most settings the average DES of HRS is at\nleast 5X higher than current stochastic network defenses, validating its\nsignificantly improved robustness-accuracy trade-off.\n"], ["2019-08-18", "http://arxiv.org/abs/1908.06401", "On the Robustness of Human Pose Estimation.", ["Sahil Shah", " Naman jain", " Abhishek Sharma", " Arjun Jain"], "  This paper provides, to the best of our knowledge, the first comprehensive\nand exhaustive study of adversarial attacks on human pose estimation. Besides\nhighlighting the important differences between well-studied classification and\nhuman pose-estimation systems w.r.t. adversarial attacks, we also provide deep\ninsights into the design choices of pose-estimation systems to shape future\nwork. We compare the robustness of several pose-estimation architectures\ntrained on the standard datasets, MPII and COCO. In doing so, we also explore\nthe problem of attacking non-classification based networks including regression\nbased networks, which has been virtually unexplored in the past.\n  We find that compared to classification and semantic segmentation, human pose\nestimation architectures are relatively robust to adversarial attacks with the\nsingle-step attacks being surprisingly ineffective. Our study show that the\nheatmap-based pose-estimation models fare better than their direct\nregression-based counterparts and that the systems which explicitly model\nanthropomorphic semantics of human body are significantly more robust. We find\nthat the targeted attacks are more difficult to obtain than untargeted ones and\nsome body-joints are easier to fool than the others. We present visualizations\nof universal perturbations to facilitate unprecedented insights into their\nworkings on pose-estimation. Additionally, we show them to generalize well\nacross different networks on both the datasets.\n"], ["2019-08-18", "http://arxiv.org/abs/1908.06566", "Adversarial Defense by Suppressing High-frequency Components.", ["Zhendong Zhang", " Cheolkon Jung", " Xiaolong Liang"], "  Recent works show that deep neural networks trained on image classification\ndataset bias towards textures. Those models are easily fooled by applying small\nhigh-frequency perturbations to clean images. In this paper, we learn robust\nimage classification models by removing high-frequency components.\nSpecifically, we develop a differentiable high-frequency suppression module\nbased on discrete Fourier transform (DFT). Combining with adversarial training,\nwe won the 5th place in the IJCAI-2019 Alibaba Adversarial AI Challenge. Our\ncode is available online.\n"], ["2019-08-17", "http://arxiv.org/abs/1908.06353", "Verification of Neural Network Control Policy Under Persistent Adversarial Perturbation.", ["Yuh-Shyang Wang", " Tsui-Wei Weng", " Luca Daniel"], "  Deep neural networks are known to be fragile to small adversarial\nperturbations. This issue becomes more critical when a neural network is\ninterconnected with a physical system in a closed loop. In this paper, we show\nhow to combine recent works on neural network certification tools (which are\nmainly used in static settings such as image classification) with robust\ncontrol theory to certify a neural network policy in a control loop.\nSpecifically, we give a sufficient condition and an algorithm to ensure that\nthe closed loop state and control constraints are satisfied when the persistent\nadversarial perturbation is l-infinity norm bounded. Our method is based on\nfinding a positively invariant set of the closed loop dynamical system, and\nthus we do not require the differentiability or the continuity of the neural\nnetwork policy. Along with the verification result, we also develop an\neffective attack strategy for neural network control systems that outperforms\nexhaustive Monte-Carlo search significantly. We show that our certification\nalgorithm works well on learned models and achieves 5 times better result than\nthe traditional Lipschitz-based method to certify the robustness of a neural\nnetwork policy on a cart pole control problem.\n"], ["2019-08-17", "http://arxiv.org/abs/1908.06281", "Nesterov Accelerated Gradient and Scale Invariance for Improving Transferability of Adversarial Examples.", ["Jiadong Lin", " Chuanbiao Song", " Kun He", " Liwei Wang", " John E. Hopcroft"], "  Recent evidence suggests that deep neural networks (DNNs) are vulnerable to\nadversarial examples, which are crafted by adding human-imperceptible\nperturbations to legitimate examples. However, most of the existing adversarial\nattacks generate adversarial examples with weak transferability, making it\ndifficult to evaluate the robustness of DNNs under the challenging black-box\nsetting. To address this issue, we propose two methods: Nesterov momentum\niterative fast gradient sign method (N-MI-FGSM) and scale-invariant attack\nmethod (SIM), to improve the transferability of adversarial examples. N-MI-FGSM\ntries a better optimizer by applying the idea of Nesterov accelerated gradient\nto gradient-based attack method. SIM leverages the scale-invariant property of\nDNNs and optimizes the generated adversarial example by a set of scaled images\nas the inputs. Further, the two methods can be naturally combined to form a\nstrong attack and enhance existing gradient attack methods. Empirical results\non ImageNet and NIPS 2017 adversarial competition show that the proposed\nmethods can generate adversarial examples with higher transferability than\nexisting competing baselines.\n"], ["2019-08-16", "http://arxiv.org/abs/1908.06062", "Adversarial point perturbations on 3D objects.", ["Daniel Liu", " Ronald Yu", " Hao Su"], "  The importance of training robust neural network grows as 3D data is\nincreasingly utilized in deep learning for vision tasks, like autonomous\ndriving. We examine this problem from the perspective of the attacker, which is\nnecessary in understanding how neural networks can be exploited, and thus\ndefended. More specifically, we propose adversarial attacks based on solving\ndifferent optimization problems, like minimizing the perceptibility of our\ngenerated adversarial examples, or maintaining a uniform density distribution\nof points across the adversarial object surfaces. Our four proposed algorithms\nfor attacking 3D point cloud classification are all highly successful on\nexisting neural networks, and we find that some of them are even effective\nagainst previously proposed point removal defenses.\n"], ["2019-08-14", "http://arxiv.org/abs/1908.05195", "DAPAS : Denoising Autoencoder to Prevent Adversarial attack in Semantic Segmentation.", ["Seung Ju Cho", " Tae Joon Jun", " Byungsoo Oh", " Daeyoung Kim"], "  Nowadays, Deep learning techniques show dramatic performance on computer\nvision area, and they even outperform human. This is a problem combined with\nthe safety of artificial intelligence, which has recently been studied a lot.\nThese attack have shown that they can fool models of image classification,\nsemantic segmentation, and object detection. We point out this attack can be\nprotected by denoise autoencoder, which is used for denoising the perturbation\nand restoring the original images. We experiment with various noise\ndistributions and verify the effect of denoise autoencoder against adversarial\nattack in semantic segmentation\n"], ["2019-08-14", "http://arxiv.org/abs/1908.05185", "Once a MAN: Towards Multi-Target Attack via Learning Multi-Target Adversarial Network Once.", ["Jiangfan Han", " Xiaoyi Dong", " Ruimao Zhang", " Dongdong Chen", " Weiming Zhang", " Nenghai Yu", " Ping Luo", " Xiaogang Wang"], "  Modern deep neural networks are often vulnerable to adversarial samples.\nBased on the first optimization-based attacking method, many following methods\nare proposed to improve the attacking performance and speed. Recently,\ngeneration-based methods have received much attention since they directly use\nfeed-forward networks to generate the adversarial samples, which avoid the\ntime-consuming iterative attacking procedure in optimization-based and\ngradient-based methods. However, current generation-based methods are only able\nto attack one specific target (category) within one model, thus making them not\napplicable to real classification systems that often have hundreds/thousands of\ncategories. In this paper, we propose the first Multi-target Adversarial\nNetwork (MAN), which can generate multi-target adversarial samples with a\nsingle model. By incorporating the specified category information into the\nintermediate features, it can attack any category of the target classification\nmodel during runtime. Experiments show that the proposed MAN can produce\nstronger attack results and also have better transferability than previous\nstate-of-the-art methods in both multi-target attack task and single-target\nattack task. We further use the adversarial samples generated by our MAN to\nimprove the robustness of the classification model. It can also achieve better\nclassification accuracy than other methods when attacked by various methods.\n"], ["2019-08-14", "http://arxiv.org/abs/1908.05008", "AdvFaces: Adversarial Face Synthesis.", ["Debayan Deb", " Jianbang Zhang", " Anil K. Jain"], "  Face recognition systems have been shown to be vulnerable to adversarial\nexamples resulting from adding small perturbations to probe images. Such\nadversarial images can lead state-of-the-art face recognition systems to\nfalsely reject a genuine subject (obfuscation attack) or falsely match to an\nimpostor (impersonation attack). Current approaches to crafting adversarial\nface images lack perceptual quality and take an unreasonable amount of time to\ngenerate them. We propose, AdvFaces, an automated adversarial face synthesis\nmethod that learns to generate minimal perturbations in the salient facial\nregions via Generative Adversarial Networks. Once AdvFaces is trained, it can\nautomatically generate imperceptible perturbations that can evade\nstate-of-the-art face matchers with attack success rates as high as 97.22% and\n24.30% for obfuscation and impersonation attacks, respectively.\n"], ["2019-08-12", "http://arxiv.org/abs/1908.04473", "On Defending Against Label Flipping Attacks on Malware Detection Systems.", ["Rahim Taheri", " Reza Javidan", " Mohammad Shojafar", " Zahra Pooranian", " Ali Miri", " Mauro Conti"], "  Label manipulation attacks are a subclass of data poisoning attacks in\nadversarial machine learning used against different applications, such as\nmalware detection. These types of attacks represent a serious threat to\ndetection systems in environments having high noise rate or uncertainty, such\nas complex networks and Internet of Thing (IoT). Recent work in the literature\nhas suggested using the $K$-Nearest Neighboring (KNN) algorithm to defend\nagainst such attacks. However, such an approach can suffer from low to wrong\ndetection accuracy. In this paper, we design an architecture to tackle the\nAndroid malware detection problem in IoT systems. We develop an attack\nmechanism based on Silhouette clustering method, modified for mobile Android\nplatforms. We proposed two Convolutional Neural Network (CNN)-type deep\nlearning algorithms against this \\emph{Silhouette Clustering-based Label\nFlipping Attack (SCLFA)}. We show the effectiveness of these two defense\nalgorithms - \\emph{Label-based Semi-supervised Defense (LSD)} and\n\\emph{clustering-based Semi-supervised Defense (CSD)} - in correcting labels\nbeing attacked. We evaluate the performance of the proposed algorithms by\nvarying the various machine learning parameters on three Android datasets:\nDrebin, Contagio, and Genome and three types of features: API, intent, and\npermission. Our evaluation shows that using random forest feature selection and\nvarying ratios of features can result in an improvement of up to 19\\% accuracy\nwhen compared with the state-of-the-art method in the literature.\n"], ["2019-08-12", "http://arxiv.org/abs/1908.04355", "Adversarial Neural Pruning.", ["Divyam Madaan", " Sung Ju Hwang"], "  It is well known that neural networks are susceptible to adversarial\nperturbations and are also computationally and memory intensive which makes it\ndifficult to deploy them in real-world applications where security and\ncomputation are constrained. In this work, we aim to obtain both robust and\nsparse networks that are applicable to such scenarios, based on the intuition\nthat latent features have a varying degree of susceptibility to adversarial\nperturbations. Specifically, we define vulnerability at the latent feature\nspace and then propose a Bayesian framework to prioritize features based on\ntheir contribution to both the original and adversarial loss, to prune\nvulnerable features and preserve the robust ones. Through quantitative\nevaluation and qualitative analysis of the perturbation to latent features, we\nshow that our sparsification method is a defense mechanism against adversarial\nattacks and the robustness indeed comes from our model's ability to prune\nvulnerable latent features that are more susceptible to adversarial\nperturbations.\n"], ["2019-08-09", "http://arxiv.org/abs/1908.03560", "On the Adversarial Robustness of Neural Networks without Weight Transport.", ["Mohamed Akrout"], "  Neural networks trained with backpropagation, the standard algorithm of deep\nlearning which uses weight transport, are easily fooled by existing\ngradient-based adversarial attacks. This class of attacks are based on certain\nsmall perturbations of the inputs to make networks misclassify them. We show\nthat less biologically implausible deep neural networks trained with feedback\nalignment, which do not use weight transport, can be harder to fool, providing\nactual robustness. Tested on MNIST, deep neural networks trained without weight\ntransport (1) have an adversarial accuracy of 98% compared to 0.03% for neural\nnetworks trained with backpropagation and (2) generate non-transferable\nadversarial examples. However, this gap decreases on CIFAR-10 but still\nsignificant particularly for small perturbation magnitude less than 1/2.\n"], ["2019-08-08", "http://arxiv.org/abs/1908.03176", "Defending Against Adversarial Iris Examples Using Wavelet Decomposition.", ["Sobhan Soleymani", " Ali Dabouei", " Jeremy Dawson", " Nasser M. Nasrabadi"], "  Deep neural networks have presented impressive performance in biometric\napplications. However, their performance is highly at risk when facing\ncarefully crafted input samples known as adversarial examples. In this paper,\nwe present three defense strategies to detect adversarial iris examples. These\ndefense strategies are based on wavelet domain denoising of the input examples\nby investigating each wavelet sub-band and removing the sub-bands that are most\naffected by the adversary. The first proposed defense strategy reconstructs\nmultiple denoised versions of the input example through manipulating the mid-\nand high-frequency components of the wavelet domain representation of the input\nexample and makes a decision upon the classification result of the majority of\nthe denoised examples. The second and third proposed defense strategies aim to\ndenoise each wavelet domain sub-band and determine the sub-bands that are most\nlikely affected by the adversary using the reconstruction error computed for\neach sub-band. We test the performance of the proposed defense strategies\nagainst several attack scenarios and compare the results with five state of the\nart defense strategies.\n"], ["2019-08-08", "http://arxiv.org/abs/1908.03173", "Universal Adversarial Audio Perturbations.", ["Sajjad Abdoli", " Luiz G. Hafemann", " Jerome Rony", " Ismail Ben Ayed", " Patrick Cardinal", " Alessandro L. Koerich"], "  We demonstrate the existence of universal adversarial perturbations, which\ncan fool a family of audio processing architectures, for both targeted and\nuntargeted attacks. To the best of our knowledge, this is the first study on\ngenerating universal adversarial perturbations for audio processing systems. We\npropose two methods for finding such perturbations. The first method is based\non an iterative, greedy approach that is well-known in computer vision: it\naggregates small perturbations to the input so as to push it to the decision\nboundary. The second method, which is the main technical contribution of this\nwork, is a novel penalty formulation, which finds targeted and untargeted\nuniversal adversarial perturbations. Differently from the greedy approach, the\npenalty method minimizes an appropriate objective function on a batch of\nsamples. Therefore, it produces more successful attacks when the number of\ntraining samples is limited. Moreover, we provide a proof that the proposed\npenalty method theoretically converges to a solution that corresponds to\nuniversal adversarial perturbations. We report comprehensive experiments,\nshowing attack success rates higher than 91.1% and 74.7% for targeted and\nuntargeted attacks, respectively.\n"], ["2019-08-07", "http://arxiv.org/abs/1908.02435", "Improved Adversarial Robustness by Reducing Open Space Risk via Tent Activations.", ["Andras Rozsa", " Terrance E. Boult"], "  Adversarial examples contain small perturbations that can remain\nimperceptible to human observers but alter the behavior of even the best\nperforming deep learning models and yield incorrect outputs. Since their\ndiscovery, adversarial examples have drawn significant attention in machine\nlearning: researchers try to reveal the reasons for their existence and improve\nthe robustness of machine learning models to adversarial perturbations. The\nstate-of-the-art defense is the computationally expensive and very time\nconsuming adversarial training via projected gradient descent (PGD). We\nhypothesize that adversarial attacks exploit the open space risk of classic\nmonotonic activation functions. This paper introduces the tent activation\nfunction with bounded open space risk and shows that tents make deep learning\nmodels more robust to adversarial attacks. We demonstrate on the MNIST dataset\nthat a classifier with tents yields an average accuracy of 91.8% against six\nwhite-box adversarial attacks, which is more than 15 percentage points above\nthe state of the art. On the CIFAR-10 dataset, our approach improves the\naverage accuracy against the six white-box adversarial attacks to 73.5% from\n41.8% achieved by adversarial training via PGD.\n"], ["2019-08-07", "http://arxiv.org/abs/1908.02802", "Investigating Decision Boundaries of Trained Neural Networks.", ["Roozbeh Yousefzadeh", " Dianne P O'Leary"], "  Deep learning models have been the subject of study from various\nperspectives, for example, their training process, interpretation,\ngeneralization error, robustness to adversarial attacks, etc. A trained model\nis defined by its decision boundaries, and therefore, many of the studies about\ndeep learning models speculate about the decision boundaries, and sometimes\nmake simplifying assumptions about them. So far, finding exact points on the\ndecision boundaries of trained deep models has been considered an intractable\nproblem. Here, we compute exact points on the decision boundaries of these\nmodels and provide mathematical tools to investigate the surfaces that define\nthe decision boundaries. Through numerical results, we confirm that some of the\nspeculations about the decision boundaries are accurate, some of the\ncomputational methods can be improved, and some of the simplifying assumptions\nmay be unreliable, for models with nonlinear activation functions. We advocate\nfor verification of simplifying assumptions and approximation methods, wherever\nthey are used. Finally, we demonstrate that the computational practices used\nfor finding adversarial examples can be improved and computing the closest\npoint on the decision boundary reveals the weakest vulnerability of a model\nagainst adversarial attack.\n"], ["2019-08-06", "http://arxiv.org/abs/1908.02374", "Explaining Deep Neural Networks Using Spectrum-Based Fault Localization.", ["Youcheng Sun", " Hana Chockler", " Xiaowei Huang", " Daniel Kroening"], "  Deep neural networks (DNNs) increasingly replace traditionally developed\nsoftware in a broad range of applications. However, in stark contrast to\ntraditional software, the black-box nature of DNNs makes it impossible to\nunderstand their outputs, creating demand for \"Explainable AI\". Explanations of\nthe outputs of the DNN are essential for the training process and are\nsupporting evidence of the adequacy of the DNN. In this paper, we show that\nspectrum-based fault localization delivers good explanations of the outputs of\nDNNs. We present an algorithm and a tool PROTOZOA, which synthesizes a ranking\nof the parts of the inputs using several spectrum-based fault localization\nmeasures. We show that the highest-ranked parts provide explanations that are\nconsistent with the standard definitions of explanations in the literature. Our\nexperimental results on ImageNet show that the explanations we generate are\nuseful visual indicators for the progress of the training of the DNN. We\ncompare the results of PROTOZOA with SHAP and show that the explanations\ngenerated by PROTOZOA are on par or superior. We also generate adversarial\nexamples using our explanations; the efficiency of this process can serve as a\nproxy metric for the quality of the explanations. Our measurements show that\nPROTOZOA's explanations yield a higher number of adversarial examples than\nthose produced by SHAP.\n"], ["2019-08-06", "http://arxiv.org/abs/1908.02199", "MetaAdvDet: Towards Robust Detection of Evolving Adversarial Attacks.", ["Chen Ma", " Chenxu Zhao", " Hailin Shi", " Li Chen", " Junhai Yong", " Dan Zeng"], "  Deep neural networks (DNNs) are vulnerable to adversarial attack which is\nmaliciously implemented by adding human-imperceptible perturbation to images\nand thus leads to incorrect prediction. Existing studies have proposed various\nmethods to detect the new adversarial attacks. However, new attack methods keep\nevolving constantly and yield new adversarial examples to bypass the existing\ndetectors. It needs to collect tens of thousands samples to train detectors,\nwhile the new attacks evolve much more frequently than the high-cost data\ncollection. Thus, this situation leads the newly evolved attack samples to\nremain in small scales. To solve such few-shot problem with the evolving\nattack, we propose a meta-learning based robust detection method to detect new\nadversarial attacks with limited examples. Specifically, the learning consists\nof a double-network framework: a task-dedicated network and a master network\nwhich alternatively learn the detection capability for either seen attack or a\nnew attack. To validate the effectiveness of our approach, we construct the\nbenchmarks with few-shot-fashion protocols based on three conventional\ndatasets, i.e. CIFAR-10, MNIST and Fashion-MNIST. Comprehensive experiments are\nconducted on them to verify the superiority of our approach with respect to the\ntraditional adversarial attack detection methods.\n"], ["2019-08-06", "http://arxiv.org/abs/1908.02256", "BlurNet: Defense by Filtering the Feature Maps.", ["Ravi Raju", " Mikko Lipasti"], "  Recently, the field of adversarial machine learning has been garnering\nattention by showing that state-of-the-art deep neural networks are vulnerable\nto adverserial examples, stemming from small perturbations being added to the\ninput image. Adversarial examples are generated by a malicious adversary by\nobtaining access to the model parameters, such as gradient information, to\nalter the input or by attacking a substitute model and transferring those\nmalicious examples over to attack the victim model. Specifically, one of these\nattack algorithms, Robust Physical Perturbations ($RP_2$), generates\nadverserial images of stop signs with black and white stickers to achieve high\ntargeted misclassification rates against standard-architecture traffic sign\nclassifiers. In this paper, we propose BlurNet, a defense against the $RP_2$\nattack. First, we motivate the defense with a frequency analysis of the first\nlayer feature maps of the network on the LISA dataset by demonstrating high\nfrequency noise is introduced into the input image by the $RP_2$ algorithm. To\nalleviate the high frequency, we introduce a depthwise convolution layer of\nstandard blur kernels after the first layer. Finally, we present a\nregularization scheme to incorporate this low-pass filtering behavior into the\ntraining regime of the network.\n"], ["2019-08-05", "http://arxiv.org/abs/1908.02658", "Random Directional Attack for Fooling Deep Neural Networks.", ["Wenjian Luo", " Chenwang Wu", " Nan Zhou", " Li Ni"], "  Deep neural networks (DNNs) have been widely used in many fields such as\nimages processing, speech recognition; however, they are vulnerable to\nadversarial examples, and this is a security issue worthy of attention. Because\nthe training process of DNNs converge the loss by updating the weights along\nthe gradient descent direction, many gradient-based methods attempt to destroy\nthe DNN model by adding perturbations in the gradient direction. Unfortunately,\nas the model is nonlinear in most cases, the addition of perturbations in the\ngradient direction does not necessarily increase loss. Thus, we propose a\nrandom directed attack (RDA) for generating adversarial examples in this paper.\nRather than limiting the gradient direction to generate an attack, RDA searches\nthe attack direction based on hill climbing and uses multiple strategies to\navoid local optima that cause attack failure. Compared with state-of-the-art\ngradient-based methods, the attack performance of RDA is very competitive.\nMoreover, RDA can attack without any internal knowledge of the model, and its\nperformance under black-box attack is similar to that of the white-box attack\nin most cases, which is difficult to achieve using existing gradient-based\nattack methods.\n"], ["2019-08-05", "http://arxiv.org/abs/1908.01517", "Adversarial Self-Defense for Cycle-Consistent GANs.", ["Dina Bashkirova", " Ben Usman", " Kate Saenko"], "  The goal of unsupervised image-to-image translation is to map images from one\ndomain to another without the ground truth correspondence between the two\ndomains. State-of-art methods learn the correspondence using large numbers of\nunpaired examples from both domains and are based on generative adversarial\nnetworks. In order to preserve the semantics of the input image, the\nadversarial objective is usually combined with a cycle-consistency loss that\npenalizes incorrect reconstruction of the input image from the translated one.\nHowever, if the target mapping is many-to-one, e.g. aerial photos to maps, such\na restriction forces the generator to hide information in low-amplitude\nstructured noise that is undetectable by human eye or by the discriminator. In\nthis paper, we show how such self-attacking behavior of unsupervised\ntranslation methods affects their performance and provide two defense\ntechniques. We perform a quantitative evaluation of the proposed techniques and\nshow that making the translation model more robust to the self-adversarial\nattack increases its generation quality and reconstruction reliability and\nmakes the model less sensitive to low-amplitude perturbations.\n"], ["2019-08-05", "http://arxiv.org/abs/1908.01469", "Automated Detection System for Adversarial Examples with High-Frequency Noises Sieve.", ["Dang Duy Thang", " Toshihiro Matsui"], "  Deep neural networks are being applied in many tasks with encouraging\nresults, and have often reached human-level performance. However, deep neural\nnetworks are vulnerable to well-designed input samples called adversarial\nexamples. In particular, neural networks tend to misclassify adversarial\nexamples that are imperceptible to humans. This paper introduces a new\ndetection system that automatically detects adversarial examples on deep neural\nnetworks. Our proposed system can mostly distinguish adversarial samples and\nbenign images in an end-to-end manner without human intervention. We exploit\nthe important role of the frequency domain in adversarial samples and propose a\nmethod that detects malicious samples in observations. When evaluated on two\nstandard benchmark datasets (MNIST and ImageNet), our method achieved an\nout-detection rate of 99.7 - 100% in many settings.\n"], ["2019-08-05", "http://arxiv.org/abs/1908.01667", "A principled approach for generating adversarial images under non-smooth dissimilarity metrics.", ["Aram-Alexandre Pooladian", " Chris Finlay", " Tim Hoheisel", " Adam Oberman"], "  Deep neural networks perform well on real world data but are prone to\nadversarial perturbations: small changes in the input easily lead to\nmisclassification. In this work, we propose an attack methodology not only for\ncases where the perturbations are measured by $\\ell_p$ norms, but in fact any\nadversarial dissimilarity metric with a closed proximal form. This includes,\nbut is not limited to, $\\ell_1, \\ell_2$, and $\\ell_\\infty$ perturbations; the\n$\\ell_0$ counting \"norm\" (i.e. true sparseness); and the total variation\nseminorm, which is a (non-$\\ell_p$) convolutional dissimilarity measuring local\npixel changes. Our approach is a natural extension of a recent adversarial\nattack method, and eliminates the differentiability requirement of the metric.\nWe demonstrate our algorithm, ProxLogBarrier, on the MNIST, CIFAR10, and\nImageNet-1k datasets. We consider undefended and defended models, and show that\nour algorithm easily transfers to various datasets. We observe that\nProxLogBarrier outperforms a host of modern adversarial attacks specialized for\nthe $\\ell_0$ case. Moreover, by altering images in the total variation\nseminorm, we shed light on a new class of perturbations that exploit\nneighboring pixel information.\n"], ["2019-08-05", "http://arxiv.org/abs/1908.01551", "Imperio: Robust Over-the-Air Adversarial Examples for Automatic Speech Recognition Systems.", ["Lea Sch\u00f6nherr", " Steffen Zeiler", " Thorsten Holz", " Dorothea Kolossa"], "  Automatic speech recognition (ASR) systems are possible to fool via targeted\nadversarial examples. These can induce the ASR to produce arbitrary\ntranscriptions in response to any type of audio signal, be it speech,\nenvironmental sounds, or music. However, in general, those adversarial examples\ndid not work in a real-world setup, where the examples are played over the air\nbut have to be fed into the ASR system directly. In some cases, where the\nadversarial examples could be successfully played over the air, the attacks\nrequire precise information about the room where the attack takes place in\norder to tailor the adversarial examples to a specific setup and are not\ntransferable to other rooms. Other attacks, which are robust in an over-the-air\nattack, are either handcrafted examples or human listeners can easily recognize\nthe target transcription, once they have been alerted to its content. In this\npaper, we demonstrate the first generic algorithm that produces adversarial\nexamples which remain robust in an over-the-air attack such that the ASR system\ntranscribes the target transcription after actually being replayed. For the\nproposed algorithm, guessing a rough approximation of the room characteristics\nis enough and no actual access to the room is required. We use the ASR system\nKaldi to demonstrate the attack and employ a room-impulse-response simulator to\nharden the adversarial examples against varying room characteristics. Further,\nthe algorithm can also utilize psychoacoustics to hide changes of the original\naudio signal below the human thresholds of hearing. We show that the\nadversarial examples work for varying room setups, but also can be tailored to\nspecific room setups. As a result, an attacker can optimize adversarial\nexamples for any target transcription and to arbitrary rooms. Additionally, the\nadversarial examples remain transferable to varying rooms with a high\nprobability.\n"], ["2019-08-04", "http://arxiv.org/abs/1908.01297", "A Restricted Black-box Adversarial Framework Towards Attacking Graph Embedding Models.", ["Heng Chang", " Yu Rong", " Tingyang Xu", " Wenbing Huang", " Honglei Zhang", " Peng Cui", " Wenwu Zhu", " Junzhou Huang"], "  With the great success of graph embedding model on both academic and industry\narea, the robustness of graph embedding against adversarial attack inevitably\nbecomes a central problem in graph learning domain. Regardless of the fruitful\nprogress, most of the current works perform the attack in a white-box fashion:\nthey need to access the model predictions and labels to construct their\nadversarial loss. However, the inaccessibility of model predictions in real\nsystems makes the white-box attack impractical to real graph learning system.\nThis paper promotes current frameworks in a more general and flexible sense --\nwe demand to attack various kinds of graph embedding model with black-box\ndriven. To this end, we begin by investigating the theoretical connections\nbetween graph signal processing and graph embedding models in a principled way\nand formulate the graph embedding model as a general graph signal process with\ncorresponding graph filter. As such, a generalized adversarial attacker:\nGF-Attack is constructed by the graph filter and feature matrix. Instead of\naccessing any knowledge of the target classifiers used in graph embedding,\nGF-Attack performs the attack only on the graph filter in a black-box attack\nfashion. To validate the generalization of GF-Attack, we construct the attacker\non four popular graph embedding models. Extensive experimental results validate\nthe effectiveness of our attacker on several benchmark datasets. Particularly\nby using our attack, even small graph perturbations like one-edge flip is able\nto consistently make a strong attack in performance to different graph\nembedding models.\n"], ["2019-08-03", "http://arxiv.org/abs/1908.01165", "Invariance-based Adversarial Attack on Neural Machine Translation Systems.", ["Akshay Chaturvedi", " Abijith KP", " Utpal Garain"], "  Recently, NLP models have been shown to be susceptible to adversarial\nattacks. In this paper, we explore adversarial attacks on neural machine\ntranslation (NMT) systems. Given a sentence in the source language, the goal of\nthe proposed attack is to change multiple words while ensuring that the\npredicted translation remains unchanged. In order to choose the word from the\nsource vocabulary, we propose a soft-attention based technique. The experiments\nare conducted on two language pairs: English-German (en-de) and English-French\n(en-fr) and two state-of-the-art NMT systems: BLSTM-based encoder-decoder with\nattention and Transformer. The proposed soft-attention based technique\noutperforms existing methods like HotFlip by a significant margin for all the\nconducted experiments The results demonstrate that state-of-the-art NMT systems\nare unable to capture the semantics of the source language.\n"], ["2019-08-02", "http://arxiv.org/abs/1908.00706", "AdvGAN++ : Harnessing latent layers for adversary generation.", ["Puneet Mangla", " Surgan Jandial", " Sakshi Varshney", " Vineeth N Balasubramanian"], "  Adversarial examples are fabricated examples, indistinguishable from the\noriginal image that mislead neural networks and drastically lower their\nperformance. Recently proposed AdvGAN, a GAN based approach, takes input image\nas a prior for generating adversaries to target a model. In this work, we show\nhow latent features can serve as better priors than input images for adversary\ngeneration by proposing AdvGAN++, a version of AdvGAN that achieves higher\nattack rates than AdvGAN and at the same time generates perceptually realistic\nimages on MNIST and CIFAR-10 datasets.\n"], ["2019-08-01", "http://arxiv.org/abs/1908.00635", "Black-box Adversarial ML Attack on Modulation Classification.", ["Muhammad Usama", " Junaid Qadir", " Ala Al-Fuqaha"], "  Recently, many deep neural networks (DNN) based modulation classification\nschemes have been proposed in the literature. We have evaluated the robustness\nof two famous such modulation classifiers (based on the techniques of\nconvolutional neural networks and long short term memory) against adversarial\nmachine learning attacks in black-box settings. We have used Carlini \\& Wagner\n(C-W) attack for performing the adversarial attack. To the best of our\nknowledge, the robustness of these modulation classifiers has not been\nevaluated through C-W attack before. Our results clearly indicate that\nstate-of-art deep machine learning-based modulation classifiers are not robust\nagainst adversarial attacks.\n"], ["2019-08-01", "http://arxiv.org/abs/1908.00656", "Robustifying deep networks for image segmentation.", ["Zheng Liu", " Jinnian Zhang", " Varun Jog", " Po-Ling Loh", " Alan B McMillan"], "  Purpose: The purpose of this study is to investigate the robustness of a\ncommonly-used convolutional neural network for image segmentation with respect\nto visually-subtle adversarial perturbations, and suggest new methods to make\nthese networks more robust to such perturbations. Materials and Methods: In\nthis retrospective study, the accuracy of brain tumor segmentation was studied\nin subjects with low- and high-grade gliomas. A three-dimensional UNet model\nwas implemented to segment four different MR series (T1-weighted, post-contrast\nT1-weighted, T2- weighted, and T2-weighted FLAIR) into four pixelwise labels\n(Gd-enhancing tumor, peritumoral edema, necrotic and non-enhancing tumor, and\nbackground). We developed attack strategies based on the Fast Gradient Sign\nMethod (FGSM), iterative FGSM (i-FGSM), and targeted iterative FGSM (ti-FGSM)\nto produce effective attacks. Additionally, we explored the effectiveness of\ndistillation and adversarial training via data augmentation to counteract\nadversarial attacks. Robustness was measured by comparing the Dice coefficient\nfor each attack method using Wilcoxon signed-rank tests. Results: Attacks based\non FGSM, i-FGSM, and ti-FGSM were effective in significantly reducing the\nquality of image segmentation with reductions in Dice coefficient by up to 65%.\nFor attack defenses, distillation performed significantly better than\nadversarial training approaches. However, all defense approaches performed\nworse compared to unperturbed test images. Conclusion: Segmentation networks\ncan be adversely affected by targeted attacks that introduce visually minor\n(and potentially undetectable) modifications to existing images. With an\nincreasing interest in applying deep learning techniques to medical imaging\ndata, it is important to quantify the ramifications of adversarial inputs\n(either intentional or unintentional).\n"], ["2019-07-31", "http://arxiv.org/abs/1908.00096", "Adversarial Robustness Curves.", ["Christina G\u00f6pfert", " Jan Philip G\u00f6pfert", " Barbara Hammer"], "  The existence of adversarial examples has led to considerable uncertainty\nregarding the trust one can justifiably put in predictions produced by\nautomated systems. This uncertainty has, in turn, lead to considerable research\neffort in understanding adversarial robustness. In this work, we take first\nsteps towards separating robustness analysis from the choice of robustness\nthreshold and norm. We propose robustness curves as a more general view of the\nrobustness behavior of a model and investigate under which circumstances they\ncan qualitatively depend on the chosen norm.\n"], ["2019-07-31", "http://arxiv.org/abs/1907.13548", "Optimal Attacks on Reinforcement Learning Policies.", ["Alessio Russo", " Alexandre Proutiere"], "  Control policies, trained using the Deep Reinforcement Learning, have been\nrecently shown to be vulnerable to adversarial attacks introducing even very\nsmall perturbations to the policy input. The attacks proposed so far have been\ndesigned using heuristics, and build on existing adversarial example crafting\ntechniques used to dupe classifiers in supervised learning. In contrast, this\npaper investigates the problem of devising optimal attacks, depending on a\nwell-defined attacker's objective, e.g., to minimize the main agent average\nreward. When the policy and the system dynamics, as well as rewards, are known\nto the attacker, a scenario referred to as a white-box attack, designing\noptimal attacks amounts to solving a Markov Decision Process. For what we call\nblack-box attacks, where neither the policy nor the system is known, optimal\nattacks can be trained using Reinforcement Learning techniques. Through\nnumerical experiments, we demonstrate the efficiency of our attacks compared to\nexisting attacks (usually based on Gradient methods). We further quantify the\npotential impact of attacks and establish its connection to the smoothness of\nthe policy under attack. Smooth policies are naturally less prone to attacks\n(this explains why Lipschitz policies, with respect to the state, are more\nresilient). Finally, we show that from the main agent perspective, the system\nuncertainties and the attacker can be modeled as a Partially Observable Markov\nDecision Process. We actually demonstrate that using Reinforcement Learning\ntechniques tailored to POMDP (e.g. using Recurrent Neural Networks) leads to\nmore resilient policies.\n"], ["2019-07-30", "http://arxiv.org/abs/1907.13124", "Impact of Adversarial Examples on Deep Learning Models for Biomedical Image Segmentation.", ["Utku Ozbulak", " Messem Arnout Van", " Neve Wesley De"], "  Deep learning models, which are increasingly being used in the field of\nmedical image analysis, come with a major security risk, namely, their\nvulnerability to adversarial examples. Adversarial examples are carefully\ncrafted samples that force machine learning models to make mistakes during\ntesting time. These malicious samples have been shown to be highly effective in\nmisguiding classification tasks. However, research on the influence of\nadversarial examples on segmentation is significantly lacking. Given that a\nlarge portion of medical imaging problems are effectively segmentation\nproblems, we analyze the impact of adversarial examples on deep learning-based\nimage segmentation models. Specifically, we expose the vulnerability of these\nmodels to adversarial examples by proposing the Adaptive Segmentation Mask\nAttack (ASMA). This novel algorithm makes it possible to craft targeted\nadversarial examples that come with (1) high intersection-over-union rates\nbetween the target adversarial mask and the prediction and (2) with\nperturbation that is, for the most part, invisible to the bare eye. We lay out\nexperimental and visual evidence by showing results obtained for the ISIC skin\nlesion segmentation challenge and the problem of glaucoma optic disc\nsegmentation. An implementation of this algorithm and additional examples can\nbe found at https://github.com/utkuozbulak/adaptive-segmentation-mask-attack.\n"], ["2019-07-30", "http://arxiv.org/abs/1907.12744", "Not All Adversarial Examples Require a Complex Defense: Identifying Over-optimized Adversarial Examples with IQR-based Logit Thresholding.", ["Utku Ozbulak", " Messem Arnout Van", " Neve Wesley De"], "  Detecting adversarial examples currently stands as one of the biggest\nchallenges in the field of deep learning. Adversarial attacks, which produce\nadversarial examples, increase the prediction likelihood of a target class for\na particular data point. During this process, the adversarial example can be\nfurther optimized, even when it has already been wrongly classified with 100%\nconfidence, thus making the adversarial example even more difficult to detect.\nFor this kind of adversarial examples, which we refer to as over-optimized\nadversarial examples, we discovered that the logits of the model provide solid\nclues on whether the data point at hand is adversarial or genuine. In this\ncontext, we first discuss the masking effect of the softmax function for the\nprediction made and explain why the logits of the model are more useful in\ndetecting over-optimized adversarial examples. To identify this type of\nadversarial examples in practice, we propose a non-parametric and\ncomputationally efficient method which relies on interquartile range, with this\nmethod becoming more effective as the image resolution increases. We support\nour observations throughout the paper with detailed experiments for different\ndatasets (MNIST, CIFAR-10, and ImageNet) and several architectures.\n"], ["2019-07-28", "http://arxiv.org/abs/1907.12138", "Are Odds Really Odd? Bypassing Statistical Detection of Adversarial Examples.", ["Hossein Hosseini", " Sreeram Kannan", " Radha Poovendran"], "  Deep learning classifiers are known to be vulnerable to adversarial examples.\nA recent paper presented at ICML 2019 proposed a statistical test detection\nmethod based on the observation that logits of noisy adversarial examples are\nbiased toward the true class. The method is evaluated on CIFAR-10 dataset and\nis shown to achieve 99% true positive rate (TPR) at only 1% false positive rate\n(FPR). In this paper, we first develop a classifier-based adaptation of the\nstatistical test method and show that it improves the detection performance. We\nthen propose Logit Mimicry Attack method to generate adversarial examples such\nthat their logits mimic those of benign images. We show that our attack\nbypasses both statistical test and classifier-based methods, reducing their TPR\nto less than 2:2% and 1:6%, respectively, even at 5% FPR. We finally show that\na classifier-based detector that is trained with logits of mimicry adversarial\nexamples can be evaded by an adaptive attacker that specifically targets the\ndetector. Furthermore, even a detector that is iteratively trained to defend\nagainst adaptive attacker cannot be made robust, indicating that statistics of\nlogits cannot be used to detect adversarial examples.\n"], ["2019-07-27", "http://arxiv.org/abs/1907.11932", "Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment.", ["Di Jin", " Zhijing Jin", " Joey Tianyi Zhou", " Peter Szolovits"], "  Machine learning algorithms are often vulnerable to adversarial examples that\nhave imperceptible alterations from the original counterparts but can fool the\nstate-of-the-art models. It is helpful to evaluate or even improve the\nrobustness of these models by exposing the maliciously crafted adversarial\nexamples. In this paper, we present TextFooler, a simple but strong baseline to\ngenerate natural adversarial text. By applying it to two fundamental natural\nlanguage tasks, text classification and textual entailment, we successfully\nattacked three target models, including the powerful pre-trained BERT, and the\nwidely used convolutional and recurrent neural networks. We demonstrate the\nadvantages of this framework in three ways: (1) effective---it outperforms\nstate-of-the-art attacks in terms of success rate and perturbation rate, (2)\nutility-preserving---it preserves semantic content and grammaticality, and\nremains correctly classified by humans, and (3) efficient---it generates\nadversarial text with computational complexity linear to the text length.\n"], ["2019-07-26", "http://arxiv.org/abs/1907.11780", "Understanding Adversarial Robustness: The Trade-off between Minimum and Average Margin.", ["Kaiwen Wu", " Yaoliang Yu"], "  Deep models, while being extremely versatile and accurate, are vulnerable to\nadversarial attacks: slight perturbations that are imperceptible to humans can\ncompletely flip the prediction of deep models. Many attack and defense\nmechanisms have been proposed, although a satisfying solution still largely\nremains elusive. In this work, we give strong evidence that during training,\ndeep models maximize the minimum margin in order to achieve high accuracy, but\nat the same time decrease the \\emph{average} margin hence hurting robustness.\nOur empirical results highlight an intrinsic trade-off between accuracy and\nrobustness for current deep model training. To further address this issue, we\npropose a new regularizer to explicitly promote average margin, and we verify\nthrough extensive experiments that it does lead to better robustness. Our\nregularized objective remains Fisher-consistent, hence asymptotically can still\nrecover the Bayes optimal classifier.\n"], ["2019-07-26", "http://arxiv.org/abs/1907.11684", "On the Design of Black-box Adversarial Examples by Leveraging Gradient-free Optimization and Operator Splitting Method.", ["Pu Zhao", " Sijia Liu", " Pin-Yu Chen", " Nghia Hoang", " Kaidi Xu", " Bhavya Kailkhura", " Xue Lin"], "  Robust machine learning is currently one of the most prominent topics which\ncould potentially help shaping a future of advanced AI platforms that not only\nperform well in average cases but also in worst cases or adverse situations.\nDespite the long-term vision, however, existing studies on black-box\nadversarial attacks are still restricted to very specific settings of threat\nmodels (e.g., single distortion metric and restrictive assumption on target\nmodel's feedback to queries) and/or suffer from prohibitively high query\ncomplexity. To push for further advances in this field, we introduce a general\nframework based on an operator splitting method, the alternating direction\nmethod of multipliers (ADMM) to devise efficient, robust black-box attacks that\nwork with various distortion metrics and feedback settings without incurring\nhigh query complexity. Due to the black-box nature of the threat model, the\nproposed ADMM solution framework is integrated with zeroth-order (ZO)\noptimization and Bayesian optimization (BO), and thus is applicable to the\ngradient-free regime. This results in two new black-box adversarial attack\ngeneration methods, ZO-ADMM and BO-ADMM. Our empirical evaluations on image\nclassification datasets show that our proposed approaches have much lower\nfunction query complexities compared to state-of-the-art attack methods, but\nachieve very competitive attack success rates.\n"], ["2019-07-24", "http://arxiv.org/abs/1907.10310", "Towards Adversarially Robust Object Detection.", ["Haichao Zhang", " Jianyu Wang"], "  Object detection is an important vision task and has emerged as an\nindispensable component in many vision system, rendering its robustness as an\nincreasingly important performance factor for practical applications. While\nobject detection models have been demonstrated to be vulnerable against\nadversarial attacks by many recent works, very few efforts have been devoted to\nimproving their robustness. In this work, we take an initial attempt towards\nthis direction. We first revisit and systematically analyze object detectors\nand many recently developed attacks from the perspective of model robustness.\nWe then present a multi-task learning perspective of object detection and\nidentify an asymmetric role of task losses. We further develop an adversarial\ntraining approach which can leverage the multiple sources of attacks for\nimproving the robustness of detection models. Extensive experiments on\nPASCAL-VOC and MS-COCO verified the effectiveness of the proposed approach.\n"], ["2019-07-24", "http://arxiv.org/abs/1907.10456", "Understanding Adversarial Attacks on Deep Learning Based Medical Image Analysis Systems.", ["Xingjun Ma", " Yuhao Niu", " Lin Gu", " Yisen Wang", " Yitian Zhao", " James Bailey", " Feng Lu"], "  Deep neural networks (DNNs) have become popular for medical image analysis\ntasks like cancer diagnosis and lesion detection. However, a recent study\ndemonstrates that medical deep learning systems can be compromised by\ncarefully-engineered adversarial examples/attacks, i.e., small imperceptible\nperturbations can fool DNNs to predict incorrectly. This raises safety concerns\nabout the deployment of deep learning systems in clinical settings. In this\npaper, we provide a deeper understanding of adversarial examples in the context\nof medical images. We find that medical DNN models can be more vulnerable to\nadversarial attacks compared to natural ones from three different viewpoints:\n1) medical image DNNs that have only a few classes are generally easier to be\nattacked; 2) the complex biological textures of medical images may lead to more\nvulnerable regions; and most importantly, 3) state-of-the-art deep networks\ndesigned for large-scale natural image processing can be overparameterized for\nmedical imaging tasks and result in high vulnerability to adversarial attacks.\nSurprisingly, we also find that medical adversarial attacks can be easily\ndetected, i.e., simple detectors can achieve over 98% detection AUCs against\nstate-of-the-art attacks, due to their fundamental feature difference from\nnormal examples. We show this is because adversarial attacks tend to attack a\nwide spread area outside the pathological regions, which results in deep\nfeatures that are fundamentally different and easily separable from normal\nfeatures. We believe these findings may be a useful basis to approach the\ndesign of secure medical deep learning systems.\n"], ["2019-07-24", "http://arxiv.org/abs/1907.10737", "Joint Adversarial Training: Incorporating both Spatial and Pixel Attacks.", ["Haichao Zhang", " Jianyu Wang"], "  Conventional adversarial training methods using attacks that manipulate the\npixel value directly and individually, leading to models that are less robust\nin face of spatial transformation-based attacks. In this paper, we propose a\njoint adversarial training method that incorporates both spatial\ntransformation-based and pixel-value based attacks for improving model\nrobustness. We introduce a spatial transformation-based attack with an explicit\nnotion of budget and develop an algorithm for spatial attack generation. We\nfurther integrate both pixel and spatial attacks into one generation model and\nshow how to leverage the complementary strengths of each other in training for\nimproving the overall model robustness. Extensive experimental results on\ndifferent benchmark datasets compared with state-of-the-art methods verified\nthe effectiveness of the proposed method.\n"], ["2019-07-24", "http://arxiv.org/abs/1907.10764", "Defense Against Adversarial Attacks Using Feature Scattering-based Adversarial Training.", ["Haichao Zhang", " Jianyu Wang"], "  We introduce a feature scattering-based adversarial training approach for\nimproving model robustness against adversarial attacks. Conventional\nadversarial training approaches leverage a supervised scheme (either targeted\nor non-targeted) in generating attacks for training, which typically suffer\nfrom issues such as label leaking as noted in recent works. Differently, the\nproposed approach generates adversarial images for training through feature\nscattering in the latent space, which is unsupervised in nature and avoids\nlabel leaking. More importantly, this new approach generates perturbed images\nin a collaborative fashion, taking the inter-sample relationships into\nconsideration. We conduct analysis on model robustness and demonstrate the\neffectiveness of the proposed approach through extensively experiments on\ndifferent datasets compared with state-of-the-art approaches.\n"], ["2019-07-24", "http://arxiv.org/abs/1907.12934", "Weakly Supervised Localization using Min-Max Entropy: an Interpretable Framework.", ["Soufiane Belharbi", " J\u00e9r\u00f4me Rony", " Jose Dolz", " Ismail Ben Ayed", " Luke McCaffrey", " Eric Granger"], "  Weakly supervised object localization (WSOL) models aim to locate objects of\ninterest in an image after being trained only on data with coarse image level\nlabels. Deep learning models for WSOL rely typically on convolutional attention\nmaps with no constraints on the regions of interest which allows them to select\nany region, making them vulnerable to false positive regions. This issue occurs\nin many application domains, e.g., medical image analysis, where\ninterpretability is central to the prediction. In order to improve the\nlocalization reliability, we propose a deep learning framework for WSOL with\npixel level localization. It is composed of two sequential sub-networks: a\nlocalizer that localizes regions of interest; followed by a classifier that\nclassifies them. Within its end-to-end training, we incorporate the prior\nknowledge stating that in an agnostic-class setup an image is more likely to\ncontain relevant --object of interest-- and irrelevant regions --noise--. Based\non the conditional entropy (CE) measured at the classifier, the localizer is\ndriven to spot relevant regions (low CE), and irrelevant regions (high CE). Our\nframework is able to recover large discriminative regions using our recursive\nerasing algorithm that we incorporate within the backpropagation during\ntraining. Moreover, the framework handles intrinsically multi-instances.\nExperimental results on public datasets with medical images (GlaS colon cancer)\nand natural images (Caltech-UCSD Birds-200-2011, Oxford flower 102) show that,\ncompared to state of the art WSOL methods, our framework can provide\nsignificant improvements in terms of image-level classification, pixel-level\nlocalization, and robustness to overfitting when dealing with few training\nsamples. A public reproducible PyTorch implementation is provided in:\nhttps://github.com/sbelharbi/wsol-min-max-entropy-interpretability .\n"], ["2019-07-23", "http://arxiv.org/abs/1907.10823", "Enhancing Adversarial Example Transferability with an Intermediate Level Attack.", ["Qian Huang", " Isay Katsman", " Horace He", " Zeqi Gu", " Serge Belongie", " Ser-Nam Lim"], "  Neural networks are vulnerable to adversarial examples, malicious inputs\ncrafted to fool trained models. Adversarial examples often exhibit black-box\ntransfer, meaning that adversarial examples for one model can fool another\nmodel. However, adversarial examples are typically overfit to exploit the\nparticular architecture and feature representation of a source model, resulting\nin sub-optimal black-box transfer attacks to other target models. We introduce\nthe Intermediate Level Attack (ILA), which attempts to fine-tune an existing\nadversarial example for greater black-box transferability by increasing its\nperturbation on a pre-specified layer of the source model, improving upon\nstate-of-the-art methods. We show that we can select a layer of the source\nmodel to perturb without any knowledge of the target models while achieving\nhigh transferability. Additionally, we provide some explanatory insights\nregarding our method and the effect of optimizing for adversarial examples\nusing intermediate feature maps.\n"], ["2019-07-21", "http://arxiv.org/abs/1907.09470", "Characterizing Attacks on Deep Reinforcement Learning.", ["Chaowei Xiao", " Xinlei Pan", " Warren He", " Jian Peng", " Mingjie Sun", " Jinfeng Yi", " Bo Li", " Dawn Song"], "  Deep reinforcement learning (DRL) has achieved great success in various\napplications. However, recent studies show that machine learning models are\nvulnerable to adversarial attacks. DRL models have been attacked by adding\nperturbations to observations. While such observation based attack is only one\naspect of potential attacks on DRL, other forms of attacks which are more\npractical require further analysis, such as manipulating environment dynamics.\nTherefore, we propose to understand the vulnerabilities of DRL from various\nperspectives and provide a thorough taxonomy of potential attacks. We conduct\nthe first set of experiments on the unexplored parts within the taxonomy. In\naddition to current observation based attacks against DRL, we propose the first\ntargeted attacks based on action space and environment dynamics. We also\nintroduce the online sequential attacks based on temporal consistency\ninformation among frames. To better estimate gradient in black-box setting, we\npropose a sampling strategy and theoretically prove its efficiency and\nestimation error bound. We conduct extensive experiments to compare the\neffectiveness of different attacks with several baselines in various\nenvironments, including game playing, robotics control, and autonomous driving.\n"], ["2019-07-17", "http://arxiv.org/abs/1907.07732", "Connecting Lyapunov Control Theory to Adversarial Attacks.", ["Arash Rahnama", " Andre T. Nguyen", " Edward Raff"], "  Significant work is being done to develop the math and tools necessary to\nbuild provable defenses, or at least bounds, against adversarial attacks of\nneural networks. In this work, we argue that tools from control theory could be\nleveraged to aid in defending against such attacks. We do this by example,\nbuilding a provable defense against a weaker adversary. This is done so we can\nfocus on the mechanisms of control theory, and illuminate its intrinsic value.\n"], ["2019-07-17", "http://arxiv.org/abs/1907.07487", "Real-time Evasion Attacks with Physical Constraints on Deep Learning-based Anomaly Detectors in Industrial Control Systems.", ["Alessandro Erba", " Riccardo Taormina", " Stefano Galelli", " Marcello Pogliani", " Michele Carminati", " Stefano Zanero", " Nils Ole Tippenhauer"], "  Recently, a number of deep learning-based anomaly detection algorithms were\nproposed to detect attacks in dynamic industrial control systems. The detectors\noperate on measured sensor data, leveraging physical process models learned a\npriori. Evading detection by such systems is challenging, as an attacker needs\nto manipulate a constrained number of sensor readings in real-time with\nrealistic perturbations according to the current state of the system. In this\nwork, we propose a number of evasion attacks (with different assumptions on the\nattacker's knowledge), and compare the attacks' cost and efficiency against\nreplay attacks. In particular, we show that a replay attack on a subset of\nsensor values can be detected easily as it violates physical constraints. In\ncontrast, our proposed attacks leverage manipulated sensor readings that\nobserve learned physical constraints of the system. Our proposed white box\nattacker uses an optimization approach with a detection oracle, while our black\nbox attacker uses an autoencoder (or a convolutional neural network) to\ntranslate anomalous data into normal data. Our proposed approaches are\nimplemented and evaluated on two different datasets pertaining to the domain of\nwater distribution networks. We then demonstrated the efficacy of the real-time\nattack on a realistic testbed. Results show that the accuracy of the detection\nalgorithms can be significantly reduced through real-time adversarial actions:\nfor the BATADAL dataset, the attacker can reduce the detection accuracy from\n0.6 to 0.14. In addition, we discuss and implement an Availability attack, in\nwhich the attacker introduces detection events with minimal changes of the\nreported data, in order to reduce confidence in the detector.\n"], ["2019-07-17", "http://arxiv.org/abs/1907.07640", "Robustness properties of Facebook's ResNeXt WSL models.", ["A. Emin Orhan"], "  We investigate the robustness properties of ResNeXt image recognition models\ntrained with billion scale weakly-supervised data (ResNeXt WSL models). These\nmodels, recently made public by Facebook AI, were trained on ~1B images from\nInstagram and fine-tuned on ImageNet. We show that these models display an\nunprecedented degree of robustness against common image corruptions and\nperturbations, as measured by the ImageNet-C and ImageNet-P benchmarks. The\nlargest of the released models, in particular, achieves state-of-the-art\nresults on both ImageNet-C and ImageNet-P by a large margin. The gains on\nImageNet-C and ImageNet-P far outpace the gains on ImageNet validation\naccuracy, suggesting the former as more useful benchmarks to measure further\nprogress in image recognition. Remarkably, the ResNeXt WSL models even achieve\na limited degree of adversarial robustness against state-of-the-art white-box\nattacks (10-step PGD attacks). However, in contrast to adversarially trained\nmodels, the robustness of the ResNeXt WSL models rapidly declines with the\nnumber of PGD steps, suggesting that these models do not achieve genuine\nadversarial robustness. Visualization of the learned features also confirms\nthis conclusion. Finally, we show that although the ResNeXt WSL models are more\nshape-biased than comparable ImageNet-trained models in a shape-texture cue\nconflict experiment, they still remain much more texture-biased than humans and\ntheir accuracy on the recently introduced \"natural adversarial examples\"\n(ImageNet-A) also remains low, suggesting that they share many of the\nunderlying characteristics of ImageNet-trained models that make these\nbenchmarks challenging.\n"], ["2019-07-16", "http://arxiv.org/abs/1907.07291", "Adversarial Security Attacks and Perturbations on Machine Learning and Deep Learning Methods.", ["Arif Siddiqi"], "  The ever-growing big data and emerging artificial intelligence (AI) demand\nthe use of machine learning (ML) and deep learning (DL) methods. Cybersecurity\nalso benefits from ML and DL methods for various types of applications. These\nmethods however are susceptible to security attacks. The adversaries can\nexploit the training and testing data of the learning models or can explore the\nworkings of those models for launching advanced future attacks. The topic of\nadversarial security attacks and perturbations within the ML and DL domains is\na recent exploration and a great interest is expressed by the security\nresearchers and practitioners. The literature covers different adversarial\nsecurity attacks and perturbations on ML and DL methods and those have their\nown presentation styles and merits. A need to review and consolidate knowledge\nthat is comprehending of this increasingly focused and growing topic of\nresearch; however, is the current demand of the research communities. In this\nreview paper, we specifically aim to target new researchers in the\ncybersecurity domain who may seek to acquire some basic knowledge on the\nmachine learning and deep learning models and algorithms, as well as some of\nthe relevant adversarial security attacks and perturbations.\n"], ["2019-07-16", "http://arxiv.org/abs/1907.07174", "Natural Adversarial Examples.", ["Dan Hendrycks", " Kevin Zhao", " Steven Basart", " Jacob Steinhardt", " Dawn Song"], "  We introduce natural adversarial examples -- real-world, unmodified, and\nnaturally occurring examples that cause classifier accuracy to significantly\ndegrade. We curate 7,500 natural adversarial examples and release them in an\nImageNet classifier test set that we call ImageNet-A. This dataset serves as a\nnew way to measure classifier robustness. Like l_p adversarial examples,\nImageNet-A examples successfully transfer to unseen or black-box classifiers.\nFor example, on ImageNet-A a DenseNet-121 obtains around 2% accuracy, an\naccuracy drop of approximately 90%. Recovering this accuracy is not simple\nbecause ImageNet-A examples exploit deep flaws in current classifiers including\ntheir over-reliance on color, texture, and background cues. We observe that\npopular training techniques for improving robustness have little effect, but we\nshow that some architectural changes can enhance robustness to natural\nadversarial examples. Future research is required to enable robust\ngeneralization to this hard ImageNet test set.\n"], ["2019-07-16", "http://arxiv.org/abs/1907.07001", "Latent Adversarial Defence with Boundary-guided Generation.", ["Xiaowei Zhou", " Ivor W. Tsang", " Jie Yin"], "  Deep Neural Networks (DNNs) have recently achieved great success in many\ntasks, which encourages DNNs to be widely used as a machine learning service in\nmodel sharing scenarios. However, attackers can easily generate adversarial\nexamples with a small perturbation to fool the DNN models to predict wrong\nlabels. To improve the robustness of shared DNN models against adversarial\nattacks, we propose a novel method called Latent Adversarial Defence (LAD). The\nproposed LAD method improves the robustness of a DNN model through adversarial\ntraining on generated adversarial examples. Different from popular attack\nmethods which are carried in the input space and only generate adversarial\nexamples of repeating patterns, LAD generates myriad of adversarial examples\nthrough adding perturbations to latent features along the normal of the\ndecision boundary which is constructed by an SVM with an attention mechanism.\nOnce adversarial examples are generated, we adversarially train the model\nthrough augmenting the training data with generated adversarial examples.\nExtensive experiments on the MNIST, SVHN, and CelebA dataset demonstrate the\neffectiveness of our model in defending against different types of adversarial\nattacks.\n"], ["2019-07-16", "http://arxiv.org/abs/1907.06826", "Adversarial Sensor Attack on LiDAR-based Perception in Autonomous Driving.", ["Yulong Cao", " Chaowei Xiao", " Benjamin Cyr", " Yimeng Zhou", " Won Park", " Sara Rampazzi", " Qi Alfred Chen", " Kevin Fu", " Z. Morley Mao"], "  In Autonomous Vehicles (AVs), one fundamental pillar is perception, which\nleverages sensors like cameras and LiDARs (Light Detection and Ranging) to\nunderstand the driving environment. Due to its direct impact on road safety,\nmultiple prior efforts have been made to study its the security of perception\nsystems. In contrast to prior work that concentrates on camera-based\nperception, in this work we perform the first security study of LiDAR-based\nperception in AV settings, which is highly important but unexplored. We\nconsider LiDAR spoofing attacks as the threat model and set the attack goal as\nspoofing obstacles close to the front of a victim AV. We find that blindly\napplying LiDAR spoofing is insufficient to achieve this goal due to the machine\nlearning-based object detection process. Thus, we then explore the possibility\nof strategically controlling the spoofed attack to fool the machine learning\nmodel. We formulate this task as an optimization problem and design modeling\nmethods for the input perturbation function and the objective function. We also\nidentify the inherent limitations of directly solving the problem using\noptimization and design an algorithm that combines optimization and global\nsampling, which improves the attack success rates to around 75%. As a case\nstudy to understand the attack impact at the AV driving decision level, we\nconstruct and evaluate two attack scenarios that may damage road safety and\nmobility. We also discuss defense directions at the AV system, sensor, and\nmachine learning model levels.\n"], ["2019-07-16", "http://arxiv.org/abs/1907.07296", "Explaining Vulnerabilities to Adversarial Machine Learning through Visual Analytics.", ["Yuxin Ma", " Tiankai Xie", " Jundong Li", " Ross Maciejewski"], "  Machine learning models are currently being deployed in a variety of\nreal-world applications where model predictions are used to make decisions\nabout healthcare, bank loans, and numerous other critical tasks. As the\ndeployment of artificial intelligence technologies becomes ubiquitous, it is\nunsurprising that adversaries have begun developing methods to manipulate\nmachine learning models to their advantage. While the visual analytics\ncommunity has developed methods for opening the black box of machine learning\nmodels, little work has focused on helping the user understand their model\nvulnerabilities in the context of adversarial attacks. In this paper, we\npresent a visual analytics framework for explaining and exploring model\nvulnerabilities to adversarial attacks. Our framework employs a multi-faceted\nvisualization scheme designed to support the analysis of data poisoning attacks\nfrom the perspective of models, data instances, features, and local structures.\nWe demonstrate our framework through two case studies on binary classifiers and\nillustrate model vulnerabilities with respect to varying attack strategies.\n"], ["2019-07-15", "http://arxiv.org/abs/1907.06800", "Graph Interpolating Activation Improves Both Natural and Robust Accuracies in Data-Efficient Deep Learning.", ["Bao Wang", " Stanley J. Osher"], "  Improving the accuracy and robustness of deep neural nets (DNNs) and adapting\nthem to small training data are primary tasks in deep learning research. In\nthis paper, we replace the output activation function of DNNs, typically the\ndata-agnostic softmax function, with a graph Laplacian-based high dimensional\ninterpolating function which, in the continuum limit, converges to the solution\nof a Laplace-Beltrami equation on a high dimensional manifold. Furthermore, we\npropose end-to-end training and testing algorithms for this new architecture.\nThe proposed DNN with graph interpolating activation integrates the advantages\nof both deep learning and manifold learning. Compared to the conventional DNNs\nwith the softmax function as output activation, the new framework demonstrates\nthe following major advantages: First, it is better applicable to\ndata-efficient learning in which we train high capacity DNNs without using a\nlarge number of training data. Second, it remarkably improves both natural\naccuracy on the clean images and robust accuracy on the adversarial images\ncrafted by both white-box and black-box adversarial attacks. Third, it is a\nnatural choice for semi-supervised learning. For reproducibility, the code is\navailable at \\url{https://github.com/BaoWangMath/DNN-DataDependentActivation}.\n"], ["2019-07-15", "http://arxiv.org/abs/1907.06565", "Recovery Guarantees for Compressible Signals with Adversarial Noise.", ["Jasjeet Dhaliwal", " Kyle Hambrook"], "  We provide recovery guarantees for compressible signals that have been\ncorrupted with noise and extend the framework introduced in [1] to defend\nneural networks against $\\ell_0$-norm and $\\ell_2$-norm attacks. Concretely,\nfor a signal that is approximately sparse in some transform domain and has been\nperturbed with noise, we provide guarantees for accurately recovering the\nsignal in the transform domain. We can then use the recovered signal to\nreconstruct the signal in its original domain while largely removing the noise.\nOur results are general as they can be directly applied to most unitary\ntransforms used in practice and hold for both $\\ell_0$-norm bounded noise and\n$\\ell_2$-norm bounded noise. In the case of $\\ell_0$-norm bounded noise, we\nprove recovery guarantees for Iterative Hard Thresholding (IHT) and Basis\nPursuit (BP). For the case of $\\ell_2$-norm bounded noise, we provide recovery\nguarantees for BP. These guarantees theoretically bolster the defense framework\nintroduced in [1] for defending neural networks against adversarial inputs.\nFinally, we experimentally demonstrate this defense framework using both IHT\nand BP against the One Pixel Attack [21], Carlini-Wagner $\\ell_0$ and $\\ell_2$\nattacks [3], Jacobian Saliency Based attack [18], and the DeepFool attack [17]\non CIFAR-10 [12], MNIST [13], and Fashion-MNIST [27] datasets. This expands\nbeyond the experimental demonstrations of [1].\n"], ["2019-07-14", "http://arxiv.org/abs/1907.06291", "Measuring the Transferability of Adversarial Examples.", ["Deyan Petrov", " Timothy M. Hospedales"], "  Adversarial examples are of wide concern due to their impact on the\nreliability of contemporary machine learning systems. Effective adversarial\nexamples are mostly found via white-box attacks. However, in some cases they\ncan be transferred across models, thus enabling them to attack black-box\nmodels. In this work we evaluate the transferability of three adversarial\nattacks - the Fast Gradient Sign Method, the Basic Iterative Method, and the\nCarlini & Wagner method, across two classes of models - the VGG class(using\nVGG16, VGG19 and an ensemble of VGG16 and VGG19), and the Inception\nclass(Inception V3, Xception, Inception Resnet V2, and an ensemble of the\nthree). We also outline the problems with the assessment of transferability in\nthe current body of research and attempt to amend them by picking specific\n\"strong\" parameters for the attacks, and by using a L-Infinity clipping\ntechnique and the SSIM metric for the final evaluation of the attack\ntransferability.\n"], ["2019-07-12", "http://arxiv.org/abs/1907.05600", "Generative Modeling by Estimating Gradients of the Data Distribution.", ["Yang Song", " Stefano Ermon"], "  We introduce a new generative model where samples are produced via Langevin\ndynamics using gradients of the data distribution estimated with score\nmatching. Because gradients might be ill-defined when the data resides on\nlow-dimensional manifolds, we perturb the data with different levels of\nGaussian noise and jointly estimate the corresponding scores, i.e., the vector\nfields of gradients of the perturbed data distribution for all noise levels.\nFor sampling, we propose an annealed Langevin dynamics where we use gradients\ncorresponding to gradually decreasing noise levels as the sampling process gets\ncloser to the data manifold. Our framework allows flexible model architectures,\nrequires no sampling during training or the use of adversarial methods, and\nprovides a learning objective that can be used for principled model\ncomparisons. Our models produce samples comparable to GANs on MNIST, CelebA and\nCIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.91 on\nCIFAR-10. Additionally, we demonstrate that our models learn effective\nrepresentations via image inpainting experiments.\n"], ["2019-07-12", "http://arxiv.org/abs/1907.05793", "Unsupervised Adversarial Attacks on Deep Feature-based Retrieval with GAN.", ["Guoping Zhao", " Mingyu Zhang", " Jiajun Liu", " Ji-Rong Wen"], "  Studies show that Deep Neural Network (DNN)-based image classification models\nare vulnerable to maliciously constructed adversarial examples. However, little\neffort has been made to investigate how DNN-based image retrieval models are\naffected by such attacks. In this paper, we introduce Unsupervised Adversarial\nAttacks with Generative Adversarial Networks (UAA-GAN) to attack deep\nfeature-based image retrieval systems. UAA-GAN is an unsupervised learning\nmodel that requires only a small amount of unlabeled data for training. Once\ntrained, it produces query-specific perturbations for query images to form\nadversarial queries. The core idea is to ensure that the attached perturbation\nis barely perceptible to human yet effective in pushing the query away from its\noriginal position in the deep feature space. UAA-GAN works with various\napplication scenarios that are based on deep features, including image\nretrieval, person Re-ID and face search. Empirical results show that UAA-GAN\ncripples retrieval performance without significant visual changes in the query\nimages. UAA-GAN generated adversarial examples are less distinguishable because\nthey tend to incorporate subtle perturbations in textured or salient areas of\nthe images, such as key body parts of human, dominant structural\npatterns/textures or edges, rather than in visually insignificant areas (e.g.,\nbackground and sky). Such tendency indicates that the model indeed learned how\nto toy with both image retrieval systems and human eyes.\n"], ["2019-07-12", "http://arxiv.org/abs/1907.05587", "Stateful Detection of Black-Box Adversarial Attacks.", ["Steven Chen", " Nicholas Carlini", " David Wagner"], "  The problem of adversarial examples, evasion attacks on machine learning\nclassifiers, has proven extremely difficult to solve. This is true even when,\nas is the case in many practical settings, the classifier is hosted as a remote\nservice and so the adversary does not have direct access to the model\nparameters.\n  This paper argues that in such settings, defenders have a much larger space\nof actions than have been previously explored. Specifically, we deviate from\nthe implicit assumption made by prior work that a defense must be a stateless\nfunction that operates on individual examples, and explore the possibility for\nstateful defenses.\n  To begin, we develop a defense designed to detect the process of adversarial\nexample generation. By keeping a history of the past queries, a defender can\ntry to identify when a sequence of queries appears to be for the purpose of\ngenerating an adversarial example. We then introduce query blinding, a new\nclass of attacks designed to bypass defenses that rely on such a defense\napproach.\n  We believe that expanding the study of adversarial examples from stateless\nclassifiers to stateful systems is not only more realistic for many black-box\nsettings, but also gives the defender a much-needed advantage in responding to\nthe adversary.\n"], ["2019-07-11", "http://arxiv.org/abs/1907.05718", "Why Blocking Targeted Adversarial Perturbations Impairs the Ability to Learn.", ["Ziv Katzir", " Yuval Elovici"], "  Despite their accuracy, neural network-based classifiers are still prone to\nmanipulation through adversarial perturbations. Those perturbations are\ndesigned to be misclassified by the neural network, while being perceptually\nidentical to some valid input. The vast majority of attack methods rely on\nwhite-box conditions, where the attacker has full knowledge of the attacked\nnetwork's parameters. This allows the attacker to calculate the network's loss\ngradient with respect to some valid input and use this gradient in order to\ncreate an adversarial example. The task of blocking white-box attacks has\nproven difficult to solve. While a large number of defense methods have been\nsuggested, they have had limited success. In this work we examine this\ndifficulty and try to understand it. We systematically explore the abilities\nand limitations of defensive distillation, one of the most promising defense\nmechanisms against adversarial perturbations suggested so far in order to\nunderstand the defense challenge. We show that contrary to commonly held\nbelief, the ability to bypass defensive distillation is not dependent on an\nattack's level of sophistication. In fact, simple approaches, such as the\nTargeted Gradient Sign Method, are capable of effectively bypassing defensive\ndistillation. We prove that defensive distillation is highly effective against\nnon-targeted attacks but is unsuitable for targeted attacks. This discovery\nleads us to realize that targeted attacks leverage the same input gradient that\nallows a network to be trained. This implies that blocking them will require\nlosing the network's ability to learn, presenting an impossible tradeoff to the\nresearch community.\n"], ["2019-07-11", "http://arxiv.org/abs/1907.05418", "Adversarial Objects Against LiDAR-Based Autonomous Driving Systems.", ["Yulong Cao", " Chaowei Xiao", " Dawei Yang", " Jing Fang", " Ruigang Yang", " Mingyan Liu", " Bo Li"], "  Deep neural networks (DNNs) are found to be vulnerable against adversarial\nexamples, which are carefully crafted inputs with a small magnitude of\nperturbation aiming to induce arbitrarily incorrect predictions. Recent studies\nshow that adversarial examples can pose a threat to real-world\nsecurity-critical applications: a \"physical adversarial Stop Sign\" can be\nsynthesized such that the autonomous driving cars will misrecognize it as\nothers (e.g., a speed limit sign). However, these image-space adversarial\nexamples cannot easily alter 3D scans of widely equipped LiDAR or radar on\nautonomous vehicles. In this paper, we reveal the potential vulnerabilities of\nLiDAR-based autonomous driving detection systems, by proposing an optimization\nbased approach LiDAR-Adv to generate adversarial objects that can evade the\nLiDAR-based detection system under various conditions. We first show the\nvulnerabilities using a blackbox evolution-based algorithm, and then explore\nhow much a strong adversary can do, using our gradient-based approach\nLiDAR-Adv. We test the generated adversarial objects on the Baidu Apollo\nautonomous driving platform and show that such physical systems are indeed\nvulnerable to the proposed attacks. We also 3D-print our adversarial objects\nand perform physical experiments to illustrate that such vulnerability exists\nin the real world. Please find more visualizations and results on the anonymous\nwebsite: https://sites.google.com/view/lidar-adv.\n"], ["2019-07-10", "http://arxiv.org/abs/1907.04774", "Metamorphic Detection of Adversarial Examples in Deep Learning Models With Affine Transformations.", ["Rohan Reddy Mekala", " Gudjon Einar Magnusson", " Adam Porter", " Mikael Lindvall", " Madeline Diep"], "  Adversarial attacks are small, carefully crafted perturbations, imperceptible\nto the naked eye; that when added to an image cause deep learning models to\nmisclassify the image with potentially detrimental outcomes. With the rise of\nartificial intelligence models in consumer safety and security intensive\nindustries such as self-driving cars, camera surveillance and face recognition,\nthere is a growing need for guarding against adversarial attacks. In this\npaper, we present an approach that uses metamorphic testing principles to\nautomatically detect such adversarial attacks. The approach can detect image\nmanipulations that are so small, that they are impossible to detect by a human\nthrough visual inspection. By applying metamorphic relations based on distance\nratio preserving affine image transformations which compare the behavior of the\noriginal and transformed image; we show that our proposed approach can\ndetermine whether or not the input image is adversarial with a high degree of\naccuracy.\n"], ["2019-07-09", "http://arxiv.org/abs/1907.04449", "Generating Adversarial Fragments with Adversarial Networks for Physical-world Implementation.", ["Zelun Kong", " Cong Liu"], "  Although deep neural networks have been widely applied in many application\ndomains, they are found to be vulnerable to adversarial attacks. A recent\npromising set of attacking techniques have been proposed, which mainly focus on\ngenerating adversarial examples under digital-world settings. Such strategies\nare unfortunately not implementable for any physical-world scenarios such as\nautonomous driving. In this paper, we present FragGAN, a new GAN-based\nframework which is capable of generating an adversarial image which differs\nfrom the original input image only through replacing a targeted fragment within\nthe image using a corresponding visually indistinguishable adversarial\nfragment. FragGAN ensures that the resulting entire image is effective in\nattacking. For any physical-world implementation, an attacker could physically\nprint out the adversarial fragment and then paste it onto the original fragment\n(e.g., a roadside sign for autonomous driving scenarios). FragGAN also enables\nclean-label attacks against image classification, as the resulting attacks may\nsucceed even without modifying any essential content of an image. Extensive\nexperiments including physical-world case studies on state-of-the-art\nautonomous steering and image classification models demonstrate that FragGAN is\nhighly effective and superior to simple extensions of existing approaches. To\nthe best of our knowledge, FragGAN is the first approach that can implement\neffective and clean-label physical-world attacks.\n"], ["2019-07-06", "http://arxiv.org/abs/1907.05274", "Affine Disentangled GAN for Interpretable and Robust AV Perception.", ["Letao Liu", " Martin Saerbeck", " Justin Dauwels"], "  Autonomous vehicles (AV) have progressed rapidly with the advancements in\ncomputer vision algorithms. The deep convolutional neural network as the main\ncontributor to this advancement has boosted the classification accuracy\ndramatically. However, the discovery of adversarial examples reveals the\ngeneralization gap between dataset and the real world. Furthermore, affine\ntransformations may also confuse computer vision based object detectors. The\ndegradation of the perception system is undesirable for safety critical systems\nsuch as autonomous vehicles. In this paper, a deep learning system is proposed:\nAffine Disentangled GAN (ADIS-GAN), which is robust against affine\ntransformations and adversarial attacks. It is demonstrated that conventional\ndata augmentation for affine transformation and adversarial attacks are\northogonal, while ADIS-GAN can handle both attacks at the same time. Useful\ninformation such as image rotation angle and scaling factor are also generated\nin ADIS-GAN. On MNIST dataset, ADIS-GAN can achieve over 98 percent\nclassification accuracy within 30 degrees rotation, and over 90 percent\nclassification accuracy against FGSM and PGD adversarial attack.\n"], ["2019-07-05", "http://arxiv.org/abs/1907.02957", "Detecting and Diagnosing Adversarial Images with Class-Conditional Capsule Reconstructions.", ["Yao Qin", " Nicholas Frosst", " Sara Sabour", " Colin Raffel", " Garrison Cottrell", " Geoffrey Hinton"], "  Adversarial examples raise questions about whether neural network models are\nsensitive to the same visual features as humans. Most of the proposed methods\nfor mitigating adversarial examples have subsequently been defeated by stronger\nattacks. Motivated by these issues, we take a different approach and propose to\ninstead detect adversarial examples based on class-conditional reconstructions\nof the input. Our method uses the reconstruction network proposed as part of\nCapsule Networks (CapsNets), but is general enough to be applied to standard\nconvolutional networks. We find that adversarial or otherwise corrupted images\nresult in much larger reconstruction errors than normal inputs, prompting a\nsimple detection method by thresholding the reconstruction error. Based on\nthese findings, we propose the Reconstructive Attack which seeks both to cause\na misclassification and a low reconstruction error. While this attack produces\nundetected adversarial examples, we find that for CapsNets the resulting\nperturbations can cause the images to appear visually more like the target\nclass. This suggests that CapsNets utilize features that are more aligned with\nhuman perception and address the central issue raised by adversarial examples.\n"], ["2019-07-04", "http://arxiv.org/abs/1907.02610", "Adversarial Robustness through Local Linearization.", ["Chongli Qin", " James Martens", " Sven Gowal", " Dilip Krishnan", " Krishnamurthy Dvijotham", " Alhussein Fawzi", " Soham De", " Robert Stanforth", " Pushmeet Kohli"], "  Adversarial training is an effective methodology for training deep neural\nnetworks that are robust against adversarial, norm-bounded perturbations.\nHowever, the computational cost of adversarial training grows prohibitively as\nthe size of the model and number of input dimensions increase. Further,\ntraining against less expensive and therefore weaker adversaries produces\nmodels that are robust against weak attacks but break down under attacks that\nare stronger. This is often attributed to the phenomenon of gradient\nobfuscation; such models have a highly non-linear loss surface in the vicinity\nof training examples, making it hard for gradient-based attacks to succeed even\nthough adversarial examples still exist. In this work, we introduce a novel\nregularizer that encourages the loss to behave linearly in the vicinity of the\ntraining data, thereby penalizing gradient obfuscation while encouraging\nrobustness. We show via extensive experiments on CIFAR-10 and ImageNet, that\nmodels trained with our regularizer avoid gradient obfuscation and can be\ntrained significantly faster than adversarial training. Using this regularizer,\nwe exceed current state of the art and achieve 47% adversarial accuracy for\nImageNet with l-infinity adversarial perturbations of radius 4/255 under an\nuntargeted, strong, white-box attack. Additionally, we match state of the art\nresults for CIFAR-10 at 8/255.\n"], ["2019-07-04", "http://arxiv.org/abs/1907.02477", "Adversarial Attacks in Sound Event Classification.", ["Vinod Subramanian", " Emmanouil Benetos", " Ning Xu", " SKoT McDonald", " Mark Sandler"], "  Adversarial attacks refer to a set of methods that perturb the input to a\nclassification model in order to fool the classifier. In this paper we apply\ndifferent gradient based adversarial attack algorithms on five deep learning\nmodels trained for sound event classification. Four of the models use\nmel-spectrogram input and one model uses raw audio input. The models represent\nstandard architectures such as convolutional, recurrent and dense networks. The\ndataset used for training is the Freesound dataset released for task 2 of the\nDCASE 2018 challenge and the models used are from participants of the challenge\nwho open sourced their code. Our experiments show that adversarial attacks can\nbe generated with high confidence and low perturbation. In addition, we show\nthat the adversarial attacks are very effective across the different models.\n"], ["2019-07-03", "http://arxiv.org/abs/1907.01996", "Robust Synthesis of Adversarial Visual Examples Using a Deep Image Prior.", ["Thomas Gittings", " Steve Schneider", " John Collomosse"], "  We present a novel method for generating robust adversarial image examples\nbuilding upon the recent `deep image prior' (DIP) that exploits convolutional\nnetwork architectures to enforce plausible texture in image synthesis.\nAdversarial images are commonly generated by perturbing images to introduce\nhigh frequency noise that induces image misclassification, but that is fragile\nto subsequent digital manipulation of the image. We show that using DIP to\nreconstruct an image under adversarial constraint induces perturbations that\nare more robust to affine deformation, whilst remaining visually imperceptible.\nFurthermore we show that our DIP approach can also be adapted to produce local\nadversarial patches (`adversarial stickers'). We demonstrate robust adversarial\nexamples over a broad gamut of images and object classes drawn from the\nImageNet dataset.\n"], ["2019-07-03", "http://arxiv.org/abs/1907.02044", "Minimally distorted Adversarial Examples with a Fast Adaptive Boundary Attack.", ["Francesco Croce", " Matthias Hein"], "  The evaluation of robustness against adversarial manipulation of neural\nnetworks-based classifiers is mainly tested with empirical attacks as the\nmethods for the exact computation, even when available, do not scale to large\nnetworks. We propose in this paper a new white-box adversarial attack wrt the\n$l_p$-norms for $p \\in \\{1,2,\\infty\\}$ aiming at finding the minimal\nperturbation necessary to change the class of a given input. It has an\nintuitive geometric meaning, yields high quality results already with one\nrestart, minimizes the size of the perturbation, so that the robust accuracy\ncan be evaluated at all possible thresholds with a single run, and comes with\nalmost no free parameters except number of iterations and restarts. It achieves\nbetter or similar robust test accuracy compared to state-of-the-art attacks\nwhich are partially specialized to one $l_p$-norm.\n"], ["2019-07-02", "http://arxiv.org/abs/1907.01216", "Efficient Cyber Attacks Detection in Industrial Control Systems Using Lightweight Neural Networks and PCA.", ["Moshe Kravchik", " Asaf Shabtai"], "  Industrial control systems (ICSs) are widely used and vital to industry and\nsociety. Their failure can have severe impact on both economics and human life.\nHence, these systems have become an attractive target for attacks, both\nphysical and cyber. A number of attack detection methods have been proposed,\nhowever they are characterized by a low detection rate, a substantial false\npositive rate, or are system specific. In this paper, we study an attack\ndetection method based on simple and lightweight neural networks, namely, 1D\nconvolutions and autoencoders. We apply these networks to both the time and\nfrequency domains of the collected data and discuss pros and cons of each\napproach. We evaluate the suggested method on three popular public datasets and\nachieve detection rates matching or exceeding previously published detection\nresults, while featuring small footprint, short training and detection times,\nand generality. We also demonstrate the effectiveness of PCA, which, given\nproper data preprocessing and feature selection, can provide high attack\ndetection scores in many settings. Finally, we study the proposed method's\nrobustness against adversarial attacks, that exploit inherent blind spots of\nneural networks to evade detection while achieving their intended physical\neffect. Our results show that the proposed method is robust to such evasion\nattacks: in order to evade detection, the attacker is forced to sacrifice the\ndesired physical impact on the system. This finding suggests that neural\nnetworks trained under the constraints of the laws of physics can be trusted\nmore than networks trained under more flexible conditions.\n"], ["2019-07-02", "http://arxiv.org/abs/1907.01197", "Treant: Training Evasion-Aware Decision Trees.", ["Stefano Calzavara", " Claudio Lucchese", " Gabriele Tolomei", " Seyum Assefa Abebe", " Salvatore Orlando"], "  Despite its success and popularity, machine learning is now recognized as\nvulnerable to evasion attacks, i.e., carefully crafted perturbations of test\ninputs designed to force prediction errors. In this paper we focus on evasion\nattacks against decision tree ensembles, which are among the most successful\npredictive models for dealing with non-perceptual problems. Even though they\nare powerful and interpretable, decision tree ensembles have received only\nlimited attention by the security and machine learning communities so far,\nleading to a sub-optimal state of the art for adversarial learning techniques.\nWe thus propose Treant, a novel decision tree learning algorithm that, on the\nbasis of a formal threat model, minimizes an evasion-aware loss function at\neach step of the tree construction. Treant is based on two key technical\ningredients: robust splitting and attack invariance, which jointly guarantee\nthe soundness of the learning process. Experimental results on three publicly\navailable datasets show that Treant is able to generate decision tree ensembles\nthat are at the same time accurate and nearly insensitive to evasion attacks,\noutperforming state-of-the-art adversarial learning techniques.\n"], ["2019-07-01", "http://arxiv.org/abs/1907.00895", "Comment on \"Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network\".", ["Roland S. Zimmermann"], "  A recent paper by Liu et al. combines the topics of adversarial training and\nBayesian Neural Networks (BNN) and suggests that adversarially trained BNNs are\nmore robust against adversarial attacks than their non-Bayesian counterparts.\nHere, I analyze the proposed defense and suggest that one needs to adjust the\nadversarial attack to incorporate the stochastic nature of a Bayesian network\nto perform an accurate evaluation of its robustness. Using this new type of\nattack I show that there appears to be no strong evidence for higher robustness\nof the adversarially trained BNNs.\n"], ["2019-07-01", "http://arxiv.org/abs/1907.01023", "Diminishing the Effect of Adversarial Perturbations via Refining Feature Representation.", ["Nader Asadi", " AmirMohammad Sarfi", " Sahba Tahsini", " Mahdi Eftekhari"], "  Deep neural networks are highly vulnerable to adversarial examples, which\nimposes severe security issues for these state-of-the-art models. Many defense\nmethods have been proposed to mitigate this problem. However, a lot of them\ndepend on modification or additional training of the target model. In this\nwork, we analytically investigate each layer representation of non-perturbed\nand perturbed images and show the effect of perturbations on each of these\nrepresentations. Accordingly, a method based on whitening coloring transform is\nproposed in order to diminish the misrepresentation of any desirable layer\ncaused by adversaries. Our method can be applied to any layer of any arbitrary\nmodel without the need of any modification or additional training. Due to the\nfact that full whitening of the layer representation is not easily\ndifferentiable, our proposed method is superbly robust against white-box\nattacks. Furthermore, we demonstrate the strength of our method against some\nstate-of-the-art black-box attacks such as Carlini-Wagner L2 attack and we show\nthat our method is able to defend against some non-constrained attacks.\n"], ["2019-07-01", "http://arxiv.org/abs/1907.01003", "Accurate, reliable and fast robustness evaluation.", ["Wieland Brendel", " Jonas Rauber", " Matthias K\u00fcmmerer", " Ivan Ustyuzhaninov", " Matthias Bethge"], "  Throughout the past five years, the susceptibility of neural networks to\nminimal adversarial perturbations has moved from a peculiar phenomenon to a\ncore issue in Deep Learning. Despite much attention, however, progress towards\nmore robust models is significantly impaired by the difficulty of evaluating\nthe robustness of neural network models. Today's methods are either fast but\nbrittle (gradient-based attacks), or they are fairly reliable but slow (score-\nand decision-based attacks). We here develop a new set of gradient-based\nadversarial attacks which (a) are more reliable in the face of gradient-masking\nthan other gradient-based attacks, (b) perform better and are more query\nefficient than current state-of-the-art gradient-based attacks, (c) can be\nflexibly adapted to a wide range of adversarial criteria and (d) require\nvirtually no hyperparameter tuning. These findings are carefully validated\nacross a diverse set of six different models and hold for L2 and L_infinity in\nboth targeted as well as untargeted scenarios. Implementations will be made\navailable in all major toolboxes (Foolbox, CleverHans and ART). Furthermore, we\nwill soon add additional content and experiments, including L0 and L1 versions\nof our attack as well as additional comparisons to other L2 and L_infinity\nattacks. We hope that this class of attacks will make robustness evaluations\neasier and more reliable, thus contributing to more signal in the search for\nmore robust machine learning models.\n"], ["2019-06-30", "http://arxiv.org/abs/1907.00374", "Fooling a Real Car with Adversarial Traffic Signs.", ["Nir Morgulis", " Alexander Kreines", " Shachar Mendelowitz", " Yuval Weisglass"], "  The attacks on the neural-network-based classifiers using adversarial images\nhave gained a lot of attention recently. An adversary can purposely generate an\nimage that is indistinguishable from a innocent image for a human being but is\nincorrectly classified by the neural networks. The adversarial images do not\nneed to be tuned to a particular architecture of the classifier - an image that\nfools one network can fool another one with a certain success rate.The\npublished works mostly concentrate on the use of modified image files for\nattacks against the classifiers trained on the model databases. Although there\nexists a general understanding that such attacks can be carried in the real\nworld as well, the works considering the real-world attacks are scarce.\nMoreover, to the best of our knowledge, there have been no reports on the\nattacks against real production-grade image classification systems.In our work\nwe present a robust pipeline for reproducible production of adversarial traffic\nsigns that can fool a wide range of classifiers, both open-source and\nproduction-grade in the real world. The efficiency of the attacks was checked\nboth with the neural-network-based classifiers and legacy computer vision\nsystems. Most of the attacks have been performed in the black-box mode, e.g.\nthe adversarial signs produced for a particular classifier were used to attack\na variety of other classifiers. The efficiency was confirmed in drive-by\nexperiments with a production-grade traffic sign recognition systems of a real\ncar.\n"], ["2019-06-28", "http://arxiv.org/abs/1906.12340", "Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty.", ["Dan Hendrycks", " Mantas Mazeika", " Saurav Kadavath", " Dawn Song"], "  Self-supervision provides effective representations for downstream tasks\nwithout requiring labels. However, existing approaches lag behind fully\nsupervised training and are often not thought beneficial beyond obviating the\nneed for annotations. We find that self-supervision can benefit robustness in a\nvariety of ways, including robustness to adversarial examples, label\ncorruption, and common input corruptions. Additionally, self-supervision\ngreatly benefits out-of-distribution detection on difficult, near-distribution\noutliers, so much so that it exceeds the performance of fully supervised\nmethods. These results demonstrate the promise of self-supervision for\nimproving robustness and uncertainty estimation and establish these tasks as\nnew axes of evaluation for future self-supervised learning research.\n"], ["2019-06-28", "http://arxiv.org/abs/1906.12269", "Certifiable Robustness and Robust Training for Graph Convolutional Networks.", ["Daniel Z\u00fcgner", " Stephan G\u00fcnnemann"], "  Recent works show that Graph Neural Networks (GNNs) are highly non-robust\nwith respect to adversarial attacks on both the graph structure and the node\nattributes, making their outcomes unreliable. We propose the first method for\ncertifiable (non-)robustness of graph convolutional networks with respect to\nperturbations of the node attributes. We consider the case of binary node\nattributes (e.g. bag-of-words) and perturbations that are L_0-bounded. If a\nnode has been certified with our method, it is guaranteed to be robust under\nany possible perturbation given the attack model. Likewise, we can certify\nnon-robustness. Finally, we propose a robust semi-supervised training procedure\nthat treats the labeled and unlabeled nodes jointly. As shown in our\nexperimental evaluation, our method significantly improves the robustness of\nthe GNN with only minimal effect on the predictive accuracy.\n"], ["2019-06-28", "http://arxiv.org/abs/1906.12061", "Learning to Cope with Adversarial Attacks.", ["Xian Yeow Lee", " Aaron Havens", " Girish Chowdhary", " Soumik Sarkar"], "  The security of Deep Reinforcement Learning (Deep RL) algorithms deployed in\nreal life applications are of a primary concern. In particular, the robustness\nof RL agents in cyber-physical systems against adversarial attacks are\nespecially vital since the cost of a malevolent intrusions can be extremely\nhigh. Studies have shown Deep Neural Networks (DNN), which forms the core\ndecision-making unit in most modern RL algorithms, are easily subjected to\nadversarial attacks. Hence, it is imperative that RL agents deployed in\nreal-life applications have the capability to detect and mitigate adversarial\nattacks in an online fashion. An example of such a framework is the\nMeta-Learned Advantage Hierarchy (MLAH) agent that utilizes a meta-learning\nframework to learn policies robustly online. Since the mechanism of this\nframework are still not fully explored, we conducted multiple experiments to\nbetter understand the framework's capabilities and limitations. Our results\nshows that the MLAH agent exhibits interesting coping behaviors when subjected\nto different adversarial attacks to maintain a nominal reward. Additionally,\nthe framework exhibits a hierarchical coping capability, based on the\nadaptability of the Master policy and sub-policies themselves. From empirical\nresults, we also observed that as the interval of adversarial attacks increase,\nthe MLAH agent can maintain a higher distribution of rewards, though at the\ncost of higher instabilities.\n"], ["2019-06-28", "http://arxiv.org/abs/1907.00098", "Robustness Guarantees for Deep Neural Networks on Videos.", ["Min Wu", " Marta Kwiatkowska"], "  The widespread adoption of deep learning models places demands on their\nrobustness. In this paper, we consider the robustness of deep neural networks\non videos, which comprise both the spatial features of individual frames\nextracted by a convolutional neural network and the temporal dynamics between\nadjacent frames captured by a recurrent neural network. To measure robustness,\nwe study the maximum safe radius problem, which computes the minimum distance\nfrom the optical flow set obtained from a given input to that of an adversarial\nexample in the norm ball. We demonstrate that, under the assumption of\nLipschitz continuity, the problem can be approximated using finite optimisation\nvia discretising the optical flow space, and the approximation has provable\nguarantees. We then show that the finite optimisation problem can be solved by\nutilising a two-player turn-based game in a cooperative setting, where the\nfirst player selects the optical flows and the second player determines the\ndimensions to be manipulated in the chosen flow. We employ an anytime approach\nto solve the game, in the sense of approximating the value of the game by\nmonotonically improving its upper and lower bounds. We exploit a gradient-based\nsearch algorithm to compute the upper bounds, and the admissible A* algorithm\nto update the lower bounds. Finally, we evaluate our framework on the UCF101\nvideo dataset.\n"], ["2019-06-27", "http://arxiv.org/abs/1906.11729", "Using Intuition from Empirical Properties to Simplify Adversarial Training Defense.", ["Guanxiong Liu", " Issa Khalil", " Abdallah Khreishah"], "  Due to the surprisingly good representation power of complex distributions,\nneural network (NN) classifiers are widely used in many tasks which include\nnatural language processing, computer vision and cyber security. In recent\nworks, people noticed the existence of adversarial examples. These adversarial\nexamples break the NN classifiers' underlying assumption that the environment\nis attack free and can easily mislead fully trained NN classifier without\nnoticeable changes. Among defensive methods, adversarial training is a popular\nchoice. However, original adversarial training with single-step adversarial\nexamples (Single-Adv) can not defend against iterative adversarial examples.\nAlthough adversarial training with iterative adversarial examples (Iter-Adv)\ncan defend against iterative adversarial examples, it consumes too much\ncomputational power and hence is not scalable. In this paper, we analyze\nIter-Adv techniques and identify two of their empirical properties. Based on\nthese properties, we propose modifications which enhance Single-Adv to perform\ncompetitively as Iter-Adv. Through preliminary evaluation, we show that the\nproposed method enhances the test accuracy of state-of-the-art (SOTA)\nSingle-Adv defensive method against iterative adversarial examples by up to\n16.93% while reducing its training cost by 28.75%.\n"], ["2019-06-27", "http://arxiv.org/abs/1906.11667", "Evolving Robust Neural Architectures to Defend from Adversarial Attacks.", ["Danilo Vasconcellos Vargas", " Shashank Kotyan"], "  Deep neural networks were shown to misclassify slightly modified input\nimages. Recently, many defenses have been proposed but none have improved\nconsistently the robustness of neural networks. Here, we propose to use attacks\nas a function evaluation to automatically search for architectures that can\nresist such attacks. Experiments on neural architecture search algorithms from\nthe literature show that although their accurate results, they are not able to\nfind robust architectures. Most of the reason for this lies in their limited\nsearch space. By creating a novel neural architecture search with options for\ndense layers to connect with convolution layers and vice-versa as well as the\naddition of multiplication, addition and concatenation layers in the search\nspace, we were able to evolve an architecture that is $58\\%$ accurate on\nadversarial samples. Interestingly, this inherent robustness of the evolved\narchitecture rivals state-of-the-art defenses such as adversarial training\nwhile being trained only on the training dataset. Moreover, the evolved\narchitecture makes use of some peculiar traits which might be useful for\ndeveloping even more robust ones. Thus, the results here demonstrate that more\nrobust architectures exist as well as opens up a new range of possibilities for\nthe development and exploration of deep neural networks using automatic\narchitecture search.\n  Code available at http://bit.ly/RobustArchitectureSearch.\n"], ["2019-06-27", "http://arxiv.org/abs/1906.11567", "Adversarial Robustness via Label-Smoothing.", ["Morgane Goibert", " Elvis Dohmatob"], "  We study Label-Smoothing as a means for improving adversarial robustness of\nsupervised deep-learning models. After establishing a thorough and unified\nframework, we propose several variations to this general method: adversarial,\nBoltzmann and second-best Label-Smoothing methods, and we explain how to\nconstruct your own one. On various datasets (MNIST, CIFAR10, SVHN) and models\n(linear models, MLPs, LeNet, ResNet), we show that Label-Smoothing in general\nimproves adversarial robustness against a variety of attacks (FGSM, BIM,\nDeepFool, Carlini-Wagner) by better taking account of the dataset geometry. The\nproposed Label-Smoothing methods have two main advantages: they can be\nimplemented as a modified cross-entropy loss, thus do not require any\nmodifications of the network architecture nor do they lead to increased\ntraining times, and they improve both standard and adversarial accuracy.\n"], ["2019-06-26", "http://arxiv.org/abs/1906.11327", "The Adversarial Robustness of Sampling.", ["Omri Ben-Eliezer", " Eylon Yogev"], "  Random sampling is a fundamental primitive in modern algorithms, statistics,\nand machine learning, used as a generic method to obtain a small yet\n\"representative\" subset of the data. In this work, we investigate the\nrobustness of sampling against adaptive adversarial attacks in a streaming\nsetting: An adversary sends a stream of elements from a universe $U$ to a\nsampling algorithm (e.g., Bernoulli sampling or reservoir sampling), with the\ngoal of making the sample \"very unrepresentative\" of the underlying data\nstream. The adversary is fully adaptive in the sense that it knows the exact\ncontent of the sample at any given point along the stream, and can choose which\nelement to send next accordingly, in an online manner.\n  Well-known results in the static setting indicate that if the full stream is\nchosen in advance (non-adaptively), then a random sample of size $\\Omega(d /\n\\varepsilon^2)$ is an $\\varepsilon$-approximation of the full data with good\nprobability, where $d$ is the VC-dimension of the underlying set system\n$(U,R)$. Does this sample size suffice for robustness against an adaptive\nadversary? The simplistic answer is \\emph{negative}: We demonstrate a set\nsystem where a constant sample size (corresponding to VC-dimension $1$)\nsuffices in the static setting, yet an adaptive adversary can make the sample\nvery unrepresentative, as long as the sample size is (strongly) sublinear in\nthe stream length, using a simple and easy-to-implement attack.\n  However, this attack is \"theoretical only\", requiring the set system size to\n(essentially) be exponential in the stream length. This is not a coincidence:\nWe show that to make Bernoulli or reservoir sampling robust against adaptive\nadversaries, the modification required is solely to replace the VC-dimension\nterm $d$ in the sample size with the cardinality term $\\log |R|$. This nearly\nmatches the bound imposed by the attack.\n"], ["2019-06-26", "http://arxiv.org/abs/1906.10973", "Defending Adversarial Attacks by Correcting logits.", ["Yifeng Li", " Lingxi Xie", " Ya Zhang", " Rui Zhang", " Yanfeng Wang", " Qi Tian"], "  Generating and eliminating adversarial examples has been an intriguing topic\nin the field of deep learning. While previous research verified that\nadversarial attacks are often fragile and can be defended via image-level\nprocessing, it remains unclear how high-level features are perturbed by such\nattacks. We investigate this issue from a new perspective, which purely relies\non logits, the class scores before softmax, to detect and defend adversarial\nattacks. Our defender is a two-layer network trained on a mixed set of clean\nand perturbed logits, with the goal being recovering the original prediction.\nUpon a wide range of adversarial attacks, our simple approach shows promising\nresults with relatively high accuracy in defense, and the defender can transfer\nacross attackers with similar properties. More importantly, our defender can\nwork in the scenarios that image data are unavailable, and enjoys high\ninterpretability especially at the semantic level.\n"], ["2019-06-25", "http://arxiv.org/abs/1906.10395", "Quantitative Verification of Neural Networks And its Security Applications.", ["Teodora Baluta", " Shiqi Shen", " Shweta Shinde", " Kuldeep S. Meel", " Prateek Saxena"], "  Neural networks are increasingly employed in safety-critical domains. This\nhas prompted interest in verifying or certifying logically encoded properties\nof neural networks. Prior work has largely focused on checking existential\nproperties, wherein the goal is to check whether there exists any input that\nviolates a given property of interest. However, neural network training is a\nstochastic process, and many questions arising in their analysis require\nprobabilistic and quantitative reasoning, i.e., estimating how many inputs\nsatisfy a given property. To this end, our paper proposes a novel and\nprincipled framework to quantitative verification of logical properties\nspecified over neural networks. Our framework is the first to provide PAC-style\nsoundness guarantees, in that its quantitative estimates are within a\ncontrollable and bounded error from the true count. We instantiate our\nalgorithmic framework by building a prototype tool called NPAQ that enables\nchecking rich properties over binarized neural networks. We show how emerging\nsecurity analyses can utilize our framework in 3 concrete point applications:\nquantifying robustness to adversarial inputs, efficacy of trojan attacks, and\nfairness/bias of given neural networks.\n"], ["2019-06-25", "http://arxiv.org/abs/1906.10773", "Are Adversarial Perturbations a Showstopper for ML-Based CAD? A Case Study on CNN-Based Lithographic Hotspot Detection.", ["Kang Liu", " Haoyu Yang", " Yuzhe Ma", " Benjamin Tan", " Bei Yu", " Evangeline F. Y. Young", " Ramesh Karri", " Siddharth Garg"], "  There is substantial interest in the use of machine learning (ML) based\ntechniques throughout the electronic computer-aided design (CAD) flow,\nparticularly those based on deep learning. However, while deep learning methods\nhave surpassed state-of-the-art performance in several applications, they have\nexhibited intrinsic susceptibility to adversarial perturbations --- small but\ndeliberate alterations to the input of a neural network, precipitating\nincorrect predictions. In this paper, we seek to investigate whether\nadversarial perturbations pose risks to ML-based CAD tools, and if so, how\nthese risks can be mitigated. To this end, we use a motivating case study of\nlithographic hotspot detection, for which convolutional neural networks (CNN)\nhave shown great promise. In this context, we show the first adversarial\nperturbation attacks on state-of-the-art CNN-based hotspot detectors;\nspecifically, we show that small (on average 0.5% modified area), functionality\npreserving and design-constraint satisfying changes to a layout can nonetheless\ntrick a CNN-based hotspot detector into predicting the modified layout as\nhotspot free (with up to 99.7% success). We propose an adversarial retraining\nstrategy to improve the robustness of CNN-based hotspot detection and show that\nthis strategy significantly improves robustness (by a factor of ~3) against\nadversarial attacks without compromising classification accuracy.\n"], ["2019-06-24", "http://arxiv.org/abs/1906.10571", "Deceptive Reinforcement Learning Under Adversarial Manipulations on Cost Signals.", ["Yunhan Huang", " Quanyan Zhu"], "  This paper studies reinforcement learning (RL) under malicious falsification\non cost signals and introduces a quantitative framework of attack models to\nunderstand the vulnerabilities of RL. Focusing on $Q$-learning, we show that\n$Q$-learning algorithms converge under stealthy attacks and bounded\nfalsifications on cost signals. We characterize the relation between the\nfalsified cost and the $Q$-factors as well as the policy learned by the\nlearning agent which provides fundamental limits for feasible offensive and\ndefensive moves. We propose a robust region in terms of the cost within which\nthe adversary can never achieve the targeted policy. We provide conditions on\nthe falsified cost which can mislead the agent to learn an adversary's favored\npolicy. A numerical case study of water reservoir control is provided to show\nthe potential hazards of RL in learning-based control systems and corroborate\nthe results.\n"], ["2019-06-22", "http://arxiv.org/abs/1906.09525", "Defending Against Adversarial Examples with K-Nearest Neighbor.", ["Chawin Sitawarin", " David Wagner"], "  Robustness is an increasingly important property of machine learning models\nas they become more and more prevalent. We propose a defense against\nadversarial examples based on a k-nearest neighbor (kNN) on the intermediate\nactivation of neural networks. Our scheme surpasses state-of-the-art defenses\non MNIST and CIFAR-10 against l2-perturbation by a significant margin. With our\nmodels, the mean perturbation norm required to fool our MNIST model is 3.07 and\n2.30 on CIFAR-10. Additionally, we propose a simple certifiable lower bound on\nthe l2-norm of the adversarial perturbation using a more specific version of\nour scheme, a 1-NN on representations learned by a Lipschitz network. Our model\nprovides a nontrivial average lower bound of the perturbation norm, comparable\nto other schemes on MNIST with similar clean accuracy.\n"], ["2019-06-21", "http://arxiv.org/abs/1906.09288", "Hiding Faces in Plain Sight: Disrupting AI Face Synthesis with Adversarial Perturbations.", ["Yuezun Li", " Xin Yang", " Baoyuan Wu", " Siwei Lyu"], "  Recent years have seen fast development in synthesizing realistic human faces\nusing AI technologies. Such fake faces can be weaponized to cause negative\npersonal and social impact. In this work, we develop technologies to defend\nindividuals from becoming victims of recent AI synthesized fake videos by\nsabotaging would-be training data. This is achieved by disrupting deep neural\nnetwork (DNN) based face detection method with specially designed imperceptible\nadversarial perturbations to reduce the quality of the detected faces. We\ndescribe attacking schemes under white-box, gray-box and black-box settings,\neach with decreasing information about the DNN based face detectors. We\nempirically show the effectiveness of our methods in disrupting\nstate-of-the-art DNN based face detectors on several datasets.\n"], ["2019-06-21", "http://arxiv.org/abs/1906.08988", "A Fourier Perspective on Model Robustness in Computer Vision.", ["Dong Yin", " Raphael Gontijo Lopes", " Jonathon Shlens", " Ekin D. Cubuk", " Justin Gilmer"], "  Achieving robustness to distributional shift is a longstanding and\nchallenging goal of computer vision. Data augmentation is a commonly used\napproach for improving robustness, however robustness gains are typically not\nuniform across corruption types. Indeed increasing performance in the presence\nof random noise is often met with reduced performance on other corruptions such\nas contrast change. Understanding when and why these sorts of trade-offs occur\nis a crucial step towards mitigating them. Towards this end, we investigate\nrecently observed trade-offs caused by Gaussian data augmentation and\nadversarial training. We find that both methods improve robustness to\ncorruptions that are concentrated in the high frequency domain while reducing\nrobustness to corruptions that are concentrated in the low frequency domain.\nThis suggests that one way to mitigate these trade-offs via data augmentation\nis to use a more diverse set of augmentations. Towards this end we observe that\nAutoAugment, a recently proposed data augmentation policy optimized for clean\naccuracy, achieves state-of-the-art robustness on the CIFAR-10-C and ImageNet-C\nbenchmarks.\n"], ["2019-06-21", "http://arxiv.org/abs/1906.09072", "Evolution Attack On Neural Networks.", ["YiGui Luo", " RuiJia Yang", " Wei Sha", " WeiYi Ding", " YouTeng Sun", " YiSi Wang"], "  Many studies have been done to prove the vulnerability of neural networks to\nadversarial example. A trained and well-behaved model can be fooled by a\nvisually imperceptible perturbation, i.e., an originally correctly classified\nimage could be misclassified after a slight perturbation. In this paper, we\npropose a black-box strategy to attack such networks using an evolution\nalgorithm. First, we formalize the generation of an adversarial example into\nthe optimization problem of perturbations that represent the noise added to an\noriginal image at each pixel. To solve this optimization problem in a black-box\nway, we find that an evolution algorithm perfectly meets our requirement since\nit can work without any gradient information. Therefore, we test various\nevolution algorithms, including a simple genetic algorithm, a\nparameter-exploring policy gradient, an OpenAI evolution strategy, and a\ncovariance matrix adaptive evolution strategy. Experimental results show that a\ncovariance matrix adaptive evolution Strategy performs best in this\noptimization problem. Additionally, we also perform several experiments to\nexplore the effect of different regularizations on improving the quality of an\nadversarial example.\n"], ["2019-06-21", "http://arxiv.org/abs/1906.09300", "Adversarial Examples to Fool Iris Recognition Systems.", ["Sobhan Soleymani", " Ali Dabouei", " Jeremy Dawson", " Nasser M. Nasrabadi"], "  Adversarial examples have recently proven to be able to fool deep learning\nmethods by adding carefully crafted small perturbation to the input space\nimage. In this paper, we study the possibility of generating adversarial\nexamples for code-based iris recognition systems. Since generating adversarial\nexamples requires back-propagation of the adversarial loss, conventional filter\nbank-based iris-code generation frameworks cannot be employed in such a setup.\nTherefore, to compensate for this shortcoming, we propose to train a deep\nauto-encoder surrogate network to mimic the conventional iris code generation\nprocedure. This trained surrogate network is then deployed to generate the\nadversarial examples using the iterative gradient sign method algorithm. We\nconsider non-targeted and targeted attacks through three attack scenarios.\nConsidering these attacks, we study the possibility of fooling an iris\nrecognition system in white-box and black-box frameworks.\n"], ["2019-06-20", "http://arxiv.org/abs/1906.11897", "On Physical Adversarial Patches for Object Detection.", ["Mark Lee", " Zico Kolter"], "  In this paper, we demonstrate a physical adversarial patch attack against\nobject detectors, notably the YOLOv3 detector. Unlike previous work on physical\nobject detection attacks, which required the patch to overlap with the objects\nbeing misclassified or avoiding detection, we show that a properly designed\npatch can suppress virtually all the detected objects in the image. That is, we\ncan place the patch anywhere in the image, causing all existing objects in the\nimage to be missed entirely by the detector, even those far away from the patch\nitself. This in turn opens up new lines of physical attacks against object\ndetection systems, which require no modification of the objects in a scene. A\ndemo of the system can be found at https://youtu.be/WXnQjbZ1e7Y.\n"], ["2019-06-19", "http://arxiv.org/abs/1907.03720", "Catfish Effect Between Internal and External Attackers:Being Semi-honest is Helpful.", ["Hanqing Liu", " Na Ruan", " Joseph K. Liu"], "  The consensus protocol named proof of work (PoW) is widely applied by\ncryptocurrencies like Bitcoin. Although security of a PoW cryptocurrency is\nalways the top priority, it is threatened by mining attacks like selfish\nmining. Researchers have proposed many mining attack models with one attacker,\nand optimized the attacker's strategy. During these mining attacks, an attacker\npursues a higher relative revenue (RR) by wasting a large amount of\ncomputational power of the honest miners at the cost of a small amount of\ncomputational power of himself. In this paper, we propose a mining attack model\nwith two phases: the original system and the multi-attacker system. It is the\nfirst model to provide both theoretical and quantitative analysis of mining\nattacks with two attackers. We explain how the original system turns into the\nmulti-attacker system by introducing two attackers: the internal attacker and\nthe external attacker. If both attackers take the attacking strategy selfish\nmining, the RR of the internal attacker in multi-attacker system will drop by\nup to 31.9% compared with his RR in original system. The external attacker will\noverestimate his RR by up to 44.6% in multiattacker system. Unexpected\ncompetitions, auctions between attackers and overestimation of attackers'\ninfluence factor are three main causes of both attackers' dropping RR. We\npropose a mining strategy named Partial Initiative Release (PIR) which is a\nsemi-honest mining strategy in multi-attacker system. In some specific\nsituations, PIR allows the attacker to get more block reward by launching an\nattack in multi-attacker system.\n"], ["2019-06-19", "http://arxiv.org/abs/1906.08416", "Improving the robustness of ImageNet classifiers using elements of human visual cognition.", ["A. Emin Orhan", " Brenden M. Lake"], "  We investigate the robustness properties of image recognition models equipped\nwith two features inspired by human vision, an explicit episodic memory and a\nshape bias, at the ImageNet scale. As reported in previous work, we show that\nan explicit episodic memory improves the robustness of image recognition models\nagainst small-norm adversarial perturbations under some threat models. It does\nnot, however, improve the robustness against more natural, and typically\nlarger, perturbations. Learning more robust features during training appears to\nbe necessary for robustness in this second sense. We show that features derived\nfrom a model that was encouraged to learn global, shape-based representations\n(Geirhos et al., 2019) do not only improve the robustness against natural\nperturbations, but when used in conjunction with an episodic memory, they also\nprovide additional robustness against adversarial perturbations. Finally, we\naddress three important design choices for the episodic memory: memory size,\ndimensionality of the memories and the retrieval method. We show that to make\nthe episodic memory more compact, it is preferable to reduce the number of\nmemories by clustering them, instead of reducing their dimensionality.\n"], ["2019-06-19", "http://arxiv.org/abs/1906.07982", "A unified view on differential privacy and robustness to adversarial examples.", ["Rafael Pinot", " Florian Yger", " C\u00e9dric Gouy-Pailler", " Jamal Atif"], "  This short note highlights some links between two lines of research within\nthe emerging topic of trustworthy machine learning: differential privacy and\nrobustness to adversarial examples. By abstracting the definitions of both\nnotions, we show that they build upon the same theoretical ground and hence\nresults obtained so far in one domain can be transferred to the other. More\nprecisely, our analysis is based on two key elements: probabilistic mappings\n(also called randomized algorithms in the differential privacy community), and\nthe Renyi divergence which subsumes a large family of divergences. We first\ngeneralize the definition of robustness against adversarial examples to\nencompass probabilistic mappings. Then we observe that Renyi-differential\nprivacy (a generalization of differential privacy recently proposed\nin~\\cite{Mironov2017RenyiDP}) and our definition of robustness share several\nsimilarities. We finally discuss how can both communities benefit from this\nconnection to transfer technical tools from one research field to the other.\n"], ["2019-06-19", "http://arxiv.org/abs/1906.07916", "Convergence of Adversarial Training in Overparametrized Networks.", ["Ruiqi Gao", " Tianle Cai", " Haochuan Li", " Liwei Wang", " Cho-Jui Hsieh", " Jason D. Lee"], "  Neural networks are vulnerable to adversarial examples, i.e. inputs that are\nimperceptibly perturbed from natural data and yet incorrectly classified by the\nnetwork. Adversarial training, a heuristic form of robust optimization that\nalternates between minimization and maximization steps, has proven to be among\nthe most successful methods to train networks that are robust against a\npre-defined family of perturbations. This paper provides a partial answer to\nthe success of adversarial training. When the inner maximization problem can be\nsolved to optimality, we prove that adversarial training finds a network of\nsmall robust train loss. When the maximization problem is solved by a heuristic\nalgorithm, we prove that adversarial training finds a network of small robust\nsurrogate train loss. The analysis technique leverages recent work on the\nanalysis of neural networks via Neural Tangent Kernel (NTK), combined with\nonline-learning when the maximization is solved by a heuristic, and the\nexpressiveness of the NTK kernel in the $\\ell_\\infty$-norm.\n"], ["2019-06-19", "http://arxiv.org/abs/1906.07920", "Global Adversarial Attacks for Assessing Deep Learning Robustness.", ["Hanbin Hu", " Mit Shah", " Jianhua Z. Huang", " Peng Li"], "  It has been shown that deep neural networks (DNNs) may be vulnerable to\nadversarial attacks, raising the concern on their robustness particularly for\nsafety-critical applications. Recognizing the local nature and limitations of\nexisting adversarial attacks, we present a new type of global adversarial\nattacks for assessing global DNN robustness. More specifically, we propose a\nnovel concept of global adversarial example pairs in which each pair of two\nexamples are close to each other but have different class labels predicted by\nthe DNN. We further propose two families of global attack methods and show that\nour methods are able to generate diverse and intriguing adversarial example\npairs at locations far from the training or testing data. Moreover, we\ndemonstrate that DNNs hardened using the strong projected gradient descent\n(PGD) based (local) adversarial training are vulnerable to the proposed global\nadversarial example pairs, suggesting that global robustness must be considered\nwhile training robust deep learning networks.\n"], ["2019-06-19", "http://arxiv.org/abs/1906.07997", "Cloud-based Image Classification Service Is Not Robust To Simple Transformations: A Forgotten Battlefield.", ["Dou Goodman", " Tao Wei"], "  Many recent works demonstrated that Deep Learning models are vulnerable to\nadversarial examples.Fortunately, generating adversarial examples usually\nrequires white-box access to the victim model, and the attacker can only access\nthe APIs opened by cloud platforms. Thus, keeping models in the cloud can\nusually give a (false) sense of security.Unfortunately, cloud-based image\nclassification service is not robust to simple transformations such as Gaussian\nNoise, Salt-and-Pepper Noise, Rotation and Monochromatization. In this\npaper,(1) we propose one novel attack method called Image Fusion(IF) attack,\nwhich achieve a high bypass rate,can be implemented only with OpenCV and is\ndifficult to defend; and (2) we make the first attempt to conduct an extensive\nempirical study of Simple Transformation (ST) attacks against real-world\ncloud-based classification services. Through evaluations on four popular cloud\nplatforms including Amazon, Google, Microsoft, Clarifai, we demonstrate that ST\nattack has a success rate of approximately 100% except Amazon approximately\n50%, IF attack have a success rate over 98% among different classification\nservices. (3) We discuss the possible defenses to address these security\nchallenges.Experiments show that our defense technology can effectively defend\nknown ST attacks.\n"], ["2019-06-19", "http://arxiv.org/abs/1906.07927", "SemanticAdv: Generating Adversarial Examples via Attribute-conditional Image Editing.", ["Haonan Qiu", " Chaowei Xiao", " Lei Yang", " Xinchen Yan", " Honglak Lee", " Bo Li"], "  Deep neural networks (DNNs) have achieved great success in various\napplications due to their strong expressive power. However, recent studies have\nshown that DNNs are vulnerable to adversarial examples which are manipulated\ninstances targeting to mislead DNNs to make incorrect predictions. Currently,\nmost such adversarial examples try to guarantee \"subtle perturbation\" by\nlimiting its $L_p$ norm. In this paper, we aim to explore the impact of\nsemantic manipulation on DNNs predictions by manipulating the semantic\nattributes of images and generate \"unrestricted adversarial examples\". Such\nsemantic based perturbation is more practical compared with pixel level\nmanipulation. In particular, we propose an algorithm SemanticAdv which\nleverages disentangled semantic factors to generate adversarial perturbation\nvia altering either single or a combination of semantic attributes. We conduct\nextensive experiments to show that the semantic based adversarial examples can\nnot only fool different learning tasks such as face verification and landmark\ndetection, but also achieve high attack success rate against real-world\nblack-box services such as Azure face verification service. Such structured\nadversarial examples with controlled semantic manipulation can shed light on\nfurther understanding about vulnerabilities of DNNs as well as potential\ndefensive approaches.\n"], ["2019-06-17", "http://arxiv.org/abs/1906.07153", "Adversarial attacks on Copyright Detection Systems.", ["Parsa Saadatpanah", " Ali Shafahi", " Tom Goldstein"], "  It is well-known that many machine learning models are susceptible to\nso-called \"adversarial attacks,\" in which an attacker evades a classifier by\nmaking small perturbations to inputs. This paper discusses how industrial\ncopyright detection tools, which serve a central role on the web, are\nsusceptible to adversarial attacks. We discuss a range of copyright detection\nsystems, and why they are particularly vulnerable to attacks. These\nvulnerabilities are especially apparent for neural network based systems. As a\nproof of concept, we describe a well-known music identification method, and\nimplement this system in the form of a neural net. We then attack this system\nusing simple gradient methods. Adversarial music created this way successfully\nfools industrial systems, including the AudioTag copyright detector and\nYouTube's Content ID system. Our goal is to raise awareness of the threats\nposed by adversarial examples in this space, and to highlight the importance of\nhardening copyright detection systems to attacks.\n"], ["2019-06-17", "http://arxiv.org/abs/1906.06919", "Improving Black-box Adversarial Attacks with a Transfer-based Prior.", ["Shuyu Cheng", " Yinpeng Dong", " Tianyu Pang", " Hang Su", " Jun Zhu"], "  We consider the black-box adversarial setting, where the adversary has to\ngenerate adversarial perturbations without access to the target models to\ncompute gradients. Previous methods tried to approximate the gradient either by\nusing a transfer gradient of a surrogate white-box model, or based on the query\nfeedback. However, these methods often suffer from low attack success rates or\npoor query efficiency since it is non-trivial to estimate the gradient in a\nhigh-dimensional space with limited information. To address these problems, we\npropose a prior-guided random gradient-free (P-RGF) method to improve black-box\nadversarial attacks, which takes the advantage of a transfer-based prior and\nthe query information simultaneously. The transfer-based prior given by the\ngradient of a surrogate model is appropriately integrated into our algorithm by\nan optimal coefficient derived by a theoretical analysis. Extensive experiments\ndemonstrate that our method requires much fewer queries to attack black-box\nmodels with higher success rates compared with the alternative state-of-the-art\nmethods.\n"], ["2019-06-17", "http://arxiv.org/abs/1906.07077", "The Attack Generator: A Systematic Approach Towards Constructing Adversarial Attacks.", ["Felix Assion", " Peter Schlicht", " Florens Gre\u00dfner", " Wiebke G\u00fcnther", " Fabian H\u00fcger", " Nico Schmidt", " Umair Rasheed"], "  Most state-of-the-art machine learning (ML) classification systems are\nvulnerable to adversarial perturbations. As a consequence, adversarial\nrobustness poses a significant challenge for the deployment of ML-based systems\nin safety- and security-critical environments like autonomous driving, disease\ndetection or unmanned aerial vehicles. In the past years we have seen an\nimpressive amount of publications presenting more and more new adversarial\nattacks. However, the attack research seems to be rather unstructured and new\nattacks often appear to be random selections from the unlimited set of possible\nadversarial attacks. With this publication, we present a structured analysis of\nthe adversarial attack creation process. By detecting different building blocks\nof adversarial attacks, we outline the road to new sets of adversarial attacks.\nWe call this the \"attack generator\". In the pursuit of this objective, we\nsummarize and extend existing adversarial perturbation taxonomies. The\nresulting taxonomy is then linked to the application context of computer vision\nsystems for autonomous vehicles, i.e. semantic segmentation and object\ndetection. Finally, in order to prove the usefulness of the attack generator,\nwe investigate existing semantic segmentation attacks with respect to the\ndetected defining components of adversarial attacks.\n"], ["2019-06-16", "http://arxiv.org/abs/1906.06784", "Interpolated Adversarial Training: Achieving Robust Neural Networks without Sacrificing Accuracy.", ["Alex Lamb", " Vikas Verma", " Juho Kannala", " Yoshua Bengio"], "  Adversarial robustness has become a central goal in deep learning, both in\ntheory and practice. However, successful methods to improve adversarial\nrobustness (such as adversarial training) greatly hurt generalization\nperformance on the clean data. This could have a major impact on how\nadversarial robustness affects real world systems (i.e. many may opt to forego\nrobustness if it can improve performance on the clean data). We propose\nInterpolated Adversarial Training, which employs recently proposed\ninterpolation based training methods in the framework of adversarial training.\nOn CIFAR-10, adversarial training increases clean test error from 5.8% to\n16.7%, whereas with our Interpolated adversarial training we retain adversarial\nrobustness while achieving a clean test error of only 6.5%. With our technique,\nthe relative error increase for the robust model is reduced from 187.9% to just\n12.1%\n"], ["2019-06-16", "http://arxiv.org/abs/1906.06765", "Defending Against Adversarial Attacks Using Random Forests.", ["Yifan Ding", " Liqiang Wang", " Huan Zhang", " Jinfeng Yi", " Deliang Fan", " Boqing Gong"], "  As deep neural networks (DNNs) have become increasingly important and\npopular, the robustness of DNNs is the key to the safety of both the Internet\nand the physical world. Unfortunately, some recent studies show that\nadversarial examples, which are hard to be distinguished from real examples,\ncan easily fool DNNs and manipulate their predictions. Upon observing that\nadversarial examples are mostly generated by gradient-based methods, in this\npaper, we first propose to use a simple yet very effective non-differentiable\nhybrid model that combines DNNs and random forests, rather than hide gradients\nfrom attackers, to defend against the attacks. Our experiments show that our\nmodel can successfully and completely defend the white-box attacks, has a lower\ntransferability, and is quite resistant to three representative types of\nblack-box attacks; while at the same time, our model achieves similar\nclassification accuracy as the original DNNs. Finally, we investigate and\nsuggest a criterion to define where to grow random forests in DNNs.\n"], ["2019-06-15", "http://arxiv.org/abs/1906.06627", "Uncovering Why Deep Neural Networks Lack Robustness: Representation Metrics that Link to Adversarial Attacks.", ["Danilo Vasconcellos Vargas", " Shashank Kotyan", " Moe Matsuki"], "  Neural networks have been shown vulnerable to adversarial samples. Slightly\nperturbed input images are able to change the classification of accurate\nmodels, showing that the representation learned is not as good as previously\nthought.To aid the development of better neural networks, it would be important\nto evaluate to what extent are current neural networks' representations\ncapturing the existing features.Here we propose a test that can evaluate neural\nnetworks using a new type of zero-shot test, entitled Raw Zero-Shot.This test\nis based on the principle that some features are present on unknown classes and\nthat unknown classes can be defined as a combination of previous learned\nfeatures without learning bias. To evaluate the soft-labels of unknown classes,\ntwo metrics are proposed.One is based on clustering validation techniques\n(Davies-Bouldin Index) and the other is based on soft-label distance of a given\ncorrect soft-label.Experiments show that such metrics are in accordance with\nthe robustness to adversarial attacks and might serve as a guidance to build\nbetter models as well as be used in loss functions to improve the models\ndirectly.Interestingly, the results suggests that dynamic routing networks such\nas CapsNet have better representation while some DNNs might be trading off\nrepresentation quality for accuracy.\n  Code available at \\url{http://bit.ly/RepresentationMetrics}.\n"], ["2019-06-14", "http://arxiv.org/abs/1906.06032", "Adversarial Training Can Hurt Generalization.", ["Aditi Raghunathan", " Sang Michael Xie", " Fanny Yang", " John C. Duchi", " Percy Liang"], "  While adversarial training can improve robust accuracy (against an\nadversary), it sometimes hurts standard accuracy (when there is no adversary).\nPrevious work has studied this tradeoff between standard and robust accuracy,\nbut only in the setting where no predictor performs well on both objectives in\nthe infinite data limit. In this paper, we show that even when the optimal\npredictor with infinite data performs well on both objectives, a tradeoff can\nstill manifest itself with finite data. Furthermore, since our construction is\nbased on a convex learning problem, we rule out optimization concerns, thus\nlaying bare a fundamental tension between robustness and generalization.\nFinally, we show that robust self-training mostly eliminates this tradeoff by\nleveraging unlabeled data.\n"], ["2019-06-14", "http://arxiv.org/abs/1906.06110", "Towards Compact and Robust Deep Neural Networks.", ["Vikash Sehwag", " Shiqi Wang", " Prateek Mittal", " Suman Jana"], "  Deep neural networks have achieved impressive performance in many\napplications but their large number of parameters lead to significant\ncomputational and storage overheads. Several recent works attempt to mitigate\nthese overheads by designing compact networks using pruning of connections.\nHowever, we observe that most of the existing strategies to design compact\nnetworks fail to preserve network robustness against adversarial examples. In\nthis work, we rigorously study the extension of network pruning strategies to\npreserve both benign accuracy and robustness of a network. Starting with a\nformal definition of the pruning procedure, including pre-training, weights\npruning, and fine-tuning, we propose a new pruning method that can create\ncompact networks while preserving both benign accuracy and robustness. Our\nmethod is based on two main insights: (1) we ensure that the training\nobjectives of the pre-training and fine-tuning steps match the training\nobjective of the desired robust model (e.g., adversarial robustness/verifiable\nrobustness), and (2) we keep the pruning strategy agnostic to pre-training and\nfine-tuning objectives. We evaluate our method on four different networks on\nthe CIFAR-10 dataset and measure benign accuracy, empirical robust accuracy,\nand verifiable robust accuracy. We demonstrate that our pruning method can\npreserve on average 93\\% benign accuracy, 92.5\\% empirical robust accuracy, and\n85.0\\% verifiable robust accuracy while compressing the tested network by\n10$\\times$.\n"], ["2019-06-14", "http://arxiv.org/abs/1906.06355", "Perceptual Based Adversarial Audio Attacks.", ["Joseph Szurley", " J. Zico Kolter"], "  Recent work has shown the possibility of adversarial attacks on automatic\nspeechrecognition (ASR) systems. However, in the vast majority of work in this\narea, theattacks have been executed only in the digital space, or have involved\nshort phrasesand static room settings. In this paper, we demonstrate a\nphysically realizableaudio adversarial attack. We base our approach\nspecifically on a psychoacoustic-property-based loss function, and automated\ngeneration of room impulse responses, to create adversarial attacks that are\nrobust when played over a speaker in multiple environments. We show that such\nattacks are possible even while being virtually imperceptible to listeners.\n"], ["2019-06-14", "http://arxiv.org/abs/1906.06086", "Copy and Paste: A Simple But Effective Initialization Method for Black-Box Adversarial Attacks.", ["Thomas Brunner", " Frederik Diehl", " Alois Knoll"], "  Many optimization methods for generating black-box adversarial examples have\nbeen proposed, but the aspect of initializing said optimizers has not been\nconsidered in much detail. We show that the choice of starting points is indeed\ncrucial, and that the performance of state-of-the-art attacks depends on it.\nFirst, we discuss desirable properties of starting points for attacking image\nclassifiers, and how they can be chosen to increase query efficiency. Notably,\nwe find that simply copying small patches from other images is a valid\nstrategy. In an evaluation on ImageNet, we show that this initialization\nreduces the number of queries required for a state-of-the-art Boundary Attack\nby 81%, significantly outperforming previous results reported for targeted\nblack-box adversarial examples.\n"], ["2019-06-14", "http://arxiv.org/abs/1906.06449", "Robust or Private? Adversarial Training Makes Models More Vulnerable to Privacy Attacks.", ["Felipe A. Mejia", " Paul Gamble", " Zigfried Hampel-Arias", " Michael Lomnitz", " Nina Lopatina", " Lucas Tindall", " Maria Alejandra Barrios"], "  Adversarial training was introduced as a way to improve the robustness of\ndeep learning models to adversarial attacks. This training method improves\nrobustness against adversarial attacks, but increases the models vulnerability\nto privacy attacks. In this work we demonstrate how model inversion attacks,\nextracting training data directly from the model, previously thought to be\nintractable become feasible when attacking a robustly trained model. The input\nspace for a traditionally trained model is dominated by adversarial examples -\ndata points that strongly activate a certain class but lack semantic meaning -\nthis makes it difficult to successfully conduct model inversion attacks. We\ndemonstrate this effect using the CIFAR-10 dataset under three different model\ninversion attacks, a vanilla gradient descent method, gradient based method at\ndifferent scales, and a generative adversarial network base attacks.\n"], ["2019-06-14", "http://arxiv.org/abs/1906.06316", "Towards Stable and Efficient Training of Verifiably Robust Neural Networks.", ["Huan Zhang", " Hongge Chen", " Chaowei Xiao", " Bo Li", " Duane Boning", " Cho-Jui Hsieh"], "  Training neural networks with verifiable robustness guarantees is\nchallenging. Several existing successful approaches utilize relatively tight\nlinear relaxation based bounds of neural network outputs, but they can slow\ndown training by a factor of hundreds and over-regularize the network.\nMeanwhile, interval bound propagation (IBP) based training is efficient and\nsignificantly outperform linear relaxation based methods on some tasks, yet it\nsuffers from stability issues since the bounds are much looser. In this paper,\nwe first interpret IBP training as training an augmented network which computes\nnon-linear bounds, thus explaining its good performance. We then propose a new\ncertified adversarial training method, CROWN-IBP, by combining the fast IBP\nbounds in the forward pass and a tight linear relaxation based bound, CROWN, in\nthe backward pass. The proposed method is computationally efficient and\nconsistently outperforms IBP baselines on training verifiably robust neural\nnetworks. We conduct large scale experiments using 53 models on MNIST,\nFashion-MNIST and CIFAR datasets. On MNIST with $\\epsilon=0.3$ and\n$\\epsilon=0.4$ ($\\ell_\\infty$ norm distortion) we achieve 7.46\\% and 12.96\\%\nverified error on test set, respectively, outperforming previous certified\ndefense methods.\n"], ["2019-06-14", "http://arxiv.org/abs/1906.06026", "Model Agnostic Dual Quality Assessment for Adversarial Machine Learning and an Analysis of Current Neural Networks and Defenses.", ["Danilo Vasconcellos Vargas", " Shashank Kotyan"], "  In adversarial machine learning, there are a huge number of attacks of\nvarious types which makes the evaluation of robustness for new models and\ndefenses a daunting task. To make matters worse, there is an inherent bias in\nattacks and defenses. Here, we organize the problems faced (model dependence,\ninsufficient evaluation, unreliable adversarial samples and perturbation\ndependent results) and propose a dual quality assessment method together with\nthe concept of robustness levels to tackle them. We validate the dual quality\nassessment on state-of-the-art models (WideResNet, ResNet, AllConv, DenseNet,\nNIN, LeNet and CapsNet) as well as the current hardest defenses proposed at\nICLR 2018 as well as the widely known adversarial training, showing that\ncurrent models and defenses are vulnerable in all levels of robustness.\nMoreover, we show that robustness to $L_0$ and $L_\\infty$ attacks differ\ngreatly and therefore duality should be taken into account for a correct\nassessment. Interestingly, a by-product of the assessment proposed is a novel\n$L_\\infty$ black-box method which requires even less perturbation than the\nOne-Pixel Attack (only 12\\% of One-Pixel Attack's amount of perturbation) to\nachieve similar results. Thus, this paper elucidates the problems of robustness\nevaluation, proposes a dual quality assessment to tackle them as well as\nanalyze the robustness of current models and defenses. Hopefully, the current\nanalysis and proposed methods would aid the development of more robust deep\nneural networks and hybrids alike.\n  Code available at: http://bit.ly/DualQualityAssessment\n"], ["2019-06-13", "http://arxiv.org/abs/1906.05599", "A Computationally Efficient Method for Defending Adversarial Deep Learning Attacks.", ["Rajeev Sahay", " Rehana Mahfuz", " Aly El Gamal"], "  The reliance on deep learning algorithms has grown significantly in recent\nyears. Yet, these models are highly vulnerable to adversarial attacks, which\nintroduce visually imperceptible perturbations into testing data to induce\nmisclassifications. The literature has proposed several methods to combat such\nadversarial attacks, but each method either fails at high perturbation values,\nrequires excessive computing power, or both. This letter proposes a\ncomputationally efficient method for defending the Fast Gradient Sign (FGS)\nadversarial attack by simultaneously denoising and compressing data.\nSpecifically, our proposed defense relies on training a fully connected\nmulti-layer Denoising Autoencoder (DAE) and using its encoder as a defense\nagainst the adversarial attack. Our results show that using this dimensionality\nreduction scheme is not only highly effective in mitigating the effect of the\nFGS attack in multiple threat models, but it also provides a 2.43x speedup in\ncomparison to defense strategies providing similar robustness against the same\nattack.\n"], ["2019-06-13", "http://arxiv.org/abs/1906.05815", "Lower Bounds for Adversarially Robust PAC Learning.", ["Dimitrios I. Diochnos", " Saeed Mahloujifar", " Mohammad Mahmoody"], "  In this work, we initiate a formal study of probably approximately correct\n(PAC) learning under evasion attacks, where the adversary's goal is to\n\\emph{misclassify} the adversarially perturbed sample point $\\widetilde{x}$,\ni.e., $h(\\widetilde{x})\\neq c(\\widetilde{x})$, where $c$ is the ground truth\nconcept and $h$ is the learned hypothesis. Previous work on PAC learning of\nadversarial examples have all modeled adversarial examples as corrupted inputs\nin which the goal of the adversary is to achieve $h(\\widetilde{x}) \\neq c(x)$,\nwhere $x$ is the original untampered instance. These two definitions of\nadversarial risk coincide for many natural distributions, such as images, but\nare incomparable in general.\n  We first prove that for many theoretically natural input spaces of high\ndimension $n$ (e.g., isotropic Gaussian in dimension $n$ under $\\ell_2$\nperturbations), if the adversary is allowed to apply up to a sublinear\n$o(||x||)$ amount of perturbations on the test instances, PAC learning requires\nsample complexity that is exponential in $n$. This is in contrast with results\nproved using the corrupted-input framework, in which the sample complexity of\nrobust learning is only polynomially more.\n  We then formalize hybrid attacks in which the evasion attack is preceded by a\npoisoning attack. This is perhaps reminiscent of \"trapdoor attacks\" in which a\npoisoning phase is involved as well, but the evasion phase here uses the\nerror-region definition of risk that aims at misclassifying the perturbed\ninstances. In this case, we show PAC learning is sometimes impossible all\ntogether, even when it is possible without the attack (e.g., due to the bounded\nVC dimension).\n"], ["2019-06-12", "http://arxiv.org/abs/1906.04948", "A Stratified Approach to Robustness for Randomly Smoothed Classifiers.", ["Guang-He Lee", " Yang Yuan", " Shiyu Chang", " Tommi S. Jaakkola"], "  Strong theoretical guarantees of robustness can be given for ensembles of\nclassifiers generated by input randomization. Specifically, an $\\ell_2$ bounded\nadversary cannot alter the ensemble prediction generated by an isotropic\nGaussian perturbation, where the radius for the adversary depends on both the\nvariance of the perturbation as well as the ensemble margin at the point of\ninterest. We build on and considerably expand this work across broad classes of\nperturbations. In particular, we offer guarantees and develop algorithms for\nthe discrete case where the adversary is $\\ell_0$ bounded. Moreover, we\nexemplify how the guarantees can be tightened with specific assumptions about\nthe function class of the classifier such as a decision tree. We empirically\nillustrate these results with and without functional restrictions across image\nand molecule datasets.\n"], ["2019-06-11", "http://arxiv.org/abs/1906.04392", "Subspace Attack: Exploiting Promising Subspaces for Query-Efficient Black-box Attacks.", ["Ziang Yan", " Yiwen Guo", " Changshui Zhang"], "  Unlike the white-box counterparts that are widely studied and readily\naccessible, adversarial examples in black-box settings are generally more\nHerculean on account of the difficulty of estimating gradients. Many methods\nachieve the task by issuing numerous queries to target classification systems,\nwhich makes the whole procedure costly and suspicious to the systems. In this\npaper, we aim at reducing the query complexity of black-box attacks in this\ncategory. We propose to exploit gradients of a few reference models which\narguably span some promising search subspaces. Experimental results show that,\nin comparison with the state-of-the-arts, our method can gain up to 2x and 4x\nreductions in the requisite mean and medium numbers of queries with much lower\nfailure rates even if the reference models are trained on a small and\ninadequate dataset disjoint to the one for training the victim model. Code and\nmodels for reproducing our results will be made publicly available.\n"], ["2019-06-11", "http://arxiv.org/abs/1906.04606", "Mimic and Fool: A Task Agnostic Adversarial Attack.", ["Akshay Chaturvedi", " Utpal Garain"], "  At present, adversarial attacks are designed in a task-specific fashion.\nHowever, for downstream computer vision tasks such as image captioning, image\nsegmentation etc., the current deep learning systems use an image classifier\nlike VGG16, ResNet50, Inception-v3 etc. as a feature extractor. Keeping this in\nmind, we propose Mimic and Fool, a task agnostic adversarial attack. Given a\nfeature extractor, the proposed attack finds an adversarial image which can\nmimic the image feature of the original image. This ensures that the two images\ngive the same (or similar) output regardless of the task. We randomly select\n1000 MSCOCO validation images for experimentation. We perform experiments on\ntwo image captioning models, Show and Tell, Show Attend and Tell and one VQA\nmodel, namely, end-to-end neural module network (N2NMN). The proposed attack\nachieves success rate of 74.0%, 81.0% and 89.6% for Show and Tell, Show Attend\nand Tell and N2NMN respectively. We also propose a slight modification to our\nattack to generate natural-looking adversarial images. In addition, it is shown\nthat the proposed attack also works for invertible architecture. Since Mimic\nand Fool only requires information about the feature extractor of the model, it\ncan be considered as a gray-box attack.\n"], ["2019-06-11", "http://arxiv.org/abs/1906.04893", "Efficient and Accurate Estimation of Lipschitz Constants for Deep Neural Networks.", ["Mahyar Fazlyab", " Alexander Robey", " Hamed Hassani", " Manfred Morari", " George J. Pappas"], "  Tight estimation of the Lipschitz constant for deep neural networks (DNNs) is\nuseful in many applications ranging from robustness certification of\nclassifiers to stability analysis of closed-loop systems with reinforcement\nlearning controllers. Existing methods in the literature for estimating the\nLipschitz constant suffer from either lack of accuracy or poor scalability. In\nthis paper, we present a convex optimization framework to compute guaranteed\nupper bounds on the Lipschitz constant of DNNs both accurately and efficiently.\nOur main idea is to interpret activation functions as gradients of convex\npotential functions. Hence, they satisfy certain properties that can be\ndescribed by quadratic constraints. This particular description allows us to\npose the Lipschitz constant estimation problem as a semidefinite program (SDP).\nThe resulting SDP can be adapted to increase either the estimation accuracy (by\ncapturing the interaction between activation functions of different layers) or\nscalability (by decomposition and parallel implementation). We illustrate the\nutility of our approach with a variety of experiments on randomly generated\nnetworks and on classifiers trained on the MNIST and Iris datasets. In\nparticular, we experimentally demonstrate that our Lipschitz bounds are the\nmost accurate compared to those in the literature. We also study the impact of\nadversarial training methods on the Lipschitz bounds of the resulting\nclassifiers and show that our bounds can be used to efficiently provide\nrobustness guarantees.\n"], ["2019-06-10", "http://arxiv.org/abs/1906.03973", "E-LPIPS: Robust Perceptual Image Similarity via Random Transformation Ensembles.", ["Markus Kettunen", " Erik H\u00e4rk\u00f6nen", " Jaakko Lehtinen"], "  It has been recently shown that the hidden variables of convolutional neural\nnetworks make for an efficient perceptual similarity metric that accurately\npredicts human judgment on relative image similarity assessment. First, we show\nthat such learned perceptual similarity metrics (LPIPS) are susceptible to\nadversarial attacks that dramatically contradict human visual similarity\njudgment. While this is not surprising in light of neural networks' well-known\nweakness to adversarial perturbations, we proceed to show that self-ensembling\nwith an infinite family of random transformations of the input --- a technique\nknown not to render classification networks robust --- is enough to turn the\nmetric robust against attack, while retaining predictive power on human\njudgments. Finally, we study the geometry imposed by our our novel\nself-ensembled metric (E-LPIPS) on the space of natural images. We find\nevidence of \"perceptual convexity\" by showing that convex combinations of\nsimilar-looking images retain appearance, and that discrete geodesics yield\nmeaningful frame interpolation and texture morphing, all without explicit\ncorrespondences.\n"], ["2019-06-10", "http://arxiv.org/abs/1906.03972", "Evaluating the Robustness of Nearest Neighbor Classifiers: A Primal-Dual Perspective.", ["Lu Wang", " Xuanqing Liu", " Jinfeng Yi", " Zhi-Hua Zhou", " Cho-Jui Hsieh"], "  We study the problem of computing the minimum adversarial perturbation of the\nNearest Neighbor (NN) classifiers. Previous attempts either conduct attacks on\ncontinuous approximations of NN models or search for the perturbation by some\nheuristic methods. In this paper, we propose the first algorithm that is able\nto compute the minimum adversarial perturbation. The main idea is to formulate\nthe problem as a list of convex quadratic programming (QP) problems that can be\nefficiently solved by the proposed algorithms for 1-NN models. Furthermore, we\nshow that dual solutions for these QP problems could give us a valid lower\nbound of the adversarial perturbation that can be used for formal robustness\nverification, giving us a nice view of attack/verification for NN models. For\n$K$-NN models with larger $K$, we show that the same formulation can help us\nefficiently compute the upper and lower bounds of the minimum adversarial\nperturbation, which can be used for attack and verification.\n"], ["2019-06-10", "http://arxiv.org/abs/1906.03849", "Robustness Verification of Tree-based Models.", ["Hongge Chen", " Huan Zhang", " Si Si", " Yang Li", " Duane Boning", " Cho-Jui Hsieh"], "  We study the robustness verification problem for tree-based models, including\ndecision trees, random forests (RFs) and gradient boosted decision trees\n(GBDTs). Formal robustness verification of decision tree ensembles involves\nfinding the exact minimal adversarial perturbation or a guaranteed lower bound\nof it. Existing approaches find the minimal adversarial perturbation by a mixed\ninteger linear programming (MILP) problem, which takes exponential time so is\nimpractical for large ensembles. Although this verification problem is\nNP-complete in general, we give a more precise complexity characterization. We\nshow that there is a simple linear time algorithm for verifying a single tree,\nand for tree ensembles, the verification problem can be cast as a max-clique\nproblem on a multi-partite graph with bounded boxicity. For low dimensional\nproblems when boxicity can be viewed as constant, this reformulation leads to a\npolynomial time algorithm. For general problems, by exploiting the boxicity of\nthe graph, we develop an efficient multi-level verification algorithm that can\ngive tight lower bounds on the robustness of decision tree ensembles, while\nallowing iterative improvement and any-time termination. OnRF/GBDT models\ntrained on 10 datasets, our algorithm is hundreds of times faster than the\nprevious approach that requires solving MILPs, and is able to give tight\nrobustness verification bounds on large GBDTs with hundreds of deep trees.\n"], ["2019-06-10", "http://arxiv.org/abs/1906.04214", "Topology Attack and Defense for Graph Neural Networks: An Optimization Perspective.", ["Kaidi Xu", " Hongge Chen", " Sijia Liu", " Pin-Yu Chen", " Tsui-Wei Weng", " Mingyi Hong", " Xue Lin"], "  Graph neural networks (GNNs) which apply the deep neural networks to graph\ndata have achieved significant performance for the task of semi-supervised node\nclassification. However, only few work has addressed the adversarial robustness\nof GNNs. In this paper, we first present a novel gradient-based attack method\nthat facilitates the difficulty of tackling discrete graph data. When comparing\nto current adversarial attacks on GNNs, the results show that by only\nperturbing a small number of edge perturbations, including addition and\ndeletion, our optimization-based attack can lead to a noticeable decrease in\nclassification performance. Moreover, leveraging our gradient-based attack, we\npropose the first optimization-based adversarial training for GNNs. Our method\nyields higher robustness against both different gradient based and greedy\nattack methods without sacrificing classification accuracy on original graph.\n"], ["2019-06-09", "http://arxiv.org/abs/1906.03612", "On the Vulnerability of Capsule Networks to Adversarial Attacks.", ["Felix Michels", " Tobias Uelwer", " Eric Upschulte", " Stefan Harmeling"], "  This paper extensively evaluates the vulnerability of capsule networks to\ndifferent adversarial attacks. Recent work suggests that these architectures\nare more robust towards adversarial attacks than other neural networks.\nHowever, our experiments show that capsule networks can be fooled as easily as\nconvolutional neural networks.\n"], ["2019-06-09", "http://arxiv.org/abs/1906.03787", "Intriguing properties of adversarial training.", ["Cihang Xie", " Alan Yuille"], "  Adversarial training is one of the main defenses against adversarial attacks.\nIn this paper, we provide the first rigorous study on diagnosing elements of\nadversarial training, which reveals two intriguing properties.\n  First, we study the role of normalization. Batch normalization (BN) is a\ncrucial element for achieving state-of-the-art performance on many vision\ntasks, but we show it may prevent networks from obtaining strong robustness in\nadversarial training. One unexpected observation is that, for models trained\nwith BN, simply removing clean images from training data largely boosts\nadversarial robustness, i.e., 18.3%. We relate this phenomenon to the\nhypothesis that clean images and adversarial images are drawn from two\ndifferent domains. This two-domain hypothesis may explain the issue of BN when\ntraining with a mixture of clean and adversarial images, as estimating\nnormalization statistics of this mixture distribution is challenging. Guided by\nthis two-domain hypothesis, we show disentangling the mixture distribution for\nnormalization, i.e., applying separate BNs to clean and adversarial images for\nstatistics estimation, achieves much stronger robustness. Additionally, we find\nthat enforcing BNs to behave consistently at training and testing can further\nenhance robustness.\n  Second, we study the role of network capacity. We find our so-called \"deep\"\nnetworks are still shallow for the task of adversarial learning. Unlike\ntraditional classification tasks where accuracy is only marginally improved by\nadding more layers to \"deep\" networks (e.g., ResNet-152), adversarial training\nexhibits a much stronger demand on deeper networks to achieve higher\nadversarial robustness. This robustness improvement can be observed\nsubstantially and consistently even by pushing the network capacity to an\nunprecedented scale, i.e., ResNet-638.\n"], ["2019-06-09", "http://arxiv.org/abs/1906.03749", "Improved Adversarial Robustness via Logit Regularization Methods.", ["Cecilia Summers", " Michael J. Dinneen"], "  While great progress has been made at making neural networks effective across\na wide range of visual tasks, most models are surprisingly vulnerable. This\nfrailness takes the form of small, carefully chosen perturbations of their\ninput, known as adversarial examples, which represent a security threat for\nlearned vision models in the wild -- a threat which should be responsibly\ndefended against in safety-critical applications of computer vision. In this\npaper, we advocate for and experimentally investigate the use of a family of\nlogit regularization techniques as an adversarial defense, which can be used in\nconjunction with other methods for creating adversarial robustness at little to\nno marginal cost. We also demonstrate that much of the effectiveness of one\nrecent adversarial defense mechanism can in fact be attributed to logit\nregularization, and show how to improve its defense against both white-box and\nblack-box attacks, in the process creating a stronger black-box attack against\nPGD-based models. We validate our methods on three datasets and include results\non both gradient-free attacks and strong gradient-based iterative attacks with\nas many as 1,000 steps.\n"], ["2019-06-09", "http://arxiv.org/abs/1906.03750", "Attacking Graph Convolutional Networks via Rewiring.", ["Yao Ma", " Suhang Wang", " Tyler Derr", " Lingfei Wu", " Jiliang Tang"], "  Graph Neural Networks (GNNs) have boosted the performance of many graph\nrelated tasks such as node classification and graph classification. Recent\nresearches show that graph neural networks are vulnerable to adversarial\nattacks, which deliberately add carefully created unnoticeable perturbation to\nthe graph structure. The perturbation is usually created by adding/deleting a\nfew edges, which might be noticeable even when the number of edges modified is\nsmall. In this paper, we propose a graph rewiring operation which affects the\ngraph in a less noticeable way compared to adding/deleting edges. We then use\nreinforcement learning to learn the attack strategy based on the proposed\nrewiring operation. Experiments on real world graphs demonstrate the\neffectiveness of the proposed framework. To understand the proposed framework,\nwe further analyze how its generated perturbation to the graph structure\naffects the output of the target model.\n"], ["2019-06-09", "http://arxiv.org/abs/1906.03563", "Towards A Unified Min-Max Framework for Adversarial Exploration and Robustness.", ["Jingkang Wang", " Tianyun Zhang", " Sijia Liu", " Pin-Yu Chen", " Jiacen Xu", " Makan Fardad", " Bo Li"], "  The worst-case training principle that minimizes the maximal adversarial\nloss, also known as adversarial training (AT), has shown to be a\nstate-of-the-art approach for enhancing adversarial robustness against\nnorm-ball bounded input perturbations. Nonetheless, min-max optimization beyond\nthe purpose of AT has not been rigorously explored in the research of\nadversarial attack and defense. In particular, given a set of risk sources\n(domains), minimizing the maximal loss induced from the domain set can be\nreformulated as a general min-max problem that is different from AT. Examples\nof this general formulation include attacking model ensembles, devising\nuniversal perturbation under multiple inputs or data transformations, and\ngeneralized AT over different types of attack models. We show that these\nproblems can be solved under a unified and theoretically principled min-max\noptimization framework. We also show that the self-adjusted domain weights\nlearned from our method provides a means to explain the difficulty level of\nattack and defense over multiple domains. Extensive experiments show that our\napproach leads to substantial performance improvement over the conventional\naveraging strategy.\n"], ["2019-06-09", "http://arxiv.org/abs/1906.04584", "Provably Robust Deep Learning via Adversarially Trained Smoothed Classifiers.", ["Hadi Salman", " Greg Yang", " Jerry Li", " Pengchuan Zhang", " Huan Zhang", " Ilya Razenshteyn", " Sebastien Bubeck"], "  Recent works have shown the effectiveness of randomized smoothing as a\nscalable technique for building neural network-based classifiers that are\nprovably robust to $\\ell_2$-norm adversarial perturbations. In this paper, we\nemploy adversarial training to improve the performance of randomized smoothing.\nWe design an adapted attack for smoothed classifiers, and we show how this\nattack can be used in an adversarial training setting to boost the provable\nrobustness of smoothed classifiers. We demonstrate through extensive\nexperimentation that our method consistently outperforms all existing provably\n$\\ell_2$-robust classifiers by a significant margin on ImageNet and CIFAR-10,\nestablishing the state-of-the-art for provable $\\ell_2$-defenses. Moreover, we\nfind that pre-training and semi-supervised learning boost adversarially trained\nsmoothed classifiers even further. Our code and trained models are available at\nhttp://github.com/Hadisalman/smoothing-adversarial .\n"], ["2019-06-08", "http://arxiv.org/abs/1906.03526", "Provably Robust Boosted Decision Stumps and Trees against Adversarial Attacks.", ["Maksym Andriushchenko", " Matthias Hein"], "  The problem of adversarial samples has been studied extensively for neural\nnetworks. However, for boosting, in particular boosted decision trees and\ndecision stumps there are almost no results, even though boosted decision\ntrees, as e.g. XGBoost, are quite popular due to their interpretability and\ngood prediction performance. We show in this paper that for boosted decision\nstumps the exact min-max optimal robust loss and test error for an\n$l_\\infty$-attack can be computed in $O(n\\,T\\log T)$, where $T$ is the number\nof decision stumps and $n$ the number of data points, as well as an optimal\nupdate of the ensemble in $O(n^2\\,T\\log T)$. While not exact, we show how to\noptimize an upper bound on the robust loss for boosted trees. Up to our\nknowledge, these are the first algorithms directly optimizing provable\nrobustness guarantees in the area of boosting. We make the code of all our\nexperiments publicly available at\nhttps://github.com/max-andr/provably-robust-boosting\n"], ["2019-06-08", "http://arxiv.org/abs/1906.03466", "Strategies to architect AI Safety: Defense to guard AI from Adversaries.", ["Rajagopal. A", " Nirmala. V"], "  The impact of designing for security of AI is critical for humanity in the AI\nera. With humans increasingly becoming dependent upon AI, there is a need for\nneural networks that work reliably, inspite of Adversarial attacks. The vision\nfor Safe and secure AI for popular use is achievable. To achieve safety of AI,\nthis paper explores strategies and a novel deep learning architecture. To guard\nAI from adversaries, paper explores combination of 3 strategies:\n  1. Introduce randomness at inference time to hide the representation learning\nfrom adversaries.\n  2. Detect presence of adversaries by analyzing the sequence of inferences.\n  3. Exploit visual similarity.\n  To realize these strategies, this paper designs a novel architecture, Dynamic\nNeural Defense, DND. This defense has 3 deep learning architectural features:\n  1. By hiding the way a neural network learns from exploratory attacks using a\nrandom computation graph, DND evades attack.\n  2. By analyzing input sequence to cloud AI inference engine with LSTM, DND\ndetects attack sequence.\n  3. By inferring with visual similar inputs generated by VAE, any AI defended\nby DND approach does not succumb to hackers.\n  Thus, a roadmap to develop reliable, safe and secure AI is presented.\n"], ["2019-06-08", "http://arxiv.org/abs/1906.03397", "Making targeted black-box evasion attacks effective and efficient.", ["Mika Juuti", " Buse Gul Atli", " N. Asokan"], "  We investigate how an adversary can optimally use its query budget for\ntargeted evasion attacks against deep neural networks in a black-box setting.\nWe formalize the problem setting and systematically evaluate what benefits the\nadversary can gain by using substitute models. We show that there is an\nexploration-exploitation tradeoff in that query efficiency comes at the cost of\neffectiveness. We present two new attack strategies for using substitute models\nand show that they are as effective as previous query-only techniques but\nrequire significantly fewer queries, by up to three orders of magnitude. We\nalso show that an agile adversary capable of switching through different attack\ntechniques can achieve pareto-optimal efficiency. We demonstrate our attack\nagainst Google Cloud Vision showing that the difficulty of black-box attacks\nagainst real-world prediction APIs is significantly easier than previously\nthought (requiring approximately 500 queries instead of approximately 20,000 as\nin previous works).\n"], ["2019-06-08", "http://arxiv.org/abs/1906.03455", "Sensitivity of Deep Convolutional Networks to Gabor Noise.", ["Kenneth T. Co", " Luis Mu\u00f1oz-Gonz\u00e1lez", " Emil C. Lupu"], "  Deep Convolutional Networks (DCNs) have been shown to be sensitive to\nUniversal Adversarial Perturbations (UAPs): input-agnostic perturbations that\nfool a model on large portions of a dataset. These UAPs exhibit interesting\nvisual patterns, but this phenomena is, as yet, poorly understood. Our work\nshows that visually similar procedural noise patterns also act as UAPs. In\nparticular, we demonstrate that different DCN architectures are sensitive to\nGabor noise patterns. This behaviour, its causes, and implications deserve\nfurther in-depth study.\n"], ["2019-06-08", "http://arxiv.org/abs/1906.03499", "ML-LOO: Detecting Adversarial Examples with Feature Attribution.", ["Puyudi Yang", " Jianbo Chen", " Cho-Jui Hsieh", " Jane-Ling Wang", " Michael I. Jordan"], "  Deep neural networks obtain state-of-the-art performance on a series of\ntasks. However, they are easily fooled by adding a small adversarial\nperturbation to input. The perturbation is often human imperceptible on image\ndata. We observe a significant difference in feature attributions of\nadversarially crafted examples from those of original ones. Based on this\nobservation, we introduce a new framework to detect adversarial examples\nthrough thresholding a scale estimate of feature attribution scores.\nFurthermore, we extend our method to include multi-layer feature attributions\nin order to tackle the attacks with mixed confidence levels. Through vast\nexperiments, our method achieves superior performances in distinguishing\nadversarial examples from popular attack methods on a variety of real data sets\namong state-of-the-art detection methods. In particular, our method is able to\ndetect adversarial examples of mixed confidence levels, and transfer between\ndifferent attacking methods.\n"], ["2019-06-08", "http://arxiv.org/abs/1906.03444", "Defending against Adversarial Attacks through Resilient Feature Regeneration.", ["Tejas Borkar", " Felix Heide", " Lina Karam"], "  Deep neural network (DNN) predictions have been shown to be vulnerable to\ncarefully crafted adversarial perturbations. Specifically, so-called universal\nadversarial perturbations are image-agnostic perturbations that can be added to\nany image and can fool a target network into making erroneous predictions.\nDeparting from existing adversarial defense strategies, which work in the image\ndomain, we present a novel defense which operates in the DNN feature domain and\neffectively defends against such universal adversarial attacks. Our approach\nidentifies pre-trained convolutional features that are most vulnerable to\nadversarial noise and deploys defender units which transform (regenerate) these\nDNN filter activations into noise-resilient features, guarding against unseen\nadversarial perturbations. The proposed defender units are trained using a\ntarget loss on synthetic adversarial perturbations, which we generate with a\nnovel efficient synthesis method. We validate the proposed method for different\nDNN architectures, and demonstrate that it outperforms existing defense\nstrategies across network architectures by more than 10% in restored accuracy.\nMoreover, we demonstrate that the approach also improves resilience of DNNs to\nother unseen adversarial attacks.\n"], ["2019-06-07", "http://arxiv.org/abs/1906.03231", "A cryptographic approach to black box adversarial machine learning.", ["Kevin Shi", " Daniel Hsu", " Allison Bishop"], "  We propose an ensemble technique for converting any classifier into a\ncomputationally secure classifier. We define a simpler security problem for\nrandom binary classifiers and prove a reduction from this model to the security\nof the overall ensemble classifier. We provide experimental evidence of the\nsecurity of our random binary classifiers, as well as empirical results of the\nadversarial accuracy of the overall ensemble to black-box attacks. Our\nconstruction crucially leverages hidden randomness in the multiclass-to-binary\nreduction.\n"], ["2019-06-07", "http://arxiv.org/abs/1906.03367", "Using learned optimizers to make models robust to input noise.", ["Luke Metz", " Niru Maheswaranathan", " Jonathon Shlens", " Jascha Sohl-Dickstein", " Ekin D. Cubuk"], "  State-of-the art vision models can achieve superhuman performance on image\nclassification tasks when testing and training data come from the same\ndistribution. However, when models are tested on corrupted images (e.g. due to\nscale changes, translations, or shifts in brightness or contrast), performance\ndegrades significantly. Here, we explore the possibility of meta-training a\nlearned optimizer that can train image classification models such that they are\nrobust to common image corruptions. Specifically, we are interested training\nmodels that are more robust to noise distributions not present in the training\ndata. We find that a learned optimizer meta-trained to produce models which are\nrobust to Gaussian noise trains models that are more robust to Gaussian noise\nat other scales compared to traditional optimizers like Adam. The effect of\nmeta-training is more complicated when targeting a more general set of noise\ndistributions, but led to improved performance on half of held-out corruption\ntasks. Our results suggest that meta-learning provides a novel approach for\nstudying and improving the robustness of deep learning models.\n"], ["2019-06-07", "http://arxiv.org/abs/1906.03310", "Adversarial Examples for Non-Parametric Methods: Attacks, Defenses and Large Sample Limits.", ["Yao-Yuan Yang", " Cyrus Rashtchian", " Yizhen Wang", " Kamalika Chaudhuri"], "  Adversarial examples have received a great deal of recent attention because\nof their potential to uncover security flaws in machine learning systems.\nHowever, most prior work on adversarial examples has been on parametric\nclassifiers, for which generic attack and defense methods are known;\nnon-parametric methods have been only considered on an ad-hoc or\nclassifier-specific basis. In this work, we take a holistic look at adversarial\nexamples for non-parametric methods. We first provide a general region-based\nattack that applies to a wide range of classifiers, including nearest\nneighbors, decision trees, and random forests. Motivated by the close\nconnection between non-parametric methods and the Bayes Optimal classifier, we\nnext exhibit a robust analogue to the Bayes Optimal, and we use it to motivate\na novel and generic defense that we call adversarial pruning. We empirically\nshow that the region-based attack and adversarial pruning defense are either\nbetter than or competitive with existing attacks and defenses for\nnon-parametric methods, while being considerably more generally applicable.\n"], ["2019-06-07", "http://arxiv.org/abs/1906.03333", "Efficient Project Gradient Descent for Ensemble Adversarial Attack.", ["Fanyou Wu", " Rado Gazo", " Eva Haviarova", " Bedrich Benes"], "  Recent advances show that deep neural networks are not robust to deliberately\ncrafted adversarial examples which many are generated by adding human\nimperceptible perturbation to clear input. Consider $l_2$ norms attacks,\nProject Gradient Descent (PGD) and the Carlini and Wagner (C\\&W) attacks are\nthe two main methods, where PGD control max perturbation for adversarial\nexamples while C\\&W approach treats perturbation as a regularization term\noptimized it with loss function together. If we carefully set parameters for\nany individual input, both methods become similar. In general, PGD attacks\nperform faster but obtains larger perturbation to find adversarial examples\nthan the C\\&W when fixing the parameters for all inputs. In this report, we\npropose an efficient modified PGD method for attacking ensemble models by\nautomatically changing ensemble weights and step size per iteration per input.\nThis method generates smaller perturbation adversarial examples than PGD method\nwhile remains efficient as compared to C\\&W method. Our method won the first\nplace in IJCAI19 Targeted Adversarial Attack competition.\n"], ["2019-06-07", "http://arxiv.org/abs/1906.02896", "Adversarial Explanations for Understanding Image Classification Decisions and Improved Neural Network Robustness.", ["Walt Woods", " Jack Chen", " Christof Teuscher"], "  For sensitive problems, such as medical imaging or fraud detection, Neural\nNetwork (NN) adoption has been slow due to concerns about their reliability,\nleading to a number of algorithms for explaining their decisions. NNs have also\nbeen found vulnerable to a class of imperceptible attacks, called adversarial\nexamples, which arbitrarily alter the output of the network. Here we\ndemonstrate both that these attacks can invalidate prior attempts to explain\nthe decisions of NNs, and that with very robust networks, the attacks\nthemselves may be leveraged as explanations with greater fidelity to the model.\nWe show that the introduction of a novel regularization technique inspired by\nthe Lipschitz constraint, alongside other proposed improvements, greatly\nimproves an NN's resistance to adversarial examples. On the ImageNet\nclassification task, we demonstrate a network with an Accuracy-Robustness Area\n(ARA) of 0.0053, an ARA 2.4x greater than the previous state of the art.\nImproving the mechanisms by which NN decisions are understood is an important\ndirection for both establishing trust in sensitive domains and learning more\nabout the stimuli to which NNs respond.\n"], ["2019-06-07", "http://arxiv.org/abs/1906.02931", "Inductive Bias of Gradient Descent based Adversarial Training on Separable Data.", ["Yan Li", " Ethan X. Fang", " Huan Xu", " Tuo Zhao"], "  Adversarial training is a principled approach for training robust neural\nnetworks. Despite of tremendous successes in practice, its theoretical\nproperties still remain largely unexplored. In this paper, we provide new\ntheoretical insights of gradient descent based adversarial training by studying\nits computational properties, specifically on its inductive bias. We take the\nbinary classification task on linearly separable data as an illustrative\nexample, where the loss asymptotically attains its infimum as the parameter\ndiverges to infinity along certain directions. Specifically, we show that when\nthe adversarial perturbation during training has bounded $\\ell_2$-norm, the\nclassifier learned by gradient descent based adversarial training converges in\ndirection to the maximum $\\ell_2$-norm margin classifier at the rate of\n$\\tilde{\\mathcal{O}}(1/\\sqrt{T})$, significantly faster than the rate\n$\\mathcal{O}(1/\\log T)$ of training with clean data. In addition, when the\nadversarial perturbation during training has bounded $\\ell_q$-norm for some\n$q\\ge 1$, the resulting classifier converges in direction to a maximum\nmixed-norm margin classifier, which has a natural interpretation of robustness,\nas being the maximum $\\ell_2$-norm margin classifier under worst-case\n$\\ell_q$-norm perturbation to the data. Our findings provide theoretical\nbackups for adversarial training that it indeed promotes robustness against\nadversarial perturbation.\n"], ["2019-06-06", "http://arxiv.org/abs/1906.02816", "Robust Attacks against Multiple Classifiers.", ["Juan C. Perdomo", " Yaron Singer"], "  We address the challenge of designing optimal adversarial noise algorithms\nfor settings where a learner has access to multiple classifiers. We demonstrate\nhow this problem can be framed as finding strategies at equilibrium in a\ntwo-player, zero-sum game between a learner and an adversary. In doing so, we\nillustrate the need for randomization in adversarial attacks. In order to\ncompute Nash equilibrium, our main technical focus is on the design of best\nresponse oracles that can then be implemented within a Multiplicative Weights\nUpdate framework to boost deterministic perturbations against a set of models\ninto optimal mixed strategies. We demonstrate the practical effectiveness of\nour approach on a series of image classification tasks using both linear\nclassifiers and deep neural networks.\n"], ["2019-06-06", "http://arxiv.org/abs/1906.02611", "Improving Robustness Without Sacrificing Accuracy with Patch Gaussian Augmentation.", ["Raphael Gontijo Lopes", " Dong Yin", " Ben Poole", " Justin Gilmer", " Ekin D. Cubuk"], "  Deploying machine learning systems in the real world requires both high\naccuracy on clean data and robustness to naturally occurring corruptions. While\narchitectural advances have led to improved accuracy, building robust models\nremains challenging. Prior work has argued that there is an inherent trade-off\nbetween robustness and accuracy, which is exemplified by standard data augment\ntechniques such as Cutout, which improves clean accuracy but not robustness,\nand additive Gaussian noise, which improves robustness but hurts accuracy. To\novercome this trade-off, we introduce Patch Gaussian, a simple augmentation\nscheme that adds noise to randomly selected patches in an input image. Models\ntrained with Patch Gaussian achieve state of the art on the CIFAR-10 and\nImageNetCommon Corruptions benchmarks while also improving accuracy on clean\ndata. We find that this augmentation leads to reduced sensitivity to high\nfrequency noise(similar to Gaussian) while retaining the ability to take\nadvantage of relevant high frequency information in the image (similar to\nCutout). Finally, we show that Patch Gaussian can be used in conjunction with\nother regularization methods and data augmentation policies such as\nAutoAugment, and improves performance on the COCO object detection benchmark.\n"], ["2019-06-06", "http://arxiv.org/abs/1906.02494", "Understanding Adversarial Behavior of DNNs by Disentangling Non-Robust and Robust Components in Performance Metric.", ["Yujun Shi", " Benben Liao", " Guangyong Chen", " Yun Liu", " Ming-Ming Cheng", " Jiashi Feng"], "  The vulnerability to slight input perturbations is a worrying yet intriguing\nproperty of deep neural networks (DNNs). Despite many previous works studying\nthe reason behind such adversarial behavior, the relationship between the\ngeneralization performance and adversarial behavior of DNNs is still little\nunderstood. In this work, we reveal such relation by introducing a metric\ncharacterizing the generalization performance of a DNN. The metric can be\ndisentangled into an information-theoretic non-robust component, responsible\nfor adversarial behavior, and a robust component. Then, we show by experiments\nthat current DNNs rely heavily on optimizing the non-robust component in\nachieving decent performance. We also demonstrate that current state-of-the-art\nadversarial training algorithms indeed try to robustify the DNNs by preventing\nthem from using the non-robust component to distinguish samples from different\ncategories. Also, based on our findings, we take a step forward and point out\nthe possible direction for achieving decent standard performance and\nadversarial robustness simultaneously. We believe that our theory could further\ninspire the community to make more interesting discoveries about the\nrelationship between standard generalization and adversarial generalization of\ndeep learning models.\n"], ["2019-06-06", "http://arxiv.org/abs/1906.02439", "Should Adversarial Attacks Use Pixel p-Norm?.", ["Ayon Sen", " Xiaojin Zhu", " Liam Marshall", " Robert Nowak"], "  Adversarial attacks aim to confound machine learning systems, while remaining\nvirtually imperceptible to humans. Attacks on image classification systems are\ntypically gauged in terms of $p$-norm distortions in the pixel feature space.\nWe perform a behavioral study, demonstrating that the pixel $p$-norm for any\n$0\\le p \\le \\infty$, and several alternative measures including earth mover's\ndistance, structural similarity index, and deep net embedding, do not fit human\nperception. Our result has the potential to improve the understanding of\nadversarial attack and defense strategies.\n"], ["2019-06-06", "http://arxiv.org/abs/1906.09453", "Image Synthesis with a Single (Robust) Classifier.", ["Shibani Santurkar", " Dimitris Tsipras", " Brandon Tran", " Andrew Ilyas", " Logan Engstrom", " Aleksander Madry"], "  We show that the basic classification framework alone can be used to tackle\nsome of the most challenging tasks in image synthesis. In contrast to other\nstate-of-the-art approaches, the toolkit we develop is rather minimal: it uses\na single, off-the-shelf classifier for all these tasks. The crux of our\napproach is that we train this classifier to be adversarially robust. It turns\nout that adversarial robustness is precisely what we need to directly\nmanipulate salient features of the input. Overall, our findings demonstrate the\nutility of robustness in the broader machine learning context. Code and models\nfor our experiments can be found at https://git.io/robust-apps.\n"], ["2019-06-05", "http://arxiv.org/abs/1906.02398", "Query-efficient Meta Attack to Deep Neural Networks.", ["Jiawei Du", " Hu Zhang", " Joey Tianyi Zhou", " Yi Yang", " Jiashi Feng"], "  Recently, several adversarial attack methods to black-box deep neural\nnetworks have been proposed and they serve as an excellent testing bed for\ninvestigating safety issues with DNNs. These methods generally take in the\nquery and corresponding feedback from the targeted DNN model and infer suitable\nattack patterns accordingly. However, due to lacking prior and inefficiency in\nleveraging the query information, these methods are mostly query-intensive. In\nthis work, we propose a meta attack strategy which is capable of attacking the\ntarget black-box model with much fewer queries. Its high query-efficiency comes\nfrom prior abstraction on training a meta attacker which can speed up the\nsearch for adversarial examples significantly. Extensive experiments on MNIST,\nCIFAR10 and tiny-Imagenet demonstrate that, our meta-attack method can\nremarkably reduce the number of model queries without sacrificing the attack\nperformance. Moreover, the obtained meta attacker is not restricted to a\nparticular model but can be reused easily with fast adaptive ability to attack\na variety of models.\n"], ["2019-06-05", "http://arxiv.org/abs/1906.02337", "MNIST-C: A Robustness Benchmark for Computer Vision.", ["Norman Mu", " Justin Gilmer"], "  We introduce the MNIST-C dataset, a comprehensive suite of 15 corruptions\napplied to the MNIST test set, for benchmarking out-of-distribution robustness\nin computer vision. Through several experiments and visualizations we\ndemonstrate that our corruptions significantly degrade performance of\nstate-of-the-art computer vision models while preserving the semantic content\nof the test images. In contrast to the popular notion of adversarial\nrobustness, our model-agnostic corruptions do not seek worst-case performance\nbut are instead designed to be broad and diverse, capturing multiple failure\nmodes of modern models. In fact, we find that several previously published\nadversarial defenses significantly degrade robustness as measured by MNIST-C.\nWe hope that our benchmark serves as a useful tool for future work in designing\nsystems that are able to learn robust feature representations that capture the\nunderlying semantics of the input.\n"], ["2019-06-05", "http://arxiv.org/abs/1906.02282", "Enhancing Gradient-based Attacks with Symbolic Intervals.", ["Shiqi Wang", " Yizheng Chen", " Ahmed Abdou", " Suman Jana"], "  Recent breakthroughs in defenses against adversarial examples, like\nadversarial training, make the neural networks robust against various classes\nof attackers (e.g., first-order gradient-based attacks). However, it is an open\nquestion whether the adversarially trained networks are truly robust under\nunknown attacks. In this paper, we present interval attacks, a new technique to\nfind adversarial examples to evaluate the robustness of neural networks.\nInterval attacks leverage symbolic interval propagation, a bound propagation\ntechnique that can exploit a broader view around the current input to locate\npromising areas containing adversarial instances, which in turn can be searched\nwith existing gradient-guided attacks. We can obtain such a broader view using\nsound bound propagation methods to track and over-approximate the errors of the\nnetwork within given input ranges. Our results show that, on state-of-the-art\nadversarially trained networks, interval attack can find on average 47%\nrelatively more violations than the state-of-the-art gradient-guided PGD\nattack.\n"], ["2019-06-05", "http://arxiv.org/abs/1906.02033", "Multi-way Encoding for Robustness.", ["Donghyun Kim", " Sarah Adel Bargal", " Jianming Zhang", " Stan Sclaroff"], "  Deep models are state-of-the-art for many computer vision tasks including\nimage classification and object detection. However, it has been shown that deep\nmodels are vulnerable to adversarial examples. We highlight how one-hot\nencoding directly contributes to this vulnerability and propose breaking away\nfrom this widely-used, but highly-vulnerable mapping. We demonstrate that by\nleveraging a different output encoding, multi-way encoding, we decorrelate\nsource and target models, making target models more secure. Our approach makes\nit more difficult for adversaries to find useful gradients for generating\nadversarial attacks of the target model. We present robustness for black-box\nand white-box attacks on four benchmark datasets. The strength of our approach\nis also presented in the form of an attack for model watermarking by\ndecorrelating a target model from a source model.\n"], ["2019-06-04", "http://arxiv.org/abs/1906.01527", "Adversarial Training Generalizes Data-dependent Spectral Norm Regularization.", ["Kevin Roth", " Yannic Kilcher", " Thomas Hofmann"], "  We establish a theoretical link between adversarial training and operator\nnorm regularization for deep neural networks. Specifically, we show that\nadversarial training is a data-dependent generalization of spectral norm\nregularization. This intriguing connection provides fundamental insights into\nthe origin of adversarial vulnerability and hints at novel ways to robustify\nand defend against adversarial attacks. We provide extensive empirical evidence\nto support our theoretical results.\n"], ["2019-06-03", "http://arxiv.org/abs/1906.01171", "Conditional Generative Models are not Robust.", ["Ethan Fetaya", " J\u00f6rn-Henrik Jacobsen", " Richard Zemel"], "  Class-conditional generative models are an increasingly popular approach to\nachieve robust classification. They are a natural choice to solve\ndiscriminative tasks in a robust manner as they jointly optimize for predictive\nperformance and accurate modeling of the input distribution. In this work, we\ninvestigate robust classification with likelihood-based conditional generative\nmodels from a theoretical and practical perspective. Our theoretical result\nreveals that it is impossible to guarantee detectability of adversarial\nexamples even for near-optimal generative classifiers. Experimentally, we show\nthat naively trained conditional generative models have poor discriminative\nperformance, making them unsuitable for classification. This is related to\noverlooked issues with training conditional generative models and we show\nmethods to improve performance. Finally, we analyze the robustness of our\nproposed conditional generative models on MNIST and CIFAR10. While we are able\nto train robust models for MNIST, robustness completely breaks down on CIFAR10.\nThis lack of robustness is related to various undesirable model properties\nmaximum likelihood fails to penalize. Our results indicate that likelihood may\nfundamentally be at odds with robust classification on challenging problems.\n"], ["2019-06-03", "http://arxiv.org/abs/1906.01121", "Adversarial Exploitation of Policy Imitation.", ["Vahid Behzadan", " William Hsu"], "  This paper investigates a class of attacks targeting the confidentiality\naspect of security in Deep Reinforcement Learning (DRL) policies. Recent\nresearch have established the vulnerability of supervised machine learning\nmodels (e.g., classifiers) to model extraction attacks. Such attacks leverage\nthe loosely-restricted ability of the attacker to iteratively query the model\nfor labels, thereby allowing for the forging of a labeled dataset which can be\nused to train a replica of the original model. In this work, we demonstrate the\nfeasibility of exploiting imitation learning techniques in launching model\nextraction attacks on DRL agents. Furthermore, we develop proof-of-concept\nattacks that leverage such techniques for black-box attacks against the\nintegrity of DRL policies. We also present a discussion on potential solution\nconcepts for mitigation techniques.\n"], ["2019-06-03", "http://arxiv.org/abs/1906.01110", "RL-Based Method for Benchmarking the Adversarial Resilience and Robustness of Deep Reinforcement Learning Policies.", ["Vahid Behzadan", " William Hsu"], "  This paper investigates the resilience and robustness of Deep Reinforcement\nLearning (DRL) policies to adversarial perturbations in the state space. We\nfirst present an approach for the disentanglement of vulnerabilities caused by\nrepresentation learning of DRL agents from those that stem from the sensitivity\nof the DRL policies to distributional shifts in state transitions. Building on\nthis approach, we propose two RL-based techniques for quantitative benchmarking\nof adversarial resilience and robustness in DRL policies against perturbations\nof state transitions. We demonstrate the feasibility of our proposals through\nexperimental evaluation of resilience and robustness in DQN, A2C, and PPO2\npolicies trained in the Cartpole environment.\n"], ["2019-06-03", "http://arxiv.org/abs/1906.00735", "Achieving Generalizable Robustness of Deep Neural Networks by Stability Training.", ["Jan Laermann", " Wojciech Samek", " Nils Strodthoff"], "  We study the recently introduced stability training as a general-purpose\nmethod to increase the robustness of deep neural networks against input\nperturbations. In particular, we explore its use as an alternative to data\naugmentation and validate its performance against a number of distortion types\nand transformations including adversarial examples. In our ImageNet-scale image\nclassification experiments stability training performs on a par or even\noutperforms data augmentation for specific transformations, while consistently\noffering improved robustness against a broader range of distortion strengths\nand types unseen during training, a considerably smaller hyperparameter\ndependence and less potentially negative side effects compared to data\naugmentation.\n"], ["2019-06-03", "http://arxiv.org/abs/1906.00698", "Adversarial Risk Bounds for Neural Networks through Sparsity based Compression.", ["Emilio Rafael Balda", " Arash Behboodi", " Niklas Koep", " Rudolf Mathar"], "  Neural networks have been shown to be vulnerable against minor adversarial\nperturbations of their inputs, especially for high dimensional data under\n$\\ell_\\infty$ attacks. To combat this problem, techniques like adversarial\ntraining have been employed to obtain models which are robust on the training\nset. However, the robustness of such models against adversarial perturbations\nmay not generalize to unseen data. To study how robustness generalizes, recent\nworks assume that the inputs have bounded $\\ell_2$-norm in order to bound the\nadversarial risk for $\\ell_\\infty$ attacks with no explicit dimension\ndependence. In this work we focus on $\\ell_\\infty$ attacks on $\\ell_\\infty$\nbounded inputs and prove margin-based bounds. Specifically, we use a\ncompression based approach that relies on efficiently compressing the set of\ntunable parameters without distorting the adversarial risk. To achieve this, we\napply the concept of effective sparsity and effective joint sparsity on the\nweight matrices of neural networks. This leads to bounds with no explicit\ndependence on the input dimension, neither on the number of classes. Our\nresults show that neural networks with approximately sparse weight matrices not\nonly enjoy enhanced robustness, but also better generalization.\n"], ["2019-06-03", "http://arxiv.org/abs/1906.00679", "The Adversarial Machine Learning Conundrum: Can The Insecurity of ML Become The Achilles' Heel of Cognitive Networks?.", ["Muhammad Usama", " Junaid Qadir", " Ala Al-Fuqaha", " Mounir Hamdi"], "  The holy grail of networking is to create \\textit{cognitive networks} that\norganize, manage, and drive themselves. Such a vision now seems attainable\nthanks in large part to the progress in the field of machine learning (ML),\nwhich has now already disrupted a number of industries and revolutionized\npractically all fields of research. But are the ML models foolproof and robust\nto security attacks to be in charge of managing the network? Unfortunately,\nmany modern ML models are easily misled by simple and easily-crafted\nadversarial perturbations, which does not bode well for the future of ML-based\ncognitive networks unless ML vulnerabilities for the cognitive networking\nenvironment are identified, addressed, and fixed. The purpose of this article\nis to highlight the problem of insecure ML and to sensitize the readers to the\ndanger of adversarial ML by showing how an easily-crafted adversarial ML\nexample can compromise the operations of the cognitive self-driving network. In\nthis paper, we demonstrate adversarial attacks on two simple yet representative\ncognitive networking applications (namely, intrusion detection and network\ntraffic classification). We also provide some guidelines to design secure ML\nmodels for cognitive networks that are robust to adversarial attacks on the ML\npipeline of cognitive networks.\n"], ["2019-06-03", "http://arxiv.org/abs/1906.00945", "Adversarial Robustness as a Prior for Learned Representations.", ["Logan Engstrom", " Andrew Ilyas", " Shibani Santurkar", " Dimitris Tsipras", " Brandon Tran", " Aleksander Madry"], "  An important goal in deep learning is to learn versatile, high-level feature\nrepresentations of input data. However, standard networks' representations seem\nto possess shortcomings that, as we illustrate, prevent them from fully\nrealizing this goal. In this work, we show that robust optimization can be\nre-cast as a tool for enforcing priors on the features learned by deep neural\nnetworks. It turns out that representations learned by robust models address\nthe aforementioned shortcomings and make significant progress towards learning\na high-level encoding of inputs. In particular, these representations are\napproximately invertible, while allowing for direct visualization and\nmanipulation of salient input features. More broadly, our results indicate\nadversarial robustness as a promising avenue for improving learned\nrepresentations. Our code and models for reproducing these results is available\nat https://git.io/robust-reps .\n"], ["2019-06-03", "http://arxiv.org/abs/1906.01040", "A Surprising Density of Illusionable Natural Speech.", ["Melody Y. Guan", " Gregory Valiant"], "  Recent work on adversarial examples has demonstrated that most natural inputs\ncan be perturbed to fool even state-of-the-art machine learning systems. But\ndoes this happen for humans as well? In this work, we investigate: what\nfraction of natural instances of speech can be turned into \"illusions\" which\neither alter humans' perception or result in different people having\nsignificantly different perceptions? We first consider the McGurk effect, the\nphenomenon by which adding a carefully chosen video clip to the audio channel\naffects the viewer's perception of what is said (McGurk and MacDonald, 1976).\nWe obtain empirical estimates that a significant fraction of both words and\nsentences occurring in natural speech have some susceptibility to this effect.\nWe also learn models for predicting McGurk illusionability. Finally we\ndemonstrate that the Yanny or Laurel auditory illusion (Pressnitzer et al.,\n2018) is not an isolated occurrence by generating several very different new\ninstances. We believe that the surprising density of illusionable natural\nspeech warrants further investigation, from the perspectives of both security\nand cognitive science. Supplementary videos are available at:\nhttps://www.youtube.com/playlist?list=PLaX7t1K-e_fF2iaenoKznCatm0RC37B_k.\n"], ["2019-06-03", "http://arxiv.org/abs/1906.00628", "Fast and Stable Interval Bounds Propagation for Training Verifiably Robust Models.", ["Pawe\u0142 Morawiecki", " Przemys\u0142aw Spurek", " Marek \u015amieja", " Jacek Tabor"], "  We present an efficient technique, which allows to train classification\nnetworks which are verifiably robust against norm-bounded adversarial attacks.\nThis framework is built upon the work of Gowal et al., who applies the interval\narithmetic to bound the activations at each layer and keeps the prediction\ninvariant to the input perturbation. While that method is faster than\ncompetitive approaches, it requires careful tuning of hyper-parameters and a\nlarge number of epochs to converge. To speed up and stabilize training, we\nsupply the cost function with an additional term, which encourages the model to\nkeep the interval bounds at hidden layers small. Experimental results\ndemonstrate that we can achieve comparable (or even better) results using a\nsmaller number of training iterations, in a more stable fashion. Moreover, the\nproposed model is not so sensitive to the exact specification of the training\nprocess, which makes it easier to use by practitioners.\n"], ["2019-06-02", "http://arxiv.org/abs/1906.00555", "Adversarially Robust Generalization Just Requires More Unlabeled Data.", ["Runtian Zhai", " Tianle Cai", " Di He", " Chen Dan", " Kun He", " John Hopcroft", " Liwei Wang"], "  Neural network robustness has recently been highlighted by the existence of\nadversarial examples. Many previous works show that the learned networks do not\nperform well on perturbed test data, and significantly more labeled data is\nrequired to achieve adversarially robust generalization. In this paper, we\ntheoretically and empirically show that with just more unlabeled data, we can\nlearn a model with better adversarially robust generalization. The key insight\nof our results is based on a risk decomposition theorem, in which the expected\nrobust risk is separated into two parts: the stability part which measures the\nprediction stability in the presence of perturbations, and the accuracy part\nwhich evaluates the standard classification accuracy. As the stability part\ndoes not depend on any label information, we can optimize this part using\nunlabeled data. We further prove that for a specific Gaussian mixture problem,\nadversarially robust generalization can be almost as easy as the standard\ngeneralization in supervised learning if a sufficiently large amount of\nunlabeled data is provided. Inspired by the theoretical findings, we further\nshow that a practical adversarial training algorithm that leverages unlabeled\ndata can improve adversarial robust generalization on MNIST and Cifar-10.\n"], ["2019-06-01", "http://arxiv.org/abs/1906.00335", "Adversarial Examples for Edge Detection: They Exist, and They Transfer.", ["Christian Cosgrove", " Alan L. Yuille"], "  Convolutional neural networks have recently advanced the state of the art in\nmany tasks including edge and object boundary detection. However, in this\npaper, we demonstrate that these edge detectors inherit a troubling property of\nneural networks: they can be fooled by adversarial examples. We show that\nadding small perturbations to an image causes HED, a CNN-based edge detection\nmodel, to fail to locate edges, to detect nonexistent edges, and even to\nhallucinate arbitrary configurations of edges. More surprisingly, we find that\nthese adversarial examples transfer to other CNN-based vision models. In\nparticular, attacks on edge detection result in significant drops in accuracy\nin models trained to perform unrelated, high-level tasks like image\nclassification and semantic segmentation. Our code will be made public.\n"], ["2019-06-01", "http://arxiv.org/abs/1906.00258", "Enhancing Transformation-based Defenses using a Distribution Classifier.", ["Connie Kou", " Hwee Kuan Lee", " Teck Khim Ng", " Ee-Chien Chang"], "  Adversarial attacks on convolutional neural networks (CNN) have gained\nsignificant attention and research efforts have focused on defense methods that\nmake the classifiers more robust. Stochastic input transformation methods have\nbeen proposed, where the idea is to randomly transform the input images to try\nto recover from the adversarial attacks. While these transformation-based\nmethods have shown considerable success at recovering from adversarial images,\nthe performance on clean images deteriorates as the magnitude of the\ntransformation increases. In this paper, we propose a defense mechanism that\ncan be integrated with existing transformation-based defenses and reduce the\ndeterioration of performance on clean images. Exploiting the fact that the\ntransformation methods are stochastic, our method samples a population of\ntransformed images and performs the final classification on distributions of\nsoftmax probabilities. We train a separate compact distribution classifier to\nrecognize distinctive features in the distributions of softmax probabilities of\ntransformed clean images. Without retraining the original CNN, our distribution\nclassifier improves the performance of transformation-based defenses on both\nclean and adversarial images, even though the distribution classifier was never\ntrained on distributions obtained from the adversarial images. Our method is\ngeneric and can be integrated with existing transformation-based methods.\n"], ["2019-06-01", "http://arxiv.org/abs/1906.00204", "Perceptual Evaluation of Adversarial Attacks for CNN-based Image Classification.", ["Sid Ahmed Fezza", " Yassine Bakhti", " Wassim Hamidouche", " Olivier D\u00e9forges"], "  Deep neural networks (DNNs) have recently achieved state-of-the-art\nperformance and provide significant progress in many machine learning tasks,\nsuch as image classification, speech processing, natural language processing,\netc. However, recent studies have shown that DNNs are vulnerable to adversarial\nattacks. For instance, in the image classification domain, adding small\nimperceptible perturbations to the input image is sufficient to fool the DNN\nand to cause misclassification. The perturbed image, called \\textit{adversarial\nexample}, should be visually as close as possible to the original image.\nHowever, all the works proposed in the literature for generating adversarial\nexamples have used the $L_{p}$ norms ($L_{0}$, $L_{2}$ and $L_{\\infty}$) as\ndistance metrics to quantify the similarity between the original image and the\nadversarial example. Nonetheless, the $L_{p}$ norms do not correlate with human\njudgment, making them not suitable to reliably assess the perceptual\nsimilarity/fidelity of adversarial examples. In this paper, we present a\ndatabase for visual fidelity assessment of adversarial examples. We describe\nthe creation of the database and evaluate the performance of fifteen\nstate-of-the-art full-reference (FR) image fidelity assessment metrics that\ncould substitute $L_{p}$ norms. The database as well as subjective scores are\npublicly available to help designing new metrics for adversarial examples and\nto facilitate future research works.\n"], ["2019-05-31", "http://arxiv.org/abs/1905.13472", "Reverse KL-Divergence Training of Prior Networks: Improved Uncertainty and Adversarial Robustness.", ["Andrey Malinin", " Mark Gales"], "  Ensemble approaches for uncertainty estimation have recently been applied to\nthe tasks of misclassification detection, out-of-distribution input detection\nand adversarial attack detection. Prior Networks have been proposed as an\napproach to efficiently emulating an ensemble of models by parameterising a\nDirichlet prior distribution over output distributions. These models have been\nshown to outperform ensemble approaches, such as Monte-Carlo Dropout, on the\ntask of out-of-distribution input detection. However, scaling Prior Networks to\ncomplex datasets with many classes is difficult using the training criteria\noriginally proposed. This paper makes two contributions. Firstly, we show that\nthe appropriate training criterion for Prior Networks is the reverse\nKL-divergence between Dirichlet distributions. Using this loss we successfully\ntrain Prior Networks on image classification datasets with up to 200 classes\nand improve out-of-distribution detection performance. Secondly, taking\nadvantage of the new training criterion, this paper investigates using Prior\nNetworks to detect adversarial attacks. It is shown that the construction of\nsuccessful adaptive whitebox attacks, which affect the prediction and evade\ndetection, against Prior Networks trained on CIFAR-10 and CIFAR-100 takes a\ngreater amount of computational effort than against standard neural networks,\nadversarially trained neural networks and dropout-defended networks.\n"], ["2019-05-31", "http://arxiv.org/abs/1905.13736", "Unlabeled Data Improves Adversarial Robustness.", ["Yair Carmon", " Aditi Raghunathan", " Ludwig Schmidt", " Percy Liang", " John C. Duchi"], "  We demonstrate, theoretically and empirically, that adversarial robustness\ncan significantly benefit from semisupervised learning. Theoretically, we\nrevisit the simple Gaussian model of Schmidt et al. that shows a sample\ncomplexity gap between standard and robust classification. We prove that this\ngap does not pertain to labels: a simple semisupervised learning procedure\n(self-training) achieves robust accuracy using the same number of labels\nrequired for standard accuracy. Empirically, we augment CIFAR-10 with 500K\nunlabeled images sourced from 80 Million Tiny Images and use robust\nself-training to outperform state-of-the-art robust accuracies by over 5 points\nin (i) $\\ell_\\infty$ robustness against several strong attacks via adversarial\ntraining and (ii) certified $\\ell_2$ and $\\ell_\\infty$ robustness via\nrandomized smoothing. On SVHN, adding the dataset's own extra training set with\nthe labels removed provides gains of 4 to 10 points, within 1 point of the gain\nfrom using the extra labels as well.\n"], ["2019-05-31", "http://arxiv.org/abs/1905.13725", "Are Labels Required for Improving Adversarial Robustness?.", ["Jonathan Uesato", " Jean-Baptiste Alayrac", " Po-Sen Huang", " Robert Stanforth", " Alhussein Fawzi", " Pushmeet Kohli"], "  Recent work has uncovered the interesting (and somewhat surprising) finding\nthat training models to be invariant to adversarial perturbations requires\nsubstantially larger datasets than those required for standard classification.\nThis result is a key hurdle in the deployment of robust machine learning models\nin many real world applications where labeled data is expensive. Our main\ninsight is that unlabeled data can be a competitive alternative to labeled data\nfor training adversarially robust models. Theoretically, we show that in a\nsimple statistical setting, the sample complexity for learning an adversarially\nrobust model from unlabeled data matches the fully supervised case up to\nconstant factors. On standard datasets like CIFAR-10, a simple Unsupervised\nAdversarial Training (UAT) approach using unlabeled data improves robust\naccuracy by 21.7% over using 4K supervised examples alone, and captures over\n95% of the improvement from the same number of labeled examples. Finally, we\nreport an improvement of 4% over the previous state-of-the-art on CIFAR-10\nagainst the strongest known attack by using additional unlabeled data from the\nuncurated 80 Million Tiny Images dataset. This demonstrates that our finding\nextends as well to the more realistic case where unlabeled data is also\nuncurated, therefore opening a new avenue for improving adversarial training.\n"], ["2019-05-30", "http://arxiv.org/abs/1905.13399", "Real-Time Adversarial Attacks.", ["Yuan Gong", " Boyang Li", " Christian Poellabauer", " Yiyu Shi"], "  In recent years, many efforts have demonstrated that modern machine learning\nalgorithms are vulnerable to adversarial attacks, where small, but carefully\ncrafted, perturbations on the input can make them fail. While these attack\nmethods are very effective, they only focus on scenarios where the target model\ntakes static input, i.e., an attacker can observe the entire original sample\nand then add a perturbation at any point of the sample. These attack approaches\nare not applicable to situations where the target model takes streaming input,\ni.e., an attacker is only able to observe past data points and add\nperturbations to the remaining (unobserved) data points of the input. In this\npaper, we propose a real-time adversarial attack scheme for machine learning\nmodels with streaming inputs.\n"], ["2019-05-30", "http://arxiv.org/abs/1905.13386", "Residual Networks as Nonlinear Systems: Stability Analysis using Linearization.", ["Kai Rothauge", " Zhewei Yao", " Zixi Hu", " Michael W. Mahoney"], "  We regard pre-trained residual networks (ResNets) as nonlinear systems and\nuse linearization, a common method used in the qualitative analysis of\nnonlinear systems, to understand the behavior of the networks under small\nperturbations of the input images. We work with ResNet-56 and ResNet-110\ntrained on the CIFAR-10 data set. We linearize these networks at the level of\nresidual units and network stages, and the singular value decomposition is used\nin the stability analysis of these components. It is found that most of the\nsingular values of the linearizations of residual units are 1 and, in spite of\nthe fact that the linearizations depend directly on the activation maps, the\nsingular values differ only slightly for different input images. However,\nadjusting the scaling of the skip connection or the values of the weights in a\nresidual unit has a significant impact on the singular value distributions.\nInspection of how random and adversarial perturbations of input images\npropagate through the network reveals that there is a dramatic jump in the\nmagnitude of adversarial perturbations towards the end of the final stage of\nthe network that is not present in the case of random perturbations. We attempt\nto gain a better understanding of this phenomenon by projecting the\nperturbations onto singular vectors of the linearizations of the residual\nunits.\n"], ["2019-05-30", "http://arxiv.org/abs/1905.13284", "Identifying Classes Susceptible to Adversarial Attacks.", ["Rangeet Pan", " Md Johirul Islam", " Shibbir Ahmed", " Hridesh Rajan"], "  Despite numerous attempts to defend deep learning based image classifiers,\nthey remain susceptible to the adversarial attacks. This paper proposes a\ntechnique to identify susceptible classes, those classes that are more easily\nsubverted. To identify the susceptible classes we use distance-based measures\nand apply them on a trained model. Based on the distance among original\nclasses, we create mapping among original classes and adversarial classes that\nhelps to reduce the randomness of a model to a significant amount in an\nadversarial setting. We analyze the high dimensional geometry among the feature\nclasses and identify the k most susceptible target classes in an adversarial\nattack. We conduct experiments using MNIST, Fashion MNIST, CIFAR-10 (ImageNet\nand ResNet-32) datasets. Finally, we evaluate our techniques in order to\ndetermine which distance-based measure works best and how the randomness of a\nmodel changes with perturbation.\n"], ["2019-05-30", "http://arxiv.org/abs/1905.13074", "Robust Sparse Regularization: Simultaneously Optimizing Neural Network Robustness and Compactness.", ["Adnan Siraj Rakin", " Zhezhi He", " Li Yang", " Yanzhi Wang", " Liqiang Wang", " Deliang Fan"], "  Deep Neural Network (DNN) trained by the gradient descent method is known to\nbe vulnerable to maliciously perturbed adversarial input, aka. adversarial\nattack. As one of the countermeasures against adversarial attack, increasing\nthe model capacity for DNN robustness enhancement was discussed and reported as\nan effective approach by many recent works. In this work, we show that\nshrinking the model size through proper weight pruning can even be helpful to\nimprove the DNN robustness under adversarial attack. For obtaining a\nsimultaneously robust and compact DNN model, we propose a multi-objective\ntraining method called Robust Sparse Regularization (RSR), through the fusion\nof various regularization techniques, including channel-wise noise injection,\nlasso weight penalty, and adversarial training. We conduct extensive\nexperiments across popular ResNet-20, ResNet-18 and VGG-16 DNN architectures to\ndemonstrate the effectiveness of RSR against popular white-box (i.e., PGD and\nFGSM) and black-box attacks. Thanks to RSR, 85% weight connections of ResNet-18\ncan be pruned while still achieving 0.68% and 8.72% improvement in clean- and\nperturbed-data accuracy respectively on CIFAR-10 dataset, in comparison to its\nPGD adversarial training baseline.\n"], ["2019-05-30", "http://arxiv.org/abs/1905.12864", "Interpretable Adversarial Training for Text.", ["Samuel Barham", " Soheil Feizi"], "  Generating high-quality and interpretable adversarial examples in the text\ndomain is a much more daunting task than it is in the image domain. This is due\npartly to the discrete nature of text, partly to the problem of ensuring that\nthe adversarial examples are still probable and interpretable, and partly to\nthe problem of maintaining label invariance under input perturbations. In order\nto address some of these challenges, we introduce sparse projected gradient\ndescent (SPGD), a new approach to crafting interpretable adversarial examples\nfor text. SPGD imposes a directional regularization constraint on input\nperturbations by projecting them onto the directions to nearby word embeddings\nwith highest cosine similarities. This constraint ensures that perturbations\nmove each word embedding in an interpretable direction (i.e., towards another\nnearby word embedding). Moreover, SPGD imposes a sparsity constraint on\nperturbations at the sentence level by ignoring word-embedding perturbations\nwhose norms are below a certain threshold. This constraint ensures that our\nmethod changes only a few words per sequence, leading to higher quality\nadversarial examples. Our experiments with the IMDB movie review dataset show\nthat the proposed SPGD method improves adversarial example interpretability and\nlikelihood (evaluated by average per-word perplexity) compared to\nstate-of-the-art methods, while suffering little to no loss in training\nperformance.\n"], ["2019-05-29", "http://arxiv.org/abs/1906.00001", "Functional Adversarial Attacks.", ["Cassidy Laidlaw", " Soheil Feizi"], "  We propose functional adversarial attacks, a novel class of threat models for\ncrafting adversarial examples to fool machine learning models. Unlike a\nstandard $\\ell_p$-ball threat model, a functional adversarial threat model\nallows only a single function to be used to perturb input features to produce\nan adversarial example. For example, a functional adversarial attack applied on\ncolors of an image can change all red pixels simultaneously to light red. Such\nglobal uniform changes in images can be less perceptible than perturbing pixels\nof the image individually. For simplicity, we refer to functional adversarial\nattacks on image colors as ReColorAdv, which is the main focus of our\nexperiments. We show that functional threat models can be combined with\nexisting additive ($\\ell_p$) threat models to generate stronger threat models\nthat allow both small, individual perturbations and large, uniform changes to\nan input. Moreover, we prove that such combinations encompass perturbations\nthat would not be allowed in either constituent threat model. In practice,\nReColorAdv can significantly reduce the accuracy of a ResNet-32 trained on\nCIFAR-10. Furthermore, to the best of our knowledge, combining ReColorAdv with\nother attacks leads to the strongest existing attack even after adversarial\ntraining.\n"], ["2019-05-29", "http://arxiv.org/abs/1905.12797", "Bandlimiting Neural Networks Against Adversarial Attacks.", ["Yuping Lin", " Kasra Ahmadi K. A.", " Hui Jiang"], "  In this paper, we study the adversarial attack and defence problem in deep\nlearning from the perspective of Fourier analysis. We first explicitly compute\nthe Fourier transform of deep ReLU neural networks and show that there exist\ndecaying but non-zero high frequency components in the Fourier spectrum of\nneural networks. We demonstrate that the vulnerability of neural networks\ntowards adversarial samples can be attributed to these insignificant but\nnon-zero high frequency components. Based on this analysis, we propose to use a\nsimple post-averaging technique to smooth out these high frequency components\nto improve the robustness of neural networks against adversarial attacks.\nExperimental results on the ImageNet dataset have shown that our proposed\nmethod is universally effective to defend many existing adversarial attacking\nmethods proposed in the literature, including FGSM, PGD, DeepFool and C&W\nattacks. Our post-averaging method is simple since it does not require any\nre-training, and meanwhile it can successfully defend over 95% of the\nadversarial samples generated by these methods without introducing any\nsignificant performance degradation (less than 1%) on the original clean\nimages.\n"], ["2019-05-29", "http://arxiv.org/abs/1905.12762", "Securing Connected & Autonomous Vehicles: Challenges Posed by Adversarial Machine Learning and The Way Forward.", ["Adnan Qayyum", " Muhammad Usama", " Junaid Qadir", " Ala Al-Fuqaha"], "  Connected and autonomous vehicles (CAVs) will form the backbone of future\nnext-generation intelligent transportation systems (ITS) providing travel\ncomfort, road safety, along with a number of value-added services. Such a\ntransformation---which will be fuelled by concomitant advances in technologies\nfor machine learning (ML) and wireless communications---will enable a future\nvehicular ecosystem that is better featured and more efficient. However, there\nare lurking security problems related to the use of ML in such a critical\nsetting where an incorrect ML decision may not only be a nuisance but can lead\nto loss of precious lives. In this paper, we present an in-depth overview of\nthe various challenges associated with the application of ML in vehicular\nnetworks. In addition, we formulate the ML pipeline of CAVs and present various\npotential security issues associated with the adoption of ML methods. In\nparticular, we focus on the perspective of adversarial ML attacks on CAVs and\noutline a solution to defend against adversarial attacks in multiple settings.\n"], ["2019-05-29", "http://arxiv.org/abs/1905.12386", "Misleading Authorship Attribution of Source Code using Adversarial Learning.", ["Erwin Quiring", " Alwin Maier", " Konrad Rieck"], "  In this paper, we present a novel attack against authorship attribution of\nsource code. We exploit that recent attribution methods rest on machine\nlearning and thus can be deceived by adversarial examples of source code. Our\nattack performs a series of semantics-preserving code transformations that\nmislead learning-based attribution but appear plausible to a developer. The\nattack is guided by Monte-Carlo tree search that enables us to operate in the\ndiscrete domain of source code. In an empirical evaluation with source code\nfrom 204 programmers, we demonstrate that our attack has a substantial effect\non two recent attribution methods, whose accuracy drops from over 88% to 1%\nunder attack. Furthermore, we show that our attack can imitate the coding style\nof developers with high accuracy and thereby induce false attributions. We\nconclude that current approaches for authorship attribution are inappropriate\nfor practical application and there is a need for resilient analysis\ntechniques.\n"], ["2019-05-29", "http://arxiv.org/abs/1905.12282", "Targeted Attacks on Deep Reinforcement Learning Agents through Adversarial Observations.", ["L\u00e9onard Hussenot", " Matthieu Geist", " Olivier Pietquin"], "  This paper deals with adversarial attacks on perceptions of neural network\npolicies in the Reinforcement Learning (RL) context. While previous approaches\nperform untargeted attacks on the state of the agent, we propose a method to\nperform targeted attacks to lure an agent into consistently following a desired\npolicy. We place ourselves in a realistic setting, where attacks are performed\non observations of the environment rather than the internal state of the agent\nand develop constant attacks instead of per-observation ones. We illustrate our\nmethod by attacking deep RL agents playing Atari games and show that universal\nadditive masks can be applied not only to degrade performance but to take\ncontrol of an agent.\n"], ["2019-05-28", "http://arxiv.org/abs/1905.13545", "High Frequency Component Helps Explain the Generalization of Convolutional Neural Networks.", ["Haohan Wang", " Xindi Wu", " Pengcheng Yin", " Eric P. Xing"], "  We investigate the relationship between the frequency spectrum of image data\nand the generalization behavior of convolutional neural networks (CNN). We\nfirst notice CNN's ability in capturing the high-frequency components of\nimages. These high-frequency components are almost imperceptible to a human.\nThus the observation can serve as one of the explanations of the existence of\nadversarial examples, and can also help verify CNN's trade-off between\nrobustness and accuracy. Our observation also immediately leads to methods that\ncan improve the adversarial robustness of trained CNN. Finally, we also utilize\nthis observation to design a (semi) black-box adversarial attack method.\n"], ["2019-05-28", "http://arxiv.org/abs/1905.12202", "Empirically Measuring Concentration: Fundamental Limits on Intrinsic Robustness.", ["Saeed Mahloujifar", " Xiao Zhang", " Mohammad Mahmoody", " David Evans"], "  Many recent works have shown that adversarial examples that fool classifiers\ncan be found by minimally perturbing a normal input. Recent theoretical\nresults, starting with Gilmer et al. (2018), show that if the inputs are drawn\nfrom a concentrated metric probability space, then adversarial examples with\nsmall perturbation are inevitable. A concentrated space has the property that\nany subset with $\\Omega(1)$ (e.g., 1/100) measure, according to the imposed\ndistribution, has small distance to almost all (e.g., 99/100) of the points in\nthe space. It is not clear, however, whether these theoretical results apply to\nactual distributions such as images. This paper presents a method for\nempirically measuring and bounding the concentration of a concrete dataset\nwhich is proven to converge to the actual concentration. We use it to\nempirically estimate the intrinsic robustness to $\\ell_\\infty$ and $\\ell_2$\nperturbations of several image classification benchmarks.\n"], ["2019-05-28", "http://arxiv.org/abs/1905.11971", "ME-Net: Towards Effective Adversarial Robustness with Matrix Estimation.", ["Yuzhe Yang", " Guo Zhang", " Dina Katabi", " Zhi Xu"], "  Deep neural networks are vulnerable to adversarial attacks. The literature is\nrich with algorithms that can easily craft successful adversarial examples. In\ncontrast, the performance of defense techniques still lags behind. This paper\nproposes ME-Net, a defense method that leverages matrix estimation (ME). In\nME-Net, images are preprocessed using two steps: first pixels are randomly\ndropped from the image; then, the image is reconstructed using ME. We show that\nthis process destroys the adversarial structure of the noise, while\nre-enforcing the global structure in the original image. Since humans typically\nrely on such global structures in classifying images, the process makes the\nnetwork mode compatible with human perception. We conduct comprehensive\nexperiments on prevailing benchmarks such as MNIST, CIFAR-10, SVHN, and\nTiny-ImageNet. Comparing ME-Net with state-of-the-art defense mechanisms shows\nthat ME-Net consistently outperforms prior techniques, improving robustness\nagainst both black-box and white-box attacks.\n"], ["2019-05-28", "http://arxiv.org/abs/1905.11832", "Snooping Attacks on Deep Reinforcement Learning.", ["Matthew Inkawhich", " Yiran Chen", " Hai Li"], "  Adversarial attacks have exposed a significant security vulnerability in\nstate-of-the-art machine learning models. Among these models include deep\nreinforcement learning agents. The existing methods for attacking reinforcement\nlearning agents assume the adversary either has access to the target agent's\nlearned parameters or the environment that the agent interacts with. In this\nwork, we propose a new class of threat models, called snooping threat models,\nthat are unique to reinforcement learning. In these snooping threat models, the\nadversary does not have the ability to personally interact with the\nenvironment, and can only eavesdrop on the action and reward signals being\nexchanged between agent and environment. We show that adversaries operating in\nthese highly constrained threat models can still launch devastating attacks\nagainst the target agent by training proxy models on related tasks and\nleveraging the transferability of adversarial examples.\n"], ["2019-05-28", "http://arxiv.org/abs/1905.11831", "Adversarial Attacks on Remote User Authentication Using Behavioural Mouse Dynamics.", ["Yi Xiang Marcus Tan", " Alfonso Iacovazzi", " Ivan Homoliak", " Yuval Elovici", " Alexander Binder"], "  Mouse dynamics is a potential means of authenticating users. Typically, the\nauthentication process is based on classical machine learning techniques, but\nrecently, deep learning techniques have been introduced for this purpose.\nAlthough prior research has demonstrated how machine learning and deep learning\nalgorithms can be bypassed by carefully crafted adversarial samples, there has\nbeen very little research performed on the topic of behavioural biometrics in\nthe adversarial domain. In an attempt to address this gap, we built a set of\nattacks, which are applications of several generative approaches, to construct\nadversarial mouse trajectories that bypass authentication models. These\ngenerated mouse sequences will serve as the adversarial samples in the context\nof our experiments. We also present an analysis of the attack approaches we\nexplored, explaining their limitations. In contrast to previous work, we\nconsider the attacks in a more realistic and challenging setting in which an\nattacker has access to recorded user data but does not have access to the\nauthentication model or its outputs. We explore three different attack\nstrategies: 1) statistics-based, 2) imitation-based, and 3) surrogate-based; we\nshow that they are able to evade the functionality of the authentication\nmodels, thereby impacting their robustness adversely. We show that\nimitation-based attacks often perform better than surrogate-based attacks,\nunless, however, the attacker can guess the architecture of the authentication\nmodel. In such cases, we propose a potential detection mechanism against\nsurrogate-based attacks.\n"], ["2019-05-28", "http://arxiv.org/abs/1905.11713", "Improving the Robustness of Deep Neural Networks via Adversarial Training with Triplet Loss.", ["Pengcheng Li", " Jinfeng Yi", " Bowen Zhou", " Lijun Zhang"], "  Recent studies have highlighted that deep neural networks (DNNs) are\nvulnerable to adversarial examples. In this paper, we improve the robustness of\nDNNs by utilizing techniques of Distance Metric Learning. Specifically, we\nincorporate Triplet Loss, one of the most popular Distance Metric Learning\nmethods, into the framework of adversarial training. Our proposed algorithm,\nAdversarial Training with Triplet Loss (AT$^2$L), substitutes the adversarial\nexample against the current model for the anchor of triplet loss to effectively\nsmooth the classification boundary. Furthermore, we propose an ensemble version\nof AT$^2$L, which aggregates different attack methods and model structures for\nbetter defense effects. Our empirical studies verify that the proposed approach\ncan significantly improve the robustness of DNNs without sacrificing accuracy.\nFinally, we demonstrate that our specially designed triplet loss can also be\nused as a regularization term to enhance other defense methods.\n"], ["2019-05-28", "http://arxiv.org/abs/1905.12105", "Certifiably Robust Interpretation in Deep Learning.", ["Alexander Levine", " Sahil Singla", " Soheil Feizi"], "  Although gradient-based saliency maps are popular methods for deep learning\ninterpretation, they can be extremely vulnerable to adversarial attacks. This\nis worrisome especially due to the lack of practical defenses for protecting\ndeep learning interpretations against attacks. In this paper, we address this\nproblem and provide two defense methods for deep learning interpretation.\nFirst, we show that a sparsified version of the popular SmoothGrad method,\nwhich computes the average saliency maps over random perturbations of the\ninput, is certifiably robust against adversarial perturbations. We obtain this\nresult by extending recent bounds for certifiably robust smooth classifiers to\nthe interpretation setting. Experiments on ImageNet samples validate our\ntheory. Second, we introduce an adversarial training approach to further\nrobustify deep learning interpretation by adding a regularization term to\npenalize the inconsistency of saliency maps between normal and crafted\nadversarial samples. Empirically, we observe that this approach not only\nimproves the robustness of deep learning interpretation to adversarial attacks,\nbut it also improves the quality of the gradient-based saliency maps.\n"], ["2019-05-28", "http://arxiv.org/abs/1905.12418", "Probabilistically True and Tight Bounds for Robust Deep Neural Network Training.", ["Salman Alsubaihi", " Adel Bibi", " Modar Alfadly", " Bernard Ghanem"], "  Training Deep Neural Networks (DNNs) that are robust to norm bounded\nadversarial attacks remains an elusive problem. While verification based\nmethods are generally too expensive to robustly train large networks, it was\ndemonstrated in Gowal et. al. that bounded input intervals can be inexpensively\npropagated per layer through large networks. This interval bound propagation\n(IBP) approach led to high robustness and was the first to be employed on large\nnetworks. However, due to the very loose nature of the IBP bounds, particularly\nfor large networks, the required training procedure is complex and involved. In\nthis paper, we closely examine the bounds of a block of layers composed of an\naffine layer followed by a ReLU nonlinearity followed by another affine layer.\nIn doing so, we propose probabilistic bounds, true bounds in expectation, that\nare provably tighter than IBP bounds in expectation. We then extend this result\nto deeper networks through blockwise propagation and show that we can achieve\norders of magnitudes tighter bounds compared to IBP. With such tight bounds, we\ndemonstrate that a simple standard training procedure can achieve the best\nrobustness-accuracy trade-off across several architectures on both MNIST and\nCIFAR10.\n"], ["2019-05-28", "http://arxiv.org/abs/1905.11736", "Cross-Domain Transferability of Adversarial Perturbations.", ["Muzammal Naseer", " Salman H. Khan", " Harris Khan", " Fahad Shahbaz Khan", " Fatih Porikli"], "  Adversarial examples reveal the blind spots of deep neural networks (DNNs)\nand represent a major concern for security-critical applications. The\ntransferability of adversarial examples makes real-world attacks possible in\nblack-box settings, where the attacker is forbidden to access the internal\nparameters of the model. The underlying assumption in most adversary generation\nmethods, whether learning an instance-specific or an instance-agnostic\nperturbation, is the direct or indirect reliance on the original\ndomain-specific data distribution. In this work, for the first time, we\ndemonstrate the existence of domain-invariant adversaries, thereby showing\ncommon adversarial space among different datasets and models. To this end, we\npropose a framework capable of launching highly transferable attacks that\ncrafts adversarial patterns to mislead networks trained on wholly different\ndomains. For instance, an adversarial function learned on Paintings, Cartoons\nor Medical images can successfully perturb ImageNet samples to fool the\nclassifier, with success rates as high as $\\sim$99\\% ($\\ell_{\\infty} \\le 10$).\nThe core of our proposed adversarial function is a generative network that is\ntrained using a relativistic supervisory signal that enables domain-invariant\nperturbations. Our approach sets the new state-of-the-art for fooling rates,\nboth under the white-box and black-box scenarios. Furthermore, despite being an\ninstance-agnostic perturbation function, our attack outperforms the\nconventionally much stronger instance-specific attack methods.\n"], ["2019-05-27", "http://arxiv.org/abs/1905.12171", "Brain-inspired reverse adversarial examples.", ["Shaokai Ye", " Sia Huat Tan", " Kaidi Xu", " Yanzhi Wang", " Chenglong Bao", " Kaisheng Ma"], "  A human does not have to see all elephants to recognize an animal as an\nelephant. On contrast, current state-of-the-art deep learning approaches\nheavily depend on the variety of training samples and the capacity of the\nnetwork. In practice, the size of network is always limited and it is\nimpossible to access all the data samples. Under this circumstance, deep\nlearning models are extremely fragile to human-imperceivable adversarial\nexamples, which impose threats to all safety critical systems. Inspired by the\nassociation and attention mechanisms of the human brain, we propose reverse\nadversarial examples method that can greatly improve models' robustness on\nunseen data. Experiments show that our reverse adversarial method can improve\naccuracy on average 19.02% on ResNet18, MobileNet, and VGG16 on unseen data\ntransformation. Besides, the proposed method is also applicable to compressed\nmodels and shows potential to compensate the robustness drop brought by model\nquantization - an absolute 30.78% accuracy improvement.\n"], ["2019-05-27", "http://arxiv.org/abs/1905.11564", "Adversarially Robust Learning Could Leverage Computational Hardness.", ["Sanjam Garg", " Somesh Jha", " Saeed Mahloujifar", " Mohammad Mahmoody"], "  Over recent years, devising classification algorithms that are robust to\nadversarial perturbations has emerged as a challenging problem. In particular,\ndeep neural nets (DNNs) seem to be susceptible to small imperceptible changes\nover test instances. In this work, we study whether there is any learning task\nfor which it is possible to design classifiers that are only robust against\npolynomial-time adversaries. Indeed, numerous cryptographic tasks (e.g.\nencryption of long messages) are only be secure against computationally bounded\nadversaries, and are indeed mpossible for computationally unbounded attackers.\nThus, it is natural to ask if the same strategy could help robust learning.\n  We show that computational limitation of attackers can indeed be useful in\nrobust learning by demonstrating a classifier for a learning task in which\ncomputational and information theoretic adversaries of bounded perturbations\nhave very different power. Namely, while computationally unbounded adversaries\ncan attack successfully and find adversarial examples with small perturbation,\npolynomial time adversaries are unable to do so unless they can break standard\ncryptographic hardness assumptions. Our results, therefore, indicate that\nperhaps a similar approach to cryptography (relying on computational hardness)\nholds promise for achieving computationally robust machine learning. We also\nshow that the existence of such learning task in which computational robustness\nbeats information theoretic robustness implies (average case) hard problems in\n$\\mathbf{NP}$.\n"], ["2019-05-27", "http://arxiv.org/abs/1905.11544", "Label Universal Targeted Attack.", ["Naveed Akhtar", " Mohammad A. A. K. Jalwana", " Mohammed Bennamoun", " Ajmal Mian"], "  We introduce Label Universal Targeted Attack (LUTA) that makes a deep model\npredict a label of attacker's choice for `any' sample of a given source class\nwith high probability. Our attack stochastically maximizes the log-probability\nof the target label for the source class with first order gradient\noptimization, while accounting for the gradient moments. It also suppresses the\nleakage of attack information to the non-source classes for avoiding the attack\nsuspicions. The perturbations resulting from our attack achieve high fooling\nratios on the large-scale ImageNet and VGGFace models, and transfer well to the\nPhysical World. Given full control over the perturbation scope in LUTA, we also\ndemonstrate it as a tool for deep model autopsy. The proposed attack reveals\ninteresting perturbation patterns and observations regarding the deep models.\n"], ["2019-05-27", "http://arxiv.org/abs/1905.11475", "Divide-and-Conquer Adversarial Detection.", ["Xuwang Yin", " Soheil Kolouri", " Gustavo K. Rohde"], "  The vulnerabilities of deep neural networks against adversarial examples have\nbecome a major concern for deploying these models in sensitive domains.\nDevising a definitive defense against such attacks is proven to be challenging,\nand the methods relying on detecting adversarial samples have been shown to be\nonly effective when the attacker is oblivious to the detection mechanism, i.e.,\nin non-adaptive attacks. In this paper, we propose an effective and practical\nmethod for detecting adaptive/dynamic adversaries. In short, we train\nadversary-robust auxiliary detectors to discriminate in-class natural examples\nfrom adversarially crafted out-of-class examples. To identify a potential\nadversary, we first obtain the estimated class of the input using the\nclassification system, and then use the corresponding detector to verify\nwhether the input is a natural example of that class, or is an adversarially\nmanipulated example. Experimental results on MNIST and CIFAR10 dataset show\nthat our method could withstand adaptive PGD attacks. Furthermore, we\ndemonstrate that with our novel training scheme our models learn significant\nmore robust representation than ordinary adversarial training.\n"], ["2019-05-27", "http://arxiv.org/abs/1905.11213", "Provable robustness against all adversarial $l_p$-perturbations for $p\\geq 1$.", ["Francesco Croce", " Matthias Hein"], "  In recent years several adversarial attacks and defenses have been proposed.\nOften seemingly robust models turn out to be non-robust when more sophisticated\nattacks are used. One way out of this dilemma are provable robustness\nguarantees. While provably robust models for specific $l_p$-perturbation models\nhave been developed, they are still vulnerable to other $l_q$-perturbations. We\npropose a new regularization scheme, MMR-Universal, for ReLU networks which\nenforces robustness wrt $l_1$- and $l_\\infty$-perturbations and show how that\nleads to provably robust models wrt any $l_p$-norm for $p\\geq 1$.\n"], ["2019-05-27", "http://arxiv.org/abs/1905.11026", "Fooling Detection Alone is Not Enough: First Adversarial Attack against Multiple Object Tracking.", ["Yunhan Jia", " Yantao Lu", " Junjie Shen", " Qi Alfred Chen", " Zhenyu Zhong", " Tao Wei"], "  Recent work in adversarial machine learning started to focus on the visual\nperception in autonomous driving and studied Adversarial Examples (AEs) for\nobject detection models. However, in such visual perception pipeline the\ndetected objects must also be tracked, in a process called Multiple Object\nTracking (MOT), to build the moving trajectories of surrounding obstacles.\nSince MOT is designed to be robust against errors in object detection, it poses\na general challenge to existing attack techniques that blindly target objection\ndetection: we find that a success rate of over 98% is needed for them to\nactually affect the tracking results, a requirement that no existing attack\ntechnique can satisfy. In this paper, we are the first to study adversarial\nmachine learning attacks against the complete visual perception pipeline in\nautonomous driving, and discover a novel attack technique, tracker hijacking,\nthat can effectively fool MOT using AEs on object detection. Using our\ntechnique, successful AEs on as few as one single frame can move an existing\nobject in to or out of the headway of an autonomous vehicle to cause potential\nsafety hazards. We perform evaluation using the Berkeley Deep Drive dataset and\nfind that on average when 3 frames are attacked, our attack can have a nearly\n100% success rate while attacks that blindly target object detection only have\nup to 25%.\n"], ["2019-05-27", "http://arxiv.org/abs/1905.11015", "Unsupervised Euclidean Distance Attack on Network Embedding.", ["Qi Xuan", " Jun Zheng", " Lihong Chen", " Shanqing Yu", " Jinyin Chen", " Dan Zhang", " Qingpeng Zhang Member"], "  Considering the wide application of network embedding methods in graph data\nmining, inspired by the adversarial attack in deep learning, this paper\nproposes a Genetic Algorithm (GA) based Euclidean Distance Attack strategy\n(EDA) to attack the network embedding, so as to prevent certain structural\ninformation from being discovered. EDA focuses on disturbing the Euclidean\ndistance between a pair of nodes in the embedding space as much as possible\nthrough minimal modifications of the network structure. Since a large number of\ndownstream network algorithms, such as community detection and node\nclassification, rely on the Euclidean distance between nodes to evaluate the\nsimilarity between them in the embedding space, EDA can be considered as a\nuniversal attack on a variety of network algorithms. Different from traditional\nsupervised attack strategies, EDA does not need labeling information, and, to\nthe best of our knowledge, is the first unsupervised network embedding attack\nmethod. We take DeepWalk as the base embedding method to develop the EDA.\nExperiments with a set of real networks demonstrate that the proposed EDA\nmethod can significantly reduce the performance of DeepWalk-based networking\nalgorithms, i.e., community detection and node classification, outperforming\nseveral heuristic attack strategies. We also show that EDA also works well on\nattacking the network algorithms based on other common network embedding\nmethods such as High-Order Proximity preserved Embedding (HOPE) and\nnon-embedding-based network algorithms such as Label Propagation Algorithm\n(LPA) and Eigenvectors of Matrices (EM). The results indicate a strong\ntransferability of the EDA method.\n"], ["2019-05-27", "http://arxiv.org/abs/1905.11468", "Scaleable input gradient regularization for adversarial robustness.", ["Chris Finlay", " Adam M Oberman"], "  In this work we revisit gradient regularization for adversarial robustness\nwith some new ingredients. First, we derive new per-image theoretical\nrobustness bounds based on local gradient information. These bounds strongly\nmotivate input gradient regularization. Second, we implement a scaleable\nversion of input gradient regularization which avoids double backpropagation:\nadversarially robust ImageNet models are trained in 33 hours on four consumer\ngrade GPUs. Finally, we show experimentally and through theoretical\ncertification that input gradient regularization is competitive with\nadversarial training. Moreover we demonstrate that gradient regularization does\nnot lead to gradient obfuscation or gradient masking.\n"], ["2019-05-27", "http://arxiv.org/abs/1905.11268", "Combating Adversarial Misspellings with Robust Word Recognition.", ["Danish Pruthi", " Bhuwan Dhingra", " Zachary C. Lipton"], "  To combat adversarial spelling mistakes, we propose placing a word\nrecognition model in front of the downstream classifier. Our word recognition\nmodels build upon the RNN semi-character architecture, introducing several new\nbackoff strategies for handling rare and unseen words. Trained to recognize\nwords corrupted by random adds, drops, swaps, and keyboard mistakes, our method\nachieves 32% relative (and 3.3% absolute) error reduction over the vanilla\nsemi-character model. Notably, our pipeline confers robustness on the\ndownstream classifier, outperforming both adversarial training and\noff-the-shelf spell checkers. Against a BERT model fine-tuned for sentiment\nanalysis, a single adversarially-chosen character attack lowers accuracy from\n90.3% to 45.8%. Our defense restores accuracy to 75%. Surprisingly, better word\nrecognition does not always entail greater robustness. Our analysis reveals\nthat robustness also depends upon a quantity that we denote the sensitivity.\n"], ["2019-05-27", "http://arxiv.org/abs/1905.12429", "Analyzing the Interpretability Robustness of Self-Explaining Models.", ["Haizhong Zheng", " Earlence Fernandes", " Atul Prakash"], "  Recently, interpretable models called self-explaining models (SEMs) have been\nproposed with the goal of providing interpretability robustness. We evaluate\nthe interpretability robustness of SEMs and show that explanations provided by\nSEMs as currently proposed are not robust to adversarial inputs. Specifically,\nwe successfully created adversarial inputs that do not change the model outputs\nbut cause significant changes in the explanations. We find that even though\ncurrent SEMs use stable co-efficients for mapping explanations to output\nlabels, they do not consider the robustness of the first stage of the model\nthat creates interpretable basis concepts from the input, leading to non-robust\nexplanations. Our work makes a case for future work to start examining how to\ngenerate interpretable basis concepts in a robust way.\n"], ["2019-05-26", "http://arxiv.org/abs/1905.11382", "State-Reification Networks: Improving Generalization by Modeling the Distribution of Hidden Representations.", ["Alex Lamb", " Jonathan Binas", " Anirudh Goyal", " Sandeep Subramanian", " Ioannis Mitliagkas", " Denis Kazakov", " Yoshua Bengio", " Michael C. Mozer"], "  Machine learning promises methods that generalize well from finite labeled\ndata. However, the brittleness of existing neural net approaches is revealed by\nnotable failures, such as the existence of adversarial examples that are\nmisclassified despite being nearly identical to a training example, or the\ninability of recurrent sequence-processing nets to stay on track without\nteacher forcing. We introduce a method, which we refer to as \\emph{state\nreification}, that involves modeling the distribution of hidden states over the\ntraining data and then projecting hidden states observed during testing toward\nthis distribution. Our intuition is that if the network can remain in a\nfamiliar manifold of hidden space, subsequent layers of the net should be well\ntrained to respond appropriately. We show that this state-reification method\nhelps neural nets to generalize better, especially when labeled data are\nsparse, and also helps overcome the challenge of achieving robust\ngeneralization with adversarial training.\n"], ["2019-05-26", "http://arxiv.org/abs/1905.10906", "Non-Determinism in Neural Networks for Adversarial Robustness.", ["Daanish Ali Khan", " Linhong Li", " Ninghao Sha", " Zhuoran Liu", " Abelino Jimenez", " Bhiksha Raj", " Rita Singh"], "  Recent breakthroughs in the field of deep learning have led to advancements\nin a broad spectrum of tasks in computer vision, audio processing, natural\nlanguage processing and other areas. In most instances where these tasks are\ndeployed in real-world scenarios, the models used in them have been shown to be\nsusceptible to adversarial attacks, making it imperative for us to address the\nchallenge of their adversarial robustness. Existing techniques for adversarial\nrobustness fall into three broad categories: defensive distillation techniques,\nadversarial training techniques, and randomized or non-deterministic model\nbased techniques. In this paper, we propose a novel neural network paradigm\nthat falls under the category of randomized models for adversarial robustness,\nbut differs from all existing techniques under this category in that it models\neach parameter of the network as a statistical distribution with learnable\nparameters. We show experimentally that this framework is highly robust to a\nvariety of white-box and black-box adversarial attacks, while preserving the\ntask-specific performance of the traditional neural network model.\n"], ["2019-05-26", "http://arxiv.org/abs/1905.10729", "Purifying Adversarial Perturbation with Adversarially Trained Auto-encoders.", ["Hebi Li", " Qi Xiao", " Shixin Tian", " Jin Tian"], "  Machine learning models are vulnerable to adversarial examples. Iterative\nadversarial training has shown promising results against strong white-box\nattacks. However, adversarial training is very expensive, and every time a\nmodel needs to be protected, such expensive training scheme needs to be\nperformed. In this paper, we propose to apply iterative adversarial training\nscheme to an external auto-encoder, which once trained can be used to protect\nother models directly. We empirically show that our model outperforms other\npurifying-based methods against white-box attacks, and transfers well to\ndirectly protect other base models with different architectures.\n"], ["2019-05-26", "http://arxiv.org/abs/1905.10900", "Enhancing ML Robustness Using Physical-World Constraints.", ["Varun Chandrasekaran", " Brian Tang", " Varsha Pendyala", " Kassem Fawaz", " Somesh Jha", " Xi Wu"], "  Recent advances in Machine Learning (ML) have demonstrated that neural\nnetworks can exceed human performance in many tasks. While generalizing well\nover natural inputs, neural networks are vulnerable to adversarial inputs -an\ninput that is ``similar'' to the original input, but misclassified by the\nmodel.\n  Existing defenses focus on Lp-norm bounded adversaries that perturb ML inputs\nin the digital space. In the real world, however, attackers can generate\nadversarial perturbations that have a large Lp-norm in the digital space.\nAdditionally, these defenses also come at a cost to accuracy, making their\napplicability questionable in the real world.\n  To defend models against such a powerful adversary, we leverage one\nconstraint on its power: the perturbation should not change the human's\nperception of the physical information; the physical world places some\nconstraints on the space of possible attacks. Two questions follow: how to\nextract and model these constraints? and how to design a classification\nparadigm that leverages these constraints to improve robustness accuracy\ntrade-off?\n  We observe that an ML model is typically a part of a larger system with\naccess to different input modalities. Utilizing these modalities, we introduce\ninvariants that limit the attacker's action space. We design a hierarchical\nclassification paradigm that enforces these invariants at inference time.\n  As a case study, we implement and evaluate our proposal in the context of the\nreal-world application of road sign classification because of its applicability\nto autonomous driving. With access to different input modalities, such as\nLiDAR, camera, and location we show how to extract invariants and develop a\nhierarchical classifier. Our results on the KITTI and GTSRB datasets show that\nwe can improve the robustness against physical attacks at minimal harm to\naccuracy.\n"], ["2019-05-26", "http://arxiv.org/abs/1905.10904", "Robust Classification using Robust Feature Augmentation.", ["Kevin Eykholt", " Swati Gupta", " Atul Prakash", " Amir Rahmati", " Pratik Vaishnavi", " Haizhong Zheng"], "  Existing deep neural networks, say for image classification, have been shown\nto be vulnerable to adversarial images that can cause a DNN misclassification,\nwithout any perceptible change to an image. In this work, we propose shock\nabsorbing robust features such as binarization, e.g., rounding, and group\nextraction, e.g., color or shape, to augment the classification pipeline,\nresulting in more robust classifiers. Experimentally, we show that augmenting\nML models with these techniques leads to improved overall robustness on\nadversarial inputs as well as significant improvements in training time. On the\nMNIST dataset, we achieved 14x speedup in training time to obtain 90%\nadversarial accuracy com-pared to the state-of-the-art adversarial training\nmethod of Madry et al., as well as retained higher adversarial accuracy over a\nbroader range of attacks. We also find robustness improvements on traffic sign\nclassification using robust feature augmentation. Finally, we give theoretical\ninsights for why one can expect robust feature augmentation to reduce\nadversarial input space\n"], ["2019-05-26", "http://arxiv.org/abs/1905.10864", "Generalizable Adversarial Attacks Using Generative Models.", ["Avishek Joey Bose", " Andre Cianflone", " William L. Hamilton"], "  Adversarial attacks on deep neural networks traditionally rely on a\nconstrained optimization paradigm, where an optimization procedure is used to\nobtain a single adversarial perturbation for a given input example. Here, we\ninstead view adversarial attacks as a generative modelling problem, with the\ngoal of producing entire distributions of adversarial examples given an\nunperturbed input. We show that this generative perspective can be used to\ndesign a unified encoder-decoder framework, which is domain-agnostic in that\nthe same framework can be employed to attack different domains with minimal\nmodification. Across three diverse domains---images, text, and graphs---our\napproach generates whitebox attacks with success rates that are competitive\nwith or superior to existing approaches, with a new state-of-the-art achieved\nin the graph domain. Finally, we demonstrate that our generative framework can\nefficiently generate a diverse set of attacks for a single given input, and is\neven capable of attacking unseen test instances in a zero-shot manner,\nexhibiting attack generalization.\n"], ["2019-05-25", "http://arxiv.org/abs/1905.11381", "Trust but Verify: An Information-Theoretic Explanation for the Adversarial Fragility of Machine Learning Systems, and a General Defense against Adversarial Attacks.", ["Jirong Yi", " Hui Xie", " Leixin Zhou", " Xiaodong Wu", " Weiyu Xu", " Raghuraman Mudumbai"], "  Deep-learning based classification algorithms have been shown to be\nsusceptible to adversarial attacks: minor changes to the input of classifiers\ncan dramatically change their outputs, while being imperceptible to humans. In\nthis paper, we present a simple hypothesis about a feature compression property\nof artificial intelligence (AI) classifiers and present theoretical arguments\nto show that this hypothesis successfully accounts for the observed fragility\nof AI classifiers to small adversarial perturbations. Drawing on ideas from\ninformation and coding theory, we propose a general class of defenses for\ndetecting classifier errors caused by abnormally small input perturbations. We\nfurther show theoretical guarantees for the performance of this detection\nmethod. We present experimental results with (a) a voice recognition system,\nand (b) a digit recognition system using the MNIST database, to demonstrate the\neffectiveness of the proposed defense methods. The ideas in this paper are\nmotivated by a simple analogy between AI classifiers and the standard Shannon\nmodel of a communication system.\n"], ["2019-05-25", "http://arxiv.org/abs/1905.10695", "Adversarial Distillation for Ordered Top-k Attacks.", ["Zekun Zhang", " Tianfu Wu"], "  Deep Neural Networks (DNNs) are vulnerable to adversarial attacks, especially\nwhite-box targeted attacks. One scheme of learning attacks is to design a\nproper adversarial objective function that leads to the imperceptible\nperturbation for any test image (e.g., the Carlini-Wagner (C&W) method). Most\nmethods address targeted attacks in the Top-1 manner. In this paper, we propose\nto learn ordered Top-k attacks (k>= 1) for image classification tasks, that is\nto enforce the Top-k predicted labels of an adversarial example to be the k\n(randomly) selected and ordered labels (the ground-truth label is exclusive).\nTo this end, we present an adversarial distillation framework: First, we\ncompute an adversarial probability distribution for any given ordered Top-k\ntargeted labels with respect to the ground-truth of a test image. Then, we\nlearn adversarial examples by minimizing the Kullback-Leibler (KL) divergence\ntogether with the perturbation energy penalty, similar in spirit to the network\ndistillation method. We explore how to leverage label semantic similarities in\ncomputing the targeted distributions, leading to knowledge-oriented attacks. In\nexperiments, we thoroughly test Top-1 and Top-5 attacks in the ImageNet-1000\nvalidation dataset using two popular DNNs trained with clean ImageNet-1000\ntrain dataset, ResNet-50 and DenseNet-121. For both models, our proposed\nadversarial distillation approach outperforms the C&W method in the Top-1\nsetting, as well as other baseline methods. Our approach shows significant\nimprovement in the Top-5 setting against a strong modified C&W method.\n"], ["2019-05-25", "http://arxiv.org/abs/1905.10615", "Adversarial Policies: Attacking Deep Reinforcement Learning.", ["Adam Gleave", " Michael Dennis", " Neel Kant", " Cody Wild", " Sergey Levine", " Stuart Russell"], "  Deep reinforcement learning (RL) policies are known to be vulnerable to\nadversarial perturbations to their observations, similar to adversarial\nexamples for classifiers. However, an attacker is not usually able to directly\nmodify another agent's observations. This might lead one to wonder: is it\npossible to attack an RL agent simply by choosing an adversarial policy acting\nin a multi-agent environment so as to create natural observations that are\nadversarial? We demonstrate the existence of adversarial policies in zero-sum\ngames between simulated humanoid robots with proprioceptive observations,\nagainst state-of-the-art victims trained via self-play to be robust to\nopponents. The adversarial policies reliably win against the victims but\ngenerate seemingly random and uncoordinated behavior. We find that these\npolicies are more successful in high-dimensional environments, and induce\nsubstantially different activations in the victim policy network than when the\nvictim plays against a normal opponent. Videos are available at\nhttp://adversarialpolicies.github.io.\n"], ["2019-05-25", "http://arxiv.org/abs/1905.10626", "Rethinking Softmax Cross-Entropy Loss for Adversarial Robustness.", ["Tianyu Pang", " Kun Xu", " Yinpeng Dong", " Chao Du", " Ning Chen", " Jun Zhu"], "  Previous work shows that adversarially robust generalization requires larger\nsample complexity, and the same dataset, e.g., CIFAR-10, which enables good\nstandard accuracy may not suffice to train robust models. Since collecting new\ntraining data could be costly, we focus on better utilizing the given data by\ninducing the regions with high sample density in the feature space, which could\nlead to locally sufficient samples for robust learning. We first formally show\nthat the softmax cross-entropy (SCE) loss and its variants convey inappropriate\nsupervisory signals, which encourage the learned feature points to spread over\nthe space sparsely in training. This inspires us to propose the Max-Mahalanobis\ncenter (MMC) loss to explicitly induce dense feature regions in order to\nbenefit robustness. Namely, the MMC loss encourages the model to concentrate on\nlearning ordered and compact representations, which gather around the preset\noptimal centers for different classes. We empirically demonstrate that applying\nthe MMC loss can significantly improve robustness even under strong adaptive\nattacks, while keeping state-of-the-art accuracy on clean inputs with little\nextra computation compared to the SCE loss.\n"], ["2019-05-24", "http://arxiv.org/abs/1905.13021", "Robustness to Adversarial Perturbations in Learning from Incomplete Data.", ["Amir Najafi", " Shin-ichi Maeda", " Masanori Koyama", " Takeru Miyato"], "  What is the role of unlabeled data in an inference problem, when the presumed\nunderlying distribution is adversarially perturbed? To provide a concrete\nanswer to this question, this paper unifies two major learning frameworks:\nSemi-Supervised Learning (SSL) and Distributionally Robust Learning (DRL). We\ndevelop a generalization theory for our framework based on a number of novel\ncomplexity measures, such as an adversarial extension of Rademacher complexity\nand its semi-supervised analogue. Moreover, our analysis is able to quantify\nthe role of unlabeled data in the generalization under a more general condition\ncompared to the existing theoretical works in SSL. Based on our framework, we\nalso present a hybrid of DRL and EM algorithms that has a guaranteed\nconvergence rate. When implemented with deep neural networks, our method shows\na comparable performance to those of the state-of-the-art on a number of\nreal-world benchmark datasets.\n"], ["2019-05-24", "http://arxiv.org/abs/1905.10029", "Power up! Robust Graph Convolutional Network against Evasion Attacks based on Graph Powering.", ["Ming Jin", " Heng Chang", " Wenwu Zhu", " Somayeh Sojoudi"], "  Graph convolutional networks (GCNs) are powerful tools for graph-structured\ndata. However, they have been recently shown to be prone to topological\nattacks. Despite substantial efforts to search for new architectures, it still\nremains a challenge to improve performance in both benign and adversarial\nsituations simultaneously. In this paper, we re-examine the fundamental\nbuilding block of GCN---the Laplacian operator---and highlight some basic flaws\nin the spatial and spectral domains. As an alternative, we propose an operator\nbased on graph powering, and prove that it enjoys a desirable property of\n\"spectral separation.\" Based on the operator, we propose a robust learning\nparadigm, where the network is trained on a family of \"'smoothed\" graphs that\nspan a spatial and spectral range for generalizability. We also use the new\noperator in replacement of the classical Laplacian to construct an architecture\nwith improved spectral robustness, expressivity and interpretability. The\nenhanced performance and robustness are demonstrated in extensive experiments.\n"], ["2019-05-24", "http://arxiv.org/abs/1905.10510", "Resisting Adversarial Attacks by $k$-Winners-Take-All.", ["Chang Xiao", " Peilin Zhong", " Changxi Zheng"], "  We propose a simple change to the current neural network structure for\ndefending against gradient-based adversarial attacks. Instead of using popular\nactivation functions (such as ReLU), we advocate the use of\n$k$-Winners-Take-All ($k$-WTA) activation, a $C^0$ discontinuous function that\npurposely invalidates the neural network model's gradient at densely\ndistributed input data points. Our proposal is theoretically rationalized. We\nshow why the discontinuities in $k$-WTA networks can largely prevent\ngradient-based search of adversarial examples and why they at the same time\nremain innocuous to the network training. This understanding is also\nempirically backed. Even without notoriously expensive adversarial training,\nthe robustness performance of our networks is comparable to conventional ReLU\nnetworks optimized by adversarial training. Furthermore, after also optimized\nthrough adversarial training, our networks outperform the state-of-the-art\nmethods under white-box attacks on various datasets that we experimented with.\n"], ["2019-05-23", "http://arxiv.org/abs/1905.09591", "A Direct Approach to Robust Deep Learning Using Adversarial Networks.", ["Huaxia Wang", " Chun-Nam Yu"], "  Deep neural networks have been shown to perform well in many classical\nmachine learning problems, especially in image classification tasks. However,\nresearchers have found that neural networks can be easily fooled, and they are\nsurprisingly sensitive to small perturbations imperceptible to humans.\nCarefully crafted input images (adversarial examples) can force a well-trained\nneural network to provide arbitrary outputs. Including adversarial examples\nduring training is a popular defense mechanism against adversarial attacks. In\nthis paper we propose a new defensive mechanism under the generative\nadversarial network (GAN) framework. We model the adversarial noise using a\ngenerative network, trained jointly with a classification discriminative\nnetwork as a minimax game. We show empirically that our adversarial network\napproach works well against black box attacks, with performance on par with\nstate-of-art methods such as ensemble adversarial training and adversarial\ntraining with projected gradient descent.\n"], ["2019-05-23", "http://arxiv.org/abs/1905.09894", "PHom-GeM: Persistent Homology for Generative Models.", ["Jeremy Charlier", " Radu State", " Jean Hilger"], "  Generative neural network models, including Generative Adversarial Network\n(GAN) and Auto-Encoders (AE), are among the most popular neural network models\nto generate adversarial data. The GAN model is composed of a generator that\nproduces synthetic data and of a discriminator that discriminates between the\ngenerator's output and the true data. AE consist of an encoder which maps the\nmodel distribution to a latent manifold and of a decoder which maps the latent\nmanifold to a reconstructed distribution. However, generative models are known\nto provoke chaotically scattered reconstructed distribution during their\ntraining, and consequently, incomplete generated adversarial distributions.\nCurrent distance measures fail to address this problem because they are not\nable to acknowledge the shape of the data manifold, i.e. its topological\nfeatures, and the scale at which the manifold should be analyzed. We propose\nPersistent Homology for Generative Models, PHom-GeM, a new methodology to\nassess and measure the distribution of a generative model. PHom-GeM minimizes\nan objective function between the true and the reconstructed distributions and\nuses persistent homology, the study of the topological features of a space at\ndifferent spatial resolutions, to compare the nature of the true and the\ngenerated distributions. Our experiments underline the potential of persistent\nhomology for Wasserstein GAN in comparison to Wasserstein AE and Variational\nAE. The experiments are conducted on a real-world data set particularly\nchallenging for traditional distance measures and generative neural network\nmodels. PHom-GeM is the first methodology to propose a topological distance\nmeasure, the bottleneck distance, for generative models used to compare\nadversarial samples in the context of credit card transactions.\n"], ["2019-05-23", "http://arxiv.org/abs/1905.09871", "Thwarting finite difference adversarial attacks with output randomization.", ["Haidar Khan", " Daniel Park", " Azer Khan", " B\u00fclent Yener"], "  Adversarial examples pose a threat to deep neural network models in a variety\nof scenarios, from settings where the adversary has complete knowledge of the\nmodel and to the opposite \"black box\" setting. Black box attacks are\nparticularly threatening as the adversary only needs access to the input and\noutput of the model. Defending against black box adversarial example generation\nattacks is paramount as currently proposed defenses are not effective. Since\nthese types of attacks rely on repeated queries to the model to estimate\ngradients over input dimensions, we investigate the use of randomization to\nthwart such adversaries from successfully creating adversarial examples.\nRandomization applied to the output of the deep neural network model has the\npotential to confuse potential attackers, however this introduces a tradeoff\nbetween accuracy and robustness. We show that for certain types of\nrandomization, we can bound the probability of introducing errors by carefully\nsetting distributional parameters. For the particular case of finite difference\nblack box attacks, we quantify the error introduced by the defense in the\nfinite difference estimate of the gradient. Lastly, we show empirically that\nthe defense can thwart two adaptive black box adversarial attack algorithms.\n"], ["2019-05-23", "http://arxiv.org/abs/1905.09797", "Interpreting Adversarially Trained Convolutional Neural Networks.", ["Tianyuan Zhang", " Zhanxing Zhu"], "  We attempt to interpret how adversarially trained convolutional neural\nnetworks (AT-CNNs) recognize objects. We design systematic approaches to\ninterpret AT-CNNs in both qualitative and quantitative ways and compare them\nwith normally trained models. Surprisingly, we find that adversarial training\nalleviates the texture bias of standard CNNs when trained on object recognition\ntasks, and helps CNNs learn a more shape-biased representation. We validate our\nhypothesis from two aspects. First, we compare the salience maps of AT-CNNs and\nstandard CNNs on clean images and images under different transformations. The\ncomparison could visually show that the prediction of the two types of CNNs is\nsensitive to dramatically different types of features. Second, to achieve\nquantitative verification, we construct additional test datasets that destroy\neither textures or shapes, such as style-transferred version of clean data,\nsaturated images and patch-shuffled ones, and then evaluate the classification\naccuracy of AT-CNNs and normal CNNs on these datasets. Our findings shed some\nlight on why AT-CNNs are more robust than those normally trained ones and\ncontribute to a better understanding of adversarial training over CNNs from an\ninterpretation perspective.\n"], ["2019-05-23", "http://arxiv.org/abs/1905.09747", "Adversarially Robust Distillation.", ["Micah Goldblum", " Liam Fowl", " Soheil Feizi", " Tom Goldstein"], "  Knowledge distillation is effective for producing small high-performance\nneural networks for classification, but these small networks are vulnerable to\nadversarial attacks. We first study how robustness transfers from robust\nteacher to student network during knowledge distillation. We find that a large\namount of robustness may be inherited by the student even when distilled on\nonly clean images. Second, we introduce Adversarially Robust Distillation (ARD)\nfor distilling robustness onto small student networks. ARD is an analogue of\nadversarial training but for distillation. In addition to producing small\nmodels with high test accuracy like conventional distillation, ARD also passes\nthe superior robustness of large networks onto the student. In our experiments,\nwe find that ARD student models decisively outperform adversarially trained\nnetworks of identical architecture on robust accuracy. Finally, we adapt recent\nfast adversarial training methods to ARD for accelerated robust distillation.\n"], ["2019-05-22", "http://arxiv.org/abs/1905.09209", "Convergence and Margin of Adversarial Training on Separable Data.", ["Zachary Charles", " Shashank Rajput", " Stephen Wright", " Dimitris Papailiopoulos"], "  Adversarial training is a technique for training robust machine learning\nmodels. To encourage robustness, it iteratively computes adversarial examples\nfor the model, and then re-trains on these examples via some update rule. This\nwork analyzes the performance of adversarial training on linearly separable\ndata, and provides bounds on the number of iterations required for large\nmargin. We show that when the update rule is given by an arbitrary empirical\nrisk minimizer, adversarial training may require exponentially many iterations\nto obtain large margin. However, if gradient or stochastic gradient update\nrules are used, only polynomially many iterations are required to find a\nlarge-margin separator. By contrast, without the use of adversarial examples,\ngradient methods may require exponentially many iterations to achieve large\nmargin. Our results are derived by showing that adversarial training with\ngradient updates minimizes a robust version of the empirical risk at a\n$\\mathcal{O}(\\ln(t)^2/t)$ rate, despite non-smoothness. We corroborate our\ntheory empirically.\n"], ["2019-05-22", "http://arxiv.org/abs/1905.09186", "Detecting Adversarial Examples and Other Misclassifications in Neural Networks by Introspection.", ["Jonathan Aigrain", " Marcin Detyniecki"], "  Despite having excellent performances for a wide variety of tasks, modern\nneural networks are unable to provide a reliable confidence value allowing to\ndetect misclassifications. This limitation is at the heart of what is known as\nan adversarial example, where the network provides a wrong prediction\nassociated with a strong confidence to a slightly modified image. Moreover,\nthis overconfidence issue has also been observed for regular errors and\nout-of-distribution data. We tackle this problem by what we call introspection,\ni.e. using the information provided by the logits of an already pretrained\nneural network. We show that by training a simple 3-layers neural network on\ntop of the logit activations, we are able to detect misclassifications at a\ncompetitive level.\n"], ["2019-05-21", "http://arxiv.org/abs/1905.08790", "DoPa: A Fast and Comprehensive CNN Defense Methodology against Physical Adversarial Attacks.", ["Zirui Xu", " Fuxun Yu", " Xiang Chen"], "  Recently, Convolutional Neural Networks (CNNs) demonstrate a considerable\nvulnerability to adversarial attacks, which can be easily misled by adversarial\nperturbations. With more aggressive methods proposed, adversarial attacks can\nbe also applied to the physical world, causing practical issues to various CNN\npowered applications. Most existing defense works for physical adversarial\nattacks only focus on eliminating explicit perturbation patterns from inputs,\nignoring interpretation and solution to CNN's intrinsic vulnerability.\nTherefore, most of them depend on considerable data processing costs and lack\nthe expected versatility to different attacks. In this paper, we propose DoPa -\na fast and comprehensive CNN defense methodology against physical adversarial\nattacks. By interpreting the CNN's vulnerability, we find that non-semantic\nadversarial perturbations can activate CNN with significantly abnormal\nactivations and even overwhelm other semantic input patterns' activations. We\nimprove the CNN recognition process by adding a self-verification stage to\nanalyze the semantics of distinguished activation patterns with only one CNN\ninference involved. Based on the detection result, we further propose a data\nrecovery methodology to defend the physical adversarial attacks. We apply such\ndetection and defense methodology into both image and audio CNN recognition\nprocess. Experiments show that our methodology can achieve an average rate of\n90% success for attack detection and 81% accuracy recovery for image physical\nadversarial attacks. Also, the proposed defense method can achieve a 92%\ndetection successful rate and 77.5% accuracy recovery for audio recognition\napplications. Moreover, the proposed defense methods are at most 2.3x faster\ncompared to the state-of-the-art defense methods, making them feasible to\nresource-constrained platforms, such as mobile devices.\n"], ["2019-05-20", "http://arxiv.org/abs/1905.08232", "Adversarially robust transfer learning.", ["Ali Shafahi", " Parsa Saadatpanah", " Chen Zhu", " Amin Ghiasi", " Christoph Studer", " David Jacobs", " Tom Goldstein"], "  Transfer learning, in which a network is trained on one task and re-purposed\non another, is often used to produce neural network classifiers when data is\nscarce or full-scale training is too costly. When the goal is to produce a\nmodel that is not only accurate but also adversarially robust, data scarcity\nand computational limitations become even more cumbersome.\n  We consider robust transfer learning, in which we transfer not only\nperformance but also robustness from a source model to a target domain. We\nstart by observing that robust networks contain robust feature extractors. By\ntraining classifiers on top of these feature extractors, we produce new models\nthat inherit the robustness of their parent networks. We then consider the case\nof fine-tuning a network by re-training end-to-end in the target domain. When\nusing lifelong learning strategies, this process preserves the robustness of\nthe source network while achieving high accuracy. By using such strategies, it\nis possible to produce accurate and robust models with little data, and without\nthe cost of adversarial training.\n"], ["2019-05-19", "http://arxiv.org/abs/1905.07831", "Testing Deep Neural Network based Image Classifiers.", ["Yuchi Tian", " Ziyuan Zhong", " Vicente Ordonez", " Baishakhi Ray"], "  Image classification is an important task in today's world with many\napplications from socio-technical to safety-critical domains. The recent advent\nof Deep Neural Network (DNN) is the key behind such a wide-spread success.\nHowever, such wide adoption comes with the concerns about the reliability of\nthese systems, as several erroneous behaviors have already been reported in\nmany sensitive and critical circumstances. Thus, it has become crucial to\nrigorously test the image classifiers to ensure high reliability. Many reported\nerroneous cases in popular neural image classifiers appear because the models\noften confuse one class with another, or show biases towards some classes over\nothers. These errors usually violate some group properties. Most existing DNN\ntesting and verification techniques focus on per image violations and thus fail\nto detect such group-level confusions or biases. In this paper, we design,\nimplement and evaluate DeepInspect, a white box testing tool, for automatically\ndetecting confusion and bias of DNN-driven image classification applications.\nWe evaluate DeepInspect using popular DNN-based image classifiers and detect\nhundreds of classification mistakes. Some of these cases are able to expose\npotential biases of the network towards certain populations. DeepInspect\nfurther reports many classification errors in state-of-the-art robust models.\n"], ["2019-05-18", "http://arxiv.org/abs/1905.07666", "What Do Adversarially Robust Models Look At?.", ["Takahiro Itazuri", " Yoshihiro Fukuhara", " Hirokatsu Kataoka", " Shigeo Morishima"], "  In this paper, we address the open question: \"What do adversarially robust\nmodels look at?\" Recently, it has been reported in many works that there exists\nthe trade-off between standard accuracy and adversarial robustness. According\nto prior works, this trade-off is rooted in the fact that adversarially robust\nand standard accurate models might depend on very different sets of features.\nHowever, it has not been well studied what kind of difference actually exists.\nIn this paper, we analyze this difference through various experiments visually\nand quantitatively. Experimental results show that adversarially robust models\nlook at things at a larger scale than standard models and pay less attention to\nfine textures. Furthermore, although it has been claimed that adversarially\nrobust features are not compatible with standard accuracy, there is even a\npositive effect by using them as pre-trained models particularly in low\nresolution datasets.\n"], ["2019-05-18", "http://arxiv.org/abs/1905.07672", "Taking Care of The Discretization Problem:A Black-Box Adversarial Image Attack in Discrete Integer Domain.", ["Yuchao Duan", " Zhe Zhao", " Lei Bu", " Fu Song"], "  Numerous methods for crafting adversarial examples were proposed recently\nwith high success rate. Since most existing machine learning based classifiers\nnormalize images into some continuous, real vector, domain firstly, attacks\noften craft adversarial examples in such domain. However, \"adversarial\"\nexamples may become benign after denormalizing them back into the discrete\ninteger domain, known as the discretization problem. This problem was mentioned\nin some work, but has received relatively little attention.\n  In this work, we first conduct a comprehensive study of existing methods and\ntools for crafting. We theoretically analyze 34 representative methods and\nempirically study 20 representative open source tools for crafting adversarial\nimages. Our study reveals that the discretization problem is far more serious\nthan originally thought. This suggests that the discretization problem should\nbe taken into account seriously when crafting adversarial examples and\nmeasuring attack success rate. As a first step towards addressing this problem\nin black-box scenario, we propose a black-box method which reduces the\nadversarial example searching problem to a derivative-free optimization\nproblem. Our method is able to craft adversarial images by derivative-free\nsearch in the discrete integer domain. Experimental results show that our\nmethod is comparable to recent white-box methods (e.g., FGSM, BIM and C\\&W) and\nachieves significantly higher success rate in terms of adversarial examples in\nthe discrete integer domain than recent black-box methods (e.g., ZOO, NES-PGD\nand Bandits). Moreover, our method is able to handle models that is\nnon-differentiable and successfully break the winner of NIPS 2017 competition\non defense with 95\\% success rate. Our results suggest that discrete\noptimization algorithms open up a promising area of research into effective\nblack-box attacks.\n"], ["2019-05-17", "http://arxiv.org/abs/1905.07387", "POPQORN: Quantifying Robustness of Recurrent Neural Networks.", ["Ching-Yun Ko", " Zhaoyang Lyu", " Tsui-Wei Weng", " Luca Daniel", " Ngai Wong", " Dahua Lin"], "  The vulnerability to adversarial attacks has been a critical issue for deep\nneural networks. Addressing this issue requires a reliable way to evaluate the\nrobustness of a network. Recently, several methods have been developed to\ncompute $\\textit{robustness quantification}$ for neural networks, namely,\ncertified lower bounds of the minimum adversarial perturbation. Such methods,\nhowever, were devised for feed-forward networks, e.g. multi-layer perceptron or\nconvolutional networks. It remains an open problem to quantify robustness for\nrecurrent networks, especially LSTM and GRU. For such networks, there exist\nadditional challenges in computing the robustness quantification, such as\nhandling the inputs at multiple steps and the interaction between gates and\nstates. In this work, we propose $\\textit{POPQORN}$\n($\\textbf{P}$ropagated-$\\textbf{o}$ut$\\textbf{p}$ut $\\textbf{Q}$uantified\nR$\\textbf{o}$bustness for $\\textbf{RN}$Ns), a general algorithm to quantify\nrobustness of RNNs, including vanilla RNNs, LSTMs, and GRUs. We demonstrate its\neffectiveness on different network architectures and show that the robustness\nquantification on individual steps can lead to new insights.\n"], ["2019-05-17", "http://arxiv.org/abs/1905.07112", "A critique of the DeepSec Platform for Security Analysis of Deep Learning Models.", ["Nicholas Carlini"], "  At IEEE S&P 2019, the paper \"DeepSec: A Uniform Platform for Security\nAnalysis of Deep Learning Model\" aims to to \"systematically evaluate the\nexisting adversarial attack and defense methods.\" While the paper's goals are\nlaudable, it fails to achieve them and presents results that are fundamentally\nflawed and misleading. We explain the flaws in the DeepSec work, along with how\nits analysis fails to meaningfully evaluate the various attacks and defenses.\nSpecifically, DeepSec (1) evaluates each defense obliviously, using attacks\ncrafted against undefended models; (2) evaluates attacks and defenses using\nincorrect implementations that greatly under-estimate their effectiveness; (3)\nevaluates the robustness of each defense as an average, not based on the most\neffective attack against that defense; (4) performs several statistical\nanalyses incorrectly and fails to report variance; and, (5) as a result of\nthese errors draws invalid conclusions and makes sweeping generalizations.\n"], ["2019-05-17", "http://arxiv.org/abs/1905.07121", "Simple Black-box Adversarial Attacks.", ["Chuan Guo", " Jacob R. Gardner", " Yurong You", " Andrew Gordon Wilson", " Kilian Q. Weinberger"], "  We propose an intriguingly simple method for the construction of adversarial\nimages in the black-box setting. In constrast to the white-box scenario,\nconstructing black-box adversarial images has the additional constraint on\nquery budget, and efficient attacks remain an open problem to date. With only\nthe mild assumption of continuous-valued confidence scores, our highly\nquery-efficient algorithm utilizes the following simple iterative principle: we\nrandomly sample a vector from a predefined orthonormal basis and either add or\nsubtract it to the target image. Despite its simplicity, the proposed method\ncan be used for both untargeted and targeted attacks -- resulting in previously\nunprecedented query efficiency in both settings. We demonstrate the efficacy\nand efficiency of our algorithm on several real world settings including the\nGoogle Cloud Vision API. We argue that our proposed algorithm should serve as a\nstrong baseline for future black-box attacks, in particular because it is\nextremely fast and its implementation requires less than 20 lines of PyTorch\ncode.\n"], ["2019-05-16", "http://arxiv.org/abs/1905.06635", "Parsimonious Black-Box Adversarial Attacks via Efficient Combinatorial Optimization.", ["Seungyong Moon", " Gaon An", " Hyun Oh Song"], "  Solving for adversarial examples with projected gradient descent has been\ndemonstrated to be highly effective in fooling the neural network based\nclassifiers. However, in the black-box setting, the attacker is limited only to\nthe query access to the network and solving for a successful adversarial\nexample becomes much more difficult. To this end, recent methods aim at\nestimating the true gradient signal based on the input queries but at the cost\nof excessive queries. We propose an efficient discrete surrogate to the\noptimization problem which does not require estimating the gradient and\nconsequently becomes free of the first order update hyperparameters to tune.\nOur experiments on Cifar-10 and ImageNet show the state of the art black-box\nattack performance with significant reduction in the required queries compared\nto a number of recently proposed methods. The source code is available at\nhttps://github.com/snu-mllab/parsimonious-blackbox-attack.\n"], ["2019-05-15", "http://arxiv.org/abs/1905.08614", "War: Detecting adversarial examples by pre-processing input data.", ["Hua Wang", " Jie Wang", " Zhaoxia Yin"], "  Deep neural networks (DNNs) have demonstrated their outstanding performance\nin many fields such as image classification and speech recognition. However,\nDNNs image classifiers are susceptible to interference from adversarial\nexamples, which ultimately leads to incorrect classification output of neural\nnetwork models. Based on this, this paper proposes a method based on War\n(WebPcompression and resize) to detect adversarial examples. The method takes\nWebP compression as the core, firstly performs WebP compression on the input\nimage, and then appropriately resizes the compressed image, so that the label\nof the adversarial example changes, thereby detecting the existence of the\nadversarial image. The experimental results show that the proposed method can\neffectively resist IFGSM, DeepFool and C&W attacks, and the recognition\naccuracy is improved by more than 10% compared with the HGD method, the\ndetection success rate of adversarial examples is 5% higher than that of the\nFeature Squeezing method. The method in this paper can effectively reduce the\nsmall noise disturbance in the adversarial image, and accurately detect the\nadversarial example according to the change of the\nsamplelabelwhileensuringtheaccuracyoftheoriginalsampleidentification.\n"], ["2019-05-15", "http://arxiv.org/abs/1905.06455", "On Norm-Agnostic Robustness of Adversarial Training.", ["Bai Li", " Changyou Chen", " Wenlin Wang", " Lawrence Carin"], "  Adversarial examples are carefully perturbed in-puts for fooling machine\nlearning models. A well-acknowledged defense method against such examples is\nadversarial training, where adversarial examples are injected into training\ndata to increase robustness. In this paper, we propose a new attack to unveil\nan undesired property of the state-of-the-art adversarial training, that is it\nfails to obtain robustness against perturbations in $\\ell_2$ and $\\ell_\\infty$\nnorms simultaneously. We discuss a possible solution to this issue and its\nlimitations as well.\n"], ["2019-05-14", "http://arxiv.org/abs/1905.05454", "Robustification of deep net classifiers by key based diversified aggregation with pre-filtering.", ["Olga Taran", " Shideh Rezaeifar", " Taras Holotyak", " Slava Voloshynovskiy"], "  In this paper, we address a problem of machine learning system vulnerability\nto adversarial attacks. We propose and investigate a Key based Diversified\nAggregation (KDA) mechanism as a defense strategy. The KDA assumes that the\nattacker (i) knows the architecture of classifier and the used defense\nstrategy, (ii) has an access to the training data set but (iii) does not know\nthe secret key. The robustness of the system is achieved by a specially\ndesigned key based randomization. The proposed randomization prevents the\ngradients' back propagation or the creating of a \"bypass\" system. The\nrandomization is performed simultaneously in several channels and a\nmulti-channel aggregation stabilizes the results of randomization by\naggregating soft outputs from each classifier in multi-channel system. The\nperformed experimental evaluation demonstrates a high robustness and\nuniversality of the KDA against the most efficient gradient based attacks like\nthose proposed by N. Carlini and D. Wagner and the non-gradient based sparse\nadversarial perturbations like OnePixel attacks.\n"], ["2019-05-13", "http://arxiv.org/abs/1905.05163", "Adversarial Examples for Electrocardiograms.", ["Xintian Han", " Yuxuan Hu", " Luca Foschini", " Larry Chinitz", " Lior Jankelson", " Rajesh Ranganath"], "  In recent years, the electrocardiogram (ECG) has seen a large diffusion in\nboth medical and commercial applications, fueled by the rise of single-lead\nversions. Single-lead ECG can be embedded in medical devices and wearable\nproducts such as the injectable Medtronic Linq monitor, the iRhythm Ziopatch\nwearable monitor, and the Apple Watch Series 4. Recently, deep neural networks\nhave been used to automatically analyze ECG tracings, outperforming even\nphysicians specialized in cardiac electrophysiology in detecting certain rhythm\nirregularities. However, deep learning classifiers have been shown to be\nbrittle to adversarial examples, which are examples created to look\nincontrovertibly belonging to a certain class to a human eye but contain subtle\nfeatures that fool the classifier into misclassifying them into the wrong\nclass. Very recently, adversarial examples have also been created for\nmedical-related tasks. Yet, traditional attack methods to create adversarial\nexamples, such as projected gradient descent (PGD) do not extend directly to\nECG signals, as they generate examples that introduce square wave artifacts\nthat are not physiologically plausible. Here, we developed a method to\nconstruct smoothed adversarial examples for single-lead ECG. First, we\nimplemented a neural network model achieving state-of-the-art performance on\nthe data from the 2017 PhysioNet/Computing-in-Cardiology Challenge for\narrhythmia detection from single lead ECG classification. For this model, we\nutilized a new technique to generate smoothed examples to produce signals that\nare 1) indistinguishable to cardiologists from the original examples and 2)\nincorrectly classified by the neural network. Finally, we show that adversarial\nexamples are not unique and provide a general technique to collate and perturb\nknown adversarial examples to create new ones.\n"], ["2019-05-13", "http://arxiv.org/abs/1905.05137", "Analyzing Adversarial Attacks Against Deep Learning for Intrusion Detection in IoT Networks.", ["Olakunle Ibitoye", " Omair Shafiq", " Ashraf Matrawy"], "  Adversarial attacks have been widely studied in the field of computer vision\nbut their impact on network security applications remains an area of open\nresearch. As IoT, 5G and AI continue to converge to realize the promise of the\nfourth industrial revolution (Industry 4.0), security incidents and events on\nIoT networks have increased. Deep learning techniques are being applied to\ndetect and mitigate many of such security threats against IoT networks.\nFeedforward Neural Networks (FNN) have been widely used for classifying\nintrusion attacks in IoT networks. In this paper, we consider a variant of the\nFNN known as the Self-normalizing Neural Network (SNN) and compare its\nperformance with the FNN for classifying intrusion attacks in an IoT network.\nOur analysis is performed using the BoT-IoT dataset from the Cyber Range Lab of\nthe center of UNSW Canberra Cyber. In our experimental results, the FNN\noutperforms the SNN for intrusion detection in IoT networks based on multiple\nperformance metrics such as accuracy, precision, and recall as well as\nmulti-classification metrics such as Cohen's Kappa score. However, when tested\nfor adversarial robustness, the SNN demonstrates better resilience against the\nadversarial samples from the IoT dataset, presenting a promising future in the\nquest for safer and more secure deep learning in IoT networks.\n"], ["2019-05-13", "http://arxiv.org/abs/1905.05186", "Harnessing the Vulnerability of Latent Layers in Adversarially Trained Models.", ["Mayank Singh", " Abhishek Sinha", " Nupur Kumari", " Harshitha Machiraju", " Balaji Krishnamurthy", " Vineeth N Balasubramanian"], "  Neural networks are vulnerable to adversarial attacks -- small visually\nimperceptible crafted noise which when added to the input drastically changes\nthe output. The most effective method of defending against these adversarial\nattacks is to use the methodology of adversarial training. We analyze the\nadversarially trained robust models to study their vulnerability against\nadversarial attacks at the level of the latent layers. Our analysis reveals\nthat contrary to the input layer which is robust to adversarial attack, the\nlatent layer of these robust models are highly susceptible to adversarial\nperturbations of small magnitude. Leveraging this information, we introduce a\nnew technique Latent Adversarial Training (LAT) which comprises of fine-tuning\nthe adversarially trained models to ensure the robustness at the feature\nlayers. We also propose Latent Attack (LA), a novel algorithm for construction\nof adversarial examples. LAT results in minor improvement in test accuracy and\nleads to a state-of-the-art adversarial accuracy against the universal\nfirst-order adversarial PGD attack which is shown for the MNIST, CIFAR-10,\nCIFAR-100 datasets.\n"], ["2019-05-11", "http://arxiv.org/abs/1905.13148", "Moving Target Defense for Deep Visual Sensing against Adversarial Examples.", ["Qun Song", " Zhenyu Yan", " Rui Tan"], "  Deep learning based visual sensing has achieved attractive accuracy but is\nshown vulnerable to adversarial example attacks. Specifically, once the\nattackers obtain the deep model, they can construct adversarial examples to\nmislead the model to yield wrong classification results. Deployable adversarial\nexamples such as small stickers pasted on the road signs and lanes have been\nshown effective in misleading advanced driver-assistance systems. Many existing\ncountermeasures against adversarial examples build their security on the\nattackers' ignorance of the defense mechanisms. Thus, they fall short of\nfollowing Kerckhoffs's principle and can be subverted once the attackers know\nthe details of the defense. This paper applies the strategy of moving target\ndefense (MTD) to generate multiple new deep models after system deployment,\nthat will collaboratively detect and thwart adversarial examples. Our MTD\ndesign is based on the adversarial examples' minor transferability to models\ndiffering from the one (e.g., the factory-designed model) used for attack\nconstruction. The post-deployment quasi-secret deep models significantly\nincrease the bar for the attackers to construct effective adversarial examples.\nWe also apply the technique of serial data fusion with early stopping to reduce\nthe inference time by a factor of up to 5 while maintaining the sensing and\ndefense performance. Extensive evaluation based on three datasets including a\nroad sign image database and a GPU-equipped Jetson embedded computing board\nshows the effectiveness of our approach.\n"], ["2019-05-10", "http://arxiv.org/abs/1905.04270", "Interpreting and Evaluating Neural Network Robustness.", ["Fuxun Yu", " Zhuwei Qin", " Chenchen Liu", " Liang Zhao", " Yanzhi Wang", " Xiang Chen"], "  Recently, adversarial deception becomes one of the most considerable threats\nto deep neural networks. However, compared to extensive research in new designs\nof various adversarial attacks and defenses, the neural networks' intrinsic\nrobustness property is still lack of thorough investigation. This work aims to\nqualitatively interpret the adversarial attack and defense mechanism through\nloss visualization, and establish a quantitative metric to evaluate the neural\nnetwork model's intrinsic robustness. The proposed robustness metric identifies\nthe upper bound of a model's prediction divergence in the given domain and thus\nindicates whether the model can maintain a stable prediction. With extensive\nexperiments, our metric demonstrates several advantages over conventional\nadversarial testing accuracy based robustness estimation: (1) it provides a\nuniformed evaluation to models with different structures and parameter scales;\n(2) it over-performs conventional accuracy based robustness estimation and\nprovides a more reliable evaluation that is invariant to different test\nsettings; (3) it can be fast generated without considerable testing cost.\n"], ["2019-05-10", "http://arxiv.org/abs/1905.04172", "On the Connection Between Adversarial Robustness and Saliency Map Interpretability.", ["Christian Etmann", " Sebastian Lunz", " Peter Maass", " Carola-Bibiane Sch\u00f6nlieb"], "  Recent studies on the adversarial vulnerability of neural networks have shown\nthat models trained to be more robust to adversarial attacks exhibit more\ninterpretable saliency maps than their non-robust counterparts. We aim to\nquantify this behavior by considering the alignment between input image and\nsaliency map. We hypothesize that as the distance to the decision boundary\ngrows,so does the alignment. This connection is strictly true in the case of\nlinear models. We confirm these theoretical findings with experiments based on\nmodels trained with a local Lipschitz regularization and identify where the\nnon-linear nature of neural networks weakens the relation.\n"], ["2019-05-10", "http://arxiv.org/abs/1905.04016", "Exact Adversarial Attack to Image Captioning via Structured Output Learning with Latent Variables.", ["Yan Xu", " Baoyuan Wu", " Fumin Shen", " Yanbo Fan", " Yong Zhang", " Heng Tao Shen", " Wei Liu"], "  In this work, we study the robustness of a CNN+RNN based image captioning\nsystem being subjected to adversarial noises. We propose to fool an image\ncaptioning system to generate some targeted partial captions for an image\npolluted by adversarial noises, even the targeted captions are totally\nirrelevant to the image content. A partial caption indicates that the words at\nsome locations in this caption are observed, while words at other locations are\nnot restricted.It is the first work to study exact adversarial attacks of\ntargeted partial captions. Due to the sequential dependencies among words in a\ncaption, we formulate the generation of adversarial noises for targeted partial\ncaptions as a structured output learning problem with latent variables. Both\nthe generalized expectation maximization algorithm and structural SVMs with\nlatent variables are then adopted to optimize the problem. The proposed methods\ngenerate very successful at-tacks to three popular CNN+RNN based image\ncaptioning models. Furthermore, the proposed attack methods are used to\nunderstand the inner mechanism of image captioning systems, providing the\nguidance to further improve automatic image captioning systems towards human\ncaptioning.\n"], ["2019-05-09", "http://arxiv.org/abs/1905.03679", "Adversarial Defense Framework for Graph Neural Network.", ["Shen Wang", " Zhengzhang Chen", " Jingchao Ni", " Xiao Yu", " Zhichun Li", " Haifeng Chen", " Philip S. Yu"], "  Graph neural network (GNN), as a powerful representation learning model on\ngraph data, attracts much attention across various disciplines. However, recent\nstudies show that GNN is vulnerable to adversarial attacks. How to make GNN\nmore robust? What are the key vulnerabilities in GNN? How to address the\nvulnerabilities and defense GNN against the adversarial attacks? In this paper,\nwe propose DefNet, an effective adversarial defense framework for GNNs. In\nparticular, we first investigate the latent vulnerabilities in every layer of\nGNNs and propose corresponding strategies including dual-stage aggregation and\nbottleneck perceptron. Then, to cope with the scarcity of training data, we\npropose an adversarial contrastive learning method to train the GNN in a\nconditional GAN manner by leveraging the high-level graph representation.\nExtensive experiments on three public datasets demonstrate the effectiveness of\nDefNet in improving the robustness of popular GNN variants, such as Graph\nConvolutional Network and GraphSAGE, under various types of adversarial\nattacks.\n"], ["2019-05-09", "http://arxiv.org/abs/1905.03517", "Mitigating Deep Learning Vulnerabilities from Adversarial Examples Attack in the Cybersecurity Domain.", ["Chris Einar San Agustin"], "  Deep learning models are known to solve classification and regression\nproblems by employing a number of epoch and training samples on a large dataset\nwith optimal accuracy. However, that doesn't mean they are attack-proof or\nunexposed to vulnerabilities. Newly deployed systems particularly on a public\nenvironment (i.e public networks) are vulnerable to attacks from various\nentities. Moreover, published research on deep learning systems (Goodfellow et\nal., 2014) have determined a significant number of attacks points and a wide\narray of attack surface that has evidence of exploitation from adversarial\nexamples. Successful exploit on these systems could lead to critical real world\nrepercussions. For instance, (1) an adversarial attack on a self-driving car\nrunning a deep reinforcement learning system yields a direct misclassification\non humans causing untoward accidents.(2) a self-driving vehicle misreading a\nred light signal may cause the car to crash to another car (3)\nmisclassification of a pedestrian lane as an intersection lane that could lead\nto car crashes. This is just the tip of the iceberg, computer vision deployment\nare not entirely focused on self-driving cars but on many other areas as well -\nthat would have definitive impact on the real-world. These vulnerabilities must\nbe mitigated at an early stage of development. It is imperative to develop and\nimplement baseline security standards at a global level prior to real-world\ndeployment.\n"], ["2019-05-09", "http://arxiv.org/abs/1905.03837", "Exploring the Hyperparameter Landscape of Adversarial Robustness.", ["Evelyn Duesterwald", " Anupama Murthi", " Ganesh Venkataraman", " Mathieu Sinn", " Deepak Vijaykeerthy"], "  Adversarial training shows promise as an approach for training models that\nare robust towards adversarial perturbation. In this paper, we explore some of\nthe practical challenges of adversarial training. We present a sensitivity\nanalysis that illustrates that the effectiveness of adversarial training hinges\non the settings of a few salient hyperparameters. We show that the robustness\nsurface that emerges across these salient parameters can be surprisingly\ncomplex and that therefore no effective one-size-fits-all parameter settings\nexist. We then demonstrate that we can use the same salient hyperparameters as\ntuning knob to navigate the tension that can arise between robustness and\naccuracy. Based on these findings, we present a practical approach that\nleverages hyperparameter optimization techniques for tuning adversarial\ntraining to maximize robustness while keeping the loss in accuracy within a\ndefined budget.\n"], ["2019-05-09", "http://arxiv.org/abs/1905.03767", "Learning Interpretable Features via Adversarially Robust Optimization.", ["Ashkan Khakzar", " Shadi Albarqouni", " Nassir Navab"], "  Neural networks are proven to be remarkably successful for classification and\ndiagnosis in medical applications. However, the ambiguity in the\ndecision-making process and the interpretability of the learned features is a\nmatter of concern. In this work, we propose a method for improving the feature\ninterpretability of neural network classifiers. Initially, we propose a\nbaseline convolutional neural network with state of the art performance in\nterms of accuracy and weakly supervised localization. Subsequently, the loss is\nmodified to integrate robustness to adversarial examples into the training\nprocess. In this work, feature interpretability is quantified via evaluating\nthe weakly supervised localization using the ground truth bounding boxes.\nInterpretability is also visually assessed using class activation maps and\nsaliency maps. The method is applied to NIH ChestX-ray14, the largest publicly\navailable chest x-rays dataset. We demonstrate that the adversarially robust\noptimization paradigm improves feature interpretability both quantitatively and\nvisually.\n"], ["2019-05-09", "http://arxiv.org/abs/1905.03828", "Universal Adversarial Perturbations for Speech Recognition Systems.", ["Paarth Neekhara", " Shehzeen Hussain", " Prakhar Pandey", " Shlomo Dubnov", " Julian McAuley", " Farinaz Koushanfar"], "  In this work, we demonstrate the existence of universal adversarial audio\nperturbations that cause mis-transcription of audio signals by automatic speech\nrecognition (ASR) systems. We propose an algorithm to find a single\nquasi-imperceptible perturbation, which when added to any arbitrary speech\nsignal, will most likely fool the victim speech recognition model. Our\nexperiments demonstrate the application of our proposed technique by crafting\naudio-agnostic universal perturbations for the state-of-the-art ASR system --\nMozilla DeepSpeech. Additionally, we show that such perturbations generalize to\na significant extent across models that are not available during training, by\nperforming a transferability test on a WaveNet based ASR system.\n"], ["2019-05-08", "http://arxiv.org/abs/1905.03434", "ROSA: Robust Salient Object Detection against Adversarial Attacks.", ["Haofeng Li", " Guanbin Li", " Yizhou Yu"], "  Recently salient object detection has witnessed remarkable improvement owing\nto the deep convolutional neural networks which can harvest powerful features\nfor images. In particular, state-of-the-art salient object detection methods\nenjoy high accuracy and efficiency from fully convolutional network (FCN) based\nframeworks which are trained from end to end and predict pixel-wise labels.\nHowever, such framework suffers from adversarial attacks which confuse neural\nnetworks via adding quasi-imperceptible noises to input images without changing\nthe ground truth annotated by human subjects. To our knowledge, this paper is\nthe first one that mounts successful adversarial attacks on salient object\ndetection models and verifies that adversarial samples are effective on a wide\nrange of existing methods. Furthermore, this paper proposes a novel end-to-end\ntrainable framework to enhance the robustness for arbitrary FCN-based salient\nobject detection models against adversarial attacks. The proposed framework\nadopts a novel idea that first introduces some new generic noise to destroy\nadversarial perturbations, and then learns to predict saliency maps for input\nimages with the introduced noise. Specifically, our proposed method consists of\na segment-wise shielding component, which preserves boundaries and destroys\ndelicate adversarial noise patterns and a context-aware restoration component,\nwhich refines saliency maps through global contrast modeling. Experimental\nresults suggest that our proposed framework improves the performance\nsignificantly for state-of-the-art models on a series of datasets.\n"], ["2019-05-08", "http://arxiv.org/abs/1905.03421", "Adversarial Image Translation: Unrestricted Adversarial Examples in Face Recognition Systems.", ["Kazuya Kakizaki", " Kosuke Yoshida"], "  Thanks to recent advances in Deep Neural Networks (DNNs), face recognition\nsystems have achieved high accuracy in classification of a large number of face\nimages. However, recent works demonstrate that DNNs could be vulnerable to\nadversarial examples and raise concerns about robustness of face recognition\nsystems. In particular adversarial examples that are not restricted to small\nperturbations could be more serious risks since conventional certified defenses\nmight be ineffective against them. To shed light on the vulnerability to this\ntype of adversarial examples, we propose a flexible and efficient method to\ngenerate unrestricted adversarial examples using image translation techniques.\nOur method enables us to translate a source image into any desired facial\nappearance with large perturbations so that target face recognition systems\ncould be deceived. Through our experiments, we demonstrate that our method\nachieves about 90% and 30% attack success rates under a white- and black-box\nsetting, respectively. We also illustrate that our translated images are\nperceptually realistic and maintain personal identity while the perturbations\nare large enough to bypass certified defenses.\n"], ["2019-05-08", "http://arxiv.org/abs/1905.03333", "Enhancing Cross-task Transferability of Adversarial Examples with Dispersion Reduction.", ["Yunhan Jia", " Yantao Lu", " Senem Velipasalar", " Zhenyu Zhong", " Tao Wei"], "  Neural networks are known to be vulnerable to carefully crafted adversarial\nexamples, and these malicious samples often transfer, i.e., they maintain their\neffectiveness even against other models. With great efforts delved into the\ntransferability of adversarial examples, surprisingly, less attention has been\npaid to its impact on real-world deep learning deployment. In this paper, we\ninvestigate the transferability of adversarial examples across a wide range of\nreal-world computer vision tasks, including image classification, explicit\ncontent detection, optical character recognition (OCR), and object detection.\nIt represents the cybercriminal's situation where an ensemble of different\ndetection mechanisms need to be evaded all at once. We propose practical attack\nthat overcomes existing attacks' limitation of requiring task-specific loss\nfunctions by targeting on the `dispersion' of internal feature map. We report\nevaluation on four different computer vision tasks provided by Google Cloud\nVision APIs to show how our approach outperforms existing attacks by degrading\nperformance of multiple CV tasks by a large margin with only modest\nperturbations.\n"], ["2019-05-07", "http://arxiv.org/abs/1905.02704", "A Comprehensive Analysis on Adversarial Robustness of Spiking Neural Networks.", ["Saima Sharmin", " Priyadarshini Panda", " Syed Shakib Sarwar", " Chankyu Lee", " Wachirawit Ponghiran", " Kaushik Roy"], "  In this era of machine learning models, their functionality is being\nthreatened by adversarial attacks. In the face of this struggle for making\nartificial neural networks robust, finding a model, resilient to these attacks,\nis very important. In this work, we present, for the first time, a\ncomprehensive analysis of the behavior of more bio-plausible networks, namely\nSpiking Neural Network (SNN) under state-of-the-art adversarial tests. We\nperform a comparative study of the accuracy degradation between conventional\nVGG-9 Artificial Neural Network (ANN) and equivalent spiking network with\nCIFAR-10 dataset in both whitebox and blackbox setting for different types of\nsingle-step and multi-step FGSM (Fast Gradient Sign Method) attacks. We\ndemonstrate that SNNs tend to show more resiliency compared to ANN under\nblack-box attack scenario. Additionally, we find that SNN robustness is largely\ndependent on the corresponding training mechanism. We observe that SNNs trained\nby spike-based backpropagation are more adversarially robust than the ones\nobtained by ANN-to-SNN conversion rules in several whitebox and blackbox\nscenarios. Finally, we also propose a simple, yet, effective framework for\ncrafting adversarial attacks from SNNs. Our results suggest that attacks\ncrafted from SNNs following our proposed method are much stronger than those\ncrafted from ANNs.\n"], ["2019-05-07", "http://arxiv.org/abs/1905.02422", "Representation of White- and Black-Box Adversarial Examples in Deep Neural Networks and Humans: A Functional Magnetic Resonance Imaging Study.", ["Chihye Han", " Wonjun Yoon", " Gihyun Kwon", " Seungkyu Nam", " Daeshik Kim"], "  The recent success of brain-inspired deep neural networks (DNNs) in solving\ncomplex, high-level visual tasks has led to rising expectations for their\npotential to match the human visual system. However, DNNs exhibit\nidiosyncrasies that suggest their visual representation and processing might be\nsubstantially different from human vision. One limitation of DNNs is that they\nare vulnerable to adversarial examples, input images on which subtle, carefully\ndesigned noises are added to fool a machine classifier. The robustness of the\nhuman visual system against adversarial examples is potentially of great\nimportance as it could uncover a key mechanistic feature that machine vision is\nyet to incorporate. In this study, we compare the visual representations of\nwhite- and black-box adversarial examples in DNNs and humans by leveraging\nfunctional magnetic resonance imaging (fMRI). We find a small but significant\ndifference in representation patterns for different (i.e. white- versus black-\nbox) types of adversarial examples for both humans and DNNs. However, human\nperformance on categorical judgment is not degraded by noise regardless of the\ntype unlike DNN. These results suggest that adversarial examples may be\ndifferentially represented in the human visual system, but unable to affect the\nperceptual experience.\n"], ["2019-05-07", "http://arxiv.org/abs/1905.02675", "An Empirical Evaluation of Adversarial Robustness under Transfer Learning.", ["Todor Davchev", " Timos Korres", " Stathi Fotiadis", " Nick Antonopoulos", " Subramanian Ramamoorthy"], "  In this work, we evaluate adversarial robustness in the context of transfer\nlearning from a source trained on CIFAR 100 to a target network trained on\nCIFAR 10. Specifically, we study the effects of using robust optimisation in\nthe source and target networks. This allows us to identify transfer learning\nstrategies under which adversarial defences are successfully retained, in\naddition to revealing potential vulnerabilities. We study the extent to which\nfeatures learnt by a fast gradient sign method (FGSM) and its iterative\nalternative (PGD) can preserve their defence properties against black and\nwhite-box attacks under three different transfer learning strategies. We find\nthat using PGD examples during training on the source task leads to more\ngeneral robust features that are easier to transfer. Furthermore, under\nsuccessful transfer, it achieves 5.2% more accuracy against white-box PGD\nattacks than suitable baselines. Overall, our empirical evaluations give\ninsights on how well adversarial robustness under transfer learning can\ngeneralise.\n"], ["2019-05-07", "http://arxiv.org/abs/1905.02463", "Adaptive Generation of Unrestricted Adversarial Inputs.", ["Isaac Dunn", " Hadrien Pouget", " Tom Melham", " Daniel Kroening"], "  Neural networks are vulnerable to adversarially-constructed perturbations of\ntheir inputs. Most research so far has considered perturbations of a fixed\nmagnitude under some $l_p$ norm. Although studying these attacks is valuable,\nthere has been increasing interest in the construction of (and robustness to)\nunrestricted attacks, which are not constrained to a small and rather\nartificial subset of all possible adversarial inputs. We introduce a novel\nalgorithm for generating such unrestricted adversarial inputs which, unlike\nprior work, is adaptive: it is able to tune its attacks to the classifier being\ntargeted. It also offers a 400-2,000x speedup over the existing state of the\nart. We demonstrate our approach by generating unrestricted adversarial inputs\nthat fool classifiers robust to perturbation-based attacks. We also show that,\nby virtue of being adaptive and unrestricted, our attack is able to defeat\nadversarial training against it.\n"], ["2019-05-06", "http://arxiv.org/abs/1905.02161", "Batch Normalization is a Cause of Adversarial Vulnerability.", ["Angus Galloway", " Anna Golubeva", " Thomas Tanay", " Medhat Moussa", " Graham W. Taylor"], "  Batch normalization (batch norm) is often used in an attempt to stabilize and\naccelerate training in deep neural networks. In many cases it indeed decreases\nthe number of parameter updates required to achieve low training error.\nHowever, it also reduces robustness to small adversarial input perturbations\nand noise by double-digit percentages, as we show on five standard datasets.\nFurthermore, substituting weight decay for batch norm is sufficient to nullify\nthe relationship between adversarial vulnerability and the input dimension. Our\nwork is consistent with a mean-field analysis that found that batch norm causes\nexploding gradients.\n"], ["2019-05-06", "http://arxiv.org/abs/1905.02175", "Adversarial Examples Are Not Bugs, They Are Features.", ["Andrew Ilyas", " Shibani Santurkar", " Dimitris Tsipras", " Logan Engstrom", " Brandon Tran", " Aleksander Madry"], "  Adversarial examples have attracted significant attention in machine\nlearning, but the reasons for their existence and pervasiveness remain unclear.\nWe demonstrate that adversarial examples can be directly attributed to the\npresence of non-robust features: features derived from patterns in the data\ndistribution that are highly predictive, yet brittle and incomprehensible to\nhumans. After capturing these features within a theoretical framework, we\nestablish their widespread existence in standard datasets. Finally, we present\na simple setting where we can rigorously tie the phenomena we observe in\npractice to a misalignment between the (human-specified) notion of robustness\nand the inherent geometry of the data.\n"], ["2019-05-05", "http://arxiv.org/abs/1905.01726", "Better the Devil you Know: An Analysis of Evasion Attacks using Out-of-Distribution Adversarial Examples.", ["Vikash Sehwag", " Arjun Nitin Bhagoji", " Liwei Song", " Chawin Sitawarin", " Daniel Cullina", " Mung Chiang", " Prateek Mittal"], "  A large body of recent work has investigated the phenomenon of evasion\nattacks using adversarial examples for deep learning systems, where the\naddition of norm-bounded perturbations to the test inputs leads to incorrect\noutput classification. Previous work has investigated this phenomenon in\nclosed-world systems where training and test inputs follow a pre-specified\ndistribution. However, real-world implementations of deep learning\napplications, such as autonomous driving and content classification are likely\nto operate in the open-world environment. In this paper, we demonstrate the\nsuccess of open-world evasion attacks, where adversarial examples are generated\nfrom out-of-distribution inputs (OOD adversarial examples). In our study, we\nuse 11 state-of-the-art neural network models trained on 3 image datasets of\nvarying complexity. We first demonstrate that state-of-the-art detectors for\nout-of-distribution data are not robust against OOD adversarial examples. We\nthen consider 5 known defenses for adversarial examples, including\nstate-of-the-art robust training methods, and show that against these defenses,\nOOD adversarial examples can achieve up to 4$\\times$ higher target success\nrates compared to adversarial examples generated from in-distribution data. We\nalso take a quantitative look at how open-world evasion attacks may affect\nreal-world systems. Finally, we present the first steps towards a robust\nopen-world machine learning system.\n"], ["2019-05-03", "http://arxiv.org/abs/1905.01034", "Transfer of Adversarial Robustness Between Perturbation Types.", ["Daniel Kang", " Yi Sun", " Tom Brown", " Dan Hendrycks", " Jacob Steinhardt"], "  We study the transfer of adversarial robustness of deep neural networks\nbetween different perturbation types. While most work on adversarial examples\nhas focused on $L_\\infty$ and $L_2$-bounded perturbations, these do not capture\nall types of perturbations available to an adversary. The present work\nevaluates 32 attacks of 5 different types against models adversarially trained\non a 100-class subset of ImageNet. Our empirical results suggest that\nevaluating on a wide range of perturbation sizes is necessary to understand\nwhether adversarial robustness transfers between perturbation types. We further\ndemonstrate that robustness against one perturbation type may not always imply\nand may sometimes hurt robustness against other perturbation types. In light of\nthese results, we recommend evaluation of adversarial defenses take place on a\ndiverse range of perturbation types and sizes.\n"], ["2019-05-02", "http://arxiv.org/abs/1905.01019", "Adversarial Training with Voronoi Constraints.", ["Marc Khoury", " Dylan Hadfield-Menell"], "  Adversarial examples are a pervasive phenomenon of machine learning models\nwhere seemingly imperceptible perturbations to the input lead to\nmisclassifications for otherwise statistically accurate models. We propose a\ngeometric framework, drawing on tools from the manifold reconstruction\nliterature, to analyze the high-dimensional geometry of adversarial examples.\nIn particular, we highlight the importance of codimension: for low-dimensional\ndata manifolds embedded in high-dimensional space there are many directions off\nthe manifold in which an adversary could construct adversarial examples.\nAdversarial examples are a natural consequence of learning a decision boundary\nthat classifies the low-dimensional data manifold well, but classifies points\nnear the manifold incorrectly. Using our geometric framework we prove that\nadversarial training is sample inefficient, and show sufficient sampling\nconditions under which nearest neighbor classifiers and ball-based adversarial\ntraining are robust. Finally we introduce adversarial training with Voronoi\nconstraints, which replaces the norm ball constraint with the Voronoi cell for\neach point in the training set. We show that adversarial training with Voronoi\nconstraints produces robust models which significantly improve over the\nstate-of-the-art on MNIST and are competitive on CIFAR-10.\n"], ["2019-05-02", "http://arxiv.org/abs/1905.00568", "Weight Map Layer for Noise and Adversarial Attack Robustness.", ["Mohammed Amer", " Tom\u00e1s Maul"], "  Convolutional neural networks (CNNs) are known for their good performance and\ngeneralization in vision-related tasks and have become state-of-the-art in both\napplication and research-based domains. However, just like other neural network\nmodels, they suffer from a susceptibility to noise and adversarial attacks. An\nadversarial defence aims at reducing a neural network's susceptibility to\nadversarial attacks through learning or architectural modifications. We propose\na weight map layer (WM) as a generic architectural addition to CNNs and show\nthat it can increase their robustness to noise and adversarial attacks. We\nfurther explain the enhanced robustness of the two WM variants introduced via\nan adaptive noise-variance amplification (ANVA) hypothesis and provide evidence\nand insights in support of it. We show that the WM layer can be integrated into\nscaled up models to increase their noise and adversarial attack robustness,\nwhile achieving the same or similar accuracy levels.\n"], ["2019-05-02", "http://arxiv.org/abs/1905.00877", "You Only Propagate Once: Accelerating Adversarial Training via Maximal Principle.", ["Dinghuai Zhang", " Tianyuan Zhang", " Yiping Lu", " Zhanxing Zhu", " Bin Dong"], "  Deep learning achieves state-of-the-art results in many tasks in computer\nvision and natural language processing. However, recent works have shown that\ndeep networks can be vulnerable to adversarial perturbations, which raised a\nserious robustness issue of deep networks. Adversarial training, typically\nformulated as a robust optimization problem, is an effective way of improving\nthe robustness of deep networks. A major drawback of existing adversarial\ntraining algorithms is the computational overhead of the generation of\nadversarial examples, typically far greater than that of the network training.\nThis leads to the unbearable overall computational cost of adversarial\ntraining. In this paper, we show that adversarial training can be cast as a\ndiscrete time differential game. Through analyzing the Pontryagin's Maximal\nPrinciple (PMP) of the problem, we observe that the adversary update is only\ncoupled with the parameters of the first layer of the network. This inspires us\nto restrict most of the forward and back propagation within the first layer of\nthe network during adversary updates. This effectively reduces the total number\nof full forward and backward propagation to only one for each group of\nadversary updates. Therefore, we refer to this algorithm YOPO (You Only\nPropagate Once). Numerical experiments demonstrate that YOPO can achieve\ncomparable defense accuracy with approximately 1/5 ~ 1/4 GPU time of the\nprojected gradient descent (PGD) algorithm. Our codes are available at\nhttps://https://github.com/a1600012888/YOPO-You-Only-Propagate-Once.\n"], ["2019-05-01", "http://arxiv.org/abs/1906.03181", "POBA-GA: Perturbation Optimized Black-Box Adversarial Attacks via Genetic Algorithm.", ["Jinyin Chen", " Mengmeng Su", " Shijing Shen", " Hui Xiong", " Haibin Zheng"], "  Most deep learning models are easily vulnerable to adversarial attacks.\nVarious adversarial attacks are designed to evaluate the robustness of models\nand develop defense model. Currently, adversarial attacks are brought up to\nattack their own target model with their own evaluation metrics. And most of\nthe black-box adversarial attack algorithms cannot achieve the expected success\nrate compared with white-box attacks. In this paper, comprehensive evaluation\nmetrics are brought up for different adversarial attack methods. A novel\nperturbation optimized black-box adversarial attack based on genetic algorithm\n(POBA-GA) is proposed for achieving white-box comparable attack performances.\nApproximate optimal adversarial examples are evolved through evolutionary\noperations including initialization, selection, crossover and mutation. Fitness\nfunction is specifically designed to evaluate the example individual in both\naspects of attack ability and perturbation control. Population diversity\nstrategy is brought up in evolutionary process to promise the approximate\noptimal perturbations obtained. Comprehensive experiments are carried out to\ntestify POBA-GA's performances. Both simulation and application results prove\nthat our method is better than current state-of-art black-box attack methods in\naspects of attack capability and perturbation control.\n"], ["2019-05-01", "http://arxiv.org/abs/1905.00441", "NATTACK: Learning the Distributions of Adversarial Examples for an Improved Black-Box Attack on Deep Neural Networks.", ["Yandong Li", " Lijun Li", " Liqiang Wang", " Tong Zhang", " Boqing Gong"], "  Powerful adversarial attack methods are vital for understanding how to\nconstruct robust deep neural networks (DNNs) and for thoroughly testing defense\ntechniques. In this paper, we propose a black-box adversarial attack algorithm\nthat can defeat both vanilla DNNs and those generated by various defense\ntechniques developed recently. Instead of searching for an \"optimal\"\nadversarial example for a benign input to a targeted DNN, our algorithm finds a\nprobability density distribution over a small region centered around the input,\nsuch that a sample drawn from this distribution is likely an adversarial\nexample, without the need of accessing the DNN's internal layers or weights.\nOur approach is universal as it can successfully attack different neural\nnetworks by a single algorithm. It is also strong; according to the testing\nagainst 2 vanilla DNNs and 13 defended ones, it outperforms state-of-the-art\nblack-box or white-box attack methods for most test cases. Additionally, our\nresults reveal that adversarial training remains one of the best defense\ntechniques, and the adversarial examples are not as transferable across\ndefended DNNs as them across vanilla DNNs.\n"], ["2019-05-01", "http://arxiv.org/abs/1905.00180", "Dropping Pixels for Adversarial Robustness.", ["Hossein Hosseini", " Sreeram Kannan", " Radha Poovendran"], "  Deep neural networks are vulnerable against adversarial examples. In this\npaper, we propose to train and test the networks with randomly subsampled\nimages with high drop rates. We show that this approach significantly improves\nrobustness against adversarial examples in all cases of bounded L0, L2 and\nL_inf perturbations, while reducing the standard accuracy by a small value. We\nargue that subsampling pixels can be thought to provide a set of robust\nfeatures for the input image and, thus, improves robustness without performing\nadversarial training.\n"], ["2019-04-30", "http://arxiv.org/abs/1904.13195", "Test Selection for Deep Learning Systems.", ["Wei Ma", " Mike Papadakis", " Anestis Tsakmalis", " Maxime Cordy", " Yves Le Traon"], "  Testing of deep learning models is challenging due to the excessive number\nand complexity of computations involved. As a result, test data selection is\nperformed manually and in an ad hoc way. This raises the question of how we can\nautomatically select candidate test data to test deep learning models. Recent\nresearch has focused on adapting test selection metrics from code-based\nsoftware testing (such as coverage) to deep learning. However, deep learning\nmodels have different attributes from code such as spread of computations\nacross the entire network reflecting training data properties, balance of\nneuron weights and redundancy (use of many more neurons than needed). Such\ndifferences make code-based metrics inappropriate to select data that can\nchallenge the models (can trigger misclassification). We thus propose a set of\ntest selection metrics based on the notion of model uncertainty (model\nconfidence on specific inputs). Intuitively, the more uncertain we are about a\ncandidate sample, the more likely it is that this sample triggers a\nmisclassification. Similarly, the samples for which we are the most uncertain,\nare the most informative and should be used to improve the model by retraining.\nWe evaluate these metrics on two widely-used image classification problems\ninvolving real and artificial (adversarial) data. We show that\nuncertainty-based metrics have a strong ability to select data that are\nmisclassified and lead to major improvement in classification accuracy during\nretraining: up to 80% more gain than random selection and other\nstate-of-the-art metrics on one dataset and up to 29% on the other.\n"], ["2019-04-30", "http://arxiv.org/abs/1904.13094", "Detecting Adversarial Examples through Nonlinear Dimensionality Reduction.", ["Francesco Crecchi", " Davide Bacciu", " Battista Biggio"], "  Deep neural networks are vulnerable to adversarial examples, i.e.,\ncarefully-perturbed inputs aimed to mislead classification. This work proposes\na detection method based on combining non-linear dimensionality reduction and\ndensity estimation techniques. Our empirical findings show that the proposed\napproach is able to effectively detect adversarial examples crafted by\nnon-adaptive attackers, i.e., not specifically tuned to bypass the detection\nmethod. Given our promising results, we plan to extend our analysis to adaptive\nattackers in future work.\n"], ["2019-04-29", "http://arxiv.org/abs/1904.13000", "Adversarial Training and Robustness for Multiple Perturbations.", ["Florian Tram\u00e8r", " Dan Boneh"], "  Defenses against adversarial examples, such as adversarial training, are\ntypically tailored to a single perturbation type (e.g., small\n$\\ell_\\infty$-noise). For other perturbations, these defenses offer no\nguarantees and, at times, even increase the model's vulnerability. Our aim is\nto understand the reasons underlying this robustness trade-off, and to train\nmodels that are simultaneously robust to multiple perturbation types. We prove\nthat a trade-off in robustness to different types of $\\ell_p$-bounded and\nspatial perturbations must exist in a natural and simple statistical setting.\nWe corroborate our formal analysis by demonstrating similar robustness\ntrade-offs on MNIST and CIFAR10. Building upon new multi-perturbation\nadversarial training schemes, and a novel efficient attack for finding\n$\\ell_1$-bounded adversarial examples, we show that no model trained against\nmultiple attacks achieves robustness competitive with that of models trained on\neach attack individually. In particular, we uncover a pernicious\ngradient-masking phenomenon on MNIST, which causes adversarial training with\nfirst-order $\\ell_\\infty, \\ell_1$ and $\\ell_2$ adversaries to achieve merely\n$50\\%$ accuracy. Our results question the viability and computational\nscalability of extending adversarial robustness, and adversarial training, to\nmultiple perturbation types.\n"], ["2019-04-29", "http://arxiv.org/abs/1904.12843", "Adversarial Training for Free!.", ["Ali Shafahi", " Mahyar Najibi", " Amin Ghiasi", " Zheng Xu", " John Dickerson", " Christoph Studer", " Larry S. Davis", " Gavin Taylor", " Tom Goldstein"], "  Adversarial training, in which a network is trained on adversarial examples,\nis one of the few defenses against adversarial attacks that withstands strong\nattacks. Unfortunately, the high cost of generating strong adversarial examples\nmakes standard adversarial training impractical on large-scale problems like\nImageNet. We present an algorithm that eliminates the overhead cost of\ngenerating adversarial examples by recycling the gradient information computed\nwhen updating model parameters. Our \"free\" adversarial training algorithm\nachieves state-of-the-art robustness on CIFAR-10 and CIFAR-100 datasets at\nnegligible additional cost compared to natural training, and can be 7 to 30\ntimes faster than other strong adversarial training methods. Using a single\nworkstation with 4 P100 GPUs and 2 days of runtime, we can train a robust model\nfor the large-scale ImageNet classification task that maintains 40% accuracy\nagainst PGD attacks.\n"], ["2019-04-27", "http://arxiv.org/abs/1904.12181", "Non-Local Context Encoder: Robust Biomedical Image Segmentation against Adversarial Attacks.", ["Xiang He", " Sibei Yang", " Guanbin Li?", " Haofeng Li", " Huiyou Chang", " Yizhou Yu"], "  Recent progress in biomedical image segmentation based on deep convolutional\nneural networks (CNNs) has drawn much attention. However, its vulnerability\ntowards adversarial samples cannot be overlooked. This paper is the first one\nthat discovers that all the CNN-based state-of-the-art biomedical image\nsegmentation models are sensitive to adversarial perturbations. This limits the\ndeployment of these methods in safety-critical biomedical fields. In this\npaper, we discover that global spatial dependencies and global contextual\ninformation in a biomedical image can be exploited to defend against\nadversarial attacks. To this end, non-local context encoder (NLCE) is proposed\nto model short- and long range spatial dependencies and encode global contexts\nfor strengthening feature activations by channel-wise attention. The NLCE\nmodules enhance the robustness and accuracy of the non-local context encoding\nnetwork (NLCEN), which learns robust enhanced pyramid feature representations\nwith NLCE modules, and then integrates the information across different levels.\nExperiments on both lung and skin lesion segmentation datasets have\ndemonstrated that NLCEN outperforms any other state-of-the-art biomedical image\nsegmentation methods against adversarial attacks. In addition, NLCE modules can\nbe applied to improve the robustness of other CNN-based biomedical image\nsegmentation methods.\n"], ["2019-04-26", "http://arxiv.org/abs/1904.11803", "Robustness Verification of Support Vector Machines.", ["Francesco Ranzato", " Marco Zanella"], "  We study the problem of formally verifying the robustness to adversarial\nexamples of support vector machines (SVMs), a major machine learning model for\nclassification and regression tasks. Following a recent stream of works on\nformal robustness verification of (deep) neural networks, our approach relies\non a sound abstract version of a given SVM classifier to be used for checking\nits robustness. This methodology is parametric on a given numerical abstraction\nof real values and, analogously to the case of neural networks, needs neither\nabstract least upper bounds nor widening operators on this abstraction. The\nstandard interval domain provides a simple instantiation of our abstraction\ntechnique, which is enhanced with the domain of reduced affine forms, which is\nan efficient abstraction of the zonotope abstract domain. This robustness\nverification technique has been fully implemented and experimentally evaluated\non SVMs based on linear and nonlinear (polynomial and radial basis function)\nkernels, which have been trained on the popular MNIST dataset of images and on\nthe recent and more challenging Fashion-MNIST dataset. The experimental results\nof our prototype SVM robustness verifier appear to be encouraging: this\nautomated verification is fast, scalable and shows significantly high\npercentages of provable robustness on the test set of MNIST, in particular\ncompared to the analogous provable robustness of neural networks.\n"], ["2019-04-24", "http://arxiv.org/abs/1904.10990", "A Robust Approach for Securing Audio Classification Against Adversarial Attacks.", ["Mohammad Esmaeilpour", " Patrick Cardinal", " Alessandro Lameiras Koerich"], "  Adversarial audio attacks can be considered as a small perturbation\nunperceptive to human ears that is intentionally added to the audio signal and\ncauses a machine learning model to make mistakes. This poses a security concern\nabout the safety of machine learning models since the adversarial attacks can\nfool such models toward the wrong predictions. In this paper we first review\nsome strong adversarial attacks that may affect both audio signals and their 2D\nrepresentations and evaluate the resiliency of the most common machine learning\nmodel, namely deep learning models and support vector machines (SVM) trained on\n2D audio representations such as short time Fourier transform (STFT), discrete\nwavelet transform (DWT) and cross recurrent plot (CRP) against several\nstate-of-the-art adversarial attacks. Next, we propose a novel approach based\non pre-processed DWT representation of audio signals and SVM to secure audio\nsystems against adversarial attacks. The proposed architecture has several\npreprocessing modules for generating and enhancing spectrograms including\ndimension reduction and smoothing. We extract features from small patches of\nthe spectrograms using speeded up robust feature (SURF) algorithm which are\nfurther used to generate a codebook using the K-Means++ algorithm. Finally,\ncodewords are used to train a SVM on the codebook of the SURF-generated\nvectors. All these steps yield to a novel approach for audio classification\nthat provides a good trade-off between accuracy and resilience. Experimental\nresults on three environmental sound datasets show the competitive performance\nof proposed approach compared to the deep neural networks both in terms of\naccuracy and robustness against strong adversarial attacks.\n"], ["2019-04-24", "http://arxiv.org/abs/1904.11042", "Physical Adversarial Textures that Fool Visual Object Tracking.", ["Rey Reza Wiyatno", " Anqi Xu"], "  We present a system for generating inconspicuous-looking textures that, when\ndisplayed in the physical world as digital or printed posters, cause visual\nobject tracking systems to become confused. For instance, as a target being\ntracked by a robot's camera moves in front of such a poster, our generated\ntexture makes the tracker lock onto it and allows the target to evade. This\nwork aims to fool seldom-targeted regression tasks, and in particular compares\ndiverse optimization strategies: non-targeted, targeted, and a new family of\nguided adversarial losses. While we use the Expectation Over Transformation\n(EOT) algorithm to generate physical adversaries that fool tracking models when\nimaged under diverse conditions, we compare the impacts of different\nconditioning variables, including viewpoint, lighting, and appearances, to find\npractical attack setups with high resulting adversarial strength and\nconvergence speed. We further showcase textures optimized solely using\nsimulated scenes can confuse real-world tracking systems.\n"], ["2019-04-23", "http://arxiv.org/abs/1904.10390", "Minimizing Perceived Image Quality Loss Through Adversarial Attack Scoping.", ["Kostiantyn Khabarlak", " Larysa Koriashkina"], "  Neural networks are now actively being used for computer vision tasks in\nsecurity critical areas such as robotics, face recognition, autonomous vehicles\nyet their safety is under question after the discovery of adversarial attacks.\nIn this paper we develop simplified adversarial attack algorithms based on a\nscoping idea, which enables execution of fast adversarial attacks that minimize\nstructural image quality (SSIM) loss, allows performing efficient transfer\nattacks with low target inference network call count and opens a possibility of\nan attack using pen-only drawings on a paper for the MNIST handwritten digit\ndataset. The presented adversarial attack analysis and the idea of attack\nscoping can be easily expanded to different datasets, thus making the paper's\nresults applicable to a wide range of practical tasks.\n"], ["2019-04-22", "http://arxiv.org/abs/1904.09804", "blessing in disguise: Designing Robust Turing Test by Employing Algorithm Unrobustness.", ["Jiaming Zhang", " Jitao Sang", " Kaiyuan Xu", " Shangxi Wu", " Yongli Hu", " Yanfeng Sun", " Jian Yu"], "  Turing test was originally proposed to examine whether machine's behavior is\nindistinguishable from a human. The most popular and practical Turing test is\nCAPTCHA, which is to discriminate algorithm from human by offering\nrecognition-alike questions. The recent development of deep learning has\nsignificantly advanced the capability of algorithm in solving CAPTCHA\nquestions, forcing CAPTCHA designers to increase question complexity. Instead\nof designing questions difficult for both algorithm and human, this study\nattempts to employ the limitations of algorithm to design robust CAPTCHA\nquestions easily solvable to human. Specifically, our data analysis observes\nthat human and algorithm demonstrates different vulnerability to visual\ndistortions: adversarial perturbation is significantly annoying to algorithm\nyet friendly to human. We are motivated to employ adversarially perturbed\nimages for robust CAPTCHA design in the context of character-based questions.\nThree modules of multi-target attack, ensemble adversarial training, and image\npreprocessing differentiable approximation are proposed to address the\ncharacteristics of character-based CAPTCHA cracking. Qualitative and\nquantitative experimental results demonstrate the effectiveness of the proposed\nsolution. We hope this study can lead to the discussions around adversarial\nattack/defense in CAPTCHA design and also inspire the future attempts in\nemploying algorithm limitation for practical usage.\n"], ["2019-04-22", "http://arxiv.org/abs/1904.10076", "Using Videos to Evaluate Image Model Robustness.", ["Keren Gu", " Brandon Yang", " Jiquan Ngiam", " Quoc Le", " Jonathon Shlens"], "  Human visual systems are robust to a wide range of image transformations that\nare challenging for artificial networks. We present the first study of image\nmodel robustness to the minute transformations found across video frames, which\nwe term \"natural robustness\". Compared to previous studies on adversarial\nexamples and synthetic distortions, natural robustness captures a more diverse\nset of common image transformations that occur in the natural environment. Our\nstudy across a dozen model architectures shows that more accurate models are\nmore robust to natural transformations, and that robustness to synthetic color\ndistortions is a good proxy for natural robustness. In examining brittleness in\nvideos, we find that majority of the brittleness found in videos lies outside\nthe typical definition of adversarial examples (99.9\\%). Finally, we\ninvestigate training techniques to reduce brittleness and find that no single\ntechnique systematically improves natural robustness across twelve tested\narchitectures.\n"], ["2019-04-21", "http://arxiv.org/abs/1904.09633", "Beyond Explainability: Leveraging Interpretability for Improved Adversarial Learning.", ["Devinder Kumar", " Ibrahim Ben-Daya", " Kanav Vats", " Jeffery Feng", " Graham Taylor and", " Alexander Wong"], "  In this study, we propose the leveraging of interpretability for tasks beyond\npurely the purpose of explainability. In particular, this study puts forward a\nnovel strategy for leveraging gradient-based interpretability in the realm of\nadversarial examples, where we use insights gained to aid adversarial learning.\nMore specifically, we introduce the concept of spatially constrained one-pixel\nadversarial perturbations, where we guide the learning of such adversarial\nperturbations towards more susceptible areas identified via gradient-based\ninterpretability. Experimental results using different benchmark datasets show\nthat such a spatially constrained one-pixel adversarial perturbation strategy\ncan noticeably improve the speed of convergence as well as produce successful\nattacks that were also visually difficult to perceive, thus illustrating an\neffective use of interpretability methods for tasks outside of the purpose of\npurely explainability.\n"], ["2019-04-20", "http://arxiv.org/abs/1904.09433", "Can Machine Learning Model with Static Features be Fooled: an Adversarial Machine Learning Approach.", ["Rahim Taheri", " Reza Javidan", " Mohammad Shojafar", " Vinod P", " Mauro Conti"], "  The widespread adoption of smartphones dramatically increases the risk of\nattacks and the spread of mobile malware, especially on the Android platform.\nMachine learning based solutions have been already used as a tool to supersede\nsignature based anti-malware systems. However, malware authors leverage\nattributes from malicious and legitimate samples to estimate statistical\ndifference in-order to create adversarial examples. Hence, to evaluate the\nvulnerability of machine learning algorithms in malware detection, we propose\nfive different attack scenarios to perturb malicious applications (apps). By\ndoing this, the classification algorithm inappropriately fits discriminant\nfunction on the set of data points, eventually yielding a higher\nmisclassification rate. Further, to distinguish the adversarial examples from\nbenign samples, we propose two defense mechanisms to counter attacks. To\nvalidate our attacks and solutions, we test our model on three different\nbenchmark datasets. We also test our methods using various classifier\nalgorithms and compare them with the state-of-the-art data poisoning method\nusing the Jacobian matrix. Promising results show that generated adversarial\nsamples can evade detection with a very high probability. Additionally, evasive\nvariants generated by our attacks models when used to harden the developed\nanti-malware system improves the detection rate.\n"], ["2019-04-19", "http://arxiv.org/abs/1904.09146", "Salient Object Detection in the Deep Learning Era: An In-Depth Survey.", ["Wenguan Wang", " Qiuxia Lai", " Huazhu Fu", " Jianbing Shen", " Haibin Ling"], "  As an important problem in computer vision, salient object detection (SOD)\nfrom images has been attracting an increasing amount of research effort over\nthe years. Recent advances in SOD, not surprisingly, are dominantly led by deep\nlearning-based solutions (named deep SOD) and reflected by hundreds of\npublished papers. To facilitate the in-depth understanding of deep SODs, in\nthis paper we provide a comprehensive survey covering various aspects ranging\nfrom algorithm taxonomy to unsolved open issues. In particular, we first review\ndeep SOD algorithms from different perspectives including network architecture,\nlevel of supervision, learning paradigm and object/instance level detection.\nFollowing that, we summarize existing SOD evaluation datasets and metrics.\nThen, we carefully compile a thorough benchmark results of SOD methods based on\nprevious work, and provide detailed analysis of the comparison results.\nMoreover, we study the performance of SOD algorithms under different\nattributes, which have been barely explored previously, by constructing a novel\nSOD dataset with rich attribute annotations. We further analyze, for the first\ntime in the field, the robustness and transferability of deep SOD models w.r.t.\nadversarial attacks. We also look into the influence of input perturbations,\nand the generalization and hardness of existing SOD datasets. Finally, we\ndiscuss several open issues and challenges of SOD, and point out possible\nresearch directions in future. All the saliency prediction maps, our\nconstructed dataset with annotations, and codes for evaluation are made\npublicly available at https://github.com/wenguanwang/SODsurvey.\n"], ["2019-04-18", "http://arxiv.org/abs/1904.08653", "Fooling automated surveillance cameras: adversarial patches to attack person detection.", ["Simen Thys", " Ranst Wiebe Van", " Toon Goedem\u00e9"], "  Adversarial attacks on machine learning models have seen increasing interest\nin the past years. By making only subtle changes to the input of a\nconvolutional neural network, the output of the network can be swayed to output\na completely different result. The first attacks did this by changing pixel\nvalues of an input image slightly to fool a classifier to output the wrong\nclass. Other approaches have tried to learn \"patches\" that can be applied to an\nobject to fool detectors and classifiers. Some of these approaches have also\nshown that these attacks are feasible in the real-world, i.e. by modifying an\nobject and filming it with a video camera. However, all of these approaches\ntarget classes that contain almost no intra-class variety (e.g. stop signs).\nThe known structure of the object is then used to generate an adversarial patch\non top of it.\n  In this paper, we present an approach to generate adversarial patches to\ntargets with lots of intra-class variety, namely persons. The goal is to\ngenerate a patch that is able successfully hide a person from a person\ndetector. An attack that could for instance be used maliciously to circumvent\nsurveillance systems, intruders can sneak around undetected by holding a small\ncardboard plate in front of their body aimed towards the surveillance camera.\n  From our results we can see that our system is able significantly lower the\naccuracy of a person detector. Our approach also functions well in real-life\nscenarios where the patch is filmed by a camera. To the best of our knowledge\nwe are the first to attempt this kind of attack on targets with a high level of\nintra-class variety like persons.\n"], ["2019-04-17", "http://arxiv.org/abs/1904.08516", "ZK-GanDef: A GAN based Zero Knowledge Adversarial Training Defense for Neural Networks.", ["Guanxiong Liu", " Issa Khalil", " Abdallah Khreishah"], "  Neural Network classifiers have been used successfully in a wide range of\napplications. However, their underlying assumption of attack free environment\nhas been defied by adversarial examples. Researchers tried to develop defenses;\nhowever, existing approaches are still far from providing effective solutions\nto this evolving problem. In this paper, we design a generative adversarial net\n(GAN) based zero knowledge adversarial training defense, dubbed ZK-GanDef,\nwhich does not consume adversarial examples during training. Therefore,\nZK-GanDef is not only efficient in training but also adaptive to new\nadversarial examples. This advantage comes at the cost of small degradation in\ntest accuracy compared to full knowledge approaches. Our experiments show that\nZK-GanDef enhances test accuracy on adversarial examples by up-to 49.17%\ncompared to zero knowledge approaches. More importantly, its test accuracy is\nclose to that of the state-of-the-art full knowledge approaches (maximum\ndegradation of 8.46%), while taking much less training time.\n"], ["2019-04-17", "http://arxiv.org/abs/1904.08444", "Defensive Quantization: When Efficiency Meets Robustness.", ["Ji Lin", " Chuang Gan", " Song Han"], "  Neural network quantization is becoming an industry standard to efficiently\ndeploy deep learning models on hardware platforms, such as CPU, GPU, TPU, and\nFPGAs. However, we observe that the conventional quantization approaches are\nvulnerable to adversarial attacks. This paper aims to raise people's awareness\nabout the security of the quantized models, and we designed a novel\nquantization methodology to jointly optimize the efficiency and robustness of\ndeep learning models. We first conduct an empirical study to show that vanilla\nquantization suffers more from adversarial attacks. We observe that the\ninferior robustness comes from the error amplification effect, where the\nquantization operation further enlarges the distance caused by amplified noise.\nThen we propose a novel Defensive Quantization (DQ) method by controlling the\nLipschitz constant of the network during quantization, such that the magnitude\nof the adversarial noise remains non-expansive during inference. Extensive\nexperiments on CIFAR-10 and SVHN datasets demonstrate that our new quantization\nmethod can defend neural networks against adversarial examples, and even\nachieves superior robustness than their full-precision counterparts while\nmaintaining the same hardware efficiency as vanilla quantization approaches. As\na by-product, DQ can also improve the accuracy of quantized models without\nadversarial attack.\n"], ["2019-04-17", "http://arxiv.org/abs/1904.08279", "Interpreting Adversarial Examples with Attributes.", ["Sadaf Gulshad", " Jan Hendrik Metzen", " Arnold Smeulders", " Zeynep Akata"], "  Deep computer vision systems being vulnerable to imperceptible and carefully\ncrafted noise have raised questions regarding the robustness of their\ndecisions. We take a step back and approach this problem from an orthogonal\ndirection. We propose to enable black-box neural networks to justify their\nreasoning both for clean and for adversarial examples by leveraging attributes,\ni.e. visually discriminative properties of objects. We rank attributes based on\ntheir class relevance, i.e. how the classification decision changes when the\ninput is visually slightly perturbed, as well as image relevance, i.e. how well\nthe attributes can be localized on both clean and perturbed images. We present\ncomprehensive experiments for attribute prediction, adversarial example\ngeneration, adversarially robust learning, and their qualitative and\nquantitative analysis using predicted attributes on three benchmark datasets.\n"], ["2019-04-17", "http://arxiv.org/abs/1904.08089", "Adversarial Defense Through Network Profiling Based Path Extraction.", ["Yuxian Qiu", " Jingwen Leng", " Cong Guo", " Quan Chen", " Chao Li", " Minyi Guo", " Yuhao Zhu"], "  Recently, researchers have started decomposing deep neural network models\naccording to their semantics or functions. Recent work has shown the\neffectiveness of decomposed functional blocks for defending adversarial\nattacks, which add small input perturbation to the input image to fool the DNN\nmodels. This work proposes a profiling-based method to decompose the DNN models\nto different functional blocks, which lead to the effective path as a new\napproach to exploring DNNs' internal organization. Specifically, the per-image\neffective path can be aggregated to the class-level effective path, through\nwhich we observe that adversarial images activate effective path different from\nnormal images. We propose an effective path similarity-based method to detect\nadversarial images with an interpretable model, which achieve better accuracy\nand broader applicability than the state-of-the-art technique.\n"], ["2019-04-17", "http://arxiv.org/abs/1904.08554", "Gotta Catch 'Em All: Using Concealed Trapdoors to Detect Adversarial Attacks on Neural Networks.", ["Shawn Shan", " Emily Willson", " Bolun Wang", " Bo Li", " Haitao Zheng", " Ben Y. Zhao"], "  Deep neural networks are vulnerable to adversarial attacks. Numerous efforts\nhave focused on defenses that either try to patch `holes' in trained models or\ntry to make it difficult or costly to compute adversarial examples exploiting\nthese holes. In our work, we explore a counter-intuitive approach of\nconstructing \"adversarial trapdoors. Unlike prior works that try to patch or\ndisguise vulnerable points in the manifold, we intentionally inject\n`trapdoors,' artificial weaknesses in the manifold that attract optimized\nperturbation into certain pre-embedded local optima. As a result, the\nadversarial generation functions naturally gravitate towards our trapdoors,\nproducing adversarial examples that the model owner can recognize through a\nknown neuron activation signature. In this paper, we introduce trapdoors and\ndescribe an implementation of trapdoors using similar strategies to\nbackdoor/Trojan attacks. We show that by proactively injecting trapdoors into\nthe models (and extracting their neuron activation signature), we can detect\nadversarial examples generated by the state of the art attacks (Projected\nGradient Descent, Optimization based CW, and Elastic Net) with high detection\nsuccess rate and negligible impact on normal inputs. These results also\ngeneralize across multiple classification domains (image recognition, face\nrecognition and traffic sign recognition). We explore different properties of\ntrapdoors, and discuss potential countermeasures (adaptive attacks) and\nmitigations.\n"], ["2019-04-17", "http://arxiv.org/abs/1904.08489", "Semantic Adversarial Attacks: Parametric Transformations That Fool Deep Classifiers.", ["Ameya Joshi", " Amitangshu Mukherjee", " Soumik Sarkar", " Chinmay Hegde"], "  Deep neural networks have been shown to exhibit an intriguing vulnerability\nto adversarial input images corrupted with imperceptible perturbations.\nHowever, the majority of adversarial attacks assume global, fine-grained\ncontrol over the image pixel space. In this paper, we consider a different\nsetting: what happens if the adversary could only alter specific attributes of\nthe input image? These would generate inputs that might be perceptibly\ndifferent, but still natural-looking and enough to fool a classifier. We\npropose a novel approach to generate such `semantic' adversarial examples by\noptimizing a particular adversarial loss over the range-space of a parametric\nconditional generative model. We demonstrate implementations of our attacks on\nbinary classifiers trained on face images, and show that such natural-looking\nsemantic adversarial examples exist. We evaluate the effectiveness of our\nattack on synthetic and real data, and present detailed comparisons with\nexisting attack methods. We supplement our empirical results with theoretical\nbounds that demonstrate the existence of such parametric adversarial examples.\n"], ["2019-04-16", "http://arxiv.org/abs/1904.07980", "Reducing Adversarial Example Transferability Using Gradient Regularization.", ["George Adam", " Petr Smirnov", " Benjamin Haibe-Kains", " Anna Goldenberg"], "  Deep learning algorithms have increasingly been shown to lack robustness to\nsimple adversarial examples (AdvX). An equally troubling observation is that\nthese adversarial examples transfer between different architectures trained on\ndifferent datasets. We investigate the transferability of adversarial examples\nbetween models using the angle between the input-output Jacobians of different\nmodels. To demonstrate the relevance of this approach, we perform case studies\nthat involve jointly training pairs of models. These case studies empirically\njustify the theoretical intuitions for why the angle between gradients is a\nfundamental quantity in AdvX transferability. Furthermore, we consider the\nasymmetry of AdvX transferability between two models of the same architecture\nand explain it in terms of differences in gradient norms between the models.\nLastly, we provide a simple modification to existing training setups that\nreduces transferability of adversarial examples between pairs of models.\n"], ["2019-04-16", "http://arxiv.org/abs/1904.07793", "AT-GAN: A Generative Attack Model for Adversarial Transferring on Generative Adversarial Nets.", ["Xiaosen Wang", " Kun He", " Chuan Guo", " Kilian Q. Weinberger", " John E. Hopcroft"], "  Recent studies have discovered the vulnerability of Deep Neural Networks\n(DNNs) to adversarial examples, which are imperceptible to humans but can\neasily fool DNNs. Existing methods for crafting adversarial examples are mainly\nbased on adding small-magnitude perturbations to the original images so that\nthe generated adversarial examples are constrained by the benign examples\nwithin a small matrix norm. In this work, we propose a new attack method called\nAT-GAN that directly generates the adversarial examples from random noise using\ngenerative adversarial nets (GANs). The key idea is to transfer a pre-trained\nGAN to generate adversarial examples for the target classifier to be attacked.\nOnce the model is transferred for attack, AT-GAN can generate diverse\nadversarial examples efficiently, making it helpful to potentially accelerate\nthe adversarial training on defenses. We evaluate AT-GAN in both semi-whitebox\nand black-box settings under typical defense methods on the MNIST handwritten\ndigit database. Empirical comparisons with existing attack baselines\ndemonstrate that AT-GAN can achieve a higher attack success rate.\n"], ["2019-04-15", "http://arxiv.org/abs/1904.07370", "Are Self-Driving Cars Secure? Evasion Attacks against Deep Neural Networks for Steering Angle Prediction.", ["Alesia Chernikova", " Alina Oprea", " Cristina Nita-Rotaru", " BaekGyu Kim"], "  Deep Neural Networks (DNNs) have tremendous potential in advancing the vision\nfor self-driving cars. However, the security of DNN models in this context\nleads to major safety implications and needs to be better understood. We\nconsider the case study of steering angle prediction from camera images, using\nthe dataset from the 2014 Udacity challenge. We demonstrate for the first time\nadversarial testing-time attacks for this application for both classification\nand regression settings. We show that minor modifications to the camera image\n(an L2 distance of 0.82 for one of the considered models) result in\nmis-classification of an image to any class of attacker's choice. Furthermore,\nour regression attack results in a significant increase in Mean Square Error\n(MSE) by a factor of 69 in the worst case.\n"], ["2019-04-15", "http://arxiv.org/abs/1904.06964", "Influence of Control Parameters and the Size of Biomedical Image Datasets on the Success of Adversarial Attacks.", ["Vassili Kovalev", " Dmitry Voynov"], "  In this paper, we study dependence of the success rate of adversarial attacks\nto the Deep Neural Networks on the biomedical image type, control parameters,\nand image dataset size. With this work, we are going to contribute towards\naccumulation of experimental results on adversarial attacks for the community\ndealing with biomedical images. The white-box Projected Gradient Descent\nattacks were examined based on 8 classification tasks and 13 image datasets\ncontaining a total of 605,080 chest X-ray and 317,000 histology images of\nmalignant tumors. We concluded that: (1) An increase of the amplitude of\nperturbation in generating malicious adversarial images leads to a growth of\nthe fraction of successful attacks for the majority of image types examined in\nthis study. (2) Histology images tend to be less sensitive to the growth of\namplitude of adversarial perturbations. (3) Percentage of successful attacks is\ngrowing with an increase of the number of iterations of the algorithm of\ngenerating adversarial perturbations with an asymptotic stabilization. (4) It\nwas found that the success of attacks dropping dramatically when the original\nconfidence of predicting image class exceeds 0.95. (5) The expected dependence\nof the percentage of successful attacks on the size of image training set was\nnot confirmed.\n"], ["2019-04-13", "http://arxiv.org/abs/1904.06606", "Exploiting Vulnerabilities of Load Forecasting Through Adversarial Attacks.", ["Yize Chen", " Yushi Tan", " Baosen Zhang"], "  Load forecasting plays a critical role in the operation and planning of power\nsystems. By using input features such as historical loads and weather\nforecasts, system operators and utilities build forecast models to guide\ndecision making in commitment and dispatch. As the forecasting techniques\nbecomes more sophisticated, however, they also become more vulnerable to\ncybersecurity threats. In this paper, we study the vulnerability of a class of\nload forecasting algorithms and analyze the potential impact on the power\nsystem operations, such as load shedding and increased dispatch costs.\nSpecifically, we propose data injection attack algorithms that require minimal\nassumptions on the ability of the adversary. The attacker does not need to have\nknowledge about the load forecasting model or the underlying power system.\nSurprisingly, our results indicate that standard load forecasting algorithms\nare quite vulnerable to the designed black-box attacks. By only injecting\nmalicious data in temperature from online weather forecast APIs, an attacker\ncould manipulate load forecasts in arbitrary directions and cause significant\nand targeted damages to system operations.\n"], ["2019-04-12", "http://arxiv.org/abs/1904.06347", "Big but Imperceptible Adversarial Perturbations via Semantic Manipulation.", ["Anand Bhattad", " Min Jin Chong", " Kaizhao Liang", " Bo Li", " David A. Forsyth"], "  Machine learning, especially deep learning, is widely applied to a range of\napplications including computer vision, robotics and natural language\nprocessing. However, it has been shown that machine learning models are\nvulnerable to adversarial examples, carefully crafted samples that deceive\nlearning models. In-depth studies on adversarial examples can help better\nunderstand potential vulnerabilities and therefore improve model robustness.\nRecent works have introduced various methods which generate adversarial\nexamples. However, all require the perturbation to be of small magnitude\n($\\mathcal{L}_p$ norm) for them to be imperceptible to humans, which is hard to\ndeploy in practice. In this paper we propose two novel methods, tAdv and cAdv,\nwhich leverage texture transfer and colorization to generate natural\nperturbation with a large $\\mathcal{L}_p$ norm. We conduct extensive\nexperiments to show that the proposed methods are general enough to attack both\nimage classification and image captioning tasks on ImageNet and MSCOCO dataset.\nIn addition, we conduct comprehensive user studies under various conditions to\nshow that our generated adversarial examples are imperceptible to humans even\nwhen the perturbations are large. We also evaluate the transferability and\nrobustness of the proposed attacks against several state-of-the-art defenses.\n"], ["2019-04-12", "http://arxiv.org/abs/1904.06026", "Cycle-Consistent Adversarial GAN: the integration of adversarial attack and defense.", ["Lingyun Jiang", " Kai Qiao", " Ruoxi Qin", " Linyuan Wang", " Jian Chen", " Haibing Bu", " Bin Yan"], "  In image classification of deep learning, adversarial examples where inputs\nintended to add small magnitude perturbations may mislead deep neural networks\n(DNNs) to incorrect results, which means DNNs are vulnerable to them. Different\nattack and defense strategies have been proposed to better research the\nmechanism of deep learning. However, those research in these networks are only\nfor one aspect, either an attack or a defense, not considering that attacks and\ndefenses should be interdependent and mutually reinforcing, just like the\nrelationship between spears and shields. In this paper, we propose\nCycle-Consistent Adversarial GAN (CycleAdvGAN) to generate adversarial\nexamples, which can learn and approximate the distribution of original\ninstances and adversarial examples. For CycleAdvGAN, once the Generator and are\ntrained, can generate adversarial perturbations efficiently for any instance,\nso as to make DNNs predict wrong, and recovery adversarial examples to clean\ninstances, so as to make DNNs predict correct. We apply CycleAdvGAN under\nsemi-white box and black-box settings on two public datasets MNIST and CIFAR10.\nUsing the extensive experiments, we show that our method has achieved the\nstate-of-the-art adversarial attack method and also efficiently improve the\ndefense ability, which make the integration of adversarial attack and defense\ncome true. In additional, it has improved attack effect only trained on the\nadversarial dataset generated by any kind of adversarial attack.\n"], ["2019-04-12", "http://arxiv.org/abs/1904.06292", "Adversarial Learning in Statistical Classification: A Comprehensive Review of Defenses Against Attacks.", ["David J. Miller", " Zhen Xiang", " George Kesidis"], "  There is great potential for damage from adversarial learning (AL) attacks on\nmachine-learning based systems. In this paper, we provide a contemporary survey\nof AL, focused particularly on defenses against attacks on statistical\nclassifiers. After introducing relevant terminology and the goals and range of\npossible knowledge of both attackers and defenders, we survey recent work on\ntest-time evasion (TTE), data poisoning (DP), and reverse engineering (RE)\nattacks and particularly defenses against same. In so doing, we distinguish\nrobust classification from anomaly detection (AD), unsupervised from\nsupervised, and statistical hypothesis-based defenses from ones that do not\nhave an explicit null (no attack) hypothesis; we identify the hyperparameters a\nparticular method requires, its computational complexity, as well as the\nperformance measures on which it was evaluated and the obtained quality. We\nthen dig deeper, providing novel insights that challenge conventional AL wisdom\nand that target unresolved issues, including: 1) robust classification versus\nAD as a defense strategy; 2) the belief that attack success increases with\nattack strength, which ignores susceptibility to AD; 3) small perturbations for\ntest-time evasion attacks: a fallacy or a requirement?; 4) validity of the\nuniversal assumption that a TTE attacker knows the ground-truth class for the\nexample to be attacked; 5) black, grey, or white box attacks as the standard\nfor defense evaluation; 6) susceptibility of query-based RE to an AD defense.\nWe also discuss attacks on the privacy of training data. We then present\nbenchmark comparisons of several defenses against TTE, RE, and backdoor DP\nattacks on images. The paper concludes with a discussion of future work.\n"], ["2019-04-12", "http://arxiv.org/abs/1904.06186", "Generating Minimal Adversarial Perturbations with Integrated Adaptive Gradients.", ["Yatie Xiao", " Chi-Man Pun"], "  Deep neural networks are easily fooled high confidence predictions for\nadversarial samples\n"], ["2019-04-12", "http://arxiv.org/abs/1904.06097", "Evaluating Robustness of Deep Image Super-Resolution against Adversarial Attacks.", ["Jun-Ho Choi", " Huan Zhang", " Jun-Hyuk Kim", " Cho-Jui Hsieh", " Jong-Seok Lee"], "  Single-image super-resolution aims to generate a high-resolution version of a\nlow-resolution image, which serves as an essential component in many computer\nvision applications. This paper investigates the robustness of deep\nlearning-based super-resolution methods against adversarial attacks, which can\nsignificantly deteriorate the super-resolved images without noticeable\ndistortion in the attacked low-resolution images. It is demonstrated that\nstate-of-the-art deep super-resolution methods are highly vulnerable to\nadversarial attacks. Different levels of robustness of different methods are\nanalyzed theoretically and experimentally. We also present analysis on\ntransferability of attacks, and feasibility of targeted attacks and universal\nattacks.\n"], ["2019-04-11", "http://arxiv.org/abs/1904.05586", "Black-Box Decision based Adversarial Attack with Symmetric $\\alpha$-stable Distribution.", ["Vignesh Srinivasan", " Ercan E. Kuruoglu", " Klaus-Robert M\u00fcller", " Wojciech Samek", " Shinichi Nakajima"], "  Developing techniques for adversarial attack and defense is an important\nresearch field for establishing reliable machine learning and its applications.\nMany existing methods employ Gaussian random variables for exploring the data\nspace to find the most adversarial (for attacking) or least adversarial (for\ndefense) point. However, the Gaussian distribution is not necessarily the\noptimal choice when the exploration is required to follow the complicated\nstructure that most real-world data distributions exhibit. In this paper, we\ninvestigate how statistics of random variables affect such random walk\nexploration. Specifically, we generalize the Boundary Attack, a\nstate-of-the-art black-box decision based attacking strategy, and propose the\nL\\'evy-Attack, where the random walk is driven by symmetric $\\alpha$-stable\nrandom variables. Our experiments on MNIST and CIFAR10 datasets show that the\nL\\'evy-Attack explores the image data space more efficiently, and significantly\nimproves the performance. Our results also give an insight into the recently\nfound fact in the whitebox attacking scenario that the choice of the norm for\nmeasuring the amplitude of the adversarial patterns is essential.\n"], ["2019-04-10", "http://arxiv.org/abs/1904.05475", "Learning to Generate Synthetic Data via Compositing.", ["Shashank Tripathi", " Siddhartha Chandra", " Amit Agrawal", " Ambrish Tyagi", " James M. Rehg", " Visesh Chari"], "  We present a task-aware approach to synthetic data generation. Our framework\nemploys a trainable synthesizer network that is optimized to produce meaningful\ntraining samples by assessing the strengths and weaknesses of a `target'\nnetwork. The synthesizer and target networks are trained in an adversarial\nmanner wherein each network is updated with a goal to outdo the other.\nAdditionally, we ensure the synthesizer generates realistic data by pairing it\nwith a discriminator trained on real-world images. Further, to make the target\nclassifier invariant to blending artefacts, we introduce these artefacts to\nbackground regions of the training images so the target does not over-fit to\nthem.\n  We demonstrate the efficacy of our approach by applying it to different\ntarget networks including a classification network on AffNIST, and two object\ndetection networks (SSD, Faster-RCNN) on different datasets. On the AffNIST\nbenchmark, our approach is able to surpass the baseline results with just half\nthe training examples. On the VOC person detection benchmark, we show\nimprovements of up to 2.7% as a result of our data augmentation. Similarly on\nthe GMU detection benchmark, we report a performance boost of 3.5% in mAP over\nthe baseline method, outperforming the previous state of the art approaches by\nup to 7.5% on specific categories.\n"], ["2019-04-10", "http://arxiv.org/abs/1904.05181", "Black-box Adversarial Attacks on Video Recognition Models.", ["Linxi Jiang", " Xingjun Ma", " Shaoxiang Chen", " James Bailey", " Yu-Gang Jiang"], "  Deep neural networks (DNNs) are known for their vulnerability to adversarial\nexamples. These are examples that have undergone small, carefully crafted\nperturbations, and which can easily fool a DNN into making misclassifications\nat test time. Thus far, the field of adversarial research has mainly focused on\nimage models, under either a white-box setting, where an adversary has full\naccess to model parameters, or a black-box setting where an adversary can only\nquery the target model for probabilities or labels. Whilst several white-box\nattacks have been proposed for video models, black-box video attacks are still\nunexplored. To close this gap, we propose the first black-box video attack\nframework, called V-BAD. V-BAD utilizes tentative perturbations transferred\nfrom image models, and partition-based rectifications found by the NES on\npartitions (patches) of tentative perturbations, to obtain good adversarial\ngradient estimates with fewer queries to the target model. V-BAD is equivalent\nto estimating the projection of an adversarial gradient on a selected subspace.\nUsing three benchmark video datasets, we demonstrate that V-BAD can craft both\nuntargeted and targeted attacks to fool two state-of-the-art deep video\nrecognition models. For the targeted attack, it achieves $>$93\\% success rate\nusing only an average of $3.4 \\sim 8.4 \\times 10^4$ queries, a similar number\nof queries to state-of-the-art black-box image attacks. This is despite the\nfact that videos often have two orders of magnitude higher dimensionality than\nstatic images. We believe that V-BAD is a promising new tool to evaluate and\nimprove the robustness of video recognition models to black-box adversarial\nattacks.\n"], ["2019-04-09", "http://arxiv.org/abs/1904.04802", "Generation & Evaluation of Adversarial Examples for Malware Obfuscation.", ["Daniel Park", " Haidar Khan", " B\u00fclent Yener"], "  There has been an increased interest in the application of convolutional\nneural networks for image based malware classification, but the susceptibility\nof neural networks to adversarial examples allows malicious actors to evade\nclassifiers. Adversarial examples are usually generated by adding small\nperturbations to the input that are unrecognizable to humans, but the same\napproach is not effective with malware. In general, these perturbations cause\nchanges in the byte sequences that change the initial functionality or result\nin un-executable binaries. We present a generative model for executable\nadversarial malware examples using obfuscation that achieves a high\nmisclassification rate, up to 100% and 98% in white-box and black-box settings\nrespectively, and demonstrates transferability. We further evaluate the\neffectiveness of the proposed method by reporting insignificant change in the\nevasion rate of our adversarial examples against popular defense strategies.\n"], ["2019-04-08", "http://arxiv.org/abs/1904.04433", "Efficient Decision-based Black-box Adversarial Attacks on Face Recognition.", ["Yinpeng Dong", " Hang Su", " Baoyuan Wu", " Zhifeng Li", " Wei Liu", " Tong Zhang", " Jun Zhu"], "  Face recognition has obtained remarkable progress in recent years due to the\ngreat improvement of deep convolutional neural networks (CNNs). However, deep\nCNNs are vulnerable to adversarial examples, which can cause fateful\nconsequences in real-world face recognition applications with\nsecurity-sensitive purposes. Adversarial attacks are widely studied as they can\nidentify the vulnerability of the models before they are deployed. In this\npaper, we evaluate the robustness of state-of-the-art face recognition models\nin the decision-based black-box attack setting, where the attackers have no\naccess to the model parameters and gradients, but can only acquire hard-label\npredictions by sending queries to the target model. This attack setting is more\npractical in real-world face recognition systems. To improve the efficiency of\nprevious methods, we propose an evolutionary attack algorithm, which can model\nthe local geometries of the search directions and reduce the dimension of the\nsearch space. Extensive experiments demonstrate the effectiveness of the\nproposed method that induces a minimum perturbation to an input face image with\nfewer queries. We also apply the proposed method to attack a real-world face\nrecognition system successfully.\n"], ["2019-04-08", "http://arxiv.org/abs/1904.04334", "A Target-Agnostic Attack on Deep Models: Exploiting Security Vulnerabilities of Transfer Learning.", ["Shahbaz Rezaei", " Xin Liu"], "  Due to the lack of enough training data and high computational cost to train\na deep neural network from scratch, transfer learning has been extensively used\nin many deep-neural-network-based applications. A commonly-used transfer\nlearning approach involves taking a part of a pre-trained model, adding a few\nlayers at the end, and re-training the new layers with a small dataset. This\napproach, while efficient and widely used, imposes a security vulnerability\nbecause the pre-trained model used in transfer learning are usually available\npublicly to everyone, including potential attackers. In this paper, we show\nthat without any additional knowledge other than the pre-trained model, an\nattacker can launch an effective and efficient brute force attack that can\ncraft instances of input to trigger each target class with high confidence. We\nassume that the attacker does not have access to any target-specific\ninformation, including samples from target classes, re-trained model, and\nprobabilities assigned by Softmax to each class, and thus called\ntarget-agnostic attack. These assumptions render all previous attacks\nimpractical, to the best of our knowledge. To evaluate the proposed attack, we\nperform a set of experiments on face recognition and speech recognition tasks\nand show the effectiveness of the attack. Our work sheds light on a fundamental\nsecurity challenge of the Softmax layer when used in transfer learning\nsettings.\n"], ["2019-04-07", "http://arxiv.org/abs/1904.03750", "JumpReLU: A Retrofit Defense Strategy for Adversarial Attacks.", ["N. Benjamin Erichson", " Zhewei Yao", " Michael W. Mahoney"], "  It has been demonstrated that very simple attacks can fool\nhighly-sophisticated neural network architectures. In particular, so-called\nadversarial examples, constructed from perturbations of input data that are\nsmall or imperceptible to humans but lead to different predictions, may lead to\nan enormous risk in certain critical applications. In light of this, there has\nbeen a great deal of work on developing adversarial training strategies to\nimprove model robustness. These training strategies are very expensive, in both\nhuman and computational time. To complement these approaches, we propose a very\nsimple and inexpensive strategy which can be used to ``retrofit'' a\npreviously-trained network to improve its resilience to adversarial attacks.\nMore concretely, we propose a new activation function---the JumpReLU---which,\nwhen used in place of a ReLU in an already-trained model, leads to a trade-off\nbetween predictive accuracy and robustness. This trade-off is controlled by the\njump size, a hyper-parameter which can be tuned during the validation stage.\nOur empirical results demonstrate that this increases model robustness,\nprotecting against adversarial attacks with substantially increased levels of\nperturbations. This is accomplished simply by retrofitting existing networks\nwith our JumpReLU activation function, without the need for retraining the\nmodel. Additionally, we demonstrate that adversarially trained (robust) models\ncan greatly benefit from retrofitting.\n"], ["2019-04-07", "http://arxiv.org/abs/1904.05747", "Malware Evasion Attack and Defense.", ["Yonghong Huang", " Utkarsh Verma", " Celeste Fralick", " Gabriel Infante-Lopez", " Brajesh Kumarz", " Carl Woodward"], "  Machine learning (ML) classifiers are vulnerable to adversarial examples. An\nadversarial example is an input sample which is slightly modified to induce\nmisclassification in an ML classifier. In this work, we investigate white-box\nand grey-box evasion attacks to an ML-based malware detector and conduct\nperformance evaluations in a real-world setting. We compare the defense\napproaches in mitigating the attacks. We propose a framework for deploying\ngrey-box and black-box attacks to malware detection systems.\n"], ["2019-04-06", "http://arxiv.org/abs/1904.03542", "On Training Robust PDF Malware Classifiers.", ["Yizheng Chen", " Shiqi Wang", " Dongdong She", " Suman Jana"], "  Although state-of-the-art PDF malware classifiers can be trained with almost\nperfect test accuracy (99%) and extremely low false positive rate (under 0.1%),\nit has been shown that even a simple adversary can evade them. A practically\nuseful malware classifier must be robust against evasion attacks. However,\nachieving such robustness is an extremely challenging task.\n  In this paper, we take the first steps towards training robust PDF malware\nclassifiers with verifiable robustness properties. For instance, a robustness\nproperty can enforce that no matter how many pages from benign documents are\ninserted into a PDF malware, the classifier must still classify it as\nmalicious. We demonstrate how the worst-case behavior of a malware classifier\nwith respect to specific robustness properties can be formally verified.\nFurthermore, we find that training classifiers that satisfy formally verified\nrobustness properties can increase the computation cost of unbounded (i.e., not\nbounded by the robustness properties) attackers by eliminating simple evasion\nattacks.\n  Specifically, we propose a new distance metric that operates on the PDF tree\nstructure and specify two classes of robustness properties including subtree\ninsertions and deletions. We utilize state-of-the-art verifiably robust\ntraining method to build robust PDF malware classifiers. Our results show that,\nwe can achieve 99% verified robust accuracy, while maintaining 99.80% accuracy\nand 0.41% false positive rate. With simple robustness properties, the\nstate-of-the-art unbounded attacker found no successful evasion on the robust\nclassifier in 6 hours. Even for a new unbounded adaptive attacker we have\ndesigned, the number of successful evasions within a fixed time budget is cut\ndown by 4x.\n"], ["2019-04-05", "http://arxiv.org/abs/1904.02884", "Evading Defenses to Transferable Adversarial Examples by Translation-Invariant Attacks.", ["Yinpeng Dong", " Tianyu Pang", " Hang Su", " Jun Zhu"], "  Deep neural networks are vulnerable to adversarial examples, which can\nmislead classifiers by adding imperceptible perturbations. An intriguing\nproperty of adversarial examples is their good transferability, making\nblack-box attacks feasible in real-world applications. Due to the threat of\nadversarial attacks, many methods have been proposed to improve the robustness.\nSeveral state-of-the-art defenses are shown to be robust against transferable\nadversarial examples. In this paper, we propose a translation-invariant attack\nmethod to generate more transferable adversarial examples against the defense\nmodels. By optimizing a perturbation over an ensemble of translated images, the\ngenerated adversarial example is less sensitive to the white-box model being\nattacked and has better transferability. To improve the efficiency of attacks,\nwe further show that our method can be implemented by convolving the gradient\nat the untranslated image with a pre-defined kernel. Our method is generally\napplicable to any gradient-based attack method. Extensive experiments on the\nImageNet dataset validate the effectiveness of the proposed method. Our best\nattack fools eight state-of-the-art defenses at an 82% success rate on average\nbased only on the transferability, demonstrating the insecurity of the current\ndefense techniques.\n"], ["2019-04-04", "http://arxiv.org/abs/1904.02841", "Minimum Uncertainty Based Detection of Adversaries in Deep Neural Networks.", ["Fatemeh Sheikholeslami", " Swayambhoo Jain", " Georgios B. Giannakis"], "  Despite their unprecedented performance in various domains, utilization of\nDeep Neural Networks (DNNs) in safety-critical environments is severely limited\nin the presence of even small adversarial perturbations. The present work\ndevelops a randomized approach to detecting such perturbations based on minimum\nuncertainty metrics that rely on sampling at the hidden layers during the DNN\ninference stage. The sampling probabilities are designed for effective\ndetection of the adversarially corrupted inputs. Being modular, the novel\ndetector of adversaries can be conveniently employed by any pre-trained DNN at\nno extra training overhead. Selecting which units to sample per hidden layer\nentails quantifying the amount of DNN output uncertainty from the viewpoint of\nBayesian neural networks, where the overall uncertainty is expressed in terms\nof its layer-wise components - what also promotes scalability. Sampling\nprobabilities are then sought by minimizing uncertainty measures\nlayer-by-layer, leading to a novel convex optimization problem that admits an\nexact solver with superlinear convergence rate. By simplifying the objective\nfunction, low-complexity approximate solvers are also developed. In addition to\nvaluable insights, these approximations link the novel approach with\nstate-of-the-art randomized adversarial detectors. The effectiveness of the\nnovel detectors in the context of competing alternatives is highlighted through\nextensive tests for various types of adversarial attacks with variable levels\nof strength.\n"], ["2019-04-04", "http://arxiv.org/abs/1904.02405", "White-to-Black: Efficient Distillation of Black-Box Adversarial Attacks.", ["Yotam Gil", " Yoav Chai", " Or Gorodissky", " Jonathan Berant"], "  Adversarial examples are important for understanding the behavior of neural\nmodels, and can improve their robustness through adversarial training. Recent\nwork in natural language processing generated adversarial examples by assuming\nwhite-box access to the attacked model, and optimizing the input directly\nagainst it (Ebrahimi et al., 2018). In this work, we show that the knowledge\nimplicit in the optimization procedure can be distilled into another more\nefficient neural network. We train a model to emulate the behavior of a\nwhite-box attack and show that it generalizes well across examples. Moreover,\nit reduces adversarial example generation time by 19x-39x. We also show that\nour approach transfers to a black-box setting, by attacking The Google\nPerspective API and exposing its vulnerability. Our attack flips the\nAPI-predicted label in 42\\% of the generated examples, while humans maintain\nhigh-accuracy in predicting the gold label.\n"], ["2019-04-03", "http://arxiv.org/abs/1904.10504", "Understanding the efficacy, reliability and resiliency of computer vision techniques for malware detection and future research directions.", ["Li Chen"], "  My research lies in the intersection of security and machine learning. This\noverview summarizes one component of my research: combining computer vision\nwith malware exploit detection for enhanced security solutions. I will present\nthe perspectives of efficacy, reliability and resiliency to formulate threat\ndetection as computer vision problems and develop state-of-the-art image-based\nmalware classification. Representing malware binary as images provides a direct\nvisualization of data samples, reduces the efforts for feature extraction, and\nconsumes the whole binary for holistic structural analysis. Employing transfer\nlearning of deep neural networks effective for large scale image classification\nto malware classification demonstrates superior classification efficacy\ncompared with classical machine learning algorithms. To enhance reliability of\nthese vision-based malware detectors, interpretation frameworks can be\nconstructed on the malware visual representations and useful for extracting\nfaithful explanation, so that security practitioners have confidence in the\nmodel before deployment. In cyber-security applications, we should always\nassume that a malware writer constantly modifies code to bypass detection.\nAddressing the resiliency of the malware detectors is equivalently important as\nefficacy and reliability. Via understanding the attack surfaces of machine\nlearning models used for malware detection, we can greatly improve the\nrobustness of the algorithms to combat malware adversaries in the wild. Finally\nI will discuss future research directions worth pursuing in this research\ncommunity.\n"], ["2019-04-03", "http://arxiv.org/abs/1904.02057", "Interpreting Adversarial Examples by Activation Promotion and Suppression.", ["Kaidi Xu", " Sijia Liu", " Gaoyuan Zhang", " Mengshu Sun", " Pu Zhao", " Quanfu Fan", " Chuang Gan", " Xue Lin"], "  It is widely known that convolutional neural networks (CNNs) are vulnerable\nto adversarial examples: images with imperceptible perturbations crafted to\nfool classifiers. However, interpretability of these perturbations is less\nexplored in the literature. This work aims to better understand the roles of\nadversarial perturbations and provide visual explanations from pixel, image and\nnetwork perspectives. We show that adversaries have a promotion-suppression\neffect (PSE) on neurons' activations and can be primarily categorized into\nthree types: i) suppression-dominated perturbations that mainly reduce the\nclassification score of the true label, ii) promotion-dominated perturbations\nthat focus on boosting the confidence of the target label, and iii) balanced\nperturbations that play a dual role in suppression and promotion. We also\nprovide image-level interpretability of adversarial examples. This links PSE of\npixel-level perturbations to class-specific discriminative image regions\nlocalized by class activation mapping (Zhou et al. 2016). Further, we examine\nthe adversarial effect through network dissection (Bau et al. 2017), which\noffers concept-level interpretability of hidden units. We show that there\nexists a tight connection between the units' sensitivity to adversarial attacks\nand their interpretability on semantic concepts. Lastly, we provide some new\ninsights from our interpretation to improve the adversarial robustness of\nnetworks.\n"], ["2019-04-03", "http://arxiv.org/abs/1904.02144", "HopSkipJumpAttack: A Query-Efficient Decision-Based Attack.", ["Jianbo Chen", " Michael I. Jordan", " Martin J. Wainwright"], "  The goal of a decision-based adversarial attack on a trained model is to\ngenerate adversarial examples based solely on observing output labels returned\nby the targeted model. We develop HopSkipJumpAttack, a family of algorithms\nbased on a novel estimate of the gradient direction using binary information at\nthe decision boundary. The proposed family includes both untargeted and\ntargeted attacks optimized for $\\ell_2$ and $\\ell_\\infty$ similarity metrics\nrespectively. Theoretical analysis is provided for the proposed algorithms and\nthe gradient direction estimate. Experiments show HopSkipJumpAttack requires\nsignificantly fewer model queries than Boundary Attack. It also achieves\ncompetitive performance in attacking several widely-used defense mechanisms.\n(HopSkipJumpAttack was named Boundary Attack++ in a previous version of the\npreprint.)\n"], ["2019-04-03", "http://arxiv.org/abs/1904.02323", "Summit: Scaling Deep Learning Interpretability by Visualizing Activation and Attribution Summarizations.", ["Fred Hohman", " Haekyu Park", " Caleb Robinson", " Duen Horng Chau"], "  Deep learning is increasingly used in decision-making tasks. However,\nunderstanding how neural networks produce final predictions remains a\nfundamental challenge. Existing work on interpreting neural network predictions\nfor images often focuses on explaining predictions for single images or\nneurons. As predictions are often computed from millions of weights that are\noptimized over millions of images, such explanations can easily miss a bigger\npicture. We present Summit, an interactive system that scalably and\nsystematically summarizes and visualizes what features a deep learning model\nhas learned and how those features interact to make predictions. Summit\nintroduces two new scalable summarization techniques: (1) activation\naggregation discovers important neurons, and (2) neuron-influence aggregation\nidentifies relationships among such neurons. Summit combines these techniques\nto create the novel attribution graph that reveals and summarizes crucial\nneuron associations and substructures that contribute to a model's outcomes.\nSummit scales to large data, such as the ImageNet dataset with 1.2M images, and\nleverages neural network feature visualization and dataset examples to help\nusers distill large, complex neural network models into compact, interactive\nvisualizations. We present neural network exploration scenarios where Summit\nhelps us discover multiple surprising insights into a prevalent, large-scale\nimage classifier's learned representations and informs future neural network\narchitecture design. The Summit visualization runs in modern web browsers and\nis open-sourced.\n"], ["2019-04-02", "http://arxiv.org/abs/1904.01231", "Adversarial Attacks against Deep Saliency Models.", ["Zhaohui Che", " Ali Borji", " Guangtao Zhai", " Suiyi Ling", " Guodong Guo", " Patrick Le Callet"], "  Currently, a plethora of saliency models based on deep neural networks have\nled great breakthroughs in many complex high-level vision tasks (e.g. scene\ndescription, object detection). The robustness of these models, however, has\nnot yet been studied. In this paper, we propose a sparse feature-space\nadversarial attack method against deep saliency models for the first time. The\nproposed attack only requires a part of the model information, and is able to\ngenerate a sparser and more insidious adversarial perturbation, compared to\ntraditional image-space attacks. These adversarial perturbations are so subtle\nthat a human observer cannot notice their presences, but the model outputs will\nbe revolutionized. This phenomenon raises security threats to deep saliency\nmodels in practical applications. We also explore some intriguing properties of\nthe feature-space attack, e.g. 1) the hidden layers with bigger receptive\nfields generate sparser perturbations, 2) the deeper hidden layers achieve\nhigher attack success rates, and 3) different loss functions and different\nattacked layers will result in diverse perturbations. Experiments indicate that\nthe proposed method is able to successfully attack different model\narchitectures across various image scenes.\n"], ["2019-04-01", "http://arxiv.org/abs/1904.01160", "Curls & Whey: Boosting Black-Box Adversarial Attacks.", ["Yucheng Shi", " Siyu Wang", " Yahong Han"], "  Image classifiers based on deep neural networks suffer from harassment caused\nby adversarial examples. Two defects exist in black-box iterative attacks that\ngenerate adversarial examples by incrementally adjusting the noise-adding\ndirection for each step. On the one hand, existing iterative attacks add noises\nmonotonically along the direction of gradient ascent, resulting in a lack of\ndiversity and adaptability of the generated iterative trajectories. On the\nother hand, it is trivial to perform adversarial attack by adding excessive\nnoises, but currently there is no refinement mechanism to squeeze redundant\nnoises. In this work, we propose Curls & Whey black-box attack to fix the above\ntwo defects. During Curls iteration, by combining gradient ascent and descent,\nwe `curl' up iterative trajectories to integrate more diversity and\ntransferability into adversarial examples. Curls iteration also alleviates the\ndiminishing marginal effect in existing iterative attacks. The Whey\noptimization further squeezes the `whey' of noises by exploiting the robustness\nof adversarial perturbation. Extensive experiments on Imagenet and\nTiny-Imagenet demonstrate that our approach achieves impressive decrease on\nnoise magnitude in l2 norm. Curls & Whey attack also shows promising\ntransferability against ensemble models as well as adversarially trained\nmodels. In addition, we extend our attack to the targeted misclassification,\neffectively reducing the difficulty of targeted attacks under black-box\ncondition.\n"], ["2019-04-01", "http://arxiv.org/abs/1904.00979", "Regional Homogeneity: Towards Learning Transferable Universal Adversarial Perturbations Against Defenses.", ["Yingwei Li", " Song Bai", " Cihang Xie", " Zhenyu Liao", " Xiaohui Shen", " Alan L. Yuille"], "  This paper focuses on learning transferable adversarial examples specifically\nagainst defense models (models to defense adversarial attacks). In particular,\nwe show that a simple universal perturbation can fool a series of\nstate-of-the-art defenses.\n  Adversarial examples generated by existing attacks are generally hard to\ntransfer to defense models. We observe the property of regional homogeneity in\nadversarial perturbations and suggest that the defenses are less robust to\nregionally homogeneous perturbations. Therefore, we propose an effective\ntransforming paradigm and a customized gradient transformer module to transform\nexisting perturbations into regionally homogeneous ones. Without explicitly\nforcing the perturbations to be universal, we observe that a well-trained\ngradient transformer module tends to output input-independent gradients (hence\nuniversal) benefiting from the under-fitting phenomenon. Thorough experiments\ndemonstrate that our work significantly outperforms the prior art attacking\nalgorithms (either image-dependent or universal ones) by an average improvement\nof 14.0% when attacking 9 defenses in the black-box setting. In addition to the\ncross-model transferability, we also verify that regionally homogeneous\nperturbations can well transfer across different vision tasks (attacking with\nthe semantic segmentation task and testing on the object detection task).\n"], ["2019-04-01", "http://arxiv.org/abs/1904.00923", "Robustness of 3D Deep Learning in an Adversarial Setting.", ["Matthew Wicker", " Marta Kwiatkowska"], "  Understanding the spatial arrangement and nature of real-world objects is of\nparamount importance to many complex engineering tasks, including autonomous\nnavigation. Deep learning has revolutionized state-of-the-art performance for\ntasks in 3D environments; however, relatively little is known about the\nrobustness of these approaches in an adversarial setting. The lack of\ncomprehensive analysis makes it difficult to justify deployment of 3D deep\nlearning models in real-world, safety-critical applications. In this work, we\ndevelop an algorithm for analysis of pointwise robustness of neural networks\nthat operate on 3D data. We show that current approaches presented for\nunderstanding the resilience of state-of-the-art models vastly overestimate\ntheir robustness. We then use our algorithm to evaluate an array of\nstate-of-the-art models in order to demonstrate their vulnerability to\nocclusion attacks. We show that, in the worst case, these networks can be\nreduced to 0% classification accuracy after the occlusion of at most 6.5% of\nthe occupied input space.\n"], ["2019-04-01", "http://arxiv.org/abs/1904.00689", "Defending against adversarial attacks by randomized diversification.", ["Olga Taran", " Shideh Rezaeifar", " Taras Holotyak", " Slava Voloshynovskiy"], "  The vulnerability of machine learning systems to adversarial attacks\nquestions their usage in many applications. In this paper, we propose a\nrandomized diversification as a defense strategy. We introduce a multi-channel\narchitecture in a gray-box scenario, which assumes that the architecture of the\nclassifier and the training data set are known to the attacker. The attacker\ndoes not only have access to a secret key and to the internal states of the\nsystem at the test time. The defender processes an input in multiple channels.\nEach channel introduces its own randomization in a special transform domain\nbased on a secret key shared between the training and testing stages. Such a\ntransform based randomization with a shared key preserves the gradients in\nkey-defined sub-spaces for the defender but it prevents gradient back\npropagation and the creation of various bypass systems for the attacker. An\nadditional benefit of multi-channel randomization is the aggregation that fuses\nsoft-outputs from all channels, thus increasing the reliability of the final\nscore. The sharing of a secret key creates an information advantage to the\ndefender. Experimental evaluation demonstrates an increased robustness of the\nproposed method to a number of known state-of-the-art attacks.\n"], ["2019-04-01", "http://arxiv.org/abs/1904.00887", "Adversarial Defense by Restricting the Hidden Space of Deep Neural Networks.", ["Aamir Mustafa", " Salman Khan", " Munawar Hayat", " Roland Goecke", " Jianbing Shen", " Ling Shao"], "  Deep neural networks are vulnerable to adversarial attacks, which can fool\nthem by adding minuscule perturbations to the input images. The robustness of\nexisting defenses suffers greatly under white-box attack settings, where an\nadversary has full knowledge about the network and can iterate several times to\nfind strong perturbations. We observe that the main reason for the existence of\nsuch perturbations is the close proximity of different class samples in the\nlearned feature space. This allows model decisions to be totally changed by\nadding an imperceptible perturbation in the inputs. To counter this, we propose\nto class-wise disentangle the intermediate feature representations of deep\nnetworks. Specifically, we force the features for each class to lie inside a\nconvex polytope that is maximally separated from the polytopes of other\nclasses. In this manner, the network is forced to learn distinct and distant\ndecision regions for each class. We observe that this simple constraint on the\nfeatures greatly enhances the robustness of learned models, even against the\nstrongest white-box attacks, without degrading the classification performance\non clean images. We report extensive evaluations in both black-box and\nwhite-box attack scenarios and show significant gains in comparison to\nstate-of-the art defenses.\n"], ["2019-03-31", "http://arxiv.org/abs/1904.01002", "On the Vulnerability of CNN Classifiers in EEG-Based BCIs.", ["Xiao Zhang", " Dongrui Wu"], "  Deep learning has been successfully used in numerous applications because of\nits outstanding performance and the ability to avoid manual feature\nengineering. One such application is electroencephalogram (EEG) based\nbrain-computer interface (BCI), where multiple convolutional neural network\n(CNN) models have been proposed for EEG classification. However, it has been\nfound that deep learning models can be easily fooled with adversarial examples,\nwhich are normal examples with small deliberate perturbations. This paper\nproposes an unsupervised fast gradient sign method (UFGSM) to attack three\npopular CNN classifiers in BCIs, and demonstrates its effectiveness. We also\nverify the transferability of adversarial examples in BCIs, which means we can\nperform attacks even without knowing the architecture and parameters of the\ntarget models, or the datasets they were trained on. To our knowledge, this is\nthe first study on the vulnerability of CNN classifiers in EEG-based BCIs, and\nhopefully will trigger more attention on the security of BCI systems.\n"], ["2019-03-29", "http://arxiv.org/abs/1903.12561", "Adversarial Robustness vs Model Compression, or Both?.", ["Shaokai Ye", " Kaidi Xu", " Sijia Liu", " Hao Cheng", " Jan-Henrik Lambrechts", " Huan Zhang", " Aojun Zhou", " Kaisheng Ma", " Yanzhi Wang", " Xue Lin"], "  It is well known that deep neural networks (DNNs) are vulnerable to\nadversarial attacks, which are implemented by adding crafted perturbations onto\nbenign examples. Min-max robust optimization based adversarial training can\nprovide a notion of security against adversarial attacks. However, adversarial\nrobustness requires a significantly larger capacity of the network than that\nfor the natural training with only benign examples. This paper proposes a\nframework of concurrent adversarial training and weight pruning that enables\nmodel compression while still preserving the adversarial robustness and\nessentially tackles the dilemma of adversarial training. Furthermore, this work\nstudies two hypotheses about weight pruning in the conventional setting and\nfinds that weight pruning is essential for reducing the network model size in\nthe adversarial setting, training a small model from scratch even with\ninherited initialization from the large model cannot achieve both adversarial\nrobustness and high standard accuracy. Code is available at\nhttps://github.com/yeshaokai/Robustness-Aware-Pruning-ADMM.\n"], ["2019-03-28", "http://arxiv.org/abs/1903.12261", "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations.", ["Dan Hendrycks", " Thomas Dietterich"], "  In this paper we establish rigorous benchmarks for image classifier\nrobustness. Our first benchmark, ImageNet-C, standardizes and expands the\ncorruption robustness topic, while showing which classifiers are preferable in\nsafety-critical applications. Then we propose a new dataset called ImageNet-P\nwhich enables researchers to benchmark a classifier's robustness to common\nperturbations. Unlike recent robustness research, this benchmark evaluates\nperformance on common corruptions and perturbations not worst-case adversarial\nperturbations. We find that there are negligible changes in relative corruption\nrobustness from AlexNet classifiers to ResNet classifiers. Afterward we\ndiscover ways to enhance corruption and perturbation robustness. We even find\nthat a bypassed adversarial defense provides substantial common perturbation\nrobustness. Together our benchmarks may aid future work toward networks that\nrobustly generalize.\n"], ["2019-03-28", "http://arxiv.org/abs/1903.11862", "Smooth Adversarial Examples.", ["Hanwei Zhang", " Yannis Avrithis", " Teddy Furon", " Laurent Amsaleg"], "  This paper investigates the visual quality of the adversarial examples.\nRecent papers propose to smooth the perturbations to get rid of high frequency\nartefacts. In this work, smoothing has a different meaning as it perceptually\nshapes the perturbation according to the visual content of the image to be\nattacked. The perturbation becomes locally smooth on the flat areas of the\ninput image, but it may be noisy on its textured areas and sharp across its\nedges.\n  This operation relies on Laplacian smoothing, well-known in graph signal\nprocessing, which we integrate in the attack pipeline. We benchmark several\nattacks with and without smoothing under a white-box scenario and evaluate\ntheir transferability. Despite the additional constraint of smoothness, our\nattack has the same probability of success at lower distortion.\n"], ["2019-03-27", "http://arxiv.org/abs/1903.11688", "Rallying Adversarial Techniques against Deep Learning for Network Security.", ["Joseph Clements", " Yuzhe Yang", " Ankur Sharma", " Hongxin Hu", " Yingjie Lao"], "  Recent advances in artificial intelligence and the increasing need for\npowerful defensive measures in the domain of network security, have led to the\nadoption of deep learning approaches for use in network intrusion detection\nsystems. These methods have achieved superior performance against conventional\nnetwork attacks, which enable the deployment of practical security systems to\nunique and dynamic sectors. Adversarial machine learning, unfortunately, has\nrecently shown that deep learning models are inherently vulnerable to\nadversarial modifications on their input data. Because of this susceptibility,\nthe deep learning models deployed to power a network defense could in fact be\nthe weakest entry point for compromising a network system. In this paper, we\nshow that by modifying on average as little as 1.38 of the input features, an\nadversary can generate malicious inputs which effectively fool a deep learning\nbased NIDS. Therefore, when designing such systems, it is crucial to consider\nthe performance from not only the conventional network security perspective but\nalso the adversarial machine learning domain.\n"], ["2019-03-27", "http://arxiv.org/abs/1903.11626", "Bridging Adversarial Robustness and Gradient Interpretability.", ["Beomsu Kim", " Junghoon Seo", " Taegyun Jeon"], "  Adversarial training is a training scheme designed to counter adversarial\nattacks by augmenting the training dataset with adversarial examples.\nSurprisingly, several studies have observed that loss gradients from\nadversarially trained DNNs are visually more interpretable than those from\nstandard DNNs. Although this phenomenon is interesting, there are only few\nworks that have offered an explanation. In this paper, we attempted to bridge\nthis gap between adversarial robustness and gradient interpretability. To this\nend, we identified that loss gradients from adversarially trained DNNs align\nbetter with human perception because adversarial training restricts gradients\ncloser to the image manifold. We then demonstrated that adversarial training\ncauses loss gradients to be quantitatively meaningful. Finally, we showed that\nunder the adversarial training framework, there exists an empirical trade-off\nbetween test accuracy and loss gradient interpretability and proposed two\npotential approaches to resolving this trade-off.\n"], ["2019-03-27", "http://arxiv.org/abs/1903.11508", "Text Processing Like Humans Do: Visually Attacking and Shielding NLP Systems.", ["Steffen Eger", " G\u00f6zde G\u00fcl \u015eahin", " Andreas R\u00fcckl\u00e9", " Ji-Ung Lee", " Claudia Schulz", " Mohsen Mesgar", " Krishnkant Swarnkar", " Edwin Simpson", " Iryna Gurevych"], "  Visual modifications to text are often used to obfuscate offensive comments\nin social media (e.g., \"!d10t\") or as a writing style (\"1337\" in \"leet speak\"),\namong other scenarios. We consider this as a new type of adversarial attack in\nNLP, a setting to which humans are very robust, as our experiments with both\nsimple and more difficult visual input perturbations demonstrate. We then\ninvestigate the impact of visual adversarial attacks on current NLP systems on\ncharacter-, word-, and sentence-level tasks, showing that both neural and\nnon-neural models are, in contrast to humans, extremely sensitive to such\nattacks, suffering performance decreases of up to 82\\%. We then explore three\nshielding methods---visual character embeddings, adversarial training, and\nrule-based recovery---which substantially improve the robustness of the models.\nHowever, the shielding methods still fall behind performances achieved in\nnon-attack scenarios, which demonstrates the difficulty of dealing with visual\nattacks.\n"], ["2019-03-27", "http://arxiv.org/abs/1903.11359", "Scaling up the randomized gradient-free adversarial attack reveals overestimation of robustness using established attacks.", ["Francesco Croce", " Jonas Rauber", " Matthias Hein"], "  Modern neural networks are highly non-robust against adversarial\nmanipulation. A significant amount of work has been invested in techniques to\ncompute lower bounds on robustness through formal guarantees and to build\nprovably robust models. However, it is still difficult to get guarantees for\nlarger networks or robustness against larger perturbations. Thus attack\nstrategies are needed to provide tight upper bounds on the actual robustness.\nWe significantly improve the randomized gradient-free attack for ReLU networks\n[9], in particular by scaling it up to large networks. We show that our attack\nachieves similar or significantly smaller robust accuracy than state-of-the-art\nattacks like PGD or the one of Carlini and Wagner, thus revealing an\noverestimation of the robustness by these state-of-the-art methods. Our attack\nis not based on a gradient descent scheme and in this sense gradient-free,\nwhich makes it less sensitive to the choice of hyperparameters as no careful\nselection of the stepsize is required.\n"], ["2019-03-26", "http://arxiv.org/abs/1903.11220", "On the Adversarial Robustness of Multivariate Robust Estimation.", ["Erhan Bayraktar", " Lifeng Lai"], "  In this paper, we investigate the adversarial robustness of multivariate\n$M$-Estimators. In the considered model, after observing the whole dataset, an\nadversary can modify all data points with the goal of maximizing inference\nerrors. We use adversarial influence function (AIF) to measure the asymptotic\nrate at which the adversary can change the inference result. We first\ncharacterize the adversary's optimal modification strategy and its\ncorresponding AIF. From the defender's perspective, we would like to design an\nestimator that has a small AIF. For the case of joint location and scale\nestimation problem, we characterize the optimal $M$-estimator that has the\nsmallest AIF. We further identify a tradeoff between robustness against\nadversarial modifications and robustness against outliers, and derive the\noptimal $M$-estimator that achieves the best tradeoff.\n"], ["2019-03-26", "http://arxiv.org/abs/1903.10826", "A geometry-inspired decision-based attack.", ["Yujia Liu", " Seyed-Mohsen Moosavi-Dezfooli", " Pascal Frossard"], "  Deep neural networks have recently achieved tremendous success in image\nclassification. Recent studies have however shown that they are easily misled\ninto incorrect classification decisions by adversarial examples. Adversaries\ncan even craft attacks by querying the model in black-box settings, where no\ninformation about the model is released except its final decision. Such\ndecision-based attacks usually require lots of queries, while real-world image\nrecognition systems might actually restrict the number of queries. In this\npaper, we propose qFool, a novel decision-based attack algorithm that can\ngenerate adversarial examples using a small number of queries. The qFool method\ncan drastically reduce the number of queries compared to previous\ndecision-based attacks while reaching the same quality of adversarial examples.\nWe also enhance our method by constraining adversarial perturbations in\nlow-frequency subspace, which can make qFool even more computationally\nefficient. Altogether, we manage to fool commercial image recognition systems\nwith a small number of queries, which demonstrates the actual effectiveness of\nour new algorithm in practice.\n"], ["2019-03-25", "http://arxiv.org/abs/1903.10586", "Defending against Whitebox Adversarial Attacks via Randomized Discretization.", ["Yuchen Zhang", " Percy Liang"], "  Adversarial perturbations dramatically decrease the accuracy of\nstate-of-the-art image classifiers. In this paper, we propose and analyze a\nsimple and computationally efficient defense strategy: inject random Gaussian\nnoise, discretize each pixel, and then feed the result into any pre-trained\nclassifier. Theoretically, we show that our randomized discretization strategy\nreduces the KL divergence between original and adversarial inputs, leading to a\nlower bound on the classification accuracy of any classifier against any\n(potentially whitebox) $\\ell_\\infty$-bounded adversarial attack. Empirically,\nwe evaluate our defense on adversarial examples generated by a strong iterative\nPGD attack. On ImageNet, our defense is more robust than adversarially-trained\nnetworks and the winning defenses of the NIPS 2017 Adversarial Attacks &\nDefenses competition.\n"], ["2019-03-25", "http://arxiv.org/abs/1903.10484", "Exploiting Excessive Invariance caused by Norm-Bounded Adversarial Robustness.", ["J\u00f6rn-Henrik Jacobsen", " Jens Behrmannn", " Nicholas Carlini", " Florian Tram\u00e8r", " Nicolas Papernot"], "  Adversarial examples are malicious inputs crafted to cause a model to\nmisclassify them. Their most common instantiation, \"perturbation-based\"\nadversarial examples introduce changes to the input that leave its true label\nunchanged, yet result in a different model prediction. Conversely,\n\"invariance-based\" adversarial examples insert changes to the input that leave\nthe model's prediction unaffected despite the underlying input's label having\nchanged.\n  In this paper, we demonstrate that robustness to perturbation-based\nadversarial examples is not only insufficient for general robustness, but\nworse, it can also increase vulnerability of the model to invariance-based\nadversarial examples. In addition to analytical constructions, we empirically\nstudy vision classifiers with state-of-the-art robustness to perturbation-based\nadversaries constrained by an $\\ell_p$ norm. We mount attacks that exploit\nexcessive model invariance in directions relevant to the task, which are able\nto find adversarial examples within the $\\ell_p$ ball. In fact, we find that\nclassifiers trained to be $\\ell_p$-norm robust are more vulnerable to\ninvariance-based adversarial examples than their undefended counterparts.\n  Excessive invariance is not limited to models trained to be robust to\nperturbation-based $\\ell_p$-norm adversaries. In fact, we argue that the term\nadversarial example is used to capture a series of model limitations, some of\nwhich may not have been discovered yet. Accordingly, we call for a set of\nprecise definitions that taxonomize and address each of these shortcomings in\nlearning.\n"], ["2019-03-25", "http://arxiv.org/abs/1903.10396", "The LogBarrier adversarial attack: making effective use of decision boundary information.", ["Chris Finlay", " Aram-Alexandre Pooladian", " Adam M. Oberman"], "  Adversarial attacks for image classification are small perturbations to\nimages that are designed to cause misclassification by a model. Adversarial\nattacks formally correspond to an optimization problem: find a minimum norm\nimage perturbation, constrained to cause misclassification. A number of\neffective attacks have been developed. However, to date, no gradient-based\nattacks have used best practices from the optimization literature to solve this\nconstrained minimization problem. We design a new untargeted attack, based on\nthese best practices, using the established logarithmic barrier method. On\naverage, our attack distance is similar or better than all state-of-the-art\nattacks on benchmark datasets (MNIST, CIFAR10, ImageNet-1K). In addition, our\nmethod performs significantly better on the most challenging images, those\nwhich normally require larger perturbations for misclassification. We employ\nthe LogBarrier attack on several adversarially defended models, and show that\nit adversarially perturbs all images more efficiently than other attacks: the\ndistance needed to perturb all images is significantly smaller with the\nLogBarrier attack than with other state-of-the-art attacks.\n"], ["2019-03-25", "http://arxiv.org/abs/1903.10219", "Robust Neural Networks using Randomized Adversarial Training.", ["Alexandre Araujo", " Rafael Pinot", " Benjamin Negrevergne", " Laurent Meunier", " Yann Chevaleyre", " Florian Yger", " Jamal Atif"], "  Since the discovery of adversarial examples in machine learning, researchers\nhave designed several techniques to train neural networks that are robust\nagainst different types of attacks (most notably $\\ell_\\infty$ and $\\ell_2$\nbased attacks). However, it has been observed that the defense mechanisms\ndesigned to protect against one type of attack often offer poor performance\nagainst the other. In this paper, we introduce Randomized Adversarial Training\n(RAT), a technique that is efficient both against $\\ell_2$ and $\\ell_\\infty$\nattacks. To obtain this result, we build upon adversarial training, a technique\nthat is efficient against $\\ell_\\infty$ attacks, and demonstrate that adding\nrandom noise at training and inference time further improves performance\nagainst \\ltwo attacks. We then show that RAT is as efficient as adversarial\ntraining against $\\ell_\\infty$ attacks while being robust against strong\n$\\ell_2$ attacks. Our final comparative experiments demonstrate that RAT\noutperforms all state-of-the-art approaches against $\\ell_2$ and $\\ell_\\infty$\nattacks.\n"], ["2019-03-24", "http://arxiv.org/abs/1903.10033", "A Formalization of Robustness for Deep Neural Networks.", ["Tommaso Dreossi", " Shromona Ghosh", " Alberto Sangiovanni-Vincentelli", " Sanjit A. Seshia"], "  Deep neural networks have been shown to lack robustness to small input\nperturbations. The process of generating the perturbations that expose the lack\nof robustness of neural networks is known as adversarial input generation. This\nprocess depends on the goals and capabilities of the adversary, In this paper,\nwe propose a unifying formalization of the adversarial input generation process\nfrom a formal methods perspective. We provide a definition of robustness that\nis general enough to capture different formulations. The expressiveness of our\nformalization is shown by modeling and comparing a variety of adversarial\nattack techniques.\n"], ["2019-03-24", "http://arxiv.org/abs/1903.09940", "Variational Inference with Latent Space Quantization for Adversarial Resilience.", ["Vinay Kyatham", " Mayank Mishra", " Tarun Kumar Yadav", " Deepak Mishra", " Prathosh AP"], "  Despite their tremendous success in modelling high-dimensional data\nmanifolds, deep neural networks suffer from the threat of adversarial attacks -\nExistence of perceptually valid input-like samples obtained through careful\nperturbation that lead to degradation in the performance of the underlying\nmodel. Major concerns with existing defense mechanisms include\nnon-generalizability across different attacks, models and large inference time.\nIn this paper, we propose a generalized defense mechanism capitalizing on the\nexpressive power of regularized latent space based generative models. We design\nan adversarial filter, devoid of access to classifier and adversaries, which\nmakes it usable in tandem with any classifier. The basic idea is to learn a\nLipschitz constrained mapping from the data manifold, incorporating adversarial\nperturbations, to a quantized latent space and re-map it to the true data\nmanifold. Specifically, we simultaneously auto-encode the data manifold and its\nperturbations implicitly through the perturbations of the regularized and\nquantized generative latent space, realized using variational inference. We\ndemonstrate the efficacy of the proposed formulation in providing resilience\nagainst multiple attack types (black and white box) and methods, while being\nalmost real-time. Our experiments show that the proposed method surpasses the\nstate-of-the-art techniques in several cases.\n"], ["2019-03-23", "http://arxiv.org/abs/1903.09799", "Improving Adversarial Robustness via Guided Complement Entropy.", ["Hao-Yun Chen", " Jhao-Hong Liang", " Shih-Chieh Chang", " Jia-Yu Pan", " Yu-Ting Chen", " Wei Wei", " Da-Cheng Juan"], "  Adversarial robustness has emerged as an important topic in deep learning as\ncarefully crafted attack samples can significantly disturb the performance of a\nmodel. Many recent methods have proposed to improve adversarial robustness by\nutilizing adversarial training or model distillation, which adds additional\nprocedures to model training. In this paper, we propose a new training paradigm\ncalled Guided Complement Entropy (GCE) that is capable of achieving\n\"adversarial defense for free,\" which involves no additional procedures in the\nprocess of improving adversarial robustness. In addition to maximizing model\nprobabilities on the ground-truth class like cross-entropy, we neutralize its\nprobabilities on the incorrect classes along with a \"guided\" term to balance\nbetween these two terms. We show in the experiments that our method achieves\nbetter model robustness with even better performance compared to the commonly\nused cross-entropy training objective. We also show that our method can be used\northogonally to adversarial training across well-known methods with noticeable\nrobustness gain. To the best of our knowledge, our approach is the first one\nthat improves model robustness without compromising performance.\n"], ["2019-03-22", "http://arxiv.org/abs/1903.10346", "Imperceptible, Robust, and Targeted Adversarial Examples for Automatic Speech Recognition.", ["Yao Qin", " Nicholas Carlini", " Ian Goodfellow", " Garrison Cottrell", " Colin Raffel"], "  Adversarial examples are inputs to machine learning models designed by an\nadversary to cause an incorrect output. So far, adversarial examples have been\nstudied most extensively in the image domain. In this domain, adversarial\nexamples can be constructed by imperceptibly modifying images to cause\nmisclassification, and are practical in the physical world. In contrast,\ncurrent targeted adversarial examples applied to speech recognition systems\nhave neither of these properties: humans can easily identify the adversarial\nperturbations, and they are not effective when played over-the-air. This paper\nmakes advances on both of these fronts. First, we develop effectively\nimperceptible audio adversarial examples (verified through a human study) by\nleveraging the psychoacoustic principle of auditory masking, while retaining\n100% targeted success rate on arbitrary full-sentence targets. Next, we make\nprogress towards physical-world over-the-air audio adversarial examples by\nconstructing perturbations which remain effective even after applying realistic\nsimulated environmental distortions.\n"], ["2019-03-21", "http://arxiv.org/abs/1904.00759", "Adversarial camera stickers: A physical camera-based attack on deep learning systems.", ["Juncheng Li", " Frank R. Schmidt", " J. Zico Kolter"], "  Recent work has documented the susceptibility of deep learning systems to\nadversarial examples, but most such attacks directly manipulate the digital\ninput to a classifier. Although a smaller line of work considers physical\nadversarial attacks, in all cases these involve manipulating the object of\ninterest, e.g., putting a physical sticker on an object to misclassify it, or\nmanufacturing an object specifically intended to be misclassified. In this\nwork, we consider an alternative question: is it possible to fool deep\nclassifiers, over all perceived objects of a certain type, by physically\nmanipulating the camera itself? We show that by placing a carefully crafted and\nmainly-translucent sticker over the lens of a camera, one can create universal\nperturbations of the observed images that are inconspicuous, yet misclassify\ntarget objects as a different (targeted) class. To accomplish this, we propose\nan iterative procedure for both updating the attack perturbation (to make it\nadversarial for a given classifier), and the threat model itself (to ensure it\nis physically realizable). For example, we show that we can achieve\nphysically-realizable attacks that fool ImageNet classifiers in a targeted\nfashion 49.6% of the time. This presents a new class of physically-realizable\nthreat models to consider in the context of adversarially robust machine\nlearning. Our demo video can be viewed at: https://youtu.be/wUVmL33Fx54\n"], ["2019-03-20", "http://arxiv.org/abs/1903.08778", "Provable Certificates for Adversarial Examples: Fitting a Ball in the Union of Polytopes.", ["Matt Jordan", " Justin Lewis", " Alexandros G. Dimakis"], "  We propose a novel method for computing exact pointwise robustness of deep\nneural networks for all convex $\\ell_p$ norms. Our algorithm, GeoCert, finds\nthe largest $\\ell_p$ ball centered at an input point $x_0$, within which the\noutput class of a given neural network with ReLU nonlinearities remains\nunchanged. We relate the problem of computing pointwise robustness of these\nnetworks to that of computing the maximum norm ball with a fixed center that\ncan be contained in a non-convex polytope. This is a challenging problem in\ngeneral, however we show that there exists an efficient algorithm to compute\nthis for polyhedral complices. Further we show that piecewise linear neural\nnetworks partition the input space into a polyhedral complex. Our algorithm has\nthe ability to almost immediately output a nontrivial lower bound to the\npointwise robustness which is iteratively improved until it ultimately becomes\ntight. We empirically show that our approach generates distance lower bounds\nthat are tighter compared to prior work, under moderate time constraints.\n"], ["2019-03-19", "http://arxiv.org/abs/1903.08333", "On the Robustness of Deep K-Nearest Neighbors.", ["Chawin Sitawarin", " David Wagner"], "  Despite a large amount of attention on adversarial examples, very few works\nhave demonstrated an effective defense against this threat. We examine Deep\nk-Nearest Neighbor (DkNN), a proposed defense that combines k-Nearest Neighbor\n(kNN) and deep learning to improve the model's robustness to adversarial\nexamples. It is challenging to evaluate the robustness of this scheme due to a\nlack of efficient algorithm for attacking kNN classifiers with large k and\nhigh-dimensional data. We propose a heuristic attack that allows us to use\ngradient descent to find adversarial examples for kNN classifiers, and then\napply it to attack the DkNN defense as well. Results suggest that our attack is\nmoderately stronger than any naive attack on kNN and significantly outperforms\nother attacks on DkNN.\n"], ["2019-03-18", "http://arxiv.org/abs/1903.07282", "Generating Adversarial Examples With Conditional Generative Adversarial Net.", ["Ping Yu", " Kaitao Song", " Jianfeng Lu"], "  Recently, deep neural networks have significant progress and successful\napplication in various fields, but they are found vulnerable to attack\ninstances, e.g., adversarial examples. State-of-art attack methods can generate\nattack images by adding small perturbation to the source image. These attack\nimages can fool the classifier but have little impact to human. Therefore, such\nattack instances are difficult to generate by searching the feature space. How\nto design an effective and robust generating method has become a spotlight.\nInspired by adversarial examples, we propose two novel generative models to\nproduce adaptive attack instances directly, in which conditional generative\nadversarial network is adopted and distinctive strategy is designed for\ntraining. Compared with the common method, such as Fast Gradient Sign Method,\nour models can reduce the generating cost and improve robustness and has about\none fifth running time for producing attack instance.\n"], ["2019-03-18", "http://arxiv.org/abs/1904.05734", "Practical Hidden Voice Attacks against Speech and Speaker Recognition Systems.", ["Hadi Abdullah", " Washington Garcia", " Christian Peeters", " Patrick Traynor", " Kevin R. B. Butler", " Joseph Wilson"], "  Voice Processing Systems (VPSes), now widely deployed, have been made\nsignificantly more accurate through the application of recent advances in\nmachine learning. However, adversarial machine learning has similarly advanced\nand has been used to demonstrate that VPSes are vulnerable to the injection of\nhidden commands - audio obscured by noise that is correctly recognized by a VPS\nbut not by human beings. Such attacks, though, are often highly dependent on\nwhite-box knowledge of a specific machine learning model and limited to\nspecific microphones and speakers, making their use across different acoustic\nhardware platforms (and thus their practicality) limited. In this paper, we\nbreak these dependencies and make hidden command attacks more practical through\nmodel-agnostic (blackbox) attacks, which exploit knowledge of the signal\nprocessing algorithms commonly used by VPSes to generate the data fed into\nmachine learning systems. Specifically, we exploit the fact that multiple\nsource audio samples have similar feature vectors when transformed by acoustic\nfeature extraction algorithms (e.g., FFTs). We develop four classes of\nperturbations that create unintelligible audio and test them against 12 machine\nlearning models, including 7 proprietary models (e.g., Google Speech API, Bing\nSpeech API, IBM Speech API, Azure Speaker API, etc), and demonstrate successful\nattacks against all targets. Moreover, we successfully use our maliciously\ngenerated audio samples in multiple hardware configurations, demonstrating\neffectiveness across both models and real systems. In so doing, we demonstrate\nthat domain-specific knowledge of audio signal processing represents a\npractical means of generating successful hidden voice command attacks.\n"], ["2019-03-17", "http://arxiv.org/abs/1903.07054", "Adversarial Attacks on Deep Neural Networks for Time Series Classification.", ["Hassan Ismail Fawaz", " Germain Forestier", " Jonathan Weber", " Lhassane Idoumghar", " Pierre-Alain Muller"], "  Time Series Classification (TSC) problems are encountered in many real life\ndata mining tasks ranging from medicine and security to human activity\nrecognition and food safety. With the recent success of deep neural networks in\nvarious domains such as computer vision and natural language processing,\nresearchers started adopting these techniques for solving time series data\nmining problems. However, to the best of our knowledge, no previous work has\nconsidered the vulnerability of deep learning models to adversarial time series\nexamples, which could potentially make them unreliable in situations where the\ndecision taken by the classifier is crucial such as in medicine and security.\nFor computer vision problems, such attacks have been shown to be very easy to\nperform by altering the image and adding an imperceptible amount of noise to\ntrick the network into wrongly classifying the input image. Following this line\nof work, we propose to leverage existing adversarial attack mechanisms to add a\nspecial noise to the input time series in order to decrease the network's\nconfidence when classifying instances at test time. Our results reveal that\ncurrent state-of-the-art deep learning time series classifiers are vulnerable\nto adversarial attacks which can have major consequences in multiple domains\nsuch as food safety and quality assurance.\n"], ["2019-03-15", "http://arxiv.org/abs/1903.06620", "On Evaluation of Adversarial Perturbations for Sequence-to-Sequence Models.", ["Paul Michel", " Xian Li", " Graham Neubig", " Juan Miguel Pino"], "  Adversarial examples --- perturbations to the input of a model that elicit\nlarge changes in the output --- have been shown to be an effective way of\nassessing the robustness of sequence-to-sequence (seq2seq) models. However,\nthese perturbations only indicate weaknesses in the model if they do not change\nthe input so significantly that it legitimately results in changes in the\nexpected output. This fact has largely been ignored in the evaluations of the\ngrowing body of related literature. Using the example of untargeted attacks on\nmachine translation (MT), we propose a new evaluation framework for adversarial\nattacks on seq2seq models that takes the semantic equivalence of the pre- and\npost-perturbation input into account. Using this framework, we demonstrate that\nexisting methods may not preserve meaning in general, breaking the\naforementioned assumption that source side perturbations should not result in\nchanges in the expected output. We further use this framework to demonstrate\nthat adding additional constraints on attacks allows for adversarial\nperturbations that are more meaning-preserving, but nonetheless largely change\nthe output sequence. Finally, we show that performing untargeted adversarial\ntraining with meaning-preserving attacks is beneficial to the model in terms of\nadversarial robustness, without hurting test performance. A toolkit\nimplementing our evaluation framework is released at\nhttps://github.com/pmichel31415/teapot-nlp.\n"], ["2019-03-15", "http://arxiv.org/abs/1903.06603", "On Certifying Non-uniform Bound against Adversarial Attacks.", ["Chen Liu", " Ryota Tomioka", " Volkan Cevher"], "  This work studies the robustness certification problem of neural network\nmodels, which aims to find certified adversary-free regions as large as\npossible around data points. In contrast to the existing approaches that seek\nregions bounded uniformly along all input features, we consider non-uniform\nbounds and use it to study the decision boundary of neural network models. We\nformulate our target as an optimization problem with nonlinear constraints.\nThen, a framework applicable for general feedforward neural networks is\nproposed to bound the output logits so that the relaxed problem can be solved\nby the augmented Lagrangian method. Our experiments show the non-uniform bounds\nhave larger volumes than uniform ones and the geometric similarity of the\nnon-uniform bounds gives a quantitative, data-agnostic metric of input\nfeatures' robustness. Further, compared with normal models, the robust models\nhave even larger non-uniform bounds and better interpretability.\n"], ["2019-03-14", "http://arxiv.org/abs/1903.06293", "A Research Agenda: Dynamic Models to Defend Against Correlated Attacks.", ["Ian Goodfellow"], "  In this article I describe a research agenda for securing machine learning\nmodels against adversarial inputs at test time. This article does not present\nresults but instead shares some of my thoughts about where I think that the\nfield needs to go. Modern machine learning works very well on I.I.D. data: data\nfor which each example is drawn {\\em independently} and for which the\ndistribution generating each example is {\\em identical}. When these assumptions\nare relaxed, modern machine learning can perform very poorly. When machine\nlearning is used in contexts where security is a concern, it is desirable to\ndesign models that perform well even when the input is designed by a malicious\nadversary. So far most research in this direction has focused on an adversary\nwho violates the {\\em identical} assumption, and imposes some kind of\nrestricted worst-case distribution shift. I argue that machine learning\nsecurity researchers should also address the problem of relaxing the {\\em\nindependence} assumption and that current strategies designed for robustness to\ndistribution shift will not do so. I recommend {\\em dynamic models} that change\neach time they are run as a potential solution path to this problem, and show\nan example of a simple attack using correlated data that can be mitigated by a\nsimple dynamic defense. This is not intended as a real-world security measure,\nbut as a recommendation to explore this research direction and develop more\nrealistic defenses.\n"], ["2019-03-14", "http://arxiv.org/abs/1903.05821", "Attribution-driven Causal Analysis for Detection of Adversarial Examples.", ["Susmit Jha", " Sunny Raj", " Steven Lawrence Fernandes", " Sumit Kumar Jha", " Somesh Jha", " Gunjan Verma", " Brian Jalaian", " Ananthram Swami"], "  Attribution methods have been developed to explain the decision of a machine\nlearning model on a given input. We use the Integrated Gradient method for\nfinding attributions to define the causal neighborhood of an input by\nincrementally masking high attribution features. We study the robustness of\nmachine learning models on benign and adversarial inputs in this neighborhood.\nOur study indicates that benign inputs are robust to the masking of high\nattribution features but adversarial inputs generated by the state-of-the-art\nadversarial attack methods such as DeepFool, FGSM, CW and PGD, are not robust\nto such masking. Further, our study demonstrates that this concentration of\nhigh-attribution features responsible for the incorrect decision is more\npronounced in physically realizable adversarial examples. This difference in\nattribution of benign and adversarial inputs can be used to detect adversarial\nexamples. Such a defense approach is independent of training data and attack\nmethod, and we demonstrate its effectiveness on digital and physically\nrealizable perturbations.\n"], ["2019-03-13", "http://arxiv.org/abs/1903.05543", "Adversarial attacks against Fact Extraction and VERification.", ["James Thorne", " Andreas Vlachos"], "  This paper describes a baseline for the second iteration of the Fact\nExtraction and VERification shared task (FEVER2.0) which explores the\nresilience of systems through adversarial evaluation. We present a collection\nof simple adversarial attacks against systems that participated in the first\nFEVER shared task. FEVER modeled the assessment of truthfulness of written\nclaims as a joint information retrieval and natural language inference task\nusing evidence from Wikipedia. A large number of participants made use of deep\nneural networks in their submissions to the shared task. The extent as to\nwhether such models understand language has been the subject of a number of\nrecent investigations and discussion in literature. In this paper, we present a\nsimple method of generating entailment-preserving and entailment-altering\nperturbations of instances by common patterns within the training data. We find\nthat a number of systems are greatly affected with absolute losses in\nclassification accuracy of up to $29\\%$ on the newly perturbed instances. Using\nthese newly generated instances, we construct a sample submission for the\nFEVER2.0 shared task. Addressing these types of attacks will aid in building\nmore robust fact-checking models, as well as suggest directions to expand the\ndatasets.\n"], ["2019-03-12", "http://arxiv.org/abs/1903.05157", "Simple Physical Adversarial Examples against End-to-End Autonomous Driving Models.", ["Adith Boloor", " Xin He", " Christopher Gill", " Yevgeniy Vorobeychik", " Xuan Zhang"], "  Recent advances in machine learning, especially techniques such as deep\nneural networks, are promoting a range of high-stakes applications, including\nautonomous driving, which often relies on deep learning for perception. While\ndeep learning for perception has been shown to be vulnerable to a host of\nsubtle adversarial manipulations of images, end-to-end demonstrations of\nsuccessful attacks, which manipulate the physical environment and result in\nphysical consequences, are scarce. Moreover, attacks typically involve\ncarefully constructed adversarial examples at the level of pixels. We\ndemonstrate the first end-to-end attacks on autonomous driving in simulation,\nusing simple physically realizable attacks: the painting of black lines on the\nroad. These attacks target deep neural network models for end-to-end autonomous\ndriving control. A systematic investigation shows that such attacks are\nsurprisingly easy to engineer, and we describe scenarios (e.g., right turns) in\nwhich they are highly effective, and others that are less vulnerable (e.g.,\ndriving straight). Further, we use network deconvolution to demonstrate that\nthe attacks succeed by inducing activation patterns similar to entirely\ndifferent scenarios used in training.\n"], ["2019-03-11", "http://arxiv.org/abs/1903.05994", "Can Adversarial Network Attack be Defended?.", ["Jinyin Chen", " Yangyang Wu", " Xiang Lin", " Qi Xuan"], "  Machine learning has been successfully applied to complex network analysis in\nvarious areas, and graph neural networks (GNNs) based methods outperform\nothers. Recently, adversarial attack on networks has attracted special\nattention since carefully crafted adversarial networks with slight\nperturbations on clean network may invalid lots of network applications, such\nas node classification, link prediction, and community detection etc. Such\nattacks are easily constructed with serious security threat to various analyze\nmethods, including traditional methods and deep models. To the best of our\nknowledge, it is the first time that defense method against network adversarial\nattack is discussed. In this paper, we are interested in the possibility of\ndefense against adversarial attack on network, and propose defense strategies\nfor GNNs against attacks. First, we propose novel adversarial training\nstrategies to improve GNNs' defensibility against attacks. Then, we\nanalytically investigate the robustness properties for GNNs granted by the use\nof smooth defense, and propose two special smooth defense strategies: smoothing\ndistillation and smoothing cross-entropy loss function. Both of them are\ncapable of smoothing gradient of GNNs, and consequently reduce the amplitude of\nadversarial gradients, which benefits gradient masking from attackers. The\ncomprehensive experiments show that our proposed strategies have great\ndefensibility against different adversarial attacks on four real-world networks\nin different network analyze tasks.\n"], ["2019-03-09", "http://arxiv.org/abs/1903.03905", "Manifold Preserving Adversarial Learning.", ["Ousmane Amadou Dia", " Elnaz Barshan", " Reza Babanezhad"], "  How to generate semantically meaningful and structurally sound adversarial\nexamples? We propose to answer this question by restricting the search for\nadversaries in the true data manifold. To this end, we introduce a stochastic\nvariational inference method to learn the data manifold, in the presence of\ncontinuous latent variables with intractable posterior distributions, without\nrequiring an a priori form for the data underlying distribution. We then\npropose a manifold perturbation strategy that ensures the cases we perturb\nremain in the manifold of the original examples and thereby generate the\nadversaries. We evaluate our approach on a number of image and text datasets.\nOur results show the effectiveness of our approach in producing coherent, and\nrealistic-looking adversaries that can evade strong defenses known to be\nresilient to traditional adversarial attacks\n"], ["2019-03-07", "http://arxiv.org/abs/1903.03029", "Attack Type Agnostic Perceptual Enhancement of Adversarial Images.", ["Bilgin Aksoy", " Alptekin Temizel"], "  Adversarial images are samples that are intentionally modified to deceive\nmachine learning systems. They are widely used in applications such as CAPTHAs\nto help distinguish legitimate human users from bots. However, the noise\nintroduced during the adversarial image generation process degrades the\nperceptual quality and introduces artificial colours; making it also difficult\nfor humans to classify images and recognise objects. In this letter, we propose\na method to enhance the perceptual quality of these adversarial images. The\nproposed method is attack type agnostic and could be used in association with\nthe existing attacks in the literature. Our experiments show that the generated\nadversarial images have lower Euclidean distance values while maintaining the\nsame adversarial attack performance. Distances are reduced by 5.88% to 41.27%\nwith an average reduction of 22% over the different attack and network types.\n"], ["2019-03-07", "http://arxiv.org/abs/1903.02926", "Out-domain examples for generative models.", ["Dario Pasquini", " Marco Mingione", " Massimo Bernaschi"], "  Deep generative models are being increasingly used in a wide variety of\napplications. However, the generative process is not fully predictable and at\ntimes, it produces an unexpected output. We will refer to those outputs as\nout-domain examples. In the present paper we show that an attacker can force a\npre-trained generator to reproduce an arbitrary out-domain example if fed by a\nsuitable adversarial input. The main assumption is that those outputs lie in an\nunexplored region of the generator's codomain and hence they have a very low\nprobability of being naturally generated. Moreover, we show that this\nadversarial input can be shaped so as to be statistically indistinguishable\nfrom the set of genuine inputs. The goal is to look for an efficient way of\nfinding these inputs in the generator's latent space.\n"], ["2019-03-06", "http://arxiv.org/abs/1903.02585", "GanDef: A GAN based Adversarial Training Defense for Neural Network Classifier.", ["Guanxiong Liu", " Issa Khalil", " Abdallah Khreishah"], "  Machine learning models, especially neural network (NN) classifiers, are\nwidely used in many applications including natural language processing,\ncomputer vision and cybersecurity. They provide high accuracy under the\nassumption of attack-free scenarios. However, this assumption has been defied\nby the introduction of adversarial examples -- carefully perturbed samples of\ninput that are usually misclassified. Many researchers have tried to develop a\ndefense against adversarial examples; however, we are still far from achieving\nthat goal. In this paper, we design a Generative Adversarial Net (GAN) based\nadversarial training defense, dubbed GanDef, which utilizes a competition game\nto regulate the feature selection during the training. We analytically show\nthat GanDef can train a classifier so it can defend against adversarial\nexamples. Through extensive evaluation on different white-box adversarial\nexamples, the classifier trained by GanDef shows the same level of test\naccuracy as those trained by state-of-the-art adversarial training defenses.\nMore importantly, GanDef-Comb, a variant of GanDef, could utilize the\ndiscriminator to achieve a dynamic trade-off between correctly classifying\noriginal and adversarial examples. As a result, it achieves the highest overall\ntest accuracy when the ratio of adversarial examples exceeds 41.7%.\n"], ["2019-03-05", "http://arxiv.org/abs/1903.01980", "Statistical Guarantees for the Robustness of Bayesian Neural Networks.", ["Luca Cardelli", " Marta Kwiatkowska", " Luca Laurenti", " Nicola Paoletti", " Andrea Patane", " Matthew Wicker"], "  We introduce a probabilistic robustness measure for Bayesian Neural Networks\n(BNNs), defined as the probability that, given a test point, there exists a\npoint within a bounded set such that the BNN prediction differs between the\ntwo. Such a measure can be used, for instance, to quantify the probability of\nthe existence of adversarial examples. Building on statistical verification\ntechniques for probabilistic models, we develop a framework that allows us to\nestimate probabilistic robustness for a BNN with statistical guarantees, i.e.,\nwith a priori error and confidence bounds. We provide experimental comparison\nfor several approximate BNN inference techniques on image classification tasks\nassociated to MNIST and a two-class subset of the GTSRB dataset. Our results\nenable quantification of uncertainty of BNN predictions in adversarial\nsettings.\n"], ["2019-03-05", "http://arxiv.org/abs/1903.01715", "L 1-norm double backpropagation adversarial defense.", ["Isma\u00efla LIMOS, LITIS Seck", " Ga\u00eblle LIMOS Loosli", " Stephane LITIS Canu"], "  Adversarial examples are a challenging open problem for deep neural networks.\nWe propose in this paper to add a penalization term that forces the decision\nfunction to be at in some regions of the input space, such that it becomes, at\nleast locally, less sensitive to attacks. Our proposition is theoretically\nmotivated and shows on a first set of carefully conducted experiments that it\nbehaves as expected when used alone, and seems promising when coupled with\nadversarial training.\n"], ["2019-03-04", "http://arxiv.org/abs/1903.01612", "Defense Against Adversarial Images using Web-Scale Nearest-Neighbor Search.", ["Abhimanyu Dubey", " der Maaten Laurens van", " Zeki Yalniz", " Yixuan Li", " Dhruv Mahajan"], "  A plethora of recent work has shown that convolutional networks are not\nrobust to adversarial images: images that are created by perturbing a sample\nfrom the data distribution as to maximize the loss on the perturbed example. In\nthis work, we hypothesize that adversarial perturbations move the image away\nfrom the image manifold in the sense that there exists no physical process that\ncould have produced the adversarial image. This hypothesis suggests that a\nsuccessful defense mechanism against adversarial images should aim to project\nthe images back onto the image manifold. We study such defense mechanisms,\nwhich approximate the projection onto the unknown image manifold by a\nnearest-neighbor search against a web-scale image database containing tens of\nbillions of images. Empirical evaluations of this defense strategy on ImageNet\nsuggest that it is very effective in attack settings in which the adversary\ndoes not have access to the image database. We also propose two novel attack\nmethods to break nearest-neighbor defenses, and demonstrate conditions under\nwhich nearest-neighbor defense fails. We perform a series of ablation\nexperiments, which suggest that there is a trade-off between robustness and\naccuracy in our defenses, that a large image database (with hundreds of\nmillions of images) is crucial to get good performance, and that careful\nconstruction the image database is important to be robust against attacks\ntailored to circumvent our defenses.\n"], ["2019-03-04", "http://arxiv.org/abs/1903.01610", "The Vulnerabilities of Graph Convolutional Networks: Stronger Attacks and Defensive Techniques.", ["Huijun Wu", " Chen Wang", " Yuriy Tyshetskiy", " Andrew Dotcherty", " Kai Lu", " Liming Zhu"], "  Graph deep learning models, such as graph convolutional networks (GCN)\nachieve remarkable performance for tasks on graph data. Similar to other types\nof deep models, graph deep learning models often suffer from adversarial\nattacks. However, compared with non-graph data, the discrete features, graph\nconnections and different definitions of imperceptible perturbations bring\nunique challenges and opportunities for the adversarial attacks and defences\nfor graph data. In this paper, we propose both attack and defence techniques.\nFor attack, we show that the discrete feature problem could easily be resolved\nby introducing integrated gradients which could accurately reflect the effect\nof perturbing certain features or edges while still benefiting from the\nparallel computations. For defence, we propose to partially learn the adjacency\nmatrix to integrate the information of distant nodes so that the prediction of\na certain target is supported by more global graph information rather than just\nfew neighbour nodes. This, therefore, makes the attacks harder since one need\nto perturb more features/edges to make the attacks succeed. Our experiments on\na number of datasets show the effectiveness of the proposed methods.\n"], ["2019-03-04", "http://arxiv.org/abs/1903.01287", "Safety Verification and Robustness Analysis of Neural Networks via Quadratic Constraints and Semidefinite Programming.", ["Mahyar Fazlyab", " Manfred Morari", " George J. Pappas"], "  Analyzing the robustness of neural networks against norm-bounded\nuncertainties and adversarial attacks has found many applications ranging from\nsafety verification to robust training. In this paper, we propose a\nsemidefinite programming (SDP) framework for safety verification and robustness\nanalysis of neural networks with general activation functions. Our main idea is\nto abstract various properties of activation functions (e.g., monotonicity,\nbounded slope, bounded values, and repetition across layers) with the formalism\nof quadratic constraints. We then analyze the safety properties of the\nabstracted network via the S-procedure and semidefinite programming. Compared\nto other semidefinite relaxations proposed in the literature, our method is\nless conservative, especially for deep networks, with an order of magnitude\nreduction in computational complexity. Furthermore, our approach is applicable\nto any activation functions.\n"], ["2019-03-04", "http://arxiv.org/abs/1903.01182", "Complement Objective Training.", ["Hao-Yun Chen", " Pei-Hsin Wang", " Chun-Hao Liu", " Shih-Chieh Chang", " Jia-Yu Pan", " Yu-Ting Chen", " Wei Wei", " Da-Cheng Juan"], "  Learning with a primary objective, such as softmax cross entropy for\nclassification and sequence generation, has been the norm for training deep\nneural networks for years. Although being a widely-adopted approach, using\ncross entropy as the primary objective exploits mostly the information from the\nground-truth class for maximizing data likelihood, and largely ignores\ninformation from the complement (incorrect) classes. We argue that, in addition\nto the primary objective, training also using a complement objective that\nleverages information from the complement classes can be effective in improving\nmodel performance. This motivates us to study a new training paradigm that\nmaximizes the likelihood of the groundtruth class while neutralizing the\nprobabilities of the complement classes. We conduct extensive experiments on\nmultiple tasks ranging from computer vision to natural language understanding.\nThe experimental results confirm that, compared to the conventional training\nwith just one primary objective, training also with the complement objective\nfurther improves the performance of the state-of-the-art models across all\ntasks. In addition to the accuracy improvement, we also show that models\ntrained with both primary and complement objectives are more robust to\nsingle-step adversarial attacks.\n"], ["2019-03-03", "http://arxiv.org/abs/1903.01015", "A Kernelized Manifold Mapping to Diminish the Effect of Adversarial Perturbations.", ["Saeid Asgari Taghanaki", " Kumar Abhishek", " Shekoofeh Azizi", " Ghassan Hamarneh"], "  The linear and non-flexible nature of deep convolutional models makes them\nvulnerable to carefully crafted adversarial perturbations. To tackle this\nproblem, we propose a non-linear radial basis convolutional feature mapping by\nlearning a Mahalanobis-like distance function. Our method then maps the\nconvolutional features onto a linearly well-separated manifold, which prevents\nsmall adversarial perturbations from forcing a sample to cross the decision\nboundary. We test the proposed method on three publicly available image\nclassification and segmentation datasets namely, MNIST, ISBI ISIC 2017 skin\nlesion segmentation, and NIH Chest X-Ray-14. We evaluate the robustness of our\nmethod to different gradient (targeted and untargeted) and non-gradient based\nattacks and compare it to several non-gradient masking defense strategies. Our\nresults demonstrate that the proposed method can increase the resilience of\ndeep convolutional neural networks to adversarial perturbations without\naccuracy drop on clean data.\n"], ["2019-03-01", "http://arxiv.org/abs/1903.01563", "Evaluating Adversarial Evasion Attacks in the Context of Wireless Communications.", ["Bryse Flowers", " R. Michael Buehrer", " William C. Headley"], "  Recent advancements in radio frequency machine learning (RFML) have\ndemonstrated the use of raw in-phase and quadrature (IQ) samples for multiple\nspectrum sensing tasks. Yet, deep learning techniques have been shown, in other\napplications, to be vulnerable to adversarial machine learning (ML) techniques,\nwhich seek to craft small perturbations that are added to the input to cause a\nmisclassification. The current work differentiates the threats that adversarial\nML poses to RFML systems based on where the attack is executed from: direct\naccess to classifier input, synchronously transmitted over the air (OTA), or\nasynchronously transmitted from a separate device. Additionally, the current\nwork develops a methodology for evaluating adversarial success in the context\nof wireless communications, where the primary metric of interest is bit error\nrate and not human perception, as is the case in image recognition. The\nmethodology is demonstrated using the well known Fast Gradient Sign Method to\nevaluate the vulnerabilities of raw IQ based Automatic Modulation\nClassification and concludes RFML is vulnerable to adversarial examples, even\nin OTA attacks. However, RFML domain specific receiver effects, which would be\nencountered in an OTA attack, can present significant impairments to\nadversarial evasion.\n"], ["2019-03-01", "http://arxiv.org/abs/1903.00585", "PuVAE: A Variational Autoencoder to Purify Adversarial Examples.", ["Uiwon Hwang", " Jaewoo Park", " Hyemi Jang", " Sungroh Yoon", " Nam Ik Cho"], "  Deep neural networks are widely used and exhibit excellent performance in\nmany areas. However, they are vulnerable to adversarial attacks that compromise\nthe network at the inference time by applying elaborately designed perturbation\nto input data. Although several defense methods have been proposed to address\nspecific attacks, other attack methods can circumvent these defense mechanisms.\nTherefore, we propose Purifying Variational Autoencoder (PuVAE), a method to\npurify adversarial examples. The proposed method eliminates an adversarial\nperturbation by projecting an adversarial example on the manifold of each\nclass, and determines the closest projection as a purified sample. We\nexperimentally illustrate the robustness of PuVAE against various attack\nmethods without any prior knowledge. In our experiments, the proposed method\nexhibits performances competitive with state-of-the-art defense methods, and\nthe inference time is approximately 130 times faster than that of Defense-GAN\nthat is the state-of-the art purifier model.\n"], ["2019-03-01", "http://arxiv.org/abs/1903.00553", "Attacking Graph-based Classification via Manipulating the Graph Structure.", ["Binghui Wang", " Neil Zhenqiang Gong"], "  Graph-based classification methods are widely used for security and privacy\nanalytics. Roughly speaking, graph-based classification methods include\ncollective classification and graph neural network. Evading a graph-based\nclassification method enables an attacker to evade detection in security\nanalytics and can be used as a privacy defense against inference attacks.\nExisting adversarial machine learning studies mainly focused on machine\nlearning for non-graph data. Only a few recent studies touched adversarial\ngraph-based classification methods. However, they focused on graph neural\nnetwork methods, leaving adversarial collective classification largely\nunexplored. We aim to bridge this gap in this work. We first propose a threat\nmodel to characterize the attack surface of a collective classification method.\nSpecifically, we characterize an attacker's background knowledge along three\ndimensions: parameters of the method, training dataset, and the complete graph;\nan attacker's goal is to evade detection via manipulating the graph structure.\nWe formulate our attack as a graph-based optimization problem, solving which\nproduces the edges that an attacker needs to manipulate to achieve its attack\ngoal. Moreover, we propose several approximation techniques to solve the\noptimization problem. We evaluate our attacks and compare them with a recent\nattack designed for graph neural networks. Results show that our attacks 1) can\neffectively evade graph-based classification methods; 2) do not require access\nto the true parameters, true training dataset, and/or complete graph; and 3)\noutperform the existing attack for evading collective classification methods\nand some graph neural network methods. We also apply our attacks to evade Sybil\ndetection using a large-scale Twitter dataset and apply our attacks as a\ndefense against attribute inference attacks using a large-scale Google+\ndataset.\n"], ["2019-02-28", "http://arxiv.org/abs/1903.00073", "On the Effectiveness of Low Frequency Perturbations.", ["Yash Sharma", " Gavin Weiguang Ding", " Marcus Brubaker"], "  Carefully crafted, often imperceptible, adversarial perturbations have been\nshown to cause state-of-the-art models to yield extremely inaccurate outputs,\nrendering them unsuitable for safety-critical application domains. In addition,\nrecent work has shown that constraining the attack space to a low frequency\nregime is particularly effective. Yet, it remains unclear whether this is due\nto generally constraining the attack search space or specifically removing high\nfrequency components from consideration. By systematically controlling the\nfrequency components of the perturbation, evaluating against the top-placing\ndefense submissions in the NeurIPS 2017 competition, we empirically show that\nperformance improvements in both optimization and generalization are yielded\nonly when low frequency components are preserved. In fact, the defended models\nbased on (ensemble) adversarial training are roughly as vulnerable to low\nfrequency perturbations as undefended models, suggesting that the purported\nrobustness of proposed defenses is reliant upon adversarial perturbations being\nhigh frequency in nature. We do find that under $\\ell_\\infty$\n$\\epsilon=16/255$, a commonly used distortion bound, low frequency\nperturbations are indeed perceptible. This questions the use of the\n$\\ell_\\infty$-norm, in particular, as a distortion metric, and suggests that\nexplicitly considering the frequency space is promising for learning robust\nmodels which better align with human perception.\n"], ["2019-02-28", "http://arxiv.org/abs/1902.11029", "Enhancing the Robustness of Deep Neural Networks by Boundary Conditional GAN.", ["Ke Sun", " Zhanxing Zhu", " Zhouchen Lin"], "  Deep neural networks have been widely deployed in various machine learning\ntasks. However, recent works have demonstrated that they are vulnerable to\nadversarial examples: carefully crafted small perturbations to cause\nmisclassification by the network. In this work, we propose a novel defense\nmechanism called Boundary Conditional GAN to enhance the robustness of deep\nneural networks against adversarial examples. Boundary Conditional GAN, a\nmodified version of Conditional GAN, can generate boundary samples with true\nlabels near the decision boundary of a pre-trained classifier. These boundary\nsamples are fed to the pre-trained classifier as data augmentation to make the\ndecision boundary more robust. We empirically show that the model improved by\nour approach consistently defenses against various types of adversarial attacks\nsuccessfully. Further quantitative investigations about the improvement of\nrobustness and visualization of decision boundaries are also provided to\njustify the effectiveness of our strategy. This new defense mechanism that uses\nboundary samples to enhance the robustness of networks opens up a new way to\ndefense adversarial attacks consistently.\n"], ["2019-02-28", "http://arxiv.org/abs/1902.11019", "Towards Understanding Adversarial Examples Systematically: Exploring Data Size, Task and Model Factors.", ["Ke Sun", " Zhanxing Zhu", " Zhouchen Lin"], "  Most previous works usually explained adversarial examples from several\nspecific perspectives, lacking relatively integral comprehension about this\nproblem. In this paper, we present a systematic study on adversarial examples\nfrom three aspects: the amount of training data, task-dependent and\nmodel-specific factors. Particularly, we show that adversarial generalization\n(i.e. test accuracy on adversarial examples) for standard training requires\nmore data than standard generalization (i.e. test accuracy on clean examples);\nand uncover the global relationship between generalization and robustness with\nrespect to the data size especially when data is augmented by generative\nmodels. This reveals the trade-off correlation between standard generalization\nand robustness in limited training data regime and their consistency when data\nsize is large enough. Furthermore, we explore how different task-dependent and\nmodel-specific factors influence the vulnerability of deep neural networks by\nextensive empirical analysis. Relevant recommendations on defense against\nadversarial attacks are provided as well. Our results outline a potential path\ntowards the luminous and systematic understanding of adversarial examples.\n"], ["2019-02-28", "http://arxiv.org/abs/1902.10899", "Adversarial Attack and Defense on Point Sets.", ["Jiancheng Yang", " Qiang Zhang", " Rongyao Fang", " Bingbing Ni", " Jinxian Liu", " Qi Tian"], "  Emergence of the utility of 3D point cloud data in critical vision tasks\n(e.g., ADAS) urges researchers to pay more attention to the robustness of 3D\nrepresentations and deep networks. To this end, we develop an attack and\ndefense scheme, dedicated to 3D point cloud data, for preventing 3D point\nclouds from manipulated as well as pursuing noise-tolerable 3D representation.\nA set of novel 3D point cloud attack operations are proposed via pointwise\ngradient perturbation and adversarial point attachment / detachment. We then\ndevelop a flexible perturbation-measurement scheme for 3D point cloud data to\ndetect potential attack data or noisy sensing data. Extensive experimental\nresults on common point cloud benchmarks demonstrate the validity of the\nproposed 3D attack and defense framework.\n"], ["2019-02-27", "http://arxiv.org/abs/1902.10758", "Stochastically Rank-Regularized Tensor Regression Networks.", ["Arinbj\u00f6rn Kolbeinsson", " Jean Kossaifi", " Yannis Panagakis", " Anima Anandkumar", " Ioanna Tzoulaki", " Paul Matthews"], "  Over-parametrization of deep neural networks has recently been shown to be\nkey to their successful training. However, it also renders them prone to\noverfitting and makes them expensive to store and train. Tensor regression\nnetworks significantly reduce the number of effective parameters in deep neural\nnetworks while retaining accuracy and the ease of training. They replace the\nflattening and fully-connected layers with a tensor regression layer, where the\nregression weights are expressed through the factors of a low-rank tensor\ndecomposition. In this paper, to further improve tensor regression networks, we\npropose a novel stochastic rank-regularization. It consists of a novel\nrandomized tensor sketching method to approximate the weights of tensor\nregression layers. We theoretically and empirically establish the link between\nour proposed stochastic rank-regularization and the dropout on low-rank tensor\nregression. Extensive experimental results with both synthetic data and real\nworld datasets (i.e., CIFAR-100 and the UK Biobank brain MRI dataset) support\nthat the proposed approach i) improves performance in both classification and\nregression tasks, ii) decreases overfitting, iii) leads to more stable training\nand iv) improves robustness to adversarial attacks and random noise.\n"], ["2019-02-27", "http://arxiv.org/abs/1902.10755", "Adversarial Attacks on Time Series.", ["Fazle Karim", " Somshubra Majumdar", " Houshang Darabi"], "  Time series classification models have been garnering significant importance\nin the research community. However, not much research has been done on\ngenerating adversarial samples for these models. These adversarial samples can\nbecome a security concern. In this paper, we propose utilizing an adversarial\ntransformation network (ATN) on a distilled model to attack various time series\nclassification models. The proposed attack on the classification model utilizes\na distilled model as a surrogate that mimics the behavior of the attacked\nclassical time series classification models. Our proposed methodology is\napplied onto 1-Nearest Neighbor Dynamic Time Warping (1-NN ) DTW, a Fully\nConnected Network and a Fully Convolutional Network (FCN), all of which are\ntrained on 42 University of California Riverside (UCR) datasets. In this paper,\nwe show both models were susceptible to attacks on all 42 datasets. To the best\nof our knowledge, such an attack on time series classification models has never\nbeen done before. Finally, we recommend future researchers that develop time\nseries classification models to incorporating adversarial data samples into\ntheir training data sets to improve resilience on adversarial samples and to\nconsider model robustness as an evaluative metric.\n"], ["2019-02-27", "http://arxiv.org/abs/1902.10674", "Communication without Interception: Defense against Deep-Learning-based Modulation Detection.", ["Muhammad Zaid Hameed", " Andras Gyorgy", " Deniz Gunduz"], "  We consider a communication scenario, in which an intruder, employing a deep\nneural network (DNN), tries to determine the modulation scheme of the\nintercepted signal. Our aim is to minimize the accuracy of the intruder, while\nguaranteeing that the intended receiver can still recover the underlying\nmessage with the highest reliability. This is achieved by constellation\nperturbation at the encoder, similarly to adversarial attacks against DNN-based\nclassifiers. In the latter perturbation is limited to be imperceptible to a\nhuman observer, while in our case perturbation is constrained so that the\nmessage can still be reliably decoded by the legitimate receiver which is\noblivious to the perturbation. Simulation results demonstrate the viability of\nour approach to make wireless communication secure against DNN-based intruders\nwith minimal sacrifice in the communication performance.\n"], ["2019-02-27", "http://arxiv.org/abs/1902.10660", "Robust Decision Trees Against Adversarial Examples.", ["Hongge Chen", " Huan Zhang", " Duane Boning", " Cho-Jui Hsieh"], "  Although adversarial examples and model robustness have been extensively\nstudied in the context of linear models and neural networks, research on this\nissue in tree-based models and how to make tree-based models robust against\nadversarial examples is still limited. In this paper, we show that tree based\nmodels are also vulnerable to adversarial examples and develop a novel\nalgorithm to learn robust trees. At its core, our method aims to optimize the\nperformance under the worst-case perturbation of input features, which leads to\na max-min saddle point problem. Incorporating this saddle point objective into\nthe decision tree building procedure is non-trivial due to the discrete nature\nof trees --- a naive approach to finding the best split according to this\nsaddle point objective will take exponential time. To make our approach\npractical and scalable, we propose efficient tree building algorithms by\napproximating the inner minimizer in this saddle point problem, and present\nefficient implementations for classical information gain based trees as well as\nstate-of-the-art tree boosting models such as XGBoost. Experimental results on\nreal world datasets demonstrate that the proposed algorithms can substantially\nimprove the robustness of tree-based models against adversarial examples.\n"], ["2019-02-26", "http://arxiv.org/abs/1902.11134", "Disentangled Deep Autoencoding Regularization for Robust Image Classification.", ["Zhenyu Duan", " Martin Renqiang Min", " Li Erran Li", " Mingbo Cai", " Yi Xu", " Bingbing Ni"], "  In spite of achieving revolutionary successes in machine learning, deep\nconvolutional neural networks have been recently found to be vulnerable to\nadversarial attacks and difficult to generalize to novel test images with\nreasonably large geometric transformations. Inspired by a recent neuroscience\ndiscovery revealing that primate brain employs disentangled shape and\nappearance representations for object recognition, we propose a general\ndisentangled deep autoencoding regularization framework that can be easily\napplied to any deep embedding based classification model for improving the\nrobustness of deep neural networks. Our framework effectively learns\ndisentangled appearance code and geometric code for robust image\nclassification, which is the first disentangling based method defending against\nadversarial attacks and complementary to standard defense methods. Extensive\nexperiments on several benchmark datasets show that, our proposed\nregularization framework leveraging disentangled embedding significantly\noutperforms traditional unregularized convolutional neural networks for image\nclassification on robustness against adversarial attacks and generalization to\nnovel test data.\n"], ["2019-02-26", "http://arxiv.org/abs/1902.09866", "Analyzing Deep Neural Networks with Symbolic Propagation: Towards Higher Precision and Faster Verification.", ["Pengfei Yang", " Jiangchao Liu", " Jianlin Li", " Liqian Chen", " Xiaowei Huang"], "  Deep neural networks (DNNs) have been shown lack of robustness for the\nvulnerability of their classification to small perturbations on the inputs.\nThis has led to safety concerns of applying DNNs to safety-critical domains.\nSeveral verification approaches have been developed to automatically prove or\ndisprove safety properties of DNNs. However, these approaches suffer from\neither the scalability problem, i.e., only small DNNs can be handled, or the\nprecision problem, i.e., the obtained bounds are loose. This paper improves on\na recent proposal of analyzing DNNs through the classic abstract interpretation\ntechnique, by a novel symbolic propagation technique. More specifically, the\nvalues of neurons are represented symbolically and propagated forwardly from\nthe input layer to the output layer, on top of abstract domains. We show that\nour approach can achieve significantly higher precision and thus can prove more\nproperties than using only abstract domains. Moreover, we show that the bounds\nderived from our approach on the hidden neurons, when applied to a\nstate-of-the-art SMT based verification tool, can improve its performance. We\nimplement our approach into a software tool and validate it over a few DNNs\ntrained on benchmark datasets such as MNIST, etc.\n"], ["2019-02-25", "http://arxiv.org/abs/1902.09592", "Verification of Non-Linear Specifications for Neural Networks.", ["Chongli Dj Qin", " Dj Krishnamurthy", " Dvijotham", " Brendan O'Donoghue", " Rudy Bunel", " Robert Stanforth", " Sven Gowal", " Jonathan Uesato", " Grzegorz Swirszcz", " Pushmeet Kohli"], "  Prior work on neural network verification has focused on specifications that\nare linear functions of the output of the network, e.g., invariance of the\nclassifier output under adversarial perturbations of the input. In this paper,\nwe extend verification algorithms to be able to certify richer properties of\nneural networks. To do this we introduce the class of convex-relaxable\nspecifications, which constitute nonlinear specifications that can be verified\nusing a convex relaxation. We show that a number of important properties of\ninterest can be modeled within this class, including conservation of energy in\na learned dynamics model of a physical system; semantic consistency of a\nclassifier's output labels under adversarial perturbations and bounding errors\nin a system that predicts the summation of handwritten digits. Our experimental\nevaluation shows that our method is able to effectively verify these\nspecifications. Moreover, our evaluation exposes the failure modes in models\nwhich cannot be verified to satisfy these specifications. Thus, emphasizing the\nimportance of training models not just to fit training data but also to be\nconsistent with specifications.\n"], ["2019-02-25", "http://arxiv.org/abs/1902.09286", "Adversarial attacks hidden in plain sight.", ["Jan Philip G\u00f6pfert", " Andr\u00e9 Artelt", " Heiko Wersing", " Barbara Hammer"], "  Convolutional neural networks have been used to achieve a string of successes\nduring recent years, but their lack of interpretability remains a serious\nissue. Adversarial examples are designed to deliberately fool neural networks\ninto making any desired incorrect classification, potentially with very high\ncertainty. Several defensive approaches increase robustness against adversarial\nattacks, demanding attacks of greater magnitude, which lead to visible\nartifacts. By considering human visual perception, we compose a technique that\nallows to hide such adversarial attacks in regions of high complexity, such\nthat they are imperceptible even to an astute observer. We carry out a user\nstudy on classifying adversarially modified images to validate the perceptual\nquality of our approach and find significant evidence for its concealment with\nregards to human visual perception.\n"], ["2019-02-24", "http://arxiv.org/abs/1902.08909", "MaskDGA: A Black-box Evasion Technique Against DGA Classifiers and Adversarial Defenses.", ["Lior Sidi", " Asaf Nadler", " Asaf Shabtai"], "  Domain generation algorithms (DGAs) are commonly used by botnets to generate\ndomain names through which bots can establish a resilient communication channel\nwith their command and control servers. Recent publications presented deep\nlearning, character-level classifiers that are able to detect algorithmically\ngenerated domain (AGD) names with high accuracy, and correspondingly,\nsignificantly reduce the effectiveness of DGAs for botnet communication. In\nthis paper we present MaskDGA, a practical adversarial learning technique that\nadds perturbation to the character-level representation of algorithmically\ngenerated domain names in order to evade DGA classifiers, without the attacker\nhaving any knowledge about the DGA classifier's architecture and parameters.\nMaskDGA was evaluated using the DMD-2018 dataset of AGD names and four recently\npublished DGA classifiers, in which the average F1-score of the classifiers\ndegrades from 0.977 to 0.495 when applying the evasion technique. An additional\nevaluation was conducted using the same classifiers but with adversarial\ndefenses implemented: adversarial re-training and distillation. The results of\nthis evaluation show that MaskDGA can be used for improving the robustness of\nthe character-level DGA classifiers against adversarial attacks, but that\nideally DGA classifiers should incorporate additional features alongside\ncharacter-level features that are demonstrated in this study to be vulnerable\nto adversarial attacks.\n"], ["2019-02-24", "http://arxiv.org/abs/1902.09062", "Adversarial Reinforcement Learning under Partial Observability in Software-Defined Networking.", ["Yi Han", " David Hubczenko", " Paul Montague", " Vel Olivier De", " Tamas Abraham", " Benjamin I. P. Rubinstein", " Christopher Leckie", " Tansu Alpcan", " Sarah Erfani"], "  Recent studies have demonstrated that reinforcement learning (RL) agents are\nsusceptible to adversarial manipulation, similar to vulnerabilities previously\ndemonstrated in the supervised setting. Accordingly focus has remained with\ncomputer vision, and full observability. This paper focuses on reinforcement\nlearning in the context of autonomous defence in Software-Defined Networking\n(SDN). We demonstrate that causative attacks---attacks that target the training\nprocess---can poison RL agents even if the attacker only has partial\nobservability of the environment. In addition, we propose an inversion defence\nmethod that aims to apply the opposite perturbation to that which an attacker\nmight use to generate their adversarial samples. Our experimental results\nillustrate that the countermeasure can effectively reduce the impact of the\ncausative attack, while not significantly affecting the training process in\nnon-attack scenarios.\n"], ["2019-02-23", "http://arxiv.org/abs/1902.08832", "Re-evaluating ADEM: A Deeper Look at Scoring Dialogue Responses.", ["Ananya B. Sai", " Mithun Das Gupta", " Mitesh M. Khapra", " Mukundhan Srinivasan"], "  Automatically evaluating the quality of dialogue responses for unstructured\ndomains is a challenging problem. ADEM(Lowe et al. 2017) formulated the\nautomatic evaluation of dialogue systems as a learning problem and showed that\nsuch a model was able to predict responses which correlate significantly with\nhuman judgements, both at utterance and system level. Their system was shown to\nhave beaten word-overlap metrics such as BLEU with large margins. We start with\nthe question of whether an adversary can game the ADEM model. We design a\nbattery of targeted attacks at the neural network based ADEM evaluation system\nand show that automatic evaluation of dialogue systems still has a long way to\ngo. ADEM can get confused with a variation as simple as reversing the word\norder in the text! We report experiments on several such adversarial scenarios\nthat draw out counterintuitive scores on the dialogue responses. We take a\nsystematic look at the scoring function proposed by ADEM and connect it to\nlinear system theory to predict the shortcomings evident in the system. We also\ndevise an attack that can fool such a system to rate a response generation\nsystem as favorable. Finally, we allude to future research directions of using\nthe adversarial attacks to design a truly automated dialogue evaluation system.\n"], ["2019-02-23", "http://arxiv.org/abs/1902.08785", "A Deep, Information-theoretic Framework for Robust Biometric Recognition.", ["Renjie Xie", " Yanzhi Chen", " Yan Wo", " Qiao Wang"], "  Deep neural networks (DNN) have been a de facto standard for nowadays\nbiometric recognition solutions. A serious, but still overlooked problem in\nthese DNN-based recognition systems is their vulnerability against adversarial\nattacks. Adversarial attacks can easily cause the output of a DNN system to\ngreatly distort with only tiny changes in its input. Such distortions can\npotentially lead to an unexpected match between a valid biometric and a\nsynthetic one constructed by a strategic attacker, raising security issue. In\nthis work, we show how this issue can be resolved by learning robust biometric\nfeatures through a deep, information-theoretic framework, which builds upon the\nrecent deep variational information bottleneck method but is carefully adapted\nto biometric recognition tasks. Empirical evaluation demonstrates that our\nmethod not only offers stronger robustness against adversarial attacks but also\nprovides better recognition performance over state-of-the-art approaches.\n"], ["2019-02-22", "http://arxiv.org/abs/1902.08412", "Adversarial Attacks on Graph Neural Networks via Meta Learning.", ["Daniel Z\u00fcgner", " Stephan G\u00fcnnemann"], "  Deep learning models for graphs have advanced the state of the art on many\ntasks. Despite their recent success, little is known about their robustness. We\ninvestigate training time attacks on graph neural networks for node\nclassification that perturb the discrete graph structure. Our core principle is\nto use meta-gradients to solve the bilevel problem underlying training-time\nattacks, essentially treating the graph as a hyperparameter to optimize. Our\nexperiments show that small graph perturbations consistently lead to a strong\ndecrease in performance for graph convolutional networks, and even transfer to\nunsupervised embeddings. Remarkably, the perturbations created by our algorithm\ncan misguide the graph neural networks such that they perform worse than a\nsimple baseline that ignores all relational information. Our attacks do not\nassume any knowledge about or access to the target classifiers.\n"], ["2019-02-22", "http://arxiv.org/abs/1902.08391", "Physical Adversarial Attacks Against End-to-End Autoencoder Communication Systems.", ["Meysam Sadeghi", " Erik G. Larsson"], "  We show that end-to-end learning of communication systems through deep neural\nnetwork (DNN) autoencoders can be extremely vulnerable to physical adversarial\nattacks. Specifically, we elaborate how an attacker can craft effective\nphysical black-box adversarial attacks. Due to the openness (broadcast nature)\nof the wireless channel, an adversary transmitter can increase the\nblock-error-rate of a communication system by orders of magnitude by\ntransmitting a well-designed perturbation signal over the channel. We reveal\nthat the adversarial attacks are more destructive than jamming attacks. We also\nshow that classical coding schemes are more robust than autoencoders against\nboth adversarial and jamming attacks. The codes are available at [1].\n"], ["2019-02-22", "http://arxiv.org/abs/1902.08722", "A Convex Relaxation Barrier to Tight Robustness Verification of Neural Networks.", ["Hadi Salman", " Greg Yang", " Huan Zhang", " Cho-Jui Hsieh", " Pengchuan Zhang"], "  Verification of neural networks enables us to gauge their robustness against\nadversarial attacks. Verification algorithms fall into two categories: exact\nverifiers that run in exponential time and relaxed verifiers that are efficient\nbut incomplete. In this paper, we unify all existing LP-relaxed verifiers, to\nthe best of our knowledge, under a general convex relaxation framework. This\nframework works for neural networks with diverse architectures and\nnonlinearities and covers both primal and dual views of robustness\nverification. We further prove strong duality between the primal and dual\nproblems under very mild conditions. Next, we perform large-scale experiments,\namounting to more than 22 CPU-years, to obtain exact solution to the\nconvex-relaxed problem that is optimal within our framework for ReLU networks.\nWe find the exact solution does not significantly improve upon the gap between\nPGD and existing relaxed verifiers for various networks trained normally or\nrobustly on MNIST and CIFAR datasets. Our results suggest there is an inherent\nbarrier to tight verification for the large class of methods captured by our\nframework. We discuss possible causes of this barrier and potential future\ndirections for bypassing it.\n"], ["2019-02-21", "http://arxiv.org/abs/1902.08336", "On the Sensitivity of Adversarial Robustness to Input Data Distributions.", ["Gavin Weiguang Ding", " Kry Yik Chau Lui", " Xiaomeng Jin", " Luyu Wang", " Ruitong Huang"], "  Neural networks are vulnerable to small adversarial perturbations. Existing\nliterature largely focused on understanding and mitigating the vulnerability of\nlearned models. In this paper, we demonstrate an intriguing phenomenon about\nthe most popular robust training method in the literature, adversarial\ntraining: Adversarial robustness, unlike clean accuracy, is sensitive to the\ninput data distribution. Even a semantics-preserving transformations on the\ninput data distribution can cause a significantly different robustness for the\nadversarial trained model that is both trained and evaluated on the new\ndistribution. Our discovery of such sensitivity on data distribution is based\non a study which disentangles the behaviors of clean accuracy and robust\naccuracy of the Bayes classifier. Empirical investigations further confirm our\nfinding. We construct semantically-identical variants for MNIST and CIFAR10\nrespectively, and show that standardly trained models achieve comparable clean\naccuracies on them, but adversarially trained models achieve significantly\ndifferent robustness accuracies. This counter-intuitive phenomenon indicates\nthat input data distribution alone can affect the adversarial robustness of\ntrained neural networks, not necessarily the tasks themselves. Lastly, we\ndiscuss the practical implications on evaluating adversarial robustness, and\nmake initial attempts to understand this complex phenomenon.\n"], ["2019-02-21", "http://arxiv.org/abs/1902.08265", "Quantifying Perceptual Distortion of Adversarial Examples.", ["Matt Jordan", " Naren Manoj", " Surbhi Goel", " Alexandros G. Dimakis"], "  Recent work has shown that additive threat models, which only permit the\naddition of bounded noise to the pixels of an image, are insufficient for fully\ncapturing the space of imperceivable adversarial examples. For example, small\nrotations and spatial transformations can fool classifiers, remain\nimperceivable to humans, but have large additive distance from the original\nimages. In this work, we leverage quantitative perceptual metrics like LPIPS\nand SSIM to define a novel threat model for adversarial attacks.\n  To demonstrate the value of quantifying the perceptual distortion of\nadversarial examples, we present and employ a unifying framework fusing\ndifferent attack styles. We first prove that our framework results in images\nthat are unattainable by attack styles in isolation. We then perform\nadversarial training using attacks generated by our framework to demonstrate\nthat networks are only robust to classes of adversarial perturbations they have\nbeen trained against, and combination attacks are stronger than any of their\nindividual components. Finally, we experimentally demonstrate that our combined\nattacks retain the same perceptual distortion but induce far higher\nmisclassification rates when compared against individual attacks.\n"], ["2019-02-21", "http://arxiv.org/abs/1902.07906", "Wasserstein Adversarial Examples via Projected Sinkhorn Iterations.", ["Eric Wong", " Frank R. Schmidt", " J. Zico Kolter"], "  A rapidly growing area of work has studied the existence of adversarial\nexamples, datapoints which have been perturbed to fool a classifier, but the\nvast majority of these works have focused primarily on threat models defined by\n$\\ell_p$ norm-bounded perturbations. In this paper, we propose a new threat\nmodel for adversarial attacks based on the Wasserstein distance. In the image\nclassification setting, such distances measure the cost of moving pixel mass,\nwhich naturally cover \"standard\" image manipulations such as scaling, rotation,\ntranslation, and distortion (and can potentially be applied to other settings\nas well). To generate Wasserstein adversarial examples, we develop a procedure\nfor projecting onto the Wasserstein ball, based upon a modified version of the\nSinkhorn iteration. The resulting algorithm can successfully attack image\nclassification models, bringing traditional CIFAR10 models down to 3% accuracy\nwithin a Wasserstein ball with radius 0.1 (i.e., moving 10% of the image mass 1\npixel), and we demonstrate that PGD-based adversarial training can improve this\nadversarial accuracy to 76%. In total, this work opens up a new direction of\nstudy in adversarial robustness, more formally considering convex metrics that\naccurately capture the invariances that we typically believe should exist in\nclassifiers. Code for all experiments in the paper is available at\nhttps://github.com/locuslab/projected_sinkhorn.\n"], ["2019-02-20", "http://arxiv.org/abs/1902.08226", "Graph Adversarial Training: Dynamically Regularizing Based on Graph Structure.", ["Fuli Feng", " Xiangnan He", " Jie Tang", " Tat-Seng Chua"], "  Recent efforts show that neural networks are vulnerable to small but\nintentional perturbations on input features in visual classification tasks. Due\nto the additional consideration of connections between examples (e.g., articles\nwith citation link tend to be in the same class), graph neural networks could\nbe more sensitive to the perturbations, since the perturbations from connected\nexamples exacerbate the impact on a target example. Adversarial Training (AT),\na dynamic regularization technique, can resist the worst-case perturbations on\ninput features and is a promising choice to improve model robustness and\ngeneralization. However, existing AT methods focus on standard classification,\nbeing less effective when training models on graph since it does not model the\nimpact from connected examples.\n  In this work, we explore adversarial training on graph, aiming to improve the\nrobustness and generalization of models learned on graph. We propose Graph\nAdversarial Training (GAT), which takes the impact from connected examples into\naccount when learning to construct and resist perturbations. We give a general\nformulation of GAT, which can be seen as a dynamic regularization scheme based\non the graph structure. To demonstrate the utility of GAT, we employ it on a\nstate-of-the-art graph neural network model --- Graph Convolutional Network\n(GCN). We conduct experiments on two citation graphs (Citeseer and Cora) and a\nknowledge graph (NELL), verifying the effectiveness of GAT which outperforms\nnormal training on GCN by 4.51% in node classification accuracy. Codes will be\nreleased upon acceptance.\n"], ["2019-02-20", "http://arxiv.org/abs/1902.07623", "advertorch v0.1: An Adversarial Robustness Toolbox based on PyTorch.", ["Gavin Weiguang Ding", " Luyu Wang", " Xiaomeng Jin"], "  advertorch is a toolbox for adversarial robustness research. It contains\nvarious implementations for attacks, defenses and robust training methods.\nadvertorch is built on PyTorch (Paszke et al., 2017), and leverages the\nadvantages of the dynamic computational graph to provide concise and efficient\nreference implementations. The code is licensed under the LGPL license and is\nopen sourced at https://github.com/BorealisAI/advertorch .\n"], ["2019-02-20", "http://arxiv.org/abs/1902.07776", "Perceptual Quality-preserving Black-Box Attack against Deep Learning Image Classifiers.", ["Diego Gragnaniello", " Francesco Marra", " Giovanni Poggi", " Luisa Verdoliva"], "  Deep neural networks provide unprecedented performance in all image\nclassification problems, taking advantage of huge amounts of data available for\ntraining. Recent studies, however, have shown their vulnerability to\nadversarial attacks, spawning an intense research effort in this field. With\nthe aim of building better systems, new countermeasures and stronger attacks\nare proposed by the day. On the attacker's side, there is growing interest for\nthe realistic black-box scenario, in which the user has no access to the neural\nnetwork parameters. The problem is to design efficient attacks which mislead\nthe neural network without compromising image quality. In this work, we propose\nto perform the black-box attack along a low-distortion path, so as to improve\nboth the attack efficiency and the perceptual quality of the adversarial image.\nNumerical experiments on real-world systems prove the effectiveness of the\nproposed approach, both in benchmark classification tasks and in key\napplications in biometrics and forensics.\n"], ["2019-02-19", "http://arxiv.org/abs/1902.06894", "There are No Bit Parts for Sign Bits in Black-Box Attacks.", ["Abdullah Al-Dujaili", " Una-May O'Reilly"], "  We present a black-box adversarial attack algorithm which sets new\nstate-of-the-art model evasion rates for query efficiency in the $\\ell_\\infty$\nand $\\ell_2$ metrics, where only loss-oracle access to the model is available.\nOn two public black-box attack challenges, the algorithm achieves the highest\nevasion rate, surpassing all of the submitted attacks. Similar performance is\nobserved on a model that is secure against substitute-model attacks. For\nstandard models trained on the MNIST, CIFAR10, and IMAGENET datasets, averaged\nover the datasets and metrics, the algorithm is 3.8x less failure-prone, and\nspends in total 2.5x fewer queries than the current state-of-the-art attacks\ncombined given a budget of 10, 000 queries per attack attempt. Notably, it\nrequires no hyperparameter tuning or any data/time-dependent prior. The\nalgorithm exploits a new approach, namely sign-based rather than\nmagnitude-based gradient estimation. This shifts the estimation from continuous\nto binary black-box optimization. With three properties of the directional\nderivative, we examine three approaches to adversarial attacks. This yields a\nsuperior algorithm breaking a standard MNIST model using just 12 queries on\naverage!\n"], ["2019-02-18", "http://arxiv.org/abs/1902.06705", "On Evaluating Adversarial Robustness.", ["Nicholas Carlini", " Anish Athalye", " Nicolas Papernot", " Wieland Brendel", " Jonas Rauber", " Dimitris Tsipras", " Ian Goodfellow", " Aleksander Madry", " Alexey Kurakin"], "  Correctly evaluating defenses against adversarial examples has proven to be\nextremely difficult. Despite the significant amount of recent work attempting\nto design defenses that withstand adaptive attacks, few have succeeded; most\npapers that propose defenses are quickly shown to be incorrect.\n  We believe a large contributing factor is the difficulty of performing\nsecurity evaluations. In this paper, we discuss the methodological foundations,\nreview commonly accepted best practices, and suggest new methods for evaluating\ndefenses to adversarial examples. We hope that both researchers developing\ndefenses as well as readers and reviewers who wish to understand the\ncompleteness of an evaluation consider our advice in order to avoid common\npitfalls.\n"], ["2019-02-18", "http://arxiv.org/abs/1902.06415", "AuxBlocks: Defense Adversarial Example via Auxiliary Blocks.", ["Yueyao Yu", " Pengfei Yu", " Wenye Li"], "  Deep learning models are vulnerable to adversarial examples, which poses an\nindisputable threat to their applications. However, recent studies observe\ngradient-masking defenses are self-deceiving methods if an attacker can realize\nthis defense. In this paper, we propose a new defense method based on appending\ninformation. We introduce the Aux Block model to produce extra outputs as a\nself-ensemble algorithm and analytically investigate the robustness mechanism\nof Aux Block. We have empirically studied the efficiency of our method against\nadversarial examples in two types of white-box attacks, and found that even in\nthe full white-box attack where an adversary can craft malicious examples from\ndefense models, our method has a more robust performance of about 54.6%\nprecision on Cifar10 dataset and 38.7% precision on Mini-Imagenet dataset.\nAnother advantage of our method is that it is able to maintain the prediction\naccuracy of the classification model on clean images, and thereby exhibits its\nhigh potential in practical applications\n"], ["2019-02-18", "http://arxiv.org/abs/1902.06626", "Mockingbird: Defending Against Deep-Learning-Based Website Fingerprinting Attacks with Adversarial Traces.", ["Mohsen Imani", " Mohammad Saidur Rahman", " Nate Mathews", " Matthew Wright"], "  Website Fingerprinting (WF) is a type of traffic analysis attack that enables\na local passive eavesdropper to infer the victim's activity even when the\ntraffic is protected by encryption, a VPN, or some other anonymity system like\nTor. Leveraging a deep-learning classifier, a WF attacker can gain up to 98%\naccuracy against Tor. Existing WF defenses are either too expensive in terms of\nbandwidth and latency overheads (e.g. two-to-three times as large or slow) or\nineffective against the latest attacks. In this paper, we explore a novel\ndefense, Mockingbird, based on the idea of adversarial examples that have been\nshown to undermine machine learning classifiers in other domains. To make the\ntechnique more robust than existing algorithms for finding adversarial\nexamples, our approach aims to make the source trace more similar to a randomly\nselected target and use the neural network only to determine when the modified\ntrace will be misclassified with high confidence. The technique drops the\naccuracy of the state-of-the-art attack hardened with adversarial training from\n95% to between 29-57%, depending on the scenario, while incurring a reasonable\n56% bandwidth overhead. The attack accuracy is generally lower than\nstate-of-the-art defenses, and much lower when considering Top-2 accuracy,\nwhile incurring lower overheads in most settings. In addition, the information\nleakage of Mockingbird is at most 1.9 bits, whereas it is at most 2.2 bits for\nW-T and 2.6 bits for WTF-PAD.\n"], ["2019-02-16", "http://arxiv.org/abs/1902.08034", "Mitigation of Adversarial Examples in RF Deep Classifiers Utilizing AutoEncoder Pre-training.", ["Silvija Kokalj-Filipovic", " Rob Miller", " Nicholas Chang", " Chi Leung Lau"], "  Adversarial examples in machine learning for images are widely publicized and\nexplored. Illustrations of misclassifications caused by slightly perturbed\ninputs are abundant and commonly known (e.g., a picture of panda imperceptibly\nperturbed to fool the classifier into incorrectly labeling it as a gibbon).\nSimilar attacks on deep learning (DL) for radio frequency (RF) signals and\ntheir mitigation strategies are scarcely addressed in the published work. Yet,\nRF adversarial examples (AdExs) with minimal waveform perturbations can cause\ndrastic, targeted misclassification results, particularly against spectrum\nsensing/survey applications (e.g. BPSK is mistaken for 8-PSK). Our research on\ndeep learning AdExs and proposed defense mechanisms are RF-centric, and\nincorporate physical world, over-the-air (OTA) effects. We herein present\ndefense mechanisms based on pre-training the target classifier using an\nautoencoder. Our results validate this approach as a viable mitigation method\nto subvert adversarial attacks against deep learning-based communications and\nradar sensing systems.\n"], ["2019-02-16", "http://arxiv.org/abs/1902.06044", "Adversarial Examples in RF Deep Learning: Detection of the Attack and its Physical Robustness.", ["Silvija Kokalj-Filipovic", " Rob Miller"], "  While research on adversarial examples in machine learning for images has\nbeen prolific, similar attacks on deep learning (DL) for radio frequency (RF)\nsignals and their mitigation strategies are scarcely addressed in the published\nwork, with only one recent publication in the RF domain [1]. RF adversarial\nexamples (AdExs) can cause drastic, targeted misclassification results mostly\nin spectrum sensing/ survey applications (e.g. BPSK mistaken for 8-PSK) with\nminimal waveform perturbation. It is not clear if the RF AdExs maintain their\neffects in the physical world, i.e., when AdExs are delivered over-the-air\n(OTA). Our research on deep learning AdExs and proposed defense mechanisms are\nRF-centric, and incorporate physical world, OTA effects. We here present\ndefense mechanisms based on statistical tests. One test to detect AdExs\nutilizes Peak-to- Average-Power-Ratio (PAPR) of the DL data points delivered\nOTA, while another statistical test uses the Softmax outputs of the DL\nclassifier, which corresponds to the probabilities the classifier assigns to\neach of the trained classes. The former test leverages the RF nature of the\ndata, and the latter is universally applicable to AdExs regardless of their\norigin. Both solutions are shown as viable mitigation methods to subvert\nadversarial attacks against communications and radar sensing systems.\n"], ["2019-02-15", "http://arxiv.org/abs/1902.05974", "DeepFault: Fault Localization for Deep Neural Networks.", ["Hasan Ferit Eniser", " Simos Gerasimou", " Alper Sen"], "  Deep Neural Networks (DNNs) are increasingly deployed in safety-critical\napplications including autonomous vehicles and medical diagnostics. To reduce\nthe residual risk for unexpected DNN behaviour and provide evidence for their\ntrustworthy operation, DNNs should be thoroughly tested. The DeepFault whitebox\nDNN testing approach presented in our paper addresses this challenge by\nemploying suspiciousness measures inspired by fault localization to establish\nthe hit spectrum of neurons and identify suspicious neurons whose weights have\nnot been calibrated correctly and thus are considered responsible for\ninadequate DNN performance. DeepFault also uses a suspiciousness-guided\nalgorithm to synthesize new inputs, from correctly classified inputs, that\nincrease the activation values of suspicious neurons. Our empirical evaluation\non several DNN instances trained on MNIST and CIFAR-10 datasets shows that\nDeepFault is effective in identifying suspicious neurons. Also, the inputs\nsynthesized by DeepFault closely resemble the original inputs, exercise the\nidentified suspicious neurons and are highly adversarial.\n"], ["2019-02-14", "http://arxiv.org/abs/1902.05586", "Can Intelligent Hyperparameter Selection Improve Resistance to Adversarial Examples?.", ["Cody Burkard", " Brent Lagesse"], "  Convolutional Neural Networks and Deep Learning classification systems in\ngeneral have been shown to be vulnerable to attack by specially crafted data\nsamples that appear to belong to one class but are instead classified as\nanother, commonly known as adversarial examples. A variety of attack strategies\nhave been proposed to craft these samples; however, there is no standard model\nthat is used to compare the success of each type of attack. Furthermore, there\nis no literature currently available that evaluates how common hyperparameters\nand optimization strategies may impact a model's ability to resist these\nsamples. This research bridges that lack of awareness and provides a means for\nthe selection of training and model parameters in future research on evasion\nattacks against convolutional neural networks. The findings of this work\nindicate that the selection of model hyperparameters does impact the ability of\na model to resist attack, although they alone cannot prevent the existence of\nadversarial examples.\n"], ["2019-02-13", "http://arxiv.org/abs/1902.04818", "The Odds are Odd: A Statistical Test for Detecting Adversarial Examples.", ["Kevin Roth", " Yannic Kilcher", " Thomas Hofmann"], "  We investigate conditions under which test statistics exist that can reliably\ndetect examples, which have been adversarially manipulated in a white-box\nattack. These statistics can be easily computed and calibrated by randomly\ncorrupting inputs. They exploit certain anomalies that adversarial attacks\nintroduce, in particular if they follow the paradigm of choosing perturbations\noptimally under p-norm constraints. Access to the log-odds is the only\nrequirement to defend models. We justify our approach empirically, but also\nprovide conditions under which detectability via the suggested test statistics\nis guaranteed to be effective. In our experiments, we show that it is even\npossible to correct test time predictions for adversarial attacks with high\naccuracy.\n"], ["2019-02-12", "http://arxiv.org/abs/1902.04416", "Examining Adversarial Learning against Graph-based IoT Malware Detection Systems.", ["Ahmed Abusnaina", " Aminollah Khormali", " Hisham Alasmary", " Jeman Park", " Afsah Anwar", " Ulku Meteriz", " Aziz Mohaisen"], "  The main goal of this study is to investigate the robustness of graph-based\nDeep Learning (DL) models used for Internet of Things (IoT) malware\nclassification against Adversarial Learning (AL). We designed two approaches to\ncraft adversarial IoT software, including Off-the-Shelf Adversarial Attack\n(OSAA) methods, using six different AL attack approaches, and Graph Embedding\nand Augmentation (GEA). The GEA approach aims to preserve the functionality and\npracticality of the generated adversarial sample through a careful embedding of\na benign sample to a malicious one. Our evaluations demonstrate that OSAAs are\nable to achieve a misclassification rate (MR) of 100%. Moreover, we observed\nthat the GEA approach is able to misclassify all IoT malware samples as benign.\n"], ["2019-02-11", "http://arxiv.org/abs/1902.04238", "Adversarial Samples on Android Malware Detection Systems for IoT Systems.", ["Xiaolei Liu", " Xiaojiang Du", " Xiaosong Zhang", " Qingxin Zhu", " Mohsen Guizani"], "  Many IoT(Internet of Things) systems run Android systems or Android-like\nsystems. With the continuous development of machine learning algorithms, the\nlearning-based Android malware detection system for IoT devices has gradually\nincreased. However, these learning-based detection models are often vulnerable\nto adversarial samples. An automated testing framework is needed to help these\nlearning-based malware detection systems for IoT devices perform security\nanalysis. The current methods of generating adversarial samples mostly require\ntraining parameters of models and most of the methods are aimed at image data.\nTo solve this problem, we propose a \\textbf{t}esting framework for\n\\textbf{l}earning-based \\textbf{A}ndroid \\textbf{m}alware \\textbf{d}etection\nsystems(TLAMD) for IoT Devices. The key challenge is how to construct a\nsuitable fitness function to generate an effective adversarial sample without\naffecting the features of the application. By introducing genetic algorithms\nand some technical improvements, our test framework can generate adversarial\nsamples for the IoT Android Application with a success rate of nearly 100\\% and\ncan perform black-box testing on the system.\n"], ["2019-02-11", "http://arxiv.org/abs/1902.07285", "A Survey: Towards a Robust Deep Neural Network in Text Domain.", ["Wenqi Wang", " Lina Wang", " Benxiao Tang", " Run Wang", " Aoshuang Ye"], "  Deep neural networks (DNNs) have shown an inherent vulnerability to\nadversarial examples which are maliciously crafted on real examples by\nattackers, aiming at making target DNNs misbehave. The threats of adversarial\nexamples are widely existed in image, voice, speech, and text recognition and\nclassification. Inspired by the previous work, researches on adversarial\nattacks and defenses in text domain develop rapidly. In order to make people\nhave a general understanding about the field, this article presents a\ncomprehensive review on adversarial examples in text. We analyze the advantages\nand shortcomings of recent adversarial examples generation methods and\nelaborate the efficiency and limitations on countermeasures. Finally, we\ndiscuss the challenges in adversarial texts and provide a research direction of\nthis aspect.\n"], ["2019-02-09", "http://arxiv.org/abs/1902.03538", "Adversarially Trained Model Compression: When Robustness Meets Efficiency.", ["Shupeng University of Rochester Gui", " Haotao Texas A&M University Wang", " Chen University of Rochester Yu", " Haichuan University of Rochester Yang", " Zhangyang Texas A&M University Wang", " Ji University of Rochester Liu"], "  The robustness of deep models to adversarial attacks has gained significant\nattention in recent years, so has the model compactness and efficiency: yet the\ntwo have been mostly studied separately, with few relationships drawn between\neach other. This paper is concerned with: how can we combine the best of both\nworlds, obtaining a robust and compact network? The answer is not as\nstraightforward as it may seem, since the two goals of model robustness and\ncompactness may contradict from time to time. We formally study this new\nquestion, by proposing a novel Adversarially Trained Model Compression (ATMC)\nframework. A unified constrained optimization formulation is designed, with an\nefficient algorithm developed. An extensive group of experiments are then\ncarefully designed and presented, demonstrating that ATMC obtains remarkably\nmore favorable trade-off among model size, accuracy and robustness, over\ncurrently available alternatives in various settings.\n"], ["2019-02-09", "http://arxiv.org/abs/1902.03380", "When Causal Intervention Meets Adversarial Examples and Image Masking for Deep Neural Networks.", ["Chao-Han Huck Yang", " Yi-Chieh Liu", " Pin-Yu Chen", " Xiaoli Ma", " Yi-Chang James Tsai"], "  Discovering and exploiting the causality in deep neural networks (DNNs) are\ncrucial challenges for understanding and reasoning causal effects (CE) on an\nexplainable visual model. \"Intervention\" has been widely used for recognizing a\ncausal relation ontologically. In this paper, we propose a causal inference\nframework for visual reasoning via do-calculus. To study the intervention\neffects on pixel-level features for causal reasoning, we introduce pixel-wise\nmasking and adversarial perturbation. In our framework, CE is calculated using\nfeatures in a latent space and perturbed prediction from a DNN-based model. We\nfurther provide the first look into the characteristics of discovered CE of\nadversarially perturbed images generated by gradient-based methods\n\\footnote{~~https://github.com/jjaacckkyy63/Causal-Intervention-AE-wAdvImg}.\nExperimental results show that CE is a competitive and robust index for\nunderstanding DNNs when compared with conventional methods such as\nclass-activation mappings (CAMs) on the Chest X-Ray-14 dataset for\nhuman-interpretable feature(s) (e.g., symptom) reasoning. Moreover, CE holds\npromises for detecting adversarial examples as it possesses distinct\ncharacteristics in the presence of adversarial perturbations.\n"], ["2019-02-08", "http://arxiv.org/abs/1902.03227", "Minimal Images in Deep Neural Networks: Fragile Object Recognition in Natural Images.", ["Sanjana Srivastava", " Guy Ben-Yosef", " Xavier Boix"], "  The human ability to recognize objects is impaired when the object is not\nshown in full. \"Minimal images\" are the smallest regions of an image that\nremain recognizable for humans. Ullman et al. 2016 show that a slight\nmodification of the location and size of the visible region of the minimal\nimage produces a sharp drop in human recognition accuracy. In this paper, we\ndemonstrate that such drops in accuracy due to changes of the visible region\nare a common phenomenon between humans and existing state-of-the-art deep\nneural networks (DNNs), and are much more prominent in DNNs. We found many\ncases where DNNs classified one region correctly and the other incorrectly,\nthough they only differed by one row or column of pixels, and were often bigger\nthan the average human minimal image size. We show that this phenomenon is\nindependent from previous works that have reported lack of invariance to minor\nmodifications in object location in DNNs. Our results thus reveal a new failure\nmode of DNNs that also affects humans to a much lesser degree. They expose how\nfragile DNN recognition ability is for natural images even without adversarial\npatterns being introduced. Bringing the robustness of DNNs in natural images to\nthe human level remains an open challenge for the community.\n"], ["2019-02-08", "http://arxiv.org/abs/1902.02947", "Understanding the One-Pixel Attack: Propagation Maps and Locality Analysis.", ["Danilo Vasconcellos Vargas", " Jiawei Su"], "  Deep neural networks were shown to be vulnerable to single pixel\nmodifications. However, the reason behind such phenomena has never been\nelucidated. Here, we propose Propagation Maps which show the influence of the\nperturbation in each layer of the network. Propagation Maps reveal that even in\nextremely deep networks such as Resnet, modification in one pixel easily\npropagates until the last layer. In fact, this initial local perturbation is\nalso shown to spread becoming a global one and reaching absolute difference\nvalues that are close to the maximum value of the original feature maps in a\ngiven layer. Moreover, we do a locality analysis in which we demonstrate that\nnearby pixels of the perturbed one in the one-pixel attack tend to share the\nsame vulnerability, revealing that the main vulnerability lies in neither\nneurons nor pixels but receptive fields. Hopefully, the analysis conducted in\nthis work together with a new technique called propagation maps shall shed\nlight into the inner workings of other adversarial samples and be the basis of\nnew defense systems to come.\n"], ["2019-02-08", "http://arxiv.org/abs/1902.03151", "Discretization based Solutions for Secure Machine Learning against Adversarial Attacks.", ["Priyadarshini Panda", " Indranil Chakraborty", " Kaushik Roy"], "  Adversarial examples are perturbed inputs that are designed (from a deep\nlearning network's (DLN) parameter gradients) to mislead the DLN during test\ntime. Intuitively, constraining the dimensionality of inputs or parameters of a\nnetwork reduces the 'space' in which adversarial examples exist. Guided by this\nintuition, we demonstrate that discretization greatly improves the robustness\nof DLNs against adversarial attacks. Specifically, discretizing the input space\n(or allowed pixel levels from 256 values or 8-bit to 4 values or 2-bit)\nextensively improves the adversarial robustness of DLNs for a substantial range\nof perturbations for minimal loss in test accuracy. Furthermore, we find that\nBinary Neural Networks (BNNs) and related variants are intrinsically more\nrobust than their full precision counterparts in adversarial scenarios.\nCombining input discretization with BNNs furthers the robustness even waiving\nthe need for adversarial training for certain magnitude of perturbation values.\nWe evaluate the effect of discretization on MNIST, CIFAR10, CIFAR100 and\nImagenet datasets. Across all datasets, we observe maximal adversarial\nresistance with 2-bit input discretization that incurs an adversarial accuracy\nloss of just ~1-2% as compared to clean test accuracy.\n"], ["2019-02-07", "http://arxiv.org/abs/1902.02826", "Robustness Of Saak Transform Against Adversarial Attacks.", ["Thiyagarajan Ramanathan", " Abinaya Manimaran", " Suya You", " C-C Jay Kuo"], "  Image classification is vulnerable to adversarial attacks. This work\ninvestigates the robustness of Saak transform against adversarial attacks\ntowards high performance image classification. We develop a complete image\nclassification system based on multi-stage Saak transform. In the Saak\ntransform domain, clean and adversarial images demonstrate different\ndistributions at different spectral dimensions. Selection of the spectral\ndimensions at every stage can be viewed as an automatic denoising process.\nMotivated by this observation, we carefully design strategies of feature\nextraction, representation and classification that increase adversarial\nrobustness. The performances with well-known datasets and attacks are\ndemonstrated by extensive experimental evaluations.\n"], ["2019-02-07", "http://arxiv.org/abs/1902.02918", "Certified Adversarial Robustness via Randomized Smoothing.", ["Jeremy M Cohen", " Elan Rosenfeld", " J. Zico Kolter"], "  We show how to turn any classifier that classifies well under Gaussian noise\ninto a new classifier that is certifiably robust to adversarial perturbations\nunder the $\\ell_2$ norm. This \"randomized smoothing\" technique has been\nproposed recently in the literature, but existing guarantees are loose. We\nprove a tight robustness guarantee in $\\ell_2$ norm for smoothing with Gaussian\nnoise. We use randomized smoothing to obtain an ImageNet classifier with e.g. a\ncertified top-1 accuracy of 49% under adversarial perturbations with $\\ell_2$\nnorm less than 0.5 (=127/255). No certified defense has been shown feasible on\nImageNet except for smoothing. On smaller-scale datasets where competing\napproaches to certified $\\ell_2$ robustness are viable, smoothing delivers\nhigher certified accuracies. Our strong empirical results suggest that\nrandomized smoothing is a promising direction for future research into\nadversarially robust classification. Code and models are available at\nhttp://github.com/locuslab/smoothing.\n"], ["2019-02-06", "http://arxiv.org/abs/1902.02041", "Fooling Neural Network Interpretations via Adversarial Model Manipulation.", ["Juyeon Heo", " Sunghwan Joo", " Taesup Moon"], "  We ask whether the neural network interpretation methods can be fooled via\nadversarial model manipulation, which is defined as a model fine-tuning step\nthat aims to radically alter the explanations without hurting the accuracy of\nthe original models, e.g., VGG19, ResNet50, and DenseNet121. By incorporating\nthe interpretation results directly in the penalty term of the objective\nfunction for fine-tuning, we show that the state-of-the-art saliency map based\ninterpreters, e.g., LRP, Grad-CAM, and SimpleGrad, can be easily fooled with\nour model manipulation. We propose two types of fooling, Passive and Active,\nand demonstrate such foolings generalize well to the entire validation set as\nwell as transfer to other interpretation methods. Our results are validated by\nboth visually showing the fooled explanations and reporting quantitative\nmetrics that measure the deviations from the original explanations. We claim\nthat the stability of neural network interpretation method with respect to our\nadversarial model manipulation is an important criterion to check for\ndeveloping robust and reliable neural network interpretation method.\n"], ["2019-02-06", "http://arxiv.org/abs/1902.02067", "Daedalus: Breaking Non-Maximum Suppression in Object Detection via Adversarial Examples.", ["Derui Wang", " Chaoran Li", " Sheng Wen", " Xiaojun Chang", " Surya Nepal", " Yang Xiang"], "  We demonstrate that Non-Maximum Suppression (NMS), which is commonly used in\nObject Detection (OD) tasks to filter redundant detection results, is no longer\nsecure. Considering that NMS has been an integral part of OD systems, thwarting\nthe functionality of NMS can result in unexpected or even lethal consequences\nfor such systems. In this paper, we propose an adversarial example attack which\ntriggers malfunctioning of NMS in end-to-end OD models. Our attack, namely\nDaedalus, compresses the dimensions of detection boxes to evade NMS. As a\nresult, the final detection output contains extremely dense false positives.\nThis can be fatal for many OD applications such as autonomous vehicle and\nsurveillance system. Our attack can be generalised to different end-to-end OD\nmodels, such that the attack cripples various OD applications. Furthermore, we\npropose a way to craft robust adversarial examples by using an ensemble of\npopular detection models as the substitutes. Considering the pervasive nature\nof model reusing in real-world OD scenarios, Daedalus examples crafted based on\nan ensemble of substitutes can launch attacks without knowing the parameters of\nthe victim models. Our experiments demonstrate that the attack effectively\nstops NMS from filtering redundant bounding boxes. As the evaluation results\nsuggest, Daedalus increases the false positive rate in detection results to\n99.9% and reduces the mean average precision scores to 0, while maintaining a\nlow cost of distortion on the original inputs. With the widespread applications\nof OD, our work shows that there are serious vulnerabilities in the fundamental\ncomponents of such systems and further investigation on them is required in\nthis area.\n"], ["2019-02-05", "http://arxiv.org/abs/1902.01686", "Fatal Brain Damage.", ["El Mahdi El Mhamdi", " Rachid Guerraoui", " Sergei Volodin"], "  The loss of a few neurons in a brain often does not result in a visible loss\nof function. We propose to advance the understanding of neural networks through\ntheir remarkable ability to sustain individual neuron failures, i.e. their\nfault tolerance. Before the last AI winter, fault tolerance in NNs was a\npopular topic as NNs were expected to be implemented in neuromorphic hardware,\nwhich for a while did not happen. Moreover, since the number of possible crash\nsubsets grows exponentially with the network size, additional assumptions are\nrequired to practically study this phenomenon for modern architectures. We\nprove a series of bounds on error propagation using justified assumptions,\napplicable to deep networks, show their location on the complexity versus\ntightness trade-off scale and test them empirically. We demonstrate how fault\ntolerance is connected to generalization and show that the data jacobian of a\nnetwork determines its fault tolerance properties. We investigate this quantity\nand show how it is interlinked with other mathematical properties of the\nnetwork such as Lipschitzness, singular values, weight matrices norms, and the\nloss gradients. Known results give a connection between the data jacobian and\nrobustness to adversarial examples, providing another piece of the puzzle.\nCombining that with our results, we call for a unifying research endeavor\nencompassing fault tolerance, generalization capacity, and robustness to\nadversarial inputs together as we demonstrate a strong connection between these\nareas. Moreover, we argue that fault tolerance is an important overlooked AI\nsafety problem since neuromorphic hardware is becoming popular again.\n"], ["2019-02-04", "http://arxiv.org/abs/1902.01147", "SNN under Attack: are Spiking Deep Belief Networks vulnerable to Adversarial Examples?.", ["Alberto Marchisio", " Giorgio Nanfa", " Faiq Khalid", " Muhammad Abdullah Hanif", " Maurizio Martina", " Muhammad Shafique"], "  Recently, many adversarial examples have emerged for Deep Neural Networks\n(DNNs) causing misclassifications. However, in-depth work still needs to be\nperformed to demonstrate such attacks and security vulnerabilities for spiking\nneural networks (SNNs), i.e. the 3rd generation NNs. This paper aims at\naddressing the fundamental questions:\"Are SNNs vulnerable to the adversarial\nattacks as well?\" and \"if yes, to what extent?\" Using a Spiking Deep Belief\nNetwork (SDBN) for the MNIST database classification, we show that the SNN\naccuracy decreases accordingly to the noise magnitude in data poisoning random\nattacks applied to the test images. Moreover, SDBNs generalization capabilities\nincrease by applying noise to the training images. We develop a novel black box\nattack methodology to automatically generate imperceptible and robust\nadversarial examples through a greedy algorithm, which is first of its kind for\nSNNs.\n"], ["2019-02-04", "http://arxiv.org/abs/1902.01148", "Theoretical evidence for adversarial robustness through randomization.", ["Rafael Pinot", " Laurent Meunier", " Alexandre Araujo", " Hisashi Kashima", " Florian Yger", " C\u00e9dric Gouy-Pailler", " Jamal Atif"], "  This paper investigates the theory of robustness against adversarial attacks.\nIt focuses on the family of randomization techniques that consist in injecting\nnoise in the network at inference time. These techniques have proven effective\nin many contexts, but lack theoretical arguments. We close this gap by\npresenting a theoretical analysis of these approaches, hence explaining why\nthey perform well in practice. More precisely, we make two new contributions.\nThe first one relates the randomization rate to robustness to adversarial\nattacks. This result applies for the general family of exponential\ndistributions, and thus extends and unifies the previous approaches. The second\ncontribution consists in devising a new upper bound on the adversarial\ngeneralization gap of randomized neural networks. We support our theoretical\nclaims with a set of experiments.\n"], ["2019-02-01", "http://arxiv.org/abs/1902.01235", "Robustness Certificates Against Adversarial Examples for ReLU Networks.", ["Sahil Singla", " Soheil Feizi"], "  While neural networks have achieved high performance in different learning\ntasks, their accuracy drops significantly in the presence of small adversarial\nperturbations to inputs. Defenses based on regularization and adversarial\ntraining are often followed by new attacks to defeat them. In this paper, we\npropose attack-agnostic robustness certificates for a multi-label\nclassification problem using a deep ReLU network. Although computing the exact\ndistance of a given input sample to the classification decision boundary\nrequires solving a non-convex optimization, we characterize two lower bounds\nfor such distances, namely the simplex certificate and the decision boundary\ncertificate. These robustness certificates leverage the piece-wise linear\nstructure of ReLU networks and use the fact that in a polyhedron around a given\nsample, the prediction function is linear. In particular, the proposed simplex\ncertificate has a closed-form, is differentiable and is an order of magnitude\nfaster to compute than the existing methods even for deep networks. In addition\nto theoretical bounds, we provide numerical results for our certificates over\nMNIST and compare them with some existing upper bounds.\n"], ["2019-02-01", "http://arxiv.org/abs/1902.00236", "Natural and Adversarial Error Detection using Invariance to Image Transformations.", ["Yuval Bahat", " Michal Irani", " Gregory Shakhnarovich"], "  We propose an approach to distinguish between correct and incorrect image\nclassifications. Our approach can detect misclassifications which either occur\n$\\it{unintentionally}$ (\"natural errors\"), or due to\n$\\it{intentional~adversarial~attacks}$ (\"adversarial errors\"), both in a single\n$\\it{unified~framework}$. Our approach is based on the observation that\ncorrectly classified images tend to exhibit robust and consistent\nclassifications under certain image transformations (e.g., horizontal flip,\nsmall image translation, etc.). In contrast, incorrectly classified images\n(whether due to adversarial errors or natural errors) tend to exhibit large\nvariations in classification results under such transformations. Our approach\ndoes not require any modifications or retraining of the classifier, hence can\nbe applied to any pre-trained classifier. We further use state of the art\ntargeted adversarial attacks to demonstrate that even when the adversary has\nfull knowledge of our method, the adversarial distortion needed for bypassing\nour detector is $\\it{no~longer~imperceptible~to~the~human~eye}$. Our approach\nobtains state-of-the-art results compared to previous adversarial detection\nmethods, surpassing them by a large margin.\n"], ["2019-02-01", "http://arxiv.org/abs/1902.01220", "Adaptive Gradient for Adversarial Perturbations Generation.", ["Yatie Xiao", " Chi-Man Pun"], "  Deep Neural Networks have achieved remarkable success in computer vision,\nnatural language processing, and audio tasks.\n"], ["2019-02-01", "http://arxiv.org/abs/1902.00577", "Robustness of Generalized Learning Vector Quantization Models against Adversarial Attacks.", ["Sascha Saralajew", " Lars Holdijk", " Maike Rees", " Thomas Villmann"], "  Adversarial attacks and the development of (deep) neural networks robust\nagainst them are currently two widely researched topics. The robustness of\nLearning Vector Quantization (LVQ) models against adversarial attacks has\nhowever not yet been studied to the same extent. We therefore present an\nextensive evaluation of three LVQ models: Generalized LVQ, Generalized Matrix\nLVQ and Generalized Tangent LVQ. The evaluation suggests that both Generalized\nLVQ and Generalized Tangent LVQ have a high base robustness, on par with the\ncurrent state-of-the-art in robust neural network methods. In contrast to this,\nGeneralized Matrix LVQ shows a high susceptibility to adversarial attacks,\nscoring consistently behind all other models. Additionally, our numerical\nevaluation indicates that increasing the number of prototypes per class\nimproves the robustness of the models.\n"], ["2019-02-01", "http://arxiv.org/abs/1902.00541", "The Efficacy of SHIELD under Different Threat Models.", ["Cory Cornelius", " Nilaksh Das", " Shang-Tse Chen", " Li Chen", " Michael E. Kounavis", " Duen Horng Chau"], "  In this appraisal paper, we evaluate the efficacy of SHIELD, a\ncompression-based defense framework for countering adversarial attacks on image\nclassification models, which was published at KDD 2018. Here, we consider\nalternative threat models not studied in the original work, where we assume\nthat an adaptive adversary is aware of the ensemble defense approach, the\ndefensive pre-processing, and the architecture and weights of the models used\nin the ensemble. We define scenarios with varying levels of threat and\nempirically analyze the proposed defense by varying the degree of information\navailable to the attacker, spanning from a full white-box attack to the\ngray-box threat model described in the original work. To evaluate the\nrobustness of the defense against an adaptive attacker, we consider the\ntargeted-attack success rate of the Projected Gradient Descent (PGD) attack,\nwhich is a strong gradient-based adversarial attack proposed in adversarial\nmachine learning research. We also experiment with training the SHIELD ensemble\nfrom scratch, which is different from re-training using a pre-trained model as\ndone in the original work. We find that the targeted PGD attack has a success\nrate of 64.3% against the original SHIELD ensemble in the full white box\nscenario, but this drops to 48.9% if the models used in the ensemble are\ntrained from scratch instead of being retrained. Our experiments further reveal\nthat an ensemble whose models are re-trained indeed have higher correlation in\nthe cosine similarity space, and models that are trained from scratch are less\nvulnerable to targeted attacks in the white-box and gray-box scenarios.\n"], ["2019-01-31", "http://arxiv.org/abs/1902.01208", "A New Family of Neural Networks Provably Resistant to Adversarial Attacks.", ["Rakshit Agrawal", " Alfaro Luca de", " David Helmbold"], "  Adversarial attacks add perturbations to the input features with the intent\nof changing the classification produced by a machine learning system. Small\nperturbations can yield adversarial examples which are misclassified despite\nbeing virtually indistinguishable from the unperturbed input. Classifiers\ntrained with standard neural network techniques are highly susceptible to\nadversarial examples, allowing an adversary to create misclassifications of\ntheir choice.\n  We introduce a new type of network unit, called MWD (max of weighed distance)\nunits that have a built-in resistant to adversarial attacks. These units are\nhighly non-linear, and we develop the techniques needed to effectively train\nthem. We show that simple interval techniques for propagating perturbation\neffects through the network enables the efficient computation of robustness\n(i.e., accuracy guarantees) for MWD networks under any perturbations, including\nadversarial attacks.\n  MWD networks are significantly more robust to input perturbations than ReLU\nnetworks. On permutation invariant MNIST, when test examples can be perturbed\nby 20% of the input range, MWD networks provably retain accuracy above 83%,\nwhile the accuracy of ReLU networks drops below 5%. The provable accuracy of\nMWD networks is superior even to the observed accuracy of ReLU networks trained\nwith the help of adversarial examples. In the absence of adversarial attacks,\nMWD networks match the performance of sigmoid networks, and have accuracy only\nslightly below that of ReLU networks.\n"], ["2019-01-31", "http://arxiv.org/abs/1902.00358", "Training Artificial Neural Networks by Generalized Likelihood Ratio Method: Exploring Brain-like Learning to Improve Robustness.", ["Li Xiao", " Yijie Peng", " Jeff Hong", " Zewu Ke", " Shuhuai Yang"], "  In this work, we propose a generalized likelihood ratio method capable of\ntraining the artificial neural networks with some biological brain-like\nmechanisms,.e.g., (a) learning by the loss value, (b) learning via neurons with\ndiscontinuous activation and loss functions. The traditional back propagation\nmethod cannot train the artificial neural networks with aforementioned\nbrain-like learning mechanisms. Numerical results show that the robustness of\nvarious artificial neural networks trained by the new method is significantly\nimproved when the input data is affected by both the natural noises and\nadversarial attacks. Code is available:\n\\url{https://github.com/LX-doctorAI/GLR_ADV} .\n"], ["2019-01-30", "http://arxiv.org/abs/1901.10861", "A Simple Explanation for the Existence of Adversarial Examples with Small Hamming Distance.", ["Adi Shamir", " Itay Safran", " Eyal Ronen", " Orr Dunkelman"], "  The existence of adversarial examples in which an imperceptible change in the\ninput can fool well trained neural networks was experimentally discovered by\nSzegedy et al in 2013, who called them \"Intriguing properties of neural\nnetworks\". Since then, this topic had become one of the hottest research areas\nwithin machine learning, but the ease with which we can switch between any two\ndecisions in targeted attacks is still far from being understood, and in\nparticular it is not clear which parameters determine the number of input\ncoordinates we have to change in order to mislead the network. In this paper we\ndevelop a simple mathematical framework which enables us to think about this\nbaffling phenomenon from a fresh perspective, turning it into a natural\nconsequence of the geometry of $\\mathbb{R}^n$ with the $L_0$ (Hamming) metric,\nwhich can be quantitatively analyzed. In particular, we explain why we should\nexpect to find targeted adversarial examples with Hamming distance of roughly\n$m$ in arbitrarily deep neural networks which are designed to distinguish\nbetween $m$ input classes.\n"], ["2019-01-30", "http://arxiv.org/abs/1901.11188", "Augmenting Model Robustness with Transformation-Invariant Attacks.", ["Houpu Yao", " Zhe Wang", " Guangyu Nie", " Yassine Mazboudi", " Yezhou Yang", " Yi Ren"], "  The vulnerability of neural networks under adversarial attacks has raised\nserious concerns and motivated extensive research. It has been shown that both\nneural networks and adversarial attacks against them can be sensitive to input\ntransformations such as linear translation and rotation, and that human vision,\nwhich is robust against adversarial attacks, is invariant to natural input\ntransformations. Based on these, this paper tests the hypothesis that model\nrobustness can be further improved when it is adversarially trained against\ntransformed attacks and transformation-invariant attacks. Experiments on MNIST,\nCIFAR-10, and restricted ImageNet show that while transformations of attacks\nalone do not affect robustness, transformation-invariant attacks can improve\nmodel robustness by 2.5\\% on MNIST, 3.7\\% on CIFAR-10, and 1.1\\% on restricted\nImageNet. We discuss the intuition behind this phenomenon.\n"], ["2019-01-29", "http://arxiv.org/abs/1901.10650", "Metric Attack and Defense for Person Re-identification.", ["Song Bai", " Yingwei Li", " Yuyin Zhou", " Qizhu Li", " Philip H. S. Torr"], "  Person re-identification (re-ID) has attracted much attention recently due to\nits great importance in video surveillance. In general, distance metrics used\nto identify two person images are expected to be robust under various\nappearance changes. However, our work observes the extreme vulnerability of\nexisting distance metrics to adversarial examples, generated by simply adding\nhuman-imperceptible perturbations to person images. Hence, the security danger\nis dramatically increased when deploying commercial re-ID systems in video\nsurveillance.\n  Although adversarial examples have been extensively applied for\nclassification analysis, it is rarely studied in metric analysis like person\nre-identification. The most likely reason is the natural gap between the\ntraining and testing of re-ID networks, that is, the predictions of a re-ID\nnetwork cannot be directly used during testing without an effective metric. In\nthis work, we bridge the gap by proposing Adversarial Metric Attack, a parallel\nmethodology to adversarial classification attacks. Comprehensive experiments\nclearly reveal the adversarial effects in re-ID systems. Meanwhile, we also\npresent an early attempt of training a metric-preserving network, thereby\ndefending the metric against adversarial attacks. At last, by benchmarking\nvarious adversarial settings, we expect that our work can facilitate the\ndevelopment of adversarial attack and defense in metric-based applications.\n"], ["2019-01-29", "http://arxiv.org/abs/1901.10513", "Adversarial Examples Are a Natural Consequence of Test Error in Noise.", ["Nic Ford", " Justin Gilmer", " Nicolas Carlini", " Dogus Cubuk"], "  Over the last few years, the phenomenon of adversarial examples ---\nmaliciously constructed inputs that fool trained machine learning models ---\nhas captured the attention of the research community, especially when the\nadversary is restricted to small modifications of a correctly handled input.\nLess surprisingly, image classifiers also lack human-level performance on\nrandomly corrupted images, such as images with additive Gaussian noise. In this\npaper we provide both empirical and theoretical evidence that these are two\nmanifestations of the same underlying phenomenon, establishing close\nconnections between the adversarial robustness and corruption robustness\nresearch programs. This suggests that improving adversarial robustness should\ngo hand in hand with improving performance in the presence of more general and\nrealistic image corruptions. Based on our results we recommend that future\nadversarial defenses consider evaluating the robustness of their methods to\ndistributional shift with benchmarks such as Imagenet-C.\n"], ["2019-01-29", "http://arxiv.org/abs/1901.10371", "On the Effect of Low-Rank Weights on Adversarial Robustness of Neural Networks.", ["Peter Langeberg", " Emilio Rafael Balda", " Arash Behboodi", " Rudolf Mathar"], "  Recently, there has been an abundance of works on designing Deep Neural\nNetworks (DNNs) that are robust to adversarial examples. In particular, a\ncentral question is which features of DNNs influence adversarial robustness\nand, therefore, can be to used to design robust DNNs. In this work, this\nproblem is studied through the lens of compression which is captured by the\nlow-rank structure of weight matrices. It is first shown that adversarial\ntraining tends to promote simultaneously low-rank and sparse structure in the\nweight matrices of neural networks. This is measured through the notions of\neffective rank and effective sparsity. In the reverse direction, when the low\nrank structure is promoted by nuclear norm regularization and combined with\nsparsity inducing regularizations, neural networks show significantly improved\nadversarial robustness. The effect of nuclear norm regularization on\nadversarial robustness is paramount when it is applied to convolutional neural\nnetworks. Although still not competing with adversarial training, this result\ncontributes to understanding the key properties of robust classifiers.\n"], ["2019-01-29", "http://arxiv.org/abs/1901.10258", "RED-Attack: Resource Efficient Decision based Attack for Machine Learning.", ["Faiq Khalid", " Hassan Ali", " Muhammad Abdullah Hanif", " Semeen Rehman", " Rehan Ahmed", " Muhammad Shafique"], "  Due to data dependency and model leakage properties, Deep Neural Networks\n(DNNs) exhibit several security vulnerabilities. Several security attacks\nexploited them but most of them require the output probability vector. These\nattacks can be mitigated by concealing the output probability vector. To\naddress this limitation, decision-based attacks have been proposed which can\nestimate the model but they require several thousand queries to generate a\nsingle untargeted attack image. However, in real-time attacks, resources and\nattack time are very crucial parameters. Therefore, in resource-constrained\nsystems, e.g., autonomous vehicles where an untargeted attack can have a\ncatastrophic effect, these attacks may not work efficiently. To address this\nlimitation, we propose a resource efficient decision-based methodology which\ngenerates the imperceptible attack, i.e., the RED-Attack, for a given black-box\nmodel. The proposed methodology follows two main steps to generate the\nimperceptible attack, i.e., classification boundary estimation and adversarial\nnoise optimization. Firstly, we propose a half-interval search-based algorithm\nfor estimating a sample on the classification boundary using a target image and\na randomly selected image from another class. Secondly, we propose an\noptimization algorithm which first, introduces a small perturbation in some\nrandomly selected pixels of the estimated sample. Then to ensure\nimperceptibility, it optimizes the distance between the perturbed and target\nsamples. For illustration, we evaluate it for CFAR-10 and German Traffic Sign\nRecognition (GTSR) using state-of-the-art networks.\n"], ["2019-01-29", "http://arxiv.org/abs/1901.10622", "Reliable Smart Road Signs.", ["Muhammed O. Sayin", " Chung-Wei Lin", " Eunsuk Kang", " Shinichi Shiraishi", " Tamer Basar"], "  In this paper, we propose a game theoretical adversarial intervention\ndetection mechanism for reliable smart road signs. A future trend in\nintelligent transportation systems is ``smart road signs\" that incorporate\nsmart codes (e.g., visible at infrared) on their surface to provide more\ndetailed information to smart vehicles. Such smart codes make road sign\nclassification problem aligned with communication settings more than\nconventional classification. This enables us to integrate well-established\nresults in communication theory, e.g., error-correction methods, into road sign\nclassification problem. Recently, vision-based road sign classification\nalgorithms have been shown to be vulnerable against (even) small scale\nadversarial interventions that are imperceptible for humans. On the other hand,\nsmart codes constructed via error-correction methods can lead to robustness\nagainst small scale intelligent or random perturbations on them. In the\nrecognition of smart road signs, however, humans are out of the loop since they\ncannot see or interpret them. Therefore, there is no equivalent concept of\nimperceptible perturbations in order to achieve a comparable performance with\nhumans. Robustness against small scale perturbations would not be sufficient\nsince the attacker can attack more aggressively without such a constraint.\nUnder a game theoretical solution concept, we seek to ensure certain measure of\nguarantees against even the worst case (intelligent) attackers that can perturb\nthe signal even at large scale. We provide a randomized detection strategy\nbased on the distance between the decoder output and the received input, i.e.,\nerror rate. Finally, we examine the performance of the proposed scheme over\nvarious scenarios.\n"], ["2019-01-28", "http://arxiv.org/abs/1901.09981", "Improving Adversarial Robustness of Ensembles with Diversity Training.", ["Sanjay Kariyappa", " Moinuddin K. Qureshi"], "  Deep Neural Networks are vulnerable to adversarial attacks even in settings\nwhere the attacker has no direct access to the model being attacked. Such\nattacks usually rely on the principle of transferability, whereby an attack\ncrafted on a surrogate model tends to transfer to the target model. We show\nthat an ensemble of models with misaligned loss gradients can provide an\neffective defense against transfer-based attacks. Our key insight is that an\nadversarial example is less likely to fool multiple models in the ensemble if\ntheir loss functions do not increase in a correlated fashion. To this end, we\npropose Diversity Training, a novel method to train an ensemble of models with\nuncorrelated loss functions. We show that our method significantly improves the\nadversarial robustness of ensembles and can also be combined with existing\nmethods to create a stronger defense.\n"], ["2019-01-28", "http://arxiv.org/abs/1901.09878", "CapsAttacks: Robust and Imperceptible Adversarial Attacks on Capsule Networks.", ["Alberto Marchisio", " Giorgio Nanfa", " Faiq Khalid", " Muhammad Abdullah Hanif", " Maurizio Martina", " Muhammad Shafique"], "  Capsule Networks preserve the hierarchical spatial relationships between\nobjects, and thereby bears a potential to surpass the performance of\ntraditional Convolutional Neural Networks (CNNs) in performing tasks like image\nclassification. A large body of work has explored adversarial examples for\nCNNs, but their effectiveness on Capsule Networks has not yet been well\nstudied. In our work, we perform an analysis to study the vulnerabilities in\nCapsule Networks to adversarial attacks. These perturbations, added to the test\ninputs, are small and imperceptible to humans, but can fool the network to\nmispredict. We propose a greedy algorithm to automatically generate targeted\nimperceptible adversarial examples in a black-box attack scenario. We show that\nthis kind of attacks, when applied to the German Traffic Sign Recognition\nBenchmark (GTSRB), mislead Capsule Networks. Moreover, we apply the same kind\nof adversarial attacks to a 5-layer CNN and a 9-layer CNN, and analyze the\noutcome, compared to the Capsule Networks to study differences in their\nbehavior.\n"], ["2019-01-28", "http://arxiv.org/abs/1901.09963", "Defense Methods Against Adversarial Examples for Recurrent Neural Networks.", ["Ishai Rosenberg", " Asaf Shabtai", " Yuval Elovici", " Lior Rokach"], "  Adversarial examples are known to mislead deep learning models to incorrectly\nclassify them, even in domains where such models achieve state-of-the-art\nperformance. Until recently, research on both attack and defense methods\nfocused on image recognition, primarily using convolutional neural networks\n(CNNs). In recent years, adversarial example generation methods for recurrent\nneural networks (RNNs) have been published, demonstrating that RNN classifiers\nare also vulnerable. In this paper, we present a novel defense method, termed\nsequence squeezing, to make RNN classifiers more robust against such attacks.\nOur method differs from previous defense methods which were designed only for\nnon-sequence based models. We also implement four additional RNN defense\nmethods inspired by current CNN defense methods. We evaluate our methods\nagainst state-of-the-art attacks in the cyber security domain where real\nadversaries (malware developers) exist, but our methods can be applied against\nany sequence based adversarial attack, e.g., in the NLP domain. Using our\nmethods we were able to decrease the effectiveness of such attack from 99.9% to\n15%.\n"], ["2019-01-28", "http://arxiv.org/abs/1901.09960", "Using Pre-Training Can Improve Model Robustness and Uncertainty.", ["Dan Hendrycks", " Kimin Lee", " Mantas Mazeika"], "  He et al. (2018) have called into question the utility of pre-training by\nshowing that training from scratch can often yield similar performance to\npre-training. We show that although pre-training may not improve performance on\ntraditional classification metrics, it improves model robustness and\nuncertainty estimates. Through extensive experiments on adversarial examples,\nlabel corruption, class imbalance, out-of-distribution detection, and\nconfidence calibration, we demonstrate large gains from pre-training and\ncomplementary effects with task-specific methods. We introduce adversarial\npre-training and show approximately a 10% absolute improvement over the\nprevious state-of-the-art in adversarial robustness. In some cases, using\npre-training without task-specific methods also surpasses the state-of-the-art,\nhighlighting the need for pre-training when evaluating future methods on\nrobustness and uncertainty tasks.\n"], ["2019-01-27", "http://arxiv.org/abs/1901.09413", "An Information-Theoretic Explanation for the Adversarial Fragility of AI Classifiers.", ["Hui Xie", " Jirong Yi", " Weiyu Xu", " Raghu Mudumbai"], "  We present a simple hypothesis about a compression property of artificial\nintelligence (AI) classifiers and present theoretical arguments to show that\nthis hypothesis successfully accounts for the observed fragility of AI\nclassifiers to small adversarial perturbations. We also propose a new method\nfor detecting when small input perturbations cause classifier errors, and show\ntheoretical guarantees for the performance of this detection method. We present\nexperimental results with a voice recognition system to demonstrate this\nmethod. The ideas in this paper are motivated by a simple analogy between AI\nclassifiers and the standard Shannon model of a communication system.\n"], ["2019-01-27", "http://arxiv.org/abs/1901.09496", "Characterizing the Shape of Activation Space in Deep Neural Networks.", ["Thomas Gebhart", " Paul Schrater", " Alan Hylton"], "  The representations learned by deep neural networks are difficult to\ninterpret in part due to their large parameter space and the complexities\nintroduced by their multi-layer structure. We introduce a method for computing\npersistent homology over the graphical activation structure of neural networks,\nwhich provides access to the task-relevant substructures activated throughout\nthe network for a given input. This topological perspective provides unique\ninsights into the distributed representations encoded by neural networks in\nterms of the shape of their activation structures. We demonstrate the value of\nthis approach by showing an alternative explanation for the existence of\nadversarial examples. By studying the topology of network activations across\nmultiple architectures and datasets, we find that adversarial perturbations do\nnot add activations that target the semantic structure of the adversarial class\nas previously hypothesized. Rather, adversarial examples are explainable as\nalterations to the dominant activation structures induced by the original\nimage, suggesting the class representations learned by deep networks are\nproblematically sparse on the input space.\n"], ["2019-01-27", "http://arxiv.org/abs/1901.09493", "Strong Black-box Adversarial Attacks on Unsupervised Machine Learning Models.", ["Anshuman Chhabra", " Abhishek Roy", " Prasant Mohapatra"], "  Machine Learning (ML) and Deep Learning (DL) models have achieved\nstate-of-the-art performance on multiple learning tasks, from vision to natural\nlanguage modelling. With the growing adoption of ML and DL to many areas of\ncomputer science, recent research has also started focusing on the security\nproperties of these models. There has been a lot of work undertaken to\nunderstand if (deep) neural network architectures are resilient to black-box\nadversarial attacks which craft perturbed input samples that fool the\nclassifier without knowing the architecture used. Recent work has also focused\non the transferability of adversarial attacks and found that adversarial\nattacks are generally easily transferable between models, datasets, and\ntechniques. However, such attacks and their analysis have not been covered from\nthe perspective of unsupervised machine learning algorithms. In this paper, we\nseek to bridge this gap through multiple contributions. We first provide a\nstrong (iterative) black-box adversarial attack that can craft adversarial\nsamples which will be incorrectly clustered irrespective of the choice of\nclustering algorithm. We choose 4 prominent clustering algorithms, and a\nreal-world dataset to show the working of the proposed adversarial algorithm.\nUsing these clustering algorithms we also carry out a simple study of\ncross-technique adversarial attack transferability.\n"], ["2019-01-26", "http://arxiv.org/abs/1901.09892", "A Black-box Attack on Neural Networks Based on Swarm Evolutionary Algorithm.", ["Xiaolei Liu", " Yuheng Luo", " Xiaosong Zhang", " Qingxin Zhu"], "  Neural networks play an increasingly important role in the field of machine\nlearning and are included in many applications in society. Unfortunately,\nneural networks suffer from adversarial samples generated to attack them.\nHowever, most of the generation approaches either assume that the attacker has\nfull knowledge of the neural network model or are limited by the type of\nattacked model. In this paper, we propose a new approach that generates a\nblack-box attack to neural networks based on the swarm evolutionary algorithm.\nBenefiting from the improvements in the technology and theoretical\ncharacteristics of evolutionary algorithms, our approach has the advantages of\neffectiveness, black-box attack, generality, and randomness. Our experimental\nresults show that both the MNIST images and the CIFAR-10 images can be\nperturbed to successful generate a black-box attack with 100\\% probability on\naverage. In addition, the proposed attack, which is successful on distilled\nneural networks with almost 100\\% probability, is resistant to defensive\ndistillation. The experimental results also indicate that the robustness of the\nartificial intelligence algorithm is related to the complexity of the model and\nthe data set. In addition, we find that the adversarial samples to some extent\nreproduce the characteristics of the sample data learned by the neural network\nmodel.\n"], ["2019-01-26", "http://arxiv.org/abs/1901.10300", "Towards Weighted-Sampling Audio Adversarial Example Attack.", ["Xiaolei Liu", " Xiaosong Zhang", " Kun Wan", " Qingxin Zhu", " Yufei Ding"], "  Recent studies have highlighted audio adversarial examples as a ubiquitous\nthreat to state-of-the-art automatic speech recognition systems. Thorough\nstudies on how to effectively generate adversarial examples are essential to\nprevent potential attacks. Despite many research on this, the efficiency and\nthe robustness of existing works are not yet satisfactory. In this paper, we\npropose~\\textit{weighted-sampling audio adversarial examples}, focusing on the\nnumbers and the weights of distortion to reinforce the attack. Further, we\napply a denoising method in the loss function to make the adversarial attack\nmore imperceptible. Experiments show that our method is the first in the field\nto generate audio adversarial examples with low noise and high audio robustness\nat the minute time-consuming level.\n"], ["2019-01-25", "http://arxiv.org/abs/1901.09113", "Generative Adversarial Networks for Black-Box API Attacks with Limited Training Data.", ["Yi Shi", " Yalin E. Sagduyu", " Kemal Davaslioglu", " Jason H. Li"], "  As online systems based on machine learning are offered to public or paid\nsubscribers via application programming interfaces (APIs), they become\nvulnerable to frequent exploits and attacks. This paper studies adversarial\nmachine learning in the practical case when there are rate limitations on API\ncalls. The adversary launches an exploratory (inference) attack by querying the\nAPI of an online machine learning system (in particular, a classifier) with\ninput data samples, collecting returned labels to build up the training data,\nand training an adversarial classifier that is functionally equivalent and\nstatistically close to the target classifier. The exploratory attack with\nlimited training data is shown to fail to reliably infer the target classifier\nof a real text classifier API that is available online to the public. In\nreturn, a generative adversarial network (GAN) based on deep learning is built\nto generate synthetic training data from a limited number of real training data\nsamples, thereby extending the training data and improving the performance of\nthe inferred classifier. The exploratory attack provides the basis to launch\nthe causative attack (that aims to poison the training process) and evasion\nattack (that aims to fool the classifier into making wrong decisions) by\nselecting training and test data samples, respectively, based on the confidence\nscores obtained from the inferred classifier. These stealth attacks with small\nfootprint (using a small number of API calls) make adversarial machine learning\npractical under the realistic case with limited training data available to the\nadversary.\n"], ["2019-01-25", "http://arxiv.org/abs/1901.08846", "Improving Adversarial Robustness via Promoting Ensemble Diversity.", ["Tianyu Pang", " Kun Xu", " Chao Du", " Ning Chen", " Jun Zhu"], "  Though deep neural networks have achieved significant progress on various\ntasks, often enhanced by model ensemble, existing high-performance models can\nbe vulnerable to adversarial attacks. Many efforts have been devoted to\nenhancing the robustness of individual networks and then constructing a\nstraightforward ensemble, e.g., by directly averaging the outputs, which\nignores the interaction among networks. This paper presents a new method that\nexplores the interaction among individual networks to improve robustness for\nensemble models. Technically, we define a new notion of ensemble diversity in\nthe adversarial setting as the diversity among non-maximal predictions of\nindividual members, and present an adaptive diversity promoting (ADP)\nregularizer to encourage the diversity, which leads to globally better\nrobustness for the ensemble by making adversarial examples difficult to\ntransfer among individual members. Our method is computationally efficient and\ncompatible with the defense methods acting on individual networks. Empirical\nresults on various datasets verify that our method can improve adversarial\nrobustness while maintaining state-of-the-art accuracy on normal examples.\n"], ["2019-01-24", "http://arxiv.org/abs/1901.09035", "Towards Interpretable Deep Neural Networks by Leveraging Adversarial Examples.", ["Yinpeng Dong", " Fan Bao", " Hang Su", " Jun Zhu"], "  Sometimes it is not enough for a DNN to produce an outcome. For example, in\napplications such as healthcare, users need to understand the rationale of the\ndecisions. Therefore, it is imperative to develop algorithms to learn models\nwith good interpretability (Doshi-Velez 2017). An important factor that leads\nto the lack of interpretability of DNNs is the ambiguity of neurons, where a\nneuron may fire for various unrelated concepts. This work aims to increase the\ninterpretability of DNNs on the whole image space by reducing the ambiguity of\nneurons. In this paper, we make the following contributions:\n  1) We propose a metric to evaluate the consistency level of neurons in a\nnetwork quantitatively.\n  2) We find that the learned features of neurons are ambiguous by leveraging\nadversarial examples.\n  3) We propose to improve the consistency of neurons on adversarial example\nsubset by an adversarial training algorithm with a consistent loss.\n"], ["2019-01-24", "http://arxiv.org/abs/1901.08360", "Cross-Entropy Loss and Low-Rank Features Have Responsibility for Adversarial Examples.", ["Kamil Nar", " Orhan Ocal", " S. Shankar Sastry", " Kannan Ramchandran"], "  State-of-the-art neural networks are vulnerable to adversarial examples; they\ncan easily misclassify inputs that are imperceptibly different than their\ntraining and test data. In this work, we establish that the use of\ncross-entropy loss function and the low-rank features of the training data have\nresponsibility for the existence of these inputs. Based on this observation, we\nsuggest that addressing adversarial examples requires rethinking the use of\ncross-entropy loss function and looking for an alternative that is more suited\nfor minimization with low-rank features. In this direction, we present a\ntraining scheme called differential training, which uses a loss function\ndefined on the differences between the features of points from opposite\nclasses. We show that differential training can ensure a large margin between\nthe decision boundary of the neural network and the points in the training\ndataset. This larger margin increases the amount of perturbation needed to flip\nthe prediction of the classifier and makes it harder to find an adversarial\nexample with small perturbations. We test differential training on a binary\nclassification task with CIFAR-10 dataset and demonstrate that it radically\nreduces the ratio of images for which an adversarial example could be found --\nnot only in the training dataset, but in the test dataset as well.\n"], ["2019-01-24", "http://arxiv.org/abs/1901.08573", "Theoretically Principled Trade-off between Robustness and Accuracy.", ["Hongyang Zhang", " Yaodong Yu", " Jiantao Jiao", " Eric P. Xing", " Laurent El Ghaoui", " Michael I. Jordan"], "  We identify a trade-off between robustness and accuracy that serves as a\nguiding principle in the design of defenses against adversarial examples.\nAlthough this problem has been widely studied empirically, much remains unknown\nconcerning the theory underlying this trade-off. In this work, we decompose the\nprediction error for adversarial examples (robust error) as the sum of the\nnatural (classification) error and boundary error, and provide a differentiable\nupper bound using the theory of classification-calibrated loss, which is shown\nto be the tightest possible upper bound uniform over all probability\ndistributions and measurable predictors. Inspired by our theoretical analysis,\nwe also design a new defense method, TRADES, to trade adversarial robustness\noff against accuracy. Our proposed algorithm performs well experimentally in\nreal-world datasets. The methodology is the foundation of our entry to the\nNeurIPS 2018 Adversarial Vision Challenge in which we won the 1st place out of\n~2,000 submissions, surpassing the runner-up approach by $11.41\\%$ in terms of\nmean $\\ell_2$ perturbation distance.\n"], ["2019-01-23", "http://arxiv.org/abs/1901.08121", "Sitatapatra: Blocking the Transfer of Adversarial Samples.", ["Ilia Shumailov", " Xitong Gao", " Yiren Zhao", " Robert Mullins", " Ross Anderson", " Cheng-Zhong Xu"], "  Convolutional Neural Networks (CNNs) are widely used to solve classification\ntasks in computer vision. However, they can be tricked into misclassifying\nspecially crafted `adversarial' samples -- and samples built to trick one model\noften work alarmingly well against other models trained on the same task. In\nthis paper we introduce Sitatapatra, a system designed to block the transfer of\nadversarial samples. It diversifies neural networks using a key, as in\ncryptography, and provides a mechanism for detecting attacks. What's more, when\nadversarial samples are detected they can typically be traced back to the\nindividual device that was used to develop them. The run-time overheads are\nminimal permitting the use of Sitatapatra on constrained systems.\n"], ["2019-01-23", "http://arxiv.org/abs/1901.07846", "SirenAttack: Generating Adversarial Audio for End-to-End Acoustic Systems.", ["Tianyu Du", " Shouling Ji", " Jinfeng Li", " Qinchen Gu", " Ting Wang", " Raheem Beyah"], "  Despite their immense popularity, deep learning-based acoustic systems are\ninherently vulnerable to adversarial attacks, wherein maliciously crafted\naudios trigger target systems to misbehave. In this paper, we present\nSirenAttack, a new class of attacks to generate adversarial audios. Compared\nwith existing attacks, SirenAttack highlights with a set of significant\nfeatures: (i) versatile -- it is able to deceive a range of end-to-end acoustic\nsystems under both white-box and black-box settings; (ii) effective -- it is\nable to generate adversarial audios that can be recognized as specific phrases\nby target acoustic systems; and (iii) stealthy -- it is able to generate\nadversarial audios indistinguishable from their benign counterparts to human\nperception. We empirically evaluate SirenAttack on a set of state-of-the-art\ndeep learning-based acoustic systems (including speech command recognition,\nspeaker recognition and sound event classification), with results showing the\nversatility, effectiveness, and stealthiness of SirenAttack. For instance, it\nachieves 99.45% attack success rate on the IEMOCAP dataset against the ResNet18\nmodel, while the generated adversarial audios are also misinterpreted by\nmultiple popular ASR platforms, including Google Cloud Speech, Microsoft Bing\nVoice, and IBM Speech-to-Text. We further evaluate three potential defense\nmethods to mitigate such attacks, including adversarial training, audio\ndownsampling, and moving average filtering, which leads to promising directions\nfor further research.\n"], ["2019-01-21", "http://arxiv.org/abs/1901.07152", "Sensitivity Analysis of Deep Neural Networks.", ["Hai Shu", " Hongtu Zhu"], "  Deep neural networks (DNNs) have achieved superior performance in various\nprediction tasks, but can be very vulnerable to adversarial examples or\nperturbations. Therefore, it is crucial to measure the sensitivity of DNNs to\nvarious forms of perturbations in real applications. We introduce a novel\nperturbation manifold and its associated influence measure to quantify the\neffects of various perturbations on DNN classifiers. Such perturbations include\nvarious external and internal perturbations to input samples and network\nparameters. The proposed measure is motivated by information geometry and\nprovides desirable invariance properties. We demonstrate that our influence\nmeasure is useful for four model building tasks: detecting potential\n'outliers', analyzing the sensitivity of model architectures, comparing network\nsensitivity between training and test sets, and locating vulnerable areas.\nExperiments show reasonably good performance of the proposed measure for the\npopular DNN models ResNet50 and DenseNet121 on CIFAR10 and MNIST datasets.\n"], ["2019-01-21", "http://arxiv.org/abs/1901.07132", "Universal Rules for Fooling Deep Neural Networks based Text Classification.", ["Di Li", " Danilo Vasconcellos Vargas", " Sakurai Kouichi"], "  Recently, deep learning based natural language processing techniques are\nbeing extensively used to deal with spam mail, censorship evaluation in social\nnetworks, among others. However, there is only a couple of works evaluating the\nvulnerabilities of such deep neural networks. Here, we go beyond attacks to\ninvestigate, for the first time, universal rules, i.e., rules that are sample\nagnostic and therefore could turn any text sample in an adversarial one. In\nfact, the universal rules do not use any information from the method itself (no\ninformation from the method, gradient information or training dataset\ninformation is used), making them black-box universal attacks. In other words,\nthe universal rules are sample and method agnostic. By proposing a\ncoevolutionary optimization algorithm we show that it is possible to create\nuniversal rules that can automatically craft imperceptible adversarial samples\n(only less than five perturbations which are close to misspelling are inserted\nin the text sample). A comparison with a random search algorithm further\njustifies the strength of the method. Thus, universal rules for fooling\nnetworks are here shown to exist. Hopefully, the results from this work will\nimpact the development of yet more sample and model agnostic attacks as well as\ntheir defenses, culminating in perhaps a new age for artificial intelligence.\n"], ["2019-01-21", "http://arxiv.org/abs/1901.06834", "Perception-in-the-Loop Adversarial Examples.", ["Mahmoud Salamati", " Sadegh Soudjani", " Rupak Majumdar"], "  We present a scalable, black box, perception-in-the-loop technique to find\nadversarial examples for deep neural network classifiers. Black box means that\nour procedure only has input-output access to the classifier, and not to the\ninternal structure, parameters, or intermediate confidence values.\nPerception-in-the-loop means that the notion of proximity between inputs can be\ndirectly queried from human participants rather than an arbitrarily chosen\nmetric. Our technique is based on covariance matrix adaptation evolution\nstrategy (CMA-ES), a black box optimization approach. CMA-ES explores the\nsearch space iteratively in a black box manner, by generating populations of\ncandidates according to a distribution, choosing the best candidates according\nto a cost function, and updating the posterior distribution to favor the best\ncandidates. We run CMA-ES using human participants to provide the fitness\nfunction, using the insight that the choice of best candidates in CMA-ES can be\nnaturally modeled as a perception task: pick the top $k$ inputs perceptually\nclosest to a fixed input. We empirically demonstrate that finding adversarial\nexamples is feasible using small populations and few iterations. We compare the\nperformance of CMA-ES on the MNIST benchmark with other black-box approaches\nusing $L_p$ norms as a cost function, and show that it performs favorably both\nin terms of success in finding adversarial examples and in minimizing the\ndistance between the original and the adversarial input. In experiments on the\nMNIST, CIFAR10, and GTSRB benchmarks, we demonstrate that CMA-ES can find\nperceptually similar adversarial inputs with a small number of iterations and\nsmall population sizes when using perception-in-the-loop. Finally, we show that\nnetworks trained specifically to be robust against $L_\\infty$ norm can still be\nsusceptible to perceptually similar adversarial examples.\n"], ["2019-01-21", "http://arxiv.org/abs/1901.06796", "Adversarial Attacks on Deep Learning Models in Natural Language Processing: A Survey.", ["Wei Emma Zhang", " Quan Z. Sheng", " Ahoud Alhazmi", " Chenliang Li"], "  With the development of high computational devices, deep neural networks\n(DNNs), in recent years, have gained significant popularity in many Artificial\nIntelligence (AI) applications. However, previous efforts have shown that DNNs\nwere vulnerable to strategically modified samples, named adversarial examples.\nThese samples are generated with some imperceptible perturbations but can fool\nthe DNNs to give false predictions. Inspired by the popularity of generating\nadversarial examples for image DNNs, research efforts on attacking DNNs for\ntextual applications emerges in recent years. However, existing perturbation\nmethods for images cannotbe directly applied to texts as text data is discrete.\nIn this article, we review research works that address this difference and\ngeneratetextual adversarial examples on DNNs. We collect, select, summarize,\ndiscuss and analyze these works in a comprehensive way andcover all the related\ninformation to make the article self-contained. Finally, drawing on the\nreviewed literature, we provide further discussions and suggestions on this\ntopic.\n"], ["2019-01-17", "http://arxiv.org/abs/1901.05674", "Easy to Fool? Testing the Anti-evasion Capabilities of PDF Malware Scanners.", ["Saeed TU Darmstadt Ehteshamifar", " Antonio xorlab Barresi", " Thomas R. ETH Zurich Gross", " Michael TU Darmstadt Pradel"], "  Malware scanners try to protect users from opening malicious documents by\nstatically or dynamically analyzing documents. However, malware developers may\napply evasions that conceal the maliciousness of a document. Given the variety\nof existing evasions, systematically assessing the impact of evasions on\nmalware scanners remains an open challenge. This paper presents a novel\nmethodology for testing the capability of malware scanners to cope with\nevasions. We apply the methodology to malicious Portable Document Format (PDF)\ndocuments and present an in-depth study of how current PDF evasions affect 41\nstate-of-the-art malware scanners. The study is based on a framework for\ncreating malicious PDF documents that use one or more evasions. Based on such\ndocuments, we measure how effective different evasions are at concealing the\nmaliciousness of a document. We find that many static and dynamic scanners can\nbe easily fooled by relatively simple evasions and that the effectiveness of\ndifferent evasions varies drastically. Our work not only is a call to arms for\nimproving current malware scanners, but by providing a large-scale corpus of\nmalicious PDF documents with evasions, we directly support the development of\nimproved tools to detect document-based malware. Moreover, our methodology\npaves the way for a quantitative evaluation of evasions in other kinds of\nmalware.\n"], ["2019-01-15", "http://arxiv.org/abs/1901.04684", "The Limitations of Adversarial Training and the Blind-Spot Attack.", ["Huan Zhang", " Hongge Chen", " Zhao Song", " Duane Boning", " Inderjit S. Dhillon", " Cho-Jui Hsieh"], "  The adversarial training procedure proposed by Madry et al. (2018) is one of\nthe most effective methods to defend against adversarial examples in deep\nneural networks (DNNs). In our paper, we shed some lights on the practicality\nand the hardness of adversarial training by showing that the effectiveness\n(robustness on test set) of adversarial training has a strong correlation with\nthe distance between a test point and the manifold of training data embedded by\nthe network. Test examples that are relatively far away from this manifold are\nmore likely to be vulnerable to adversarial attacks. Consequentially, an\nadversarial training based defense is susceptible to a new class of attacks,\nthe \"blind-spot attack\", where the input images reside in \"blind-spots\" (low\ndensity regions) of the empirical distribution of training data but is still on\nthe ground-truth data manifold. For MNIST, we found that these blind-spots can\nbe easily found by simply scaling and shifting image pixel values. Most\nimportantly, for large datasets with high dimensional and complex data manifold\n(CIFAR, ImageNet, etc), the existence of blind-spots in adversarial training\nmakes defending on any valid test examples difficult due to the curse of\ndimensionality and the scarcity of training data. Additionally, we find that\nblind-spots also exist on provable defenses including (Wong & Kolter, 2018) and\n(Sinha et al., 2018) because these trainable robustness certificates can only\nbe practically optimized on a limited set of training data.\n"], ["2019-01-13", "http://arxiv.org/abs/1901.03706", "Generating Adversarial Perturbation with Root Mean Square Gradient.", ["Yatie Xiao", " Chi-Man Pun", " Jizhe Zhou"], "  Deep Neural Models are vulnerable to adversarial perturbations in\nclassification. Many attack methods generate adversarial examples with large\npixel modification and low cosine similarity with original images. In this\npaper, we propose an adversarial method generating perturbations based on root\nmean square gradient which formulates adversarial perturbation size in root\nmean square level and update gradient in direction, due to updating gradients\nwith adaptive and root mean square stride, our method map origin, and\ncorresponding adversarial image directly which shows good transferability in\nadversarial examples generation. We evaluate several traditional perturbations\ncreating ways in image classification with our methods. Experimental results\nshow that our approach works well and outperform recent techniques in the\nchange of misclassifying image classification with slight pixel modification,\nand excellent efficiency in fooling deep network models.\n"], ["2019-01-12", "http://arxiv.org/abs/1901.03808", "ECGadv: Generating Adversarial Electrocardiogram to Misguide Arrhythmia Classification System.", ["Huangxun Chen", " Chenyu Huang", " Qianyi Huang", " Qian Zhang", " Wei Wang"], "  Deep neural networks (DNNs)-powered Electrocardiogram (ECG) diagnosis systems\nrecently achieve promising progress to take over tedious examinations by\ncardiologists. However, their vulnerability to adversarial attacks still lack\ncomprehensive investigation. The existing attacks in image domain could not be\ndirectly applicable due to the distinct properties of ECGs in visualization and\ndynamic properties. Thus, this paper takes a step to thoroughly explore\nadversarial attacks on the DNN-powered ECG diagnosis system. We analyze the\nproperties of ECGs to design effective attacks schemes under two attacks models\nrespectively. Our results demonstrate the blind spots of DNN-powered diagnosis\nsystems under adversarial attacks, which calls attention to adequate\ncountermeasures.\n"], ["2019-01-11", "http://arxiv.org/abs/1901.03583", "Explaining Vulnerabilities of Deep Learning to Adversarial Malware Binaries.", ["Luca Demetrio", " Battista Biggio", " Giovanni Lagorio", " Fabio Roli", " Alessandro Armando"], "  Recent work has shown that deep-learning algorithms for malware detection are\nalso susceptible to adversarial examples, i.e., carefully-crafted perturbations\nto input malware that enable misleading classification. Although this has\nquestioned their suitability for this task, it is not yet clear why such\nalgorithms are easily fooled also in this particular application domain. In\nthis work, we take a first step to tackle this issue by leveraging explainable\nmachine-learning algorithms developed to interpret the black-box decisions of\ndeep neural networks. In particular, we use an explainable technique known as\nfeature attribution to identify the most influential input features\ncontributing to each decision, and adapt it to provide meaningful explanations\nto the classification of malware binaries. In this case, we find that a\nrecently-proposed convolutional neural network does not learn any meaningful\ncharacteristic for malware detection from the data and text sections of\nexecutable files, but rather tends to learn to discriminate between benign and\nmalware samples based on the characteristics found in the file header. Based on\nthis finding, we propose a novel attack algorithm that generates adversarial\nmalware binaries by only changing few tens of bytes in the file header. With\nrespect to the other state-of-the-art attack algorithms, our attack does not\nrequire injecting any padding bytes at the end of the file, and it is much more\nefficient, as it requires manipulating much fewer bytes.\n"], ["2019-01-10", "http://arxiv.org/abs/1901.03398", "Characterizing and evaluating adversarial examples for Offline Handwritten Signature Verification.", ["Luiz G. Hafemann", " Robert Sabourin", " Luiz S. Oliveira"], "  The phenomenon of Adversarial Examples is attracting increasing interest from\nthe Machine Learning community, due to its significant impact to the security\nof Machine Learning systems. Adversarial examples are similar (from a\nperceptual notion of similarity) to samples from the data distribution, that\n\"fool\" a machine learning classifier. For computer vision applications, these\nare images with carefully crafted but almost imperceptible changes, that are\nmisclassified. In this work, we characterize this phenomenon under an existing\ntaxonomy of threats to biometric systems, in particular identifying new attacks\nfor Offline Handwritten Signature Verification systems. We conducted an\nextensive set of experiments on four widely used datasets: MCYT-75, CEDAR,\nGPDS-160 and the Brazilian PUC-PR, considering both a CNN-based system and a\nsystem using a handcrafted feature extractor (CLBP). We found that attacks that\naim to get a genuine signature rejected are easy to generate, even in a limited\nknowledge scenario, where the attacker does not have access to the trained\nclassifier nor the signatures used for training. Attacks that get a forgery to\nbe accepted are harder to produce, and often require a higher level of noise -\nin most cases, no longer \"imperceptible\" as previous findings in object\nrecognition. We also evaluated the impact of two countermeasures on the success\nrate of the attacks and the amount of noise required for generating successful\nattacks.\n"], ["2019-01-10", "http://arxiv.org/abs/1901.03037", "Image Transformation can make Neural Networks more robust against Adversarial Examples.", ["Dang Duy Thang", " Toshihiro Matsui"], "  Neural networks are being applied in many tasks related to IoT with\nencouraging results. For example, neural networks can precisely detect human,\nobjects and animal via surveillance camera for security purpose. However,\nneural networks have been recently found vulnerable to well-designed input\nsamples that called adversarial examples. Such issue causes neural networks to\nmisclassify adversarial examples that are imperceptible to humans. We found\ngiving a rotation to an adversarial example image can defeat the effect of\nadversarial examples. Using MNIST number images as the original images, we\nfirst generated adversarial examples to neural network recognizer, which was\ncompletely fooled by the forged examples. Then we rotated the adversarial image\nand gave them to the recognizer to find the recognizer to regain the correct\nrecognition. Thus, we empirically confirmed rotation to images can protect\npattern recognizer based on neural networks from adversarial example attacks.\n"], ["2019-01-09", "http://arxiv.org/abs/1901.03006", "Extending Adversarial Attacks and Defenses to Deep 3D Point Cloud Classifiers.", ["Daniel Liu", " Ronald Yu", " Hao Su"], "  3D object classification and segmentation using deep neural networks has been\nextremely successful. As the problem of identifying 3D objects has many\nsafety-critical applications, the neural networks have to be robust against\nadversarial changes to the input data set. There is a growing body of research\non generating human-imperceptible adversarial attacks and defenses against them\nin the 2D image classification domain. However, 3D objects have various\ndifferences with 2D images, and this specific domain has not been rigorously\nstudied so far.\n  We present a preliminary evaluation of adversarial attacks on deep 3D point\ncloud classifiers, namely PointNet and PointNet++, by evaluating both white-box\nand black-box adversarial attacks that were proposed for 2D images and\nextending those attacks to reduce the perceptibility of the perturbations in 3D\nspace. We also show the high effectiveness of simple defenses against those\nattacks by proposing new defenses that exploit the unique structure of 3D point\nclouds. Finally, we attempt to explain the effectiveness of the defenses\nthrough the intrinsic structures of both the point clouds and the neural\nnetwork architectures. Overall, we find that networks that process 3D point\ncloud data are weak to adversarial attacks, but they are also more easily\ndefensible compared to 2D image classifiers. Our investigation will provide the\ngroundwork for future studies on improving the robustness of deep neural\nnetworks that handle 3D data.\n"], ["2019-01-08", "http://arxiv.org/abs/1901.02229", "Interpretable BoW Networks for Adversarial Example Detection.", ["Krishna Kanth Nakka", " Mathieu Salzmann"], "  The standard approach to providing interpretability to deep convolutional\nneural networks (CNNs) consists of visualizing either their feature maps, or\nthe image regions that contribute the most to the prediction. In this paper, we\nintroduce an alternative strategy to interpret the results of a CNN. To this\nend, we leverage a Bag of visual Word representation within the network and\nassociate a visual and semantic meaning to the corresponding codebook elements\nvia the use of a generative adversarial network. The reason behind the\nprediction for a new sample can then be interpreted by looking at the visual\nrepresentation of the most highly activated codeword. We then propose to\nexploit our interpretable BoW networks for adversarial example detection. To\nthis end, we build upon the intuition that, while adversarial samples look very\nsimilar to real images, to produce incorrect predictions, they should activate\ncodewords with a significantly different visual representation. We therefore\ncast the adversarial example detection problem as that of comparing the input\nimage with the most highly activated visual codeword. As evidenced by our\nexperiments, this allows us to outperform the state-of-the-art adversarial\nexample detection methods on standard benchmarks, independently of the attack\nstrategy.\n"], ["2019-01-07", "http://arxiv.org/abs/1901.01677", "Image Super-Resolution as a Defense Against Adversarial Attacks.", ["Aamir Mustafa", " Salman H. Khan", " Munawar Hayat", " Jianbing Shen", " Ling Shao"], "  Convolutional Neural Networks have achieved significant success across\nmultiple computer vision tasks. However, they are vulnerable to carefully\ncrafted, human-imperceptible adversarial noise patterns which constrain their\ndeployment in critical security-sensitive systems. This paper proposes a\ncomputationally efficient image enhancement approach that provides a strong\ndefense mechanism to effectively mitigate the effect of such adversarial\nperturbations. We show that deep image restoration networks learn mapping\nfunctions that can bring off-the-manifold adversarial samples onto the natural\nimage manifold, thus restoring classification towards correct classes. A\ndistinguishing feature of our approach is that, in addition to providing\nrobustness against attacks, it simultaneously enhances image quality and\nretains models performance on clean images. Furthermore, the proposed method\ndoes not modify the classifier or requires a separate mechanism to detect\nadversarial images. The effectiveness of the scheme has been demonstrated\nthrough extensive experiments, where it has proven a strong defense in gray-box\nsettings. The proposed scheme is simple and has the following advantages: (1)\nit does not require any model training or parameter optimization, (2) it\ncomplements other existing defense mechanisms, (3) it is agnostic to the\nattacked model and attack type and (4) it provides superior performance across\nall popular attack algorithms. Our codes are publicly available at\nhttps://github.com/aamir-mustafa/super-resolution-adversarial-defense.\n"], ["2019-01-05", "http://arxiv.org/abs/1901.09657", "Fake News Detection via NLP is Vulnerable to Adversarial Attacks.", ["Zhixuan Zhou", " Huankang Guan", " Meghana Moorthy Bhat", " Justin Hsu"], "  News plays a significant role in shaping people's beliefs and opinions. Fake\nnews has always been a problem, which wasn't exposed to the mass public until\nthe past election cycle for the 45th President of the United States. While\nquite a few detection methods have been proposed to combat fake news since\n2015, they focus mainly on linguistic aspects of an article without any fact\nchecking. In this paper, we argue that these models have the potential to\nmisclassify fact-tampering fake news as well as under-written real news.\nThrough experiments on Fakebox, a state-of-the-art fake news detector, we show\nthat fact tampering attacks can be effective. To address these weaknesses, we\nargue that fact checking should be adopted in conjunction with linguistic\ncharacteristics analysis, so as to truly separate fake news from real news. A\ncrowdsourced knowledge graph is proposed as a straw man solution to collecting\ntimely facts about news events.\n"], ["2019-01-04", "http://arxiv.org/abs/1901.01223", "Adversarial Examples Versus Cloud-based Detectors: A Black-box Empirical Study.", ["Xurong Li", " Shouling Ji", " Meng Han", " Juntao Ji", " Zhenyu Ren", " Yushan Liu", " Chunming Wu"], "  Deep learning has been broadly leveraged by major cloud providers, such as\nGoogle, AWS and Baidu, to offer various computer vision related services\nincluding image classification, object identification, illegal image detection,\netc. While recent works extensively demonstrated that deep learning\nclassification models are vulnerable to adversarial examples, cloud-based image\ndetection models, which are more complicated than classifiers, may also have\nsimilar security concern but not get enough attention yet. In this paper, we\nmainly focus on the security issues of real-world cloud-based image detectors.\nSpecifically, (1) based on effective semantic segmentation, we propose four\nattacks to generate semantics-aware adversarial examples via only interacting\nwith black-box APIs; and (2) we make the first attempt to conduct an extensive\nempirical study of black-box attacks against real-world cloud-based image\ndetectors. Through the comprehensive evaluations on five major cloud platforms:\nAWS, Azure, Google Cloud, Baidu Cloud, and Alibaba Cloud, we demonstrate that\nour image processing based attacks can reach a success rate of approximately\n100%, and the semantic segmentation based attacks have a success rate over 90%\namong different detection services, such as violence, politician, and\npornography detection. We also proposed several possible defense strategies for\nthese security challenges in the real-life situation.\n"], ["2019-01-02", "http://arxiv.org/abs/1901.00546", "Multi-Label Adversarial Perturbations.", ["Qingquan Song", " Haifeng Jin", " Xiao Huang", " Xia Hu"], "  Adversarial examples are delicately perturbed inputs, which aim to mislead\nmachine learning models towards incorrect outputs. While most of the existing\nwork focuses on generating adversarial perturbations in multi-class\nclassification problems, many real-world applications fall into the multi-label\nsetting in which one instance could be associated with more than one label. For\nexample, a spammer may generate adversarial spams with malicious advertising\nwhile maintaining the other labels such as topic labels unchanged. To analyze\nthe vulnerability and robustness of multi-label learning models, we investigate\nthe generation of multi-label adversarial perturbations. This is a challenging\ntask due to the uncertain number of positive labels associated with one\ninstance, as well as the fact that multiple labels are usually not mutually\nexclusive with each other. To bridge this gap, in this paper, we propose a\ngeneral attacking framework targeting on multi-label classification problem and\nconduct a premier analysis on the perturbations for deep neural networks.\nLeveraging the ranking relationships among labels, we further design a\nranking-based framework to attack multi-label ranking algorithms. We specify\nthe connection between the two proposed frameworks and separately design two\nspecific methods grounded on each of them to generate targeted multi-label\nperturbations. Experiments on real-world multi-label image classification and\nranking problems demonstrate the effectiveness of our proposed frameworks and\nprovide insights of the vulnerability of multi-label deep learning models under\ndiverse targeted attacking strategies. Several interesting findings including\nan unpolished defensive strategy, which could potentially enhance the\ninterpretability and robustness of multi-label deep learning models, are\nfurther presented and discussed at the end.\n"], ["2019-01-02", "http://arxiv.org/abs/1901.00532", "Adversarial Robustness May Be at Odds With Simplicity.", ["Preetum Nakkiran"], "  Current techniques in machine learning are so far are unable to learn\nclassifiers that are robust to adversarial perturbations. However, they are\nable to learn non-robust classifiers with very high accuracy, even in the\npresence of random perturbations. Towards explaining this gap, we highlight the\nhypothesis that $\\textit{robust classification may require more complex\nclassifiers (i.e. more capacity) than standard classification.}$\n  In this note, we show that this hypothesis is indeed possible, by giving\nseveral theoretical examples of classification tasks and sets of \"simple\"\nclassifiers for which: (1) There exists a simple classifier with high standard\naccuracy, and also high accuracy under random $\\ell_\\infty$ noise. (2) Any\nsimple classifier is not robust: it must have high adversarial loss with\n$\\ell_\\infty$ perturbations. (3) Robust classification is possible, but only\nwith more complex classifiers (exponentially more complex, in some examples).\n  Moreover, $\\textit{there is a quantitative trade-off between robustness and\nstandard accuracy among simple classifiers.}$ This suggests an alternate\nexplanation of this phenomenon, which appears in practice: the tradeoff may\noccur not because the classification task inherently requires such a tradeoff\n(as in [Tsipras-Santurkar-Engstrom-Turner-Madry `18]), but because the\nstructure of our current classifiers imposes such a tradeoff.\n"], ["2019-01-01", "http://arxiv.org/abs/1901.00054", "A Noise-Sensitivity-Analysis-Based Test Prioritization Technique for Deep Neural Networks.", ["Long Zhang", " Xuechao Sun", " Yong Li", " Zhenyu Zhang"], "  Deep neural networks (DNNs) have been widely used in the fields such as\nnatural language processing, computer vision and image recognition. But several\nstudies have been shown that deep neural networks can be easily fooled by\nartificial examples with some perturbations, which are widely known as\nadversarial examples. Adversarial examples can be used to attack deep neural\nnetworks or to improve the robustness of deep neural networks. A common way of\ngenerating adversarial examples is to first generate some noises and then add\nthem into original examples. In practice, different examples have different\nnoise-sensitive. To generate an effective adversarial example, it may be\nnecessary to add a lot of noise to low noise-sensitive example, which may make\nthe adversarial example meaningless. In this paper, we propose a\nnoise-sensitivity-analysis-based test prioritization technique to pick out\nexamples by their noise sensitivity. We construct an experiment to validate our\napproach on four image sets and two DNN models, which shows that examples are\nsensitive to noise and our method can effectively pick out examples by their\nnoise sensitivity.\n"], ["2018-12-27", "http://arxiv.org/abs/1812.10812", "DeepBillboard: Systematic Physical-World Testing of Autonomous Driving Systems.", ["Husheng Zhou", " Wei Li", " Yuankun Zhu", " Yuqun Zhang", " Bei Yu", " Lingming Zhang", " Cong Liu"], "  Deep Neural Networks (DNNs) have been widely applied in many autonomous\nsystems such as autonomous driving. Recently, DNN testing has been intensively\nstudied to automatically generate adversarial examples, which inject\nsmall-magnitude perturbations into inputs to test DNNs under extreme\nsituations. While existing testing techniques prove to be effective, they\nmostly focus on generating digital adversarial perturbations (particularly for\nautonomous driving), e.g., changing image pixels, which may never happen in\nphysical world. There is a critical missing piece in the literature on\nautonomous driving testing: understanding and exploiting both digital and\nphysical adversarial perturbation generation for impacting steering decisions.\nIn this paper, we present DeepBillboard, a systematic physical-world testing\napproach targeting at a common and practical driving scenario: drive-by\nbillboards. DeepBillboard is capable of generating a robust and resilient\nprintable adversarial billboard, which works under dynamic changing driving\nconditions including viewing angle, distance, and lighting. The objective is to\nmaximize the possibility, degree, and duration of the steering-angle errors of\nan autonomous vehicle driving by the generated adversarial billboard. We have\nextensively evaluated the efficacy and robustness of DeepBillboard through\nconducting both digital and physical-world experiments. Results show that\nDeepBillboard is effective for various steering models and scenes. Furthermore,\nDeepBillboard is sufficiently robust and resilient for generating\nphysical-world adversarial billboard tests for real-world driving under various\nweather conditions. To the best of our knowledge, this is the first study\ndemonstrating the possibility of generating realistic and continuous\nphysical-world tests for practical autonomous driving systems.\n"], ["2018-12-26", "http://arxiv.org/abs/1812.10528", "Adversarial Attack and Defense on Graph Data: A Survey.", ["Lichao Sun", " Ji Wang", " Philip S. Yu", " Bo Li"], "  Deep neural networks (DNNs) have been widely applied in various applications\ninvolving image, text, audio, and graph data. However, recent studies have\nshown that DNNs are vulnerable to adversarial attack. Though there are several\nworks studying adversarial attack and defense on domains such as images and\ntext processing, it is difficult to directly transfer the learned knowledge to\ngraph data due to its representation challenge. Given the importance of graph\nanalysis, increasing number of works start to analyze the robustness of machine\nlearning models on graph. Nevertheless, current studies considering adversarial\nbehaviors on graph data usually focus on specific types of attacks with certain\nassumptions. In addition, each work proposes its own mathematical formulation\nwhich makes the comparison among different methods difficult. Therefore, in\nthis paper, we aim to survey existing adversarial attack strategies on graph\ndata and provide an unified problem formulation which can cover all current\nadversarial learning studies on graph. We also compare different attacks on\ngraph data and discuss their corresponding contributions and limitations.\nFinally, we discuss several future research directions in this area.\n"], ["2018-12-25", "http://arxiv.org/abs/1812.10199", "A Multiversion Programming Inspired Approach to Detecting Audio Adversarial Examples.", ["Qiang Zeng", " Jianhai Su", " Chenglong Fu", " Golam Kayas", " Lannan Luo"], "  Adversarial examples (AEs) are crafted by adding human-imperceptible\nperturbations to inputs such that a machine-learning based classifier\nincorrectly labels them. They have become a severe threat to the\ntrustworthiness of machine learning. While AEs in the image domain have been\nwell studied, audio AEs are less investigated. Recently, multiple techniques\nare proposed to generate audio AEs, which makes countermeasures against them an\nurgent task. Our experiments show that, given an AE, the transcription results\nby different Automatic Speech Recognition (ASR) systems differ significantly,\nas they use different architectures, parameters, and training datasets.\nInspired by Multiversion Programming, we propose a novel audio AE detection\napproach, which utilizes multiple off-the-shelf ASR systems to determine\nwhether an audio input is an AE. The evaluation shows that the detection\nachieves accuracies over 98.6%.\n"], ["2018-12-25", "http://arxiv.org/abs/1812.10085", "A Data-driven Adversarial Examples Recognition Framework via Adversarial Feature Genome.", ["Li Chen", " Hailun Ding", " Qi Li", " Jiawei Zhu", " Jian Peng", " Haifeng Li"], "  Convolutional neural networks (CNNs) are easily spoofed by adversarial\nexamples which lead to wrong classification results. Most of the defense\nmethods focus only on how to improve the robustness of CNNs or to detect\nadversarial examples. They are incapable of detecting and correctly classifying\nadversarial examples simultaneously. We find that adversarial examples and\noriginal images have diverse representations in the feature space, and this\ndifference grows as layers go deeper, which we call Adversarial Feature\nSeparability (AFS). Inspired by AFS, we propose a defense framework based on\nAdversarial Feature Genome (AFG), which can detect and correctly classify\nadversarial examples into original classes simultaneously. AFG is an innovative\nencoding for both image and adversarial example. It consists of group features\nand a mixed label. With group features which are visual representations of\nadversarial and original images via group visualization method, one can detect\nadversarial examples because of ASF of group features. With a mixed label, one\ncan trace back to the original label of an adversarial example. Then, the\nclassification of adversarial example is modeled as a multi-label\nclassification trained on the AFG dataset, which can get the original class of\nadversarial example. Experiments show that the proposed framework not only\neffectively detects adversarial examples from different attack algorithms, but\nalso correctly classifies adversarial examples. Our framework potentially gives\na new perspective, i.e., a data-driven way, to improve the robustness of a CNN\nmodel.\n"], ["2018-12-25", "http://arxiv.org/abs/1812.10061", "Noise Flooding for Detecting Audio Adversarial Examples Against Automatic Speech Recognition.", ["Krishan Rajaratnam", " Jugal Kalita"], "  Neural models enjoy widespread use across a variety of tasks and have grown\nto become crucial components of many industrial systems. Despite their\neffectiveness and extensive popularity, they are not without their exploitable\nflaws. Initially applied to computer vision systems, the generation of\nadversarial examples is a process in which seemingly imperceptible\nperturbations are made to an image, with the purpose of inducing a deep\nlearning based classifier to misclassify the image. Due to recent trends in\nspeech processing, this has become a noticeable issue in speech recognition\nmodels. In late 2017, an attack was shown to be quite effective against the\nSpeech Commands classification model. Limited-vocabulary speech classifiers,\nsuch as the Speech Commands model, are used quite frequently in a variety of\napplications, particularly in managing automated attendants in telephony\ncontexts. As such, adversarial examples produced by this attack could have\nreal-world consequences. While previous work in defending against these\nadversarial examples has investigated using audio preprocessing to reduce or\ndistort adversarial noise, this work explores the idea of flooding particular\nfrequency bands of an audio signal with random noise in order to detect\nadversarial examples. This technique of flooding, which does not require\nretraining or modifying the model, is inspired by work done in computer vision\nand builds on the idea that speech classifiers are relatively robust to natural\nnoise. A combined defense incorporating 5 different frequency bands for\nflooding the signal with noise outperformed other existing defenses in the\naudio space, detecting adversarial examples with 91.8% precision and 93.5%\nrecall.\n"], ["2018-12-25", "http://arxiv.org/abs/1812.10049", "PPD: Permutation Phase Defense Against Adversarial Examples in Deep Learning.", ["Mehdi Jafarnia-Jahromi", " Tasmin Chowdhury", " Hsin-Tai Wu", " Sayandev Mukherjee"], "  Deep neural networks have demonstrated cutting edge performance on various\ntasks including classification. However, it is well known that adversarially\ndesigned imperceptible perturbation of the input can mislead advanced\nclassifiers. In this paper, Permutation Phase Defense (PPD), is proposed as a\nnovel method to resist adversarial attacks. PPD combines random permutation of\nthe image with phase component of its Fourier transform. The basic idea behind\nthis approach is to turn adversarial defense problems analogously into\nsymmetric cryptography, which relies solely on safekeeping of the keys for\nsecurity. In PPD, safe keeping of the selected permutation ensures\neffectiveness against adversarial attacks. Testing PPD on MNIST and CIFAR-10\ndatasets yielded state-of-the-art robustness against the most powerful\nadversarial attacks currently available.\n"], ["2018-12-25", "http://arxiv.org/abs/1812.10217", "Seeing isn't Believing: Practical Adversarial Attack Against Object Detectors.", ["Yue Zhao", " Hong Zhu", " Ruigang Liang", " Qintao Shen", " Shengzhi Zhang", " Kai Chen"], "  In this paper, we presented systematic solutions to build robust and\npractical AEs against real world object detectors. Particularly, for Hiding\nAttack (HA), we proposed the feature-interference reinforcement (FIR) method\nand the enhanced realistic constraints generation (ERG) to enhance robustness,\nand for Appearing Attack (AA), we proposed the nested-AE, which combines two\nAEs together to attack object detectors in both long and short distance. We\nalso designed diverse styles of AEs to make AA more surreptitious. Evaluation\nresults show that our AEs can attack the state-of-the-art real-time object\ndetectors (i.e., YOLO V3 and faster-RCNN) at the success rate up to 92.4% with\nvarying distance from 1m to 25m and angles from -60{\\deg} to 60{\\deg}. Our AEs\nare also demonstrated to be highly transferable, capable of attacking another\nthree state-of-the-art black-box models with high success rate.\n"], ["2018-12-24", "http://arxiv.org/abs/1812.11017", "DUP-Net: Denoiser and Upsampler Network for 3D Adversarial Point Clouds Defense.", ["Hang Zhou", " Kejiang Chen", " Weiming Zhang", " Han Fang", " Wenbo Zhou", " Nenghai Yu"], "  Neural networks are vulnerable to adversarial examples, which poses a threat\nto their application in security sensitive systems. We propose a Denoiser and\nUPsampler Network (DUP-Net) structure as defenses for 3D adversarial point\ncloud classification, where the two modules reconstruct surface smoothness by\ndropping or adding points. In this paper, statistical outlier removal (SOR) and\na data-driven upsampling network are considered as denoiser and upsampler\nrespectively. Compared with baseline defenses, DUP-Net has three advantages.\nFirst, with DUP-Net as a defense, the target model is more robust to white-box\nadversarial attacks. Second, the statistical outlier removal provides added\nrobustness since it is a non-differentiable denoising operation. Third, the\nupsampler network can be trained on a small dataset and defends well against\nadversarial attacks generated from other point cloud datasets. We conduct\nvarious experiments to validate that DUP-Net is very effective as defense in\npractice. Our best defense eliminates 83.8% of C&W and l_2 loss based attack\n(point shifting), 50.0% of C&W and Hausdorff distance loss based attack (point\nadding) and 9.0% of saliency map based attack (point dropping) under 200\ndropped points on PointNet.\n"], ["2018-12-23", "http://arxiv.org/abs/1812.09803", "Guessing Smart: Biased Sampling for Efficient Black-Box Adversarial Attacks.", ["Thomas Brunner", " Frederik Diehl", " Michael Truong Le", " Alois Knoll"], "  We consider adversarial examples for image classification in the black-box\ndecision-based setting. Here, an attacker cannot access confidence scores, but\nonly the final label. Most attacks for this scenario are either unreliable or\ninefficient. Focusing on the latter, we show that a specific class of attacks,\nBoundary Attacks, can be reinterpreted as a biased sampling framework that\ngains efficiency from domain knowledge. We identify three such biases, image\nfrequency, regional masks and surrogate gradients, and evaluate their\nperformance against an ImageNet classifier. We show that the combination of\nthese biases outperforms the state of the art by a wide margin. We also\nshowcase an efficient way to attack the Google Cloud Vision API, where we craft\nconvincing perturbations with just a few hundred queries. Finally, the methods\nwe propose have also been found to work very well against strong defenses: Our\ntargeted attack won second place in the NeurIPS 2018 Adversarial Vision\nChallenge.\n"], ["2018-12-23", "http://arxiv.org/abs/1812.09660", "Markov Game Modeling of Moving Target Defense for Strategic Detection of Threats in Cloud Networks.", ["Ankur Chowdhary", " Sailik Sengupta", " Dijiang Huang", " Subbarao Kambhampati"], "  The processing and storage of critical data in large-scale cloud networks\nnecessitate the need for scalable security solutions. It has been shown that\ndeploying all possible security measures incurs a cost on performance by using\nup valuable computing and networking resources which are the primary selling\npoints for cloud service providers. Thus, there has been a recent interest in\ndeveloping Moving Target Defense (MTD) mechanisms that helps one optimize the\njoint objective of maximizing security while ensuring that the impact on\nperformance is minimized. Often, these techniques model the problem of\nmulti-stage attacks by stealthy adversaries as a single-step attack detection\ngame using graph connectivity measures as a heuristic to measure performance,\nthereby (1) losing out on valuable information that is inherently present in\ngraph-theoretic models designed for large cloud networks, and (2) coming up\nwith certain strategies that have asymmetric impacts on performance. In this\nwork, we leverage knowledge in attack graphs of a cloud network in formulating\na zero-sum Markov Game and use the Common Vulnerability Scoring System (CVSS)\nto come up with meaningful utility values for this game. Then, we show that the\noptimal strategy of placing detecting mechanisms against an adversary is\nequivalent to computing the mixed Min-max Equilibrium of the Markov Game. We\ncompare the gains obtained by using our method to other techniques presently\nused in cloud network security, thereby showing its effectiveness. Finally, we\nhighlight how the method was used for a small real-world cloud system.\n"], ["2018-12-22", "http://arxiv.org/abs/1812.09638", "Exploiting the Inherent Limitation of L0 Adversarial Examples.", ["Fei Zuo", " Bokai Yang", " Xiaopeng Li", " Lannan Luo", " Qiang Zeng"], "  Despite the great achievements made by neural networks on tasks such as image\nclassification, they are brittle and vulnerable to adversarial example (AE)\nattacks, which are crafted by adding human-imperceptible perturbations to\ninputs in order that a neural-network-based classifier incorrectly labels them.\nIn particular, L0 AEs are a category of widely discussed threats where\nadversaries are restricted in the number of pixels that they can corrupt.\nHowever, our observation is that, while L0 attacks modify as few pixels as\npossible, they tend to cause large-amplitude perturbations to the modified\npixels. We consider this as an inherent limitation of L0 AEs, and thwart such\nattacks by both detecting and rectifying them. The main novelty of the proposed\ndetector is that we convert the AE detection problem into a comparison problem\nby exploiting the inherent limitation of L0 attacks. More concretely, given an\nimage I, it is pre-processed to obtain another image I' . A Siamese network,\nwhich is known to be effective in comparison, takes I and I' as the input pair\nto determine whether I is an AE. A trained Siamese network automatically and\nprecisely captures the discrepancies between I and I' to detect L0\nperturbations. In addition, we show that the pre-processing technique,\ninpainting, used for detection can also work as an effective defense, which has\na high probability of removing the adversarial influence of L0 perturbations.\nThus, our system, called AEPECKER, demonstrates not only high AE detection\naccuracies, but also a notable capability to correct the classification\nresults.\n"], ["2018-12-21", "http://arxiv.org/abs/1812.09431", "Dissociable neural representations of adversarially perturbed images in deep neural networks and the human brain.", ["Chi Zhang", " Xiaohan Duan", " Linyuan Wang", " Yongli Li", " Bin Yan", " Guoen Hu", " Ruyuan Zhang", " Li Tong"], "  Despite the remarkable similarities between deep neural networks (DNN) and\nthe human brain as shown in previous studies, the fact that DNNs still fall\nbehind humans in many visual tasks suggests that considerable differences still\nexist between the two systems. To probe their dissimilarities, we leverage\nadversarial noise (AN) and adversarial interference (AI) images that yield\ndistinct recognition performance in a prototypical DNN (AlexNet) and human\nvision. The evoked activity by regular (RE) and adversarial images in both\nsystems is thoroughly compared. We find that representational similarity\nbetween RE and adversarial images in the human brain resembles their perceptual\nsimilarity. However, such representation-perception association is disrupted in\nthe DNN. Especially, the representational similarity between RE and AN images\nidiosyncratically increases from low- to high-level layers. Furthermore,\nforward encoding modeling reveals that the DNN-brain hierarchical\ncorrespondence proposed in previous studies only holds when the two systems\nprocess RE and AI images but not AN images. These results might be due to the\ndeterministic modeling approach of current DNNs. Taken together, our results\nprovide a complementary perspective on the comparison between DNNs and the\nhuman brain, and highlight the need to characterize their differences to\nfurther bridge artificial and human intelligence research.\n"], ["2018-12-19", "http://arxiv.org/abs/1812.08108", "Enhancing Robustness of Deep Neural Networks Against Adversarial Malware Samples: Principles, Framework, and AICS'2019 Challenge.", ["Deqiang Li", " Qianmu Li", " Yanfang Ye", " Shouhuai Xu"], "  Malware continues to be a major cyber threat, despite the tremendous effort\nthat has been made to combat them. The number of malware in the wild steadily\nincreases over time, meaning that we must resort to automated defense\ntechniques. This naturally calls for machine learning based malware detection.\nHowever, machine learning is known to be vulnerable to adversarial evasion\nattacks that manipulate a small number of features to make classifiers wrongly\nrecognize a malware sample as a benign one. The state-of-the-art is that there\nare no effective countermeasures against these attacks. Inspired by the\nAICS'2019 Challenge, we systematize a number of principles for enhancing the\nrobustness of neural networks against adversarial malware evasion attacks. Some\nof these principles have been scattered in the literature, but others are\nproposed in this paper for the first time. Under the guidance of these\nprinciples, we propose a framework and an accompanying training algorithm,\nwhich are then applied to the AICS'2019 challenge. Our experimental results\nhave been submitted to the challenge organizer for evaluation.\n"], ["2018-12-18", "http://arxiv.org/abs/1812.08329", "PROVEN: Certifying Robustness of Neural Networks with a Probabilistic Approach.", ["Tsui-Wei Weng", " Pin-Yu Chen", " Lam M. Nguyen", " Mark S. Squillante", " Ivan Oseledets", " Luca Daniel"], "  With deep neural networks providing state-of-the-art machine learning models\nfor numerous machine learning tasks, quantifying the robustness of these models\nhas become an important area of research. However, most of the research\nliterature merely focuses on the \\textit{worst-case} setting where the input of\nthe neural network is perturbed with noises that are constrained within an\n$\\ell_p$ ball; and several algorithms have been proposed to compute certified\nlower bounds of minimum adversarial distortion based on such worst-case\nanalysis. In this paper, we address these limitations and extend the approach\nto a \\textit{probabilistic} setting where the additive noises can follow a\ngiven distributional characterization. We propose a novel probabilistic\nframework PROVEN to PRObabilistically VErify Neural networks with statistical\nguarantees -- i.e., PROVEN certifies the probability that the classifier's\ntop-1 prediction cannot be altered under any constrained $\\ell_p$ norm\nperturbation to a given input. Importantly, we show that it is possible to\nderive closed-form probabilistic certificates based on current state-of-the-art\nneural network robustness verification frameworks. Hence, the probabilistic\ncertificates provided by PROVEN come naturally and with almost no overhead when\nobtaining the worst-case certified lower bounds from existing methods such as\nFast-Lin, CROWN and CNN-Cert. Experiments on small and large MNIST and CIFAR\nneural network models demonstrate our probabilistic approach can achieve up to\naround $75\\%$ improvement in the robustness certification with at least a\n$99.99\\%$ confidence compared with the worst-case robustness certificate\ndelivered by CROWN.\n"], ["2018-12-17", "http://arxiv.org/abs/1812.06815", "Spartan Networks: Self-Feature-Squeezing Neural Networks for increased robustness in adversarial settings.", ["Fran\u00e7ois Menet", " Paul Berthier", " Jos\u00e9 M. Fernandez", " Michel Gagnon"], "  Deep learning models are vulnerable to adversarial examples which are input\nsamples modified in order to maximize the error on the system. We introduce\nSpartan Networks, resistant deep neural networks that do not require input\npreprocessing nor adversarial training. These networks have an adversarial\nlayer designed to discard some information of the network, thus forcing the\nsystem to focus on relevant input. This is done using a new activation function\nto discard data. The added layer trains the neural network to filter-out\nusually-irrelevant parts of its input. Our performance evaluation shows that\nSpartan Networks have a slightly lower precision but report a higher robustness\nunder attack when compared to unprotected models. Results of this study of\nAdversarial AI as a new attack vector are based on tests conducted on the MNIST\ndataset.\n"], ["2018-12-17", "http://arxiv.org/abs/1812.06626", "Designing Adversarially Resilient Classifiers using Resilient Feature Engineering.", ["Kevin Eykholt", " Atul Prakash"], "  We provide a methodology, resilient feature engineering, for creating\nadversarially resilient classifiers. According to existing work, adversarial\nattacks identify weakly correlated or non-predictive features learned by the\nclassifier during training and design the adversarial noise to utilize these\nfeatures. Therefore, highly predictive features should be used first during\nclassification in order to determine the set of possible output labels. Our\nmethodology focuses the problem of designing resilient classifiers into a\nproblem of designing resilient feature extractors for these highly predictive\nfeatures. We provide two theorems, which support our methodology. The Serial\nComposition Resilience and Parallel Composition Resilience theorems show that\nthe output of adversarially resilient feature extractors can be combined to\ncreate an equally resilient classifier. Based on our theoretical results, we\noutline the design of an adversarially resilient classifier.\n"], ["2018-12-17", "http://arxiv.org/abs/1812.08342", "A Survey of Safety and Trustworthiness of Deep Neural Networks.", ["Xiaowei Huang", " Daniel Kroening", " Wenjie Ruan", " James Sharp", " Youcheng Sun", " Emese Thamo", " Min Wu", " Xinping Yi"], "  In the past few years, significant progress has been made on deep neural\nnetworks (DNNs) in achieving human-level performance on several long-standing\ntasks. With the broader deployment of DNNs on various applications, the\nconcerns on its safety and trustworthiness have been raised in public,\nespecially after the widely reported fatal incidents of self-driving cars.\nResearch to address these concerns is very active, with many papers released in\nthe past few years. This survey paper conducts a review of the current research\neffort on making DNNs safe and trustworthy, by focusing on four aspects:\nverification, testing, adversarial attack and defence, and interpretability. In\ntotal, we surveyed 178 papers, most of which published after 2017.\n"], ["2018-12-16", "http://arxiv.org/abs/1812.06570", "Defense-VAE: A Fast and Accurate Defense against Adversarial Attacks.", ["Xiang Li", " Shihao Ji"], "  Deep neural networks (DNNs) have been enormously successful across a variety\nof prediction tasks. However, recent research shows that DNNs are particularly\nvulnerable to adversarial attacks, which poses a serious threat to their\napplications in security-sensitive systems. In this paper, we propose a simple\nyet effective defense algorithm Defense-VAE that uses variational autoencoder\n(VAE) to purge adversarial perturbations from contaminated images. The proposed\nmethod is generic and can defend white-box and black-box attacks without the\nneed of retraining the original CNN classifiers, and can further strengthen the\ndefense by retraining CNN or end-to-end finetuning the whole pipeline. In\naddition, the proposed method is very efficient compared to the\noptimization-based alternatives, such as Defense-GAN, since no iterative\noptimization is needed for online prediction. Extensive experiments on MNIST,\nFashion-MNIST, CelebA and CIFAR-10 demonstrate the superior defense accuracy of\nDefense-VAE compared to Defense-GAN, while being 50x faster than the latter.\nThis makes Defense-VAE widely deployable in real-time security-sensitive\nsystems. Our source code can be found at\nhttps://github.com/lxuniverse/defense-vae.\n"], ["2018-12-15", "http://arxiv.org/abs/1812.07385", "Perturbation Analysis of Learning Algorithms: A Unifying Perspective on Generation of Adversarial Examples.", ["Emilio Rafael Balda", " Arash Behboodi", " Rudolf Mathar"], "  Despite the tremendous success of deep neural networks in various learning\nproblems, it has been observed that adding an intentionally designed\nadversarial perturbation to inputs of these architectures leads to erroneous\nclassification with high confidence in the prediction. In this work, we propose\na general framework based on the perturbation analysis of learning algorithms\nwhich consists of convex programming and is able to recover many current\nadversarial attacks as special cases. The framework can be used to propose\nnovel attacks against learning algorithms for classification and regression\ntasks under various new constraints with closed form solutions in many\ninstances. In particular we derive new attacks against classification\nalgorithms which are shown to achieve comparable performances to notable\nexisting attacks. The framework is then used to generate adversarial\nperturbations for regression tasks which include single pixel and single subset\nattacks. By applying this method to autoencoding and image colorization tasks,\nit is shown that adversarial perturbations can effectively perturb the output\nof regression tasks as well.\n"], ["2018-12-15", "http://arxiv.org/abs/1812.06371", "Trust Region Based Adversarial Attack on Neural Networks.", ["Zhewei Yao", " Amir Gholami", " Peng Xu", " Kurt Keutzer", " Michael Mahoney"], "  Deep Neural Networks are quite vulnerable to adversarial perturbations.\nCurrent state-of-the-art adversarial attack methods typically require very time\nconsuming hyper-parameter tuning, or require many iterations to solve an\noptimization based adversarial attack. To address this problem, we present a\nnew family of trust region based adversarial attacks, with the goal of\ncomputing adversarial perturbations efficiently. We propose several attacks\nbased on variants of the trust region optimization method. We test the proposed\nmethods on Cifar-10 and ImageNet datasets using several different models\nincluding AlexNet, ResNet-50, VGG-16, and DenseNet-121 models. Our methods\nachieve comparable results with the Carlini-Wagner (CW) attack, but with\nsignificant speed up of up to $37\\times$, for the VGG-16 model on a Titan Xp\nGPU. For the case of ResNet-50 on ImageNet, we can bring down its\nclassification accuracy to less than 0.1\\% with at most $1.5\\%$ relative\n$L_\\infty$ (or $L_2$) perturbation requiring only $1.02$ seconds as compared to\n$27.04$ seconds for the CW attack. We have open sourced our method which can be\naccessed at [1].\n"], ["2018-12-14", "http://arxiv.org/abs/1812.05793", "Adversarial Sample Detection for Deep Neural Network through Model Mutation Testing.", ["Jingyi Wang", " Guoliang Dong", " Jun Sun", " Xinyu Wang", " Peixin Zhang"], "  Deep neural networks (DNN) have been shown to be useful in a wide range of\napplications. However, they are also known to be vulnerable to adversarial\nsamples. By transforming a normal sample with some carefully crafted human\nimperceptible perturbations, even highly accurate DNN make wrong decisions.\nMultiple defense mechanisms have been proposed which aim to hinder the\ngeneration of such adversarial samples. However, a recent work show that most\nof them are ineffective. In this work, we propose an alternative approach to\ndetect adversarial samples at runtime. Our main observation is that adversarial\nsamples are much more sensitive than normal samples if we impose random\nmutations on the DNN. We thus first propose a measure of `sensitivity' and show\nempirically that normal samples and adversarial samples have distinguishable\nsensitivity. We then integrate statistical hypothesis testing and model\nmutation testing to check whether an input sample is likely to be normal or\nadversarial at runtime by measuring its sensitivity. We evaluated our approach\non the MNIST and CIFAR10 datasets. The results show that our approach detects\nadversarial samples generated by state-of-the-art attacking methods efficiently\nand accurately.\n"], ["2018-12-13", "http://arxiv.org/abs/1812.05271", "TextBugger: Generating Adversarial Text Against Real-world Applications.", ["Jinfeng Li", " Shouling Ji", " Tianyu Du", " Bo Li", " Ting Wang"], "  Deep Learning-based Text Understanding (DLTU) is the backbone technique\nbehind various applications, including question answering, machine translation,\nand text classification. Despite its tremendous popularity, the security\nvulnerabilities of DLTU are still largely unknown, which is highly concerning\ngiven its increasing use in security-sensitive applications such as sentiment\nanalysis and toxic content detection. In this paper, we show that DLTU is\ninherently vulnerable to adversarial text attacks, in which maliciously crafted\ntexts trigger target DLTU systems and services to misbehave. Specifically, we\npresent TextBugger, a general attack framework for generating adversarial\ntexts. In contrast to prior works, TextBugger differs in significant ways: (i)\neffective -- it outperforms state-of-the-art attacks in terms of attack success\nrate; (ii) evasive -- it preserves the utility of benign text, with 94.9\\% of\nthe adversarial text correctly recognized by human readers; and (iii) efficient\n-- it generates adversarial text with computational complexity sub-linear to\nthe text length. We empirically evaluate TextBugger on a set of real-world DLTU\nsystems and services used for sentiment analysis and toxic content detection,\ndemonstrating its effectiveness, evasiveness, and efficiency. For instance,\nTextBugger achieves 100\\% success rate on the IMDB dataset based on Amazon AWS\nComprehend within 4.61 seconds and preserves 97\\% semantic similarity. We\nfurther discuss possible defense mechanisms to mitigate such attack and the\nadversary's potential countermeasures, which leads to promising directions for\nfurther research.\n"], ["2018-12-13", "http://arxiv.org/abs/1812.05720", "Why ReLU networks yield high-confidence predictions far away from the training data and how to mitigate the problem.", ["Matthias Hein", " Maksym Andriushchenko", " Julian Bitterwolf"], "  Classifiers used in the wild, in particular for safety-critical systems,\nshould not only have good generalization properties but also should know when\nthey don't know, in particular make low confidence predictions far away from\nthe training data. We show that ReLU type neural networks which yield a\npiecewise linear classifier function fail in this regard as they produce almost\nalways high confidence predictions far away from the training data. For bounded\ndomains like images we propose a new robust optimization technique similar to\nadversarial training which enforces low confidence predictions far away from\nthe training data. We show that this technique is surprisingly effective in\nreducing the confidence of predictions far away from the training data while\nmaintaining high confidence predictions and test error on the original\nclassification task compared to standard training.\n"], ["2018-12-12", "http://arxiv.org/abs/1812.05013", "Thwarting Adversarial Examples: An $L_0$-RobustSparse Fourier Transform.", ["Mitali Bafna", " Jack Murtagh", " Nikhil Vyas"], "  We give a new algorithm for approximating the Discrete Fourier transform of\nan approximately sparse signal that has been corrupted by worst-case $L_0$\nnoise, namely a bounded number of coordinates of the signal have been corrupted\narbitrarily. Our techniques generalize to a wide range of linear\ntransformations that are used in data analysis such as the Discrete Cosine and\nSine transforms, the Hadamard transform, and their high-dimensional analogs. We\nuse our algorithm to successfully defend against well known $L_0$ adversaries\nin the setting of image classification. We give experimental results on the\nJacobian-based Saliency Map Attack (JSMA) and the Carlini Wagner (CW) $L_0$\nattack on the MNIST and Fashion-MNIST datasets as well as the Adversarial Patch\non the ImageNet dataset.\n"], ["2018-12-11", "http://arxiv.org/abs/1812.04293", "Mix'n'Squeeze: Thwarting Adaptive Adversarial Samples Using Randomized Squeezing.", ["Kumar Sharad", " Giorgia Azzurra Marson", " Hien Thi Thu Truong", " Ghassan Karame"], "  Deep Learning (DL) has been shown to be particularly vulnerable to\nadversarial samples. To combat adversarial strategies, numerous defenses have\nbeen proposed in the literature. Among these, feature squeezing emerges as an\neffective defense by reducing unnecessary features without changing the DL\nmodel. However, feature squeezing is a static defense and does not resist\nadaptive attacks. Namely, feature squeezing is a deterministic process: as soon\nas an adversarial sample is found, this sample will always succeed against the\nclassifier.\n  In this work, we address this problem and introduce Mix'n'Squeeze, the first\nrandomized feature squeezing defense that leverages key-based randomness and is\nsecure against adaptive whitebox adversaries. Our defense consists of\npre-processing the classifier inputs by embedding carefully selected randomness\nwithin each feature, before applying feature squeezing, so that an adaptive\nwhitebox attacker can no longer predict the effect of their own perturbations\non the resulting sample. We thoroughly implement and evaluate Mix'n'Squeeze in\nthe context of image classification in light of the various reported strategies\nto generate adversarial samples. We also analyze the resilience of\nMix'n'Squeeze with respect to state of the art adaptive strategies and we show\nthat---in contrast to common belief---Mix'n'Squeeze does not hamper the\nclassifier's accuracy while significantly decreasing the success probability of\nan adaptive whitebox adversary.\n"], ["2018-12-11", "http://arxiv.org/abs/1812.04599", "Adversarial Framing for Image and Video Classification.", ["Konrad Zolna", " Michal Zajac", " Negar Rostamzadeh", " Pedro O. Pinheiro"], "  Neural networks are prone to adversarial attacks. In general, such attacks\ndeteriorate the quality of the input by either slightly modifying most of its\npixels, or by occluding it with a patch. In this paper, we propose a method\nthat keeps the image unchanged and only adds an adversarial framing on the\nborder of the image. We show empirically that our method is able to\nsuccessfully attack state-of-the-art methods on both image and video\nclassification problems. Notably, the proposed method results in a universal\nattack which is very fast at test time. Source code can be found at\nhttps://github.com/zajaczajac/adv_framing .\n"], ["2018-12-10", "http://arxiv.org/abs/1812.03705", "Defending Against Universal Perturbations With Shared Adversarial Training.", ["Chaithanya Kumar Mummadi", " Thomas Brox", " Jan Hendrik Metzen"], "  Classifiers such as deep neural networks have been shown to be vulnerable\nagainst adversarial perturbations on problems with high-dimensional input\nspace. While adversarial training improves the robustness of image classifiers\nagainst such adversarial perturbations, it leaves them sensitive to\nperturbations on a non-negligible fraction of the inputs. In this work, we show\nthat adversarial training is more effective in preventing universal\nperturbations, where the same perturbation needs to fool a classifier on many\ninputs. Moreover, we investigate the trade-off between robustness against\nuniversal perturbations and performance on unperturbed data and propose an\nextension of adversarial training that handles this trade-off more gracefully.\nWe present results for image classification and semantic segmentation to\nshowcase that universal perturbations that fool a model hardened with\nadversarial training become clearly perceptible and show patterns of the target\nscene.\n"], ["2018-12-08", "http://arxiv.org/abs/1812.03413", "Learning Transferable Adversarial Examples via Ghost Networks.", ["Yingwei Li", " Song Bai", " Yuyin Zhou", " Cihang Xie", " Zhishuai Zhang", " Alan Yuille"], "  Recent development of adversarial attacks has proven that ensemble-based\nmethods outperform traditional, non-ensemble ones in black-box attack. However,\nthese methods generally require a family of diverse models, and ensembling them\ntogether afterward, both of which are computationally expensive. In this paper,\nwe propose Ghost Networks to generate transferable adversarial examples\nefficiently. The critical principle of ghost networks is to apply feature-level\nperturbations to an existing model to potentially create a huge set of diverse\nmodels. After that, models are subsequently fused by longitudinal ensemble.\nCompared to traditional ensemble methods, both steps require almost no extra\ntime and space consumption. Extensive experimental results suggest that the\nnumber of networks is essential for improving the transferability of\nadversarial examples, but it is less necessary to independently train different\nnetworks and ensemble them in an intensive aggregation way. Instead, our work\ncan be used as a computationally cheap and easily applied plug-in to improve\nadversarial approaches both in single-model and multi-model attack, compatible\nwith residual and non-residual networks. By reproducing the NeurIPS 2017\nadversarial competition, our method outperforms the No.1 attack submission by a\nlarge margin, demonstrating its effectiveness and efficiency.\n"], ["2018-12-08", "http://arxiv.org/abs/1812.03411", "Feature Denoising for Improving Adversarial Robustness.", ["Cihang Xie", " Yuxin Wu", " der Maaten Laurens van", " Alan Yuille", " Kaiming He"], "  Adversarial attacks to image classification systems present challenges to\nconvolutional networks and opportunities for understanding them. This study\nsuggests that adversarial perturbations on images lead to noise in the features\nconstructed by these networks. Motivated by this observation, we develop new\nnetwork architectures that increase adversarial robustness by performing\nfeature denoising. Specifically, our networks contain blocks that denoise the\nfeatures using non-local means or other filters; the entire networks are\ntrained end-to-end. When combined with adversarial training, our feature\ndenoising networks substantially improve the state-of-the-art in adversarial\nrobustness in both white-box and black-box attack settings. On ImageNet, under\n10-iteration PGD white-box attacks where prior art has 27.9% accuracy, our\nmethod achieves 55.7%; even under extreme 2000-iteration PGD white-box attacks,\nour method secures 42.6% accuracy. Our method was ranked first in Competition\non Adversarial Attacks and Defenses (CAAD) 2018 --- it achieved 50.6%\nclassification accuracy on a secret, ImageNet-like test dataset against 48\nunknown attackers, surpassing the runner-up approach by ~10%. Code is available\nat https://github.com/facebookresearch/ImageNet-Adversarial-Training.\n"], ["2018-12-08", "http://arxiv.org/abs/1812.03405", "AutoGAN: Robust Classifier Against Adversarial Attacks.", ["Blerta Lindqvist", " Shridatt Sugrim", " Rauf Izmailov"], "  Classifiers fail to classify correctly input images that have been\npurposefully and imperceptibly perturbed to cause misclassification. This\nsusceptability has been shown to be consistent across classifiers, regardless\nof their type, architecture or parameters. Common defenses against adversarial\nattacks modify the classifer boundary by training on additional adversarial\nexamples created in various ways. In this paper, we introduce AutoGAN, which\ncounters adversarial attacks by enhancing the lower-dimensional manifold\ndefined by the training data and by projecting perturbed data points onto it.\nAutoGAN mitigates the need for knowing the attack type and magnitude as well as\nthe need for having adversarial samples of the attack. Our approach uses a\nGenerative Adversarial Network (GAN) with an autoencoder generator and a\ndiscriminator that also serves as a classifier. We test AutoGAN against\nadversarial samples generated with state-of-the-art Fast Gradient Sign Method\n(FGSM) as well as samples generated with random Gaussian noise, both using the\nMNIST dataset. For different magnitudes of perturbation in training and\ntesting, AutoGAN can surpass the accuracy of FGSM method by up to 25\\% points\non samples perturbed using FGSM. Without an augmented training dataset, AutoGAN\nachieves an accuracy of 89\\% compared to 1\\% achieved by FGSM method on FGSM\ntesting adversarial samples.\n"], ["2018-12-08", "http://arxiv.org/abs/1812.03303", "Detecting Adversarial Examples in Convolutional Neural Networks.", ["Stefanos Pertigkiozoglou", " Petros Maragos"], "  The great success of convolutional neural networks has caused a massive\nspread of the use of such models in a large variety of Computer Vision\napplications. However, these models are vulnerable to certain inputs, the\nadversarial examples, which although are not easily perceived by humans, they\ncan lead a neural network to produce faulty results. This paper focuses on the\ndetection of adversarial examples, which are created for convolutional neural\nnetworks that perform image classification. We propose three methods for\ndetecting possible adversarial examples and after we analyze and compare their\nperformance, we combine their best aspects to develop an even more robust\napproach. The first proposed method is based on the regularization of the\nfeature vector that the neural network produces as output. The second method\ndetects adversarial examples by using histograms, which are created from the\noutputs of the hidden layers of the neural network. These histograms create a\nfeature vector which is used as the input of an SVM classifier, which\nclassifies the original input either as an adversarial or as a real input.\nFinally, for the third method we introduce the concept of the residual image,\nwhich contains information about the parts of the input pattern that are\nignored by the neural network. This method aims at the detection of possible\nadversarial examples, by using the residual image and reinforcing the parts of\nthe input pattern that are ignored by the neural network. Each one of these\nmethods has some novelties and by combining them we can further improve the\ndetection results. For the proposed methods and their combination, we present\nthe results of detecting adversarial examples on the MNIST dataset. The\ncombination of the proposed methods offers some improvements over similar state\nof the art approaches.\n"], ["2018-12-07", "http://arxiv.org/abs/1812.03190", "Deep-RBF Networks Revisited: Robust Classification with Rejection.", ["Pourya Habib Zadeh", " Reshad Hosseini", " Suvrit Sra"], "  One of the main drawbacks of deep neural networks, like many other\nclassifiers, is their vulnerability to adversarial attacks. An important reason\nfor their vulnerability is assigning high confidence to regions with few or\neven no feature points. By feature points, we mean a nonlinear transformation\nof the input space extracting a meaningful representation of the input data. On\nthe other hand, deep-RBF networks assign high confidence only to the regions\ncontaining enough feature points, but they have been discounted due to the\nwidely-held belief that they have the vanishing gradient problem. In this\npaper, we revisit the deep-RBF networks by first giving a general formulation\nfor them, and then proposing a family of cost functions thereof inspired by\nmetric learning. In the proposed deep-RBF learning algorithm, the vanishing\ngradient problem does not occur. We make these networks robust to adversarial\nattack by adding the reject option to their output layer. Through several\nexperiments on the MNIST dataset, we demonstrate that our proposed method not\nonly achieves significant classification accuracy but is also very resistant to\nvarious adversarial attacks.\n"], ["2018-12-07", "http://arxiv.org/abs/1812.03087", "Combatting Adversarial Attacks through Denoising and Dimensionality Reduction: A Cascaded Autoencoder Approach.", ["Rajeev Sahay", " Rehana Mahfuz", " Aly El Gamal"], "  Machine Learning models are vulnerable to adversarial attacks that rely on\nperturbing the input data. This work proposes a novel strategy using\nAutoencoder Deep Neural Networks to defend a machine learning model against two\ngradient-based attacks: The Fast Gradient Sign attack and Fast Gradient attack.\nFirst we use an autoencoder to denoise the test data, which is trained with\nboth clean and corrupted data. Then, we reduce the dimension of the denoised\ndata using the hidden layer representation of another autoencoder. We perform\nthis experiment for multiple values of the bound of adversarial perturbations,\nand consider different numbers of reduced dimensions. When the test data is\npreprocessed using this cascaded pipeline, the tested deep neural network\nclassifier yields a much higher accuracy, thus mitigating the effect of the\nadversarial perturbation.\n"], ["2018-12-06", "http://arxiv.org/abs/1812.02891", "Adversarial Defense of Image Classification Using a Variational Auto-Encoder.", ["Yi Luo", " Henry Pfister"], "  Deep neural networks are known to be vulnerable to adversarial attacks. This\nexposes them to potential exploits in security-sensitive applications and\nhighlights their lack of robustness. This paper uses a variational auto-encoder\n(VAE) to defend against adversarial attacks for image classification tasks.\nThis VAE defense has a few nice properties: (1) it is quite flexible and its\nuse of randomness makes it harder to attack; (2) it can learn disentangled\nrepresentations that prevent blurry reconstruction; and (3) a patch-wise VAE\ndefense strategy is used that does not require retraining for different size\nimages. For moderate to severe attacks, this system outperforms or closely\nmatches the performance of JPEG compression, with the best quality parameter.\nIt also has more flexibility and potential for improvement via training.\n"], ["2018-12-06", "http://arxiv.org/abs/1812.02885", "Adversarial Attacks, Regression, and Numerical Stability Regularization.", ["Andre T. Nguyen", " Edward Raff"], "  Adversarial attacks against neural networks in a regression setting are a\ncritical yet understudied problem. In this work, we advance the state of the\nart by investigating adversarial attacks against regression networks and by\nformulating a more effective defense against these attacks. In particular, we\ntake the perspective that adversarial attacks are likely caused by numerical\ninstability in learned functions. We introduce a stability inducing,\nregularization based defense against adversarial attacks in the regression\nsetting. Our new and easy to implement defense is shown to outperform prior\napproaches and to improve the numerical stability of learned functions.\n"], ["2018-12-06", "http://arxiv.org/abs/1812.02606", "The Limitations of Model Uncertainty in Adversarial Settings.", ["Kathrin Grosse", " David Pfaff", " Michael T. Smith", " Michael Backes"], "  Machine learning models are vulnerable to adversarial examples: minor\nperturbations to input samples intended to deliberately cause\nmisclassification. Many defenses have led to an arms race-we thus study a\npromising, recent trend in this setting, Bayesian uncertainty measures. These\nmeasures allow a classifier to provide principled confidence and uncertainty\nfor an input, where the latter refers to how usual the input is. We focus on\nGaussian processes (GP), a classifier providing such principled uncertainty and\nconfidence measures. Using correctly classified benign data as comparison, GP's\nintrinsic uncertainty and confidence deviate for misclassified benign samples\nand misclassified adversarial examples. We therefore introduce\nhigh-confidence-low-uncertainty adversarial examples: adversarial examples\ncrafted maximizing GP confidence and minimizing GP uncertainty. Visual\ninspection shows HCLU adversarial examples are malicious, and resemble the\noriginal rather than the target class. HCLU adversarial examples also transfer\nto other classifiers. We focus on transferability to other algorithms providing\nuncertainty measures, and find that a Bayesian neural network confidently\nmisclassifies HCLU adversarial examples. We conclude that uncertainty and\nconfidence, even in the Bayesian sense, can be circumvented by both white-box\nand black-box attackers.\n"], ["2018-12-06", "http://arxiv.org/abs/1812.02575", "Prior Networks for Detection of Adversarial Attacks.", ["Andrey Malinin", " Mark Gales"], "  Adversarial examples are considered a serious issue for safety critical\napplications of AI, such as finance, autonomous vehicle control and medicinal\napplications. Though significant work has resulted in increased robustness of\nsystems to these attacks, systems are still vulnerable to well-crafted attacks.\nTo address this problem, several adversarial attack detection methods have been\nproposed. However, a system can still be vulnerable to adversarial samples that\nare designed to specifically evade these detection methods. One recent\ndetection scheme that has shown good performance is based on uncertainty\nestimates derived from Monte-Carlo dropout ensembles. Prior Networks, a new\nmethod of estimating predictive uncertainty, has been shown to outperform\nMonte-Carlo dropout on a range of tasks. One of the advantages of this approach\nis that the behaviour of a Prior Network can be explicitly tuned to, for\nexample, predict high uncertainty in regions where there are no training data\nsamples. In this work, Prior Networks are applied to adversarial attack\ndetection using measures of uncertainty in a similar fashion to Monte-Carlo\nDropout. Detection based on measures of uncertainty derived from DNNs and\nMonte-Carlo dropout ensembles are used as a baseline. Prior Networks are shown\nto significantly out-perform these baseline approaches over a range of\nadversarial attacks in both detection of whitebox and blackbox configurations.\nEven when the adversarial attacks are constructed with full knowledge of the\ndetection mechanism, it is shown to be highly challenging to successfully\ngenerate an adversarial sample.\n"], ["2018-12-06", "http://arxiv.org/abs/1812.02524", "Towards Leveraging the Information of Gradients in Optimization-based Adversarial Attack.", ["Jingyang Zhang", " Hsin-Pai Cheng", " Chunpeng Wu", " Hai Li", " Yiran Chen"], "  In recent years, deep neural networks demonstrated state-of-the-art\nperformance in a large variety of tasks and therefore have been adopted in many\napplications. On the other hand, the latest studies revealed that neural\nnetworks are vulnerable to adversarial examples obtained by carefully adding\nsmall perturbation to legitimate samples. Based upon the observation, many\nattack methods were proposed. Among them, the optimization-based CW attack is\nthe most powerful as the produced adversarial samples present much less\ndistortion compared to other methods. The better attacking effect, however,\ncomes at the cost of running more iterations and thus longer computation time\nto reach desirable results. In this work, we propose to leverage the\ninformation of gradients as a guidance during the search of adversaries. More\nspecifically, directly incorporating the gradients into the perturbation can be\nregarded as a constraint added to the optimization process. We intuitively and\nempirically prove the rationality of our method in reducing the search space.\nOur experiments show that compared to the original CW attack, the proposed\nmethod requires fewer iterations towards adversarial samples, obtaining a\nhigher success rate and resulting in smaller $\\ell_2$ distortion.\n"], ["2018-12-06", "http://arxiv.org/abs/1812.02843", "Fooling Network Interpretation in Image Classification.", ["Akshayvarun Subramanya", " Vipin Pillai", " Hamed Pirsiavash"], "  Deep neural networks have been shown to be fooled rather easily using\nadversarial attack algorithms. Practical methods such as adversarial patches\nhave been shown to be extremely effective in causing misclassification.\nHowever, these patches are highlighted using standard network interpretation\nalgorithms, thus revealing the identity of the adversary. We show that it is\npossible to create adversarial patches which not only fool the prediction, but\nalso change what we interpret regarding the cause of the prediction. Moreover,\nwe introduce our attack as a controlled setting to measure the accuracy of\ninterpretation algorithms. We show this using extensive experiments for\nGrad-CAM interpretation that transfers to occluding patch interpretation as\nwell. We believe our algorithms can facilitate developing more robust network\ninterpretation tools that truly explain the network's underlying decision\nmaking process.\n"], ["2018-12-06", "http://arxiv.org/abs/1812.02637", "Max-Margin Adversarial (MMA) Training: Direct Input Space Margin Maximization through Adversarial Training.", ["Gavin Weiguang Ding", " Yash Sharma", " Kry Yik Chau Lui", " Ruitong Huang"], "  We study adversarial robustness of neural networks from a margin maximization\nperspective, where margins are defined as the distances from inputs to a\nclassifier's decision boundary. In theory, we show that maximizing margins can\nbe achieved by minimizing the adversarial loss on the decision boundary at the\n\"shortest successful perturbation\". This max-margin perspective also provides\nan alternative interpretation on adversarial training with a fixed perturbation\nmagnitude $\\epsilon$: adversarial training is maximizing either a lower bound\nor an upper bound of the margin. Motivated by our theoretical analysis, we\npropose Max-Margin Adversarial (MMA) training to directly maximize the margins.\nInstead of adversarial training with a fixed $\\epsilon$, MMA offers an\nimprovement by selecting the margin as the \"correct\" $\\epsilon$ individually\nfor each point. We demonstrate MMA training's efficacy and analyze its\nproperties on the MNIST and CIFAR10 datasets w.r.t. $\\ell_\\infty$ and $\\ell_2$\nrobustness.\n"], ["2018-12-05", "http://arxiv.org/abs/1812.02737", "On Configurable Defense against Adversarial Example Attacks.", ["Bo Luo", " Min Li", " Yu Li", " Qiang Xu"], "  Machine learning systems based on deep neural networks (DNNs) have gained\nmainstream adoption in many applications. Recently, however, DNNs are shown to\nbe vulnerable to adversarial example attacks with slight perturbations on the\ninputs. Existing defense mechanisms against such attacks try to improve the\noverall robustness of the system, but they do not differentiate different\ntargeted attacks even though the corresponding impacts may vary significantly.\nTo tackle this problem, we propose a novel configurable defense mechanism in\nthis work, wherein we are able to flexibly tune the robustness of the system\nagainst different targeted attacks to satisfy application requirements. This is\nachieved by refining the DNN loss function with an attack sensitive matrix to\nrepresent the impacts of different targeted attacks. Experimental results on\nCIFAR-10 and GTSRB data sets demonstrate the efficacy of the proposed solution.\n"], ["2018-12-05", "http://arxiv.org/abs/1812.02132", "SADA: Semantic Adversarial Diagnostic Attacks for Autonomous Applications.", ["Abdullah Hamdi", " Matthias M\u00fcller", " Bernard Ghanem"], "  One major factor impeding more widespread adoption of deep neural networks\n(DNNs) is their issues with robustness, which is essential for safety critical\napplications such as autonomous driving. This has motivated much recent work on\nadversarial attacks for DNNs, which mostly focus on pixel-level perturbations\nvoid of semantic meaning. In contrast, we present a general framework for\nadversarial black-box attacks on trained agents, which covers semantic\nperturbations to the environment of the agent performing the task as well as\npixel-level attacks. To do this, we re-frame the adversarial attack problem as\nlearning a distribution of parameters that always fool the agent. In the\nsemantic case, our proposed adversary (denoted as BBGAN) is trained to sample\nparameters that describe the environment with which the black-box agent\ninteracts, such that the agent performs its dedicated task poorly in this\nenvironment. We apply BBGAN on three different tasks (primarily targeting\naspects of autonomous navigation): object detection, self-driving, and\nautonomous UAV racing. On these tasks, BBGAN can generate failure cases that\nconsistently fool an agent. We also demonstrate the usefulness of our framework\nas an analysis tool by visualizing systemic failure cases and uncovering\nsemantic insights about the agents themselves.\n"], ["2018-12-05", "http://arxiv.org/abs/1812.01821", "Regularized Ensembles and Transferability in Adversarial Learning.", ["Yifan Chen", " Yevgeniy Vorobeychik"], "  Despite the considerable success of convolutional neural networks in a broad\narray of domains, recent research has shown these to be vulnerable to small\nadversarial perturbations, commonly known as adversarial examples. Moreover,\nsuch examples have shown to be remarkably portable, or transferable, from one\nmodel to another, enabling highly successful black-box attacks. We explore this\nissue of transferability and robustness from two dimensions: first, considering\nthe impact of conventional $l_p$ regularization as well as replacing the top\nlayer with a linear support vector machine (SVM), and second, the value of\ncombining regularized models into an ensemble. We show that models trained with\ndifferent regularizers present barriers to transferability, as does partial\ninformation about the models comprising the ensemble.\n"], ["2018-12-04", "http://arxiv.org/abs/1812.01647", "Rigorous Agent Evaluation: An Adversarial Approach to Uncover Catastrophic Failures.", ["Jonathan Dj Uesato", " Ananya Dj Kumar", " Csaba Dj Szepesvari", " Tom Dj Erez", " Avraham Dj Ruderman", " Keith Dj Anderson", " Dj Krishmamurthy", " Dvijotham", " Nicolas Heess", " Pushmeet Kohli"], "  This paper addresses the problem of evaluating learning systems in safety\ncritical domains such as autonomous driving, where failures can have\ncatastrophic consequences. We focus on two problems: searching for scenarios\nwhen learned agents fail and assessing their probability of failure. The\nstandard method for agent evaluation in reinforcement learning, Vanilla Monte\nCarlo, can miss failures entirely, leading to the deployment of unsafe agents.\nWe demonstrate this is an issue for current agents, where even matching the\ncompute used for training is sometimes insufficient for evaluation. To address\nthis shortcoming, we draw upon the rare event probability estimation literature\nand propose an adversarial evaluation approach. Our approach focuses evaluation\non adversarially chosen situations, while still providing unbiased estimates of\nfailure probabilities. The key difficulty is in identifying these adversarial\nsituations -- since failures are rare there is little signal to drive\noptimization. To solve this we propose a continuation approach that learns\nfailure modes in related but less robust agents. Our approach also allows reuse\nof data already collected for training the agent. We demonstrate the efficacy\nof adversarial evaluation on two standard domains: humanoid control and\nsimulated driving. Experimental results show that our methods can find\ncatastrophic failures and estimate failures rates of agents multiple orders of\nmagnitude faster than standard evaluation schemes, in minutes to hours rather\nthan days.\n"], ["2018-12-04", "http://arxiv.org/abs/1812.01804", "Random Spiking and Systematic Evaluation of Defenses Against Adversarial Examples.", ["Huangyi Ge", " Sze Yiu Chau", " Bruno Ribeiro", " Ninghui Li"], "  Image classifiers often suffer from adversarial examples, which are generated\nby strategically adding a small amount of noise to input images to trick\nclassifiers into misclassification. Over the years, many defense mechanisms\nhave been proposed, and different researchers have made seemingly contradictory\nclaims on their effectiveness. We present an analysis of possible adversarial\nmodels, and propose an evaluation framework for comparing different defense\nmechanisms. As part of the framework, we introduced a more powerful and\nrealistic adversary strategy. We propose a new defense mechanism called Random\nSpiking (RS), which generalizes dropout and introduces random noises in the\ntraining process in a controlled manner. Evaluations under our proposed\nframework suggest RS delivers better protection against adversarial examples\nthan many existing schemes.\n"], ["2018-12-03", "http://arxiv.org/abs/1812.00740", "Disentangling Adversarial Robustness and Generalization.", ["David Stutz", " Matthias Hein", " Bernt Schiele"], "  Obtaining deep networks that are robust against adversarial examples and\ngeneralize well is an open problem. A recent hypothesis even states that both\nrobust and accurate models are impossible, i.e., adversarial robustness and\ngeneralization are conflicting goals. In an effort to clarify the relationship\nbetween robustness and generalization, we assume an underlying, low-dimensional\ndata manifold and show that: 1. regular adversarial examples leave the\nmanifold; 2. adversarial examples constrained to the manifold, i.e.,\non-manifold adversarial examples, exist; 3. on-manifold adversarial examples\nare generalization errors, and on-manifold adversarial training boosts\ngeneralization; 4. regular robustness and generalization are not necessarily\ncontradicting goals. These assumptions imply that both robust and accurate\nmodels are possible. However, different models (architectures, training\nstrategies etc.) can exhibit different robustness and generalization\ncharacteristics. To confirm our claims, we present extensive experiments on\nsynthetic data (with known manifold) as well as on EMNIST, Fashion-MNIST and\nCelebA.\n"], ["2018-12-03", "http://arxiv.org/abs/1812.00891", "Interpretable Deep Learning under Fire.", ["Xinyang Zhang", " Ningfei Wang", " Hua Shen", " Shouling Ji", " Xiapu Luo", " Ting Wang"], "  Providing explanations for deep neural network (DNN) models is crucial for\ntheir use in security-sensitive domains. A plethora of interpretation models\nhave been proposed to help users understand the inner workings of DNNs: how\ndoes a DNN arrive at a specific decision for a given input? The improved\ninterpretability is believed to offer a sense of security by involving human in\nthe decision-making process. Yet, due to its data-driven nature, the\ninterpretability itself is potentially susceptible to malicious manipulations,\nabout which little is known thus far.\n  Here we bridge this gap by conducting the first systematic study on the\nsecurity of interpretable deep learning systems (IDLSes). We show that existing\n\\imlses are highly vulnerable to adversarial manipulations. Specifically, we\npresent ADV^2, a new class of attacks that generate adversarial inputs not only\nmisleading target DNNs but also deceiving their coupled interpretation models.\nThrough empirical evaluation against four major types of IDLSes on benchmark\ndatasets and in security-critical applications (e.g., skin cancer diagnosis),\nwe demonstrate that with ADV^2 the adversary is able to arbitrarily designate\nan input's prediction and interpretation. Further, with both analytical and\nempirical evidence, we identify the prediction-interpretation gap as one root\ncause of this vulnerability -- a DNN and its interpretation model are often\nmisaligned, resulting in the possibility of exploiting both models\nsimultaneously. Finally, we explore potential countermeasures against ADV^2,\nincluding leveraging its low transferability and incorporating it in an\nadversarial training framework. Our findings shed light on designing and\noperating IDLSes in a more secure and informative fashion, leading to several\npromising research directions.\n"], ["2018-12-03", "http://arxiv.org/abs/1812.01198", "Adversarial Example Decomposition.", ["Horace He", " Aaron Lou", " Qingxuan Jiang", " Isay Katsman", " Serge Belongie", " Ser-Nam Lim"], "  Research has shown that widely used deep neural networks are vulnerable to\ncarefully crafted adversarial perturbations. Moreover, these adversarial\nperturbations often transfer across models. We hypothesize that adversarial\nweakness is composed of three sources of bias: architecture, dataset, and\nrandom initialization. We show that one can decompose adversarial examples into\nan architecture-dependent component, data-dependent component, and\nnoise-dependent component and that these components behave intuitively. For\nexample, noise-dependent components transfer poorly to all other models, while\narchitecture-dependent components transfer better to retrained models with the\nsame architecture. In addition, we demonstrate that these components can be\nrecombined to improve transferability without sacrificing efficacy on the\noriginal model.\n"], ["2018-12-02", "http://arxiv.org/abs/1812.00483", "Model-Reuse Attacks on Deep Learning Systems.", ["Yujie Ji", " Xinyang Zhang", " Shouling Ji", " Xiapu Luo", " Ting Wang"], "  Many of today's machine learning (ML) systems are built by reusing an array\nof, often pre-trained, primitive models, each fulfilling distinct functionality\n(e.g., feature extraction). The increasing use of primitive models\nsignificantly simplifies and expedites the development cycles of ML systems.\nYet, because most of such models are contributed and maintained by untrusted\nsources, their lack of standardization or regulation entails profound security\nimplications, about which little is known thus far.\n  In this paper, we demonstrate that malicious primitive models pose immense\nthreats to the security of ML systems. We present a broad class of {\\em\nmodel-reuse} attacks wherein maliciously crafted models trigger host ML systems\nto misbehave on targeted inputs in a highly predictable manner. By empirically\nstudying four deep learning systems (including both individual and ensemble\nsystems) used in skin cancer screening, speech recognition, face verification,\nand autonomous steering, we show that such attacks are (i) effective - the host\nsystems misbehave on the targeted inputs as desired by the adversary with high\nprobability, (ii) evasive - the malicious models function indistinguishably\nfrom their benign counterparts on non-targeted inputs, (iii) elastic - the\nmalicious models remain effective regardless of various system design choices\nand tuning strategies, and (iv) easy - the adversary needs little prior\nknowledge about the data used for system tuning or inference. We provide\nanalytical justification for the effectiveness of model-reuse attacks, which\npoints to the unprecedented complexity of today's primitive models. This issue\nthus seems fundamental to many ML systems. We further discuss potential\ncountermeasures and their challenges, which lead to several promising research\ndirections.\n"], ["2018-12-02", "http://arxiv.org/abs/1812.00552", "Universal Perturbation Attack Against Image Retrieval.", ["Jie Li", " Rongrong Ji", " Hong Liu", " Xiaopeng Hong", " Yue Gao", " Qi Tian"], "  Universal adversarial perturbations (UAPs), a.k.a. input-agnostic\nperturbations, has been proved to exist and be able to fool cutting-edge deep\nlearning models on most of the data samples. Existing UAP methods mainly focus\non attacking image classification models. Nevertheless, little attention has\nbeen paid to attacking image retrieval systems. In this paper, we make the\nfirst attempt in attacking image retrieval systems. Concretely, image retrieval\nattack is to make the retrieval system return irrelevant images to the query at\nthe top ranking list. It plays an important role to corrupt the neighbourhood\nrelationships among features in image retrieval attack. To this end, we propose\na novel method to generate retrieval-against UAP to break the neighbourhood\nrelationships of image features via degrading the corresponding ranking metric.\nTo expand the attack method to scenarios with varying input sizes or\nuntouchable network parameters, a multi-scale random resizing scheme and a\nranking distillation strategy are proposed. We evaluate the proposed method on\nfour widely-used image retrieval datasets, and report a significant performance\ndrop in terms of different metrics, such as mAP and mP@10. Finally, we test our\nattack methods on the real-world visual search engine, i.e., Google Images,\nwhich demonstrates the practical potentials of our methods.\n"], ["2018-12-01", "http://arxiv.org/abs/1812.01713", "FineFool: Fine Object Contour Attack via Attention.", ["Jinyin Chen", " Haibin Zheng", " Hui Xiong", " Mengmeng Su"], "  Machine learning models have been shown vulnerable to adversarial attacks\nlaunched by adversarial examples which are carefully crafted by attacker to\ndefeat classifiers. Deep learning models cannot escape the attack either. Most\nof adversarial attack methods are focused on success rate or perturbations\nsize, while we are more interested in the relationship between adversarial\nperturbation and the image itself. In this paper, we put forward a novel\nadversarial attack based on contour, named FineFool. Finefool not only has\nbetter attack performance compared with other state-of-art white-box attacks in\naspect of higher attack success rate and smaller perturbation, but also capable\nof visualization the optimal adversarial perturbation via attention on object\ncontour. To the best of our knowledge, Finefool is for the first time combines\nthe critical feature of the original clean image with the optimal perturbations\nin a visible manner. Inspired by the correlations between adversarial\nperturbations and object contour, slighter perturbations is produced via\nfocusing on object contour features, which is more imperceptible and difficult\nto be defended, especially network add-on defense methods with the trade-off\nbetween perturbations filtering and contour feature loss. Compared with\nexisting state-of-art attacks, extensive experiments are conducted to show that\nFinefool is capable of efficient attack against defensive deep models.\n"], ["2018-12-01", "http://arxiv.org/abs/1812.00292", "SentiNet: Detecting Physical Attacks Against Deep Learning Systems.", ["Edward Chou", " Florian Tram\u00e8r", " Giancarlo Pellegrino", " Dan Boneh"], "  SentiNet is a novel detection framework for physical attacks on neural\nnetworks, a class of attacks that constrains an adversarial region to a visible\nportion of an image. Physical attacks have been shown to be robust and flexible\ntechniques suited for deployment in real-world scenarios. Unlike most other\nadversarial detection works, SentiNet does not require training a model or\npreknowledge of an attack prior to detection. This attack-agnostic approach is\nappealing due to the large number of possible mechanisms and vectors of attack\nan attack-specific defense would have to consider. By leveraging the neural\nnetwork's susceptibility to attacks and by using techniques from model\ninterpretability and object detection as detection mechanisms, SentiNet turns a\nweakness of a model into a strength. We demonstrate the effectiveness of\nSentiNet on three different attacks - i.e., adversarial examples, data\npoisoning attacks, and trojaned networks - that have large variations in\ndeployment mechanisms, and show that our defense is able to achieve very\ncompetitive performance metrics for all three threats, even against strong\nadaptive adversaries with full knowledge of SentiNet.\n"], ["2018-12-01", "http://arxiv.org/abs/1812.00239", "Building robust classifiers through generation of confident out of distribution examples.", ["Kumar Sricharan", " Ashok Srivastava"], "  Deep learning models are known to be overconfident in their predictions on\nout of distribution inputs. There have been several pieces of work to address\nthis issue, including a number of approaches for building Bayesian neural\nnetworks, as well as closely related work on detection of out of distribution\nsamples. Recently, there has been work on building classifiers that are robust\nto out of distribution samples by adding a regularization term that maximizes\nthe entropy of the classifier output on out of distribution data. To\napproximate out of distribution samples (which are not known apriori), a GAN\nwas used for generation of samples at the edges of the training distribution.\nIn this paper, we introduce an alternative GAN based approach for building a\nrobust classifier, where the idea is to use the GAN to explicitly generate out\nof distribution samples that the classifier is confident on (low entropy), and\nhave the classifier maximize the entropy for these samples. We showcase the\neffectiveness of our approach relative to state-of-the-art on hand-written\ncharacters as well as on a variety of natural image datasets.\n"], ["2018-12-01", "http://arxiv.org/abs/1812.00181", "Effects of Loss Functions And Target Representations on Adversarial Robustness.", ["Sean Saito", " Sujoy Roy"], "  Understanding and evaluating the robustness of neural networks under\nadversarial settings is a subject of growing interest. Attacks proposed in the\nliterature usually work with models trained to minimize cross-entropy loss and\noutput softmax probabilities. In this work, we present interesting experimental\nresults that suggest the importance of considering other loss functions and\ntarget representations, specifically, (1) training on mean-squared error and\n(2) representing targets as codewords generated from a random codebook. We\nevaluate the robustness of neural networks that implement these proposed\nmodifications using existing attacks, showing an increase in accuracy against\nuntargeted attacks of up to 98.7\\% and a decrease of targeted attack success\nrates of up to 99.8\\%. Our model demonstrates more robustness compared to its\nconventional counterpart even against attacks that are tailored to our\nmodifications. Furthermore, we find that the parameters of our modified model\nhave significantly smaller Lipschitz bounds, an important measure correlated\nwith a model's sensitivity to adversarial perturbations.\n"], ["2018-12-01", "http://arxiv.org/abs/1812.00151", "Discrete Adversarial Attacks and Submodular Optimization with Applications to Text Classification.", ["Qi Lei", " Lingfei Wu", " Pin-Yu Chen", " Alexandros G. Dimakis", " Inderjit S. Dhillon", " Michael Witbrock"], "  Adversarial examples are carefully constructed modifications to an input that\ncompletely change the output of a classifier but are imperceptible to humans.\nDespite these successful attacks for continuous data (such as image and audio\nsamples), generating adversarial examples for discrete structures such as text\nhas proven significantly more challenging. In this paper we formulate the\nattacks with discrete input on a set function as an optimization task. We prove\nthat this set function is submodular for some popular neural network text\nclassifiers under simplifying assumption. This finding guarantees a $1-1/e$\napproximation factor for attacks that use the greedy algorithm. Meanwhile, we\nshow how to use the gradient of the attacked classifier to guide the greedy\nsearch. Empirical studies with our proposed optimization scheme show\nsignificantly improved attack ability and efficiency, on three different text\nclassification tasks over various baselines. We also use a joint sentence and\nword paraphrasing technique to maintain the original semantics and syntax of\nthe text. This is validated by a human subject evaluation in subjective metrics\non the quality and semantic coherence of our generated adversarial text.\n"], ["2018-11-30", "http://arxiv.org/abs/1811.12641", "Transferable Adversarial Attacks for Image and Video Object Detection.", ["Xingxing Wei", " Siyuan Liang", " Xiaochun Cao", " Jun Zhu"], "  Adversarial examples have been demonstrated to threaten many computer vision\ntasks including object detection. However, the existing attacking methods for\nobject detection have two limitations: poor transferability, which denotes that\nthe generated adversarial examples have low success rate to attack other kinds\nof detection methods, and high computation cost, which means that they need\nmore time to generate an adversarial image, and therefore are difficult to deal\nwith the video data. To address these issues, we utilize a generative mechanism\nto obtain the adversarial image and video. In this way, the processing time is\nreduced. To enhance the transferability, we destroy the feature maps extracted\nfrom the feature network, which usually constitutes the basis of object\ndetectors. The proposed method is based on the Generative Adversarial Network\n(GAN) framework, where we combine the high-level class loss and low-level\nfeature loss to jointly train the adversarial example generator. A series of\nexperiments conducted on PASCAL VOC and ImageNet VID datasets show that our\nmethod can efficiently generate image and video adversarial examples, and more\nimportantly, these adversarial examples have better transferability, and thus,\nare able to simultaneously attack two kinds of representative object detection\nmodels: proposal based models like Faster-RCNN, and regression based models\nlike SSD.\n"], ["2018-11-30", "http://arxiv.org/abs/1811.12673", "ComDefend: An Efficient Image Compression Model to Defend Adversarial Examples.", ["Xiaojun Jia", " Xingxing Wei", " Xiaochun Cao", " Hassan Foroosh"], "  Deep neural networks (DNNs) have been demonstrated to be vulnerable to\nadversarial examples. Specifically, adding imperceptible perturbations to clean\nimages can fool the well trained deep neural networks. In this paper, we\npropose an end-to-end image compression model to defend adversarial examples:\n\\textbf{ComDefend}. The proposed model consists of a compression convolutional\nneural network (ComCNN) and a reconstruction convolutional neural network\n(ResCNN). The ComCNN is used to maintain the structure information of the\noriginal image and purify adversarial perturbations. And the ResCNN is used to\nreconstruct the original image with high quality. In other words, ComDefend can\ntransform the adversarial image to its clean version, which is then fed to the\ntrained classifier. Our method is a pre-processing module, and does not modify\nthe classifier's structure during the whole process. Therefore, it can be\ncombined with other model-specific defense models to jointly improve the\nclassifier's robustness. A series of experiments conducted on MNIST, CIFAR10\nand ImageNet show that the proposed method outperforms the state-of-the-art\ndefense methods, and is consistently effective to protect classifiers against\nadversarial attacks.\n"], ["2018-11-30", "http://arxiv.org/abs/1812.00037", "Adversarial Defense by Stratified Convolutional Sparse Coding.", ["Bo Sun", " Nian-hsuan Tsai", " Fangchen Liu", " Ronald Yu", " Hao Su"], "  We propose an adversarial defense method that achieves state-of-the-art\nperformance among attack-agnostic adversarial defense methods while also\nmaintaining robustness to input resolution, scale of adversarial perturbation,\nand scale of dataset size. Based on convolutional sparse coding, we construct a\nstratified low-dimensional quasi-natural image space that faithfully\napproximates the natural image space while also removing adversarial\nperturbations. We introduce a novel Sparse Transformation Layer (STL) in\nbetween the input image and the first layer of the neural network to\nefficiently project images into our quasi-natural image space. Our experiments\nshow state-of-the-art performance of our method compared to other\nattack-agnostic adversarial defense methods in various adversarial settings.\n"], ["2018-11-29", "http://arxiv.org/abs/1811.12395", "CNN-Cert: An Efficient Framework for Certifying Robustness of Convolutional Neural Networks.", ["Akhilan Boopathy", " Tsui-Wei Weng", " Pin-Yu Chen", " Sijia Liu", " Luca Daniel"], "  Verifying robustness of neural network classifiers has attracted great\ninterests and attention due to the success of deep neural networks and their\nunexpected vulnerability to adversarial perturbations. Although finding minimum\nadversarial distortion of neural networks (with ReLU activations) has been\nshown to be an NP-complete problem, obtaining a non-trivial lower bound of\nminimum distortion as a provable robustness guarantee is possible. However,\nmost previous works only focused on simple fully-connected layers (multilayer\nperceptrons) and were limited to ReLU activations. This motivates us to propose\na general and efficient framework, CNN-Cert, that is capable of certifying\nrobustness on general convolutional neural networks. Our framework is general\n-- we can handle various architectures including convolutional layers,\nmax-pooling layers, batch normalization layer, residual blocks, as well as\ngeneral activation functions; our approach is efficient -- by exploiting the\nspecial structure of convolutional layers, we achieve up to 17 and 11 times of\nspeed-up compared to the state-of-the-art certification algorithms (e.g.\nFast-Lin, CROWN) and 366 times of speed-up compared to the dual-LP approach\nwhile our algorithm obtains similar or even better verification bounds. In\naddition, CNN-Cert generalizes state-of-the-art algorithms e.g. Fast-Lin and\nCROWN. We demonstrate by extensive experiments that our method outperforms\nstate-of-the-art lower-bound-based certification algorithms in terms of both\nbound quality and speed.\n"], ["2018-11-29", "http://arxiv.org/abs/1811.12335", "Bayesian Adversarial Spheres: Bayesian Inference and Adversarial Examples in a Noiseless Setting.", ["Artur Bekasov", " Iain Murray"], "  Modern deep neural network models suffer from adversarial examples, i.e.\nconfidently misclassified points in the input space. It has been shown that\nBayesian neural networks are a promising approach for detecting adversarial\npoints, but careful analysis is problematic due to the complexity of these\nmodels. Recently Gilmer et al. (2018) introduced adversarial spheres, a toy\nset-up that simplifies both practical and theoretical analysis of the problem.\nIn this work, we use the adversarial sphere set-up to understand the properties\nof approximate Bayesian inference methods for a linear model in a noiseless\nsetting. We compare predictions of Bayesian and non-Bayesian methods,\nshowcasing the advantages of the former, although revealing open challenges for\ndeep learning applications.\n"], ["2018-11-29", "http://arxiv.org/abs/1811.12601", "Adversarial Examples as an Input-Fault Tolerance Problem.", ["Angus Galloway", " Anna Golubeva", " Graham W. Taylor"], "  We analyze the adversarial examples problem in terms of a model's fault\ntolerance with respect to its input. Whereas previous work focuses on\narbitrarily strict threat models, i.e., $\\epsilon$-perturbations, we consider\narbitrary valid inputs and propose an information-based characteristic for\nevaluating tolerance to diverse input faults.\n"], ["2018-11-29", "http://arxiv.org/abs/1811.12470", "Analyzing Federated Learning through an Adversarial Lens.", ["Arjun Nitin Bhagoji", " Supriyo Chakraborty", " Prateek Mittal", " Seraphin Calo"], "  Federated learning distributes model training among a multitude of agents,\nwho, guided by privacy concerns, perform training using their local data but\nshare only model parameter updates, for iterative aggregation at the server. In\nthis work, we explore the threat of model poisoning attacks on federated\nlearning initiated by a single, non-colluding malicious agent where the\nadversarial objective is to cause the model to misclassify a set of chosen\ninputs with high confidence. We explore a number of strategies to carry out\nthis attack, starting with simple boosting of the malicious agent's update to\novercome the effects of other agents' updates. To increase attack stealth, we\npropose an alternating minimization strategy, which alternately optimizes for\nthe training loss and the adversarial objective. We follow up by using\nparameter estimation for the benign agents' updates to improve on attack\nsuccess. Finally, we use a suite of interpretability techniques to generate\nvisual explanations of model decisions for both benign and malicious models and\nshow that the explanations are nearly visually indistinguishable. Our results\nindicate that even a highly constrained adversary can carry out model poisoning\nattacks while simultaneously maintaining stealth, thus highlighting the\nvulnerability of the federated learning setting and the need to develop\neffective defense strategies.\n"], ["2018-11-28", "http://arxiv.org/abs/1811.11875", "Adversarial Attacks for Optical Flow-Based Action Recognition Classifiers.", ["Nathan Inkawhich", " Matthew Inkawhich", " Yiran Chen", " Hai Li"], "  The success of deep learning research has catapulted deep models into\nproduction systems that our society is becoming increasingly dependent on,\nespecially in the image and video domains. However, recent work has shown that\nthese largely uninterpretable models exhibit glaring security vulnerabilities\nin the presence of an adversary. In this work, we develop a powerful untargeted\nadversarial attack for action recognition systems in both white-box and\nblack-box settings. Action recognition models differ from image-classification\nmodels in that their inputs contain a temporal dimension, which we explicitly\ntarget in the attack. Drawing inspiration from image classifier attacks, we\ncreate new attacks which achieve state-of-the-art success rates on a two-stream\nclassifier trained on the UCF-101 dataset. We find that our attacks can\nsignificantly degrade a model's performance with sparsely and imperceptibly\nperturbed examples. We also demonstrate the transferability of our attacks to\nblack-box action recognition systems.\n"], ["2018-11-28", "http://arxiv.org/abs/1811.11553", "Strike (with) a Pose: Neural Networks Are Easily Fooled by Strange Poses of Familiar Objects.", ["Michael A. Alcorn", " Qi Li", " Zhitao Gong", " Chengfei Wang", " Long Mai", " Wei-Shinn Ku", " Anh Nguyen"], "  Despite excellent performance on stationary test sets, deep neural networks\n(DNNs) can fail to generalize to out-of-distribution (OoD) inputs, including\nnatural, non-adversarial ones, which are common in real-world settings. In this\npaper, we present a framework for discovering DNN failures that harnesses 3D\nrenderers and 3D models. That is, we estimate the parameters of a 3D renderer\nthat cause a target DNN to misbehave in response to the rendered image. Using\nour framework and a self-assembled dataset of 3D objects, we investigate the\nvulnerability of DNNs to OoD poses of well-known objects in ImageNet. For\nobjects that are readily recognized by DNNs in their canonical poses, DNNs\nincorrectly classify 97% of their pose space. In addition, DNNs are highly\nsensitive to slight pose perturbations. Importantly, adversarial poses transfer\nacross models and datasets. We find that 99.9% and 99.4% of the poses\nmisclassified by Inception-v3 also transfer to the AlexNet and ResNet-50 image\nclassifiers trained on the same ImageNet dataset, respectively, and 75.5%\ntransfer to the YOLOv3 object detector trained on MS COCO.\n"], ["2018-11-28", "http://arxiv.org/abs/1811.11493", "A randomized gradient-free attack on ReLU networks.", ["Francesco Croce", " Matthias Hein"], "  It has recently been shown that neural networks but also other classifiers\nare vulnerable to so called adversarial attacks e.g. in object recognition an\nalmost non-perceivable change of the image changes the decision of the\nclassifier. Relatively fast heuristics have been proposed to produce these\nadversarial inputs but the problem of finding the optimal adversarial input,\nthat is with the minimal change of the input, is NP-hard. While methods based\non mixed-integer optimization which find the optimal adversarial input have\nbeen developed, they do not scale to large networks. Currently, the attack\nscheme proposed by Carlini and Wagner is considered to produce the best\nadversarial inputs. In this paper we propose a new attack scheme for the class\nof ReLU networks based on a direct optimization on the resulting linear\nregions. In our experimental validation we improve in all except one experiment\nout of 18 over the Carlini-Wagner attack with a relative improvement of up to\n9\\%. As our approach is based on the geometrical structure of ReLU networks, it\nis less susceptible to defences targeting their functional properties.\n"], ["2018-11-28", "http://arxiv.org/abs/1811.11402", "Adversarial Machine Learning And Speech Emotion Recognition: Utilizing Generative Adversarial Networks For Robustness.", ["Siddique Latif", " Rajib Rana", " Junaid Qadir"], "  Deep learning has undoubtedly offered tremendous improvements in the\nperformance of state-of-the-art speech emotion recognition (SER) systems.\nHowever, recent research on adversarial examples poses enormous challenges on\nthe robustness of SER systems by showing the susceptibility of deep neural\nnetworks to adversarial examples as they rely only on small and imperceptible\nperturbations. In this study, we evaluate how adversarial examples can be used\nto attack SER systems and propose the first black-box adversarial attack on SER\nsystems. We also explore potential defenses including adversarial training and\ngenerative adversarial network (GAN) to enhance robustness. Experimental\nevaluations suggest various interesting aspects of the effective utilization of\nadversarial examples useful for achieving robustness for SER systems opening up\nopportunities for researchers to further innovate in this space.\n"], ["2018-11-27", "http://arxiv.org/abs/1811.11304", "Universal Adversarial Training.", ["Ali Shafahi", " Mahyar Najibi", " Zheng Xu", " John Dickerson", " Larry S. Davis", " Tom Goldstein"], "  Standard adversarial attacks change the predicted class label of an image by\nadding specially tailored small perturbations to its pixels. In contrast, a\nuniversal perturbation is an update that can be added to any image in a broad\nclass of images, while still changing the predicted class label. We study the\nefficient generation of universal adversarial perturbations, and also efficient\nmethods for hardening networks to these attacks. We propose a simple\noptimization-based universal attack that reduces the top-1 accuracy of various\nnetwork architectures on ImageNet to less than 20%, while learning the\nuniversal perturbation 13X faster than the standard method. To defend against\nthese perturbations, we propose universal adversarial training, which models\nthe problem of robust classifier generation as a two-player min-max game. This\nmethod is much faster and more scalable than conventional adversarial training\nwith a strong adversary (PGD), and yet yields models that are extremely\nresistant to universal attacks, and comparably resistant to standard\n(per-instance) black box attacks. We also discover a rather fascinating\nside-effect of universal adversarial training: attacks built for universally\nrobust models transfer better to other (black box) models than those built with\nconventional adversarial training.\n"], ["2018-11-27", "http://arxiv.org/abs/1811.11079", "Robust Classification of Financial Risk.", ["Suproteem K. Sarkar", " Kojin Oshiba", " Daniel Giebisch", " Yaron Singer"], "  Algorithms are increasingly common components of high-impact decision-making,\nand a growing body of literature on adversarial examples in laboratory settings\nindicates that standard machine learning models are not robust. This suggests\nthat real-world systems are also susceptible to manipulation or\nmisclassification, which especially poses a challenge to machine learning\nmodels used in financial services. We use the loan grade classification problem\nto explore how machine learning models are sensitive to small changes in\nuser-reported data, using adversarial attacks documented in the literature and\nan original, domain-specific attack. Our work shows that a robust optimization\nalgorithm can build models for financial services that are resistant to\nmisclassification on perturbations. To the best of our knowledge, this is the\nfirst study of adversarial attacks and defenses for deep learning in financial\nservices.\n"], ["2018-11-27", "http://arxiv.org/abs/1811.11310", "Using Attribution to Decode Dataset Bias in Neural Network Models for Chemistry.", ["Kevin McCloskey", " Ankur Taly", " Federico Monti", " Michael P. Brenner", " Lucy Colwell"], "  Deep neural networks have achieved state of the art accuracy at classifying\nmolecules with respect to whether they bind to specific protein targets. A key\nbreakthrough would occur if these models could reveal the fragment\npharmacophores that are causally involved in binding. Extracting chemical\ndetails of binding from the networks could potentially lead to scientific\ndiscoveries about the mechanisms of drug actions. But doing so requires shining\nlight into the black box that is the trained neural network model, a task that\nhas proved difficult across many domains. Here we show how the binding\nmechanism learned by deep neural network models can be interrogated, using a\nrecently described attribution method. We first work with carefully constructed\nsynthetic datasets, in which the 'fragment logic' of binding is fully known. We\nfind that networks that achieve perfect accuracy on held out test datasets\nstill learn spurious correlations due to biases in the datasets, and we are\nable to exploit this non-robustness to construct adversarial examples that fool\nthe model. The dataset bias makes these models unreliable for accurately\nrevealing information about the mechanisms of protein-ligand binding. In light\nof our findings, we prescribe a test that checks for dataset bias given a\nhypothesis. If the test fails, it indicates that either the model must be\nsimplified or regularized and/or that the training dataset requires\naugmentation.\n"], ["2018-11-27", "http://arxiv.org/abs/1811.10828", "A Frank-Wolfe Framework for Efficient and Effective Adversarial Attacks.", ["Jinghui Chen", " Dongruo Zhou", " Jinfeng Yi", " Quanquan Gu"], "  Depending on how much information an adversary can access to, adversarial\nattacks can be classified as white-box attack and black-box attack. For\nwhite-box attack, optimization-based attack algorithms such as projected\ngradient descent (PGD) can achieve relatively high attack success rates within\nmoderate iterates. However, they tend to generate adversarial examples near or\nupon the boundary of the perturbation set, resulting in large distortion.\nFurthermore, their corresponding black-box attack algorithms also suffer from\nhigh query complexities, thereby limiting their practical usefulness. In this\npaper, we focus on the problem of developing efficient and effective\noptimization-based adversarial attack algorithms. In particular, we propose a\nnovel adversarial attack framework for both white-box and black-box settings\nbased on a variant of Frank-Wolfe algorithm. We show in theory that the\nproposed attack algorithms are efficient with an $O(1/\\sqrt{T})$ convergence\nrate. The empirical results of attacking the ImageNet and MNIST datasets also\nverify the efficiency and effectiveness of the proposed algorithms. More\nspecifically, our proposed algorithms attain the best attack performances in\nboth white-box and black-box attacks among all baselines, and are more time and\nquery efficient than the state-of-the-art.\n"], ["2018-11-26", "http://arxiv.org/abs/1811.10745", "ResNets Ensemble via the Feynman-Kac Formalism to Improve Natural and Robust Accuracies.", ["Bao Wang", " Binjie Yuan", " Zuoqiang Shi", " Stanley J. Osher"], "  Empirical adversarial risk minimization (EARM) is a widely used mathematical\nframework to robustly train deep neural nets (DNNs) that are resistant to\nadversarial attacks. However, both natural and robust accuracies, in\nclassifying clean and adversarial images, respectively, of the trained robust\nmodels are far from satisfactory. In this work, we unify the theory of optimal\ncontrol of transport equations with the practice of training and testing of\nResNets. Based on this unified viewpoint, we propose a simple yet effective\nResNets ensemble algorithm to boost the accuracy of the robustly trained model\non both clean and adversarial images. The proposed algorithm consists of two\ncomponents: First, we modify the base ResNets by injecting a variance specified\nGaussian noise to the output of each residual mapping. Second, we average over\nthe production of multiple jointly trained modified ResNets to get the final\nprediction. These two steps give an approximation to the Feynman-Kac formula\nfor representing the solution of a transport equation with viscosity, or a\nconvection-diffusion equation. For the CIFAR10 benchmark, this simple algorithm\nleads to a robust model with a natural accuracy of {\\bf 85.62}\\% on clean\nimages and a robust accuracy of ${\\bf 57.94 \\%}$ under the 20 iterations of the\nIFGSM attack, which outperforms the current state-of-the-art in defending\nagainst IFGSM attack on the CIFAR10. Both natural and robust accuracies of the\nproposed ResNets ensemble can be improved dynamically as the building block\nResNet advances. The code is available at:\n\\url{https://github.com/BaoWangMath/EnResNet}.\n"], ["2018-11-26", "http://arxiv.org/abs/1811.10716", "Bilateral Adversarial Training: Towards Fast Training of More Robust Models Against Adversarial Attacks.", ["Jianyu Wang", " Haichao Zhang"], "  In this paper, we study fast training of adversarially robust models. From\nthe analyses of the state-of-the-art defense method, i.e., the multi-step\nadversarial training, we hypothesize that the gradient magnitude links to the\nmodel robustness. Motivated by this, we propose to perturb both the image and\nthe label during training, which we call Bilateral Adversarial Training (BAT).\nTo generate the adversarial label, we derive an closed-form heuristic solution.\nTo generate the adversarial image, we use one-step targeted attack with the\ntarget label being the most confusing class. In the experiment, we first show\nthat random start and the most confusing target attack effectively prevent the\nlabel leaking and gradient masking problem. Then coupled with the adversarial\nlabel part, our model significantly improves the state-of-the-art results. For\nexample, against PGD100 white-box attack with cross-entropy loss, on CIFAR10,\nwe achieve 63.7\\% versus 47.2\\%; on SVHN, we achieve 59.1\\% versus 42.1\\%. At\nlast, the experiment on the very (computationally) challenging ImageNet dataset\nfurther demonstrates the effectiveness of our fast method.\n"], ["2018-11-25", "http://arxiv.org/abs/1811.09982", "Is Data Clustering in Adversarial Settings Secure?.", ["Battista Biggio", " Ignazio Pillai", " Samuel Rota Bul\u00f2", " Davide Ariu", " Marcello Pelillo", " Fabio Roli"], "  Clustering algorithms have been increasingly adopted in security applications\nto spot dangerous or illicit activities. However, they have not been originally\ndevised to deal with deliberate attack attempts that may aim to subvert the\nclustering process itself. Whether clustering can be safely adopted in such\nsettings remains thus questionable. In this work we propose a general framework\nthat allows one to identify potential attacks against clustering algorithms,\nand to evaluate their impact, by making specific assumptions on the adversary's\ngoal, knowledge of the attacked system, and capabilities of manipulating the\ninput data. We show that an attacker may significantly poison the whole\nclustering process by adding a relatively small percentage of attack samples to\nthe input data, and that some attack samples may be obfuscated to be hidden\nwithin some existing clusters. We present a case study on single-linkage\nhierarchical clustering, and report experiments on clustering of malware\nsamples and handwritten digits.\n"], ["2018-11-24", "http://arxiv.org/abs/1811.09831", "Attention, Please! Adversarial Defense via Attention Rectification and Preservation.", ["Shangxi Wu", " Jitao Sang", " Kaiyuan Xu", " Jiaming Zhang", " Yanfeng Sun", " Liping Jing", " Jian Yu"], "  This study provides a new understanding of the adversarial attack problem by\nexamining the correlation between adversarial attack and visual attention\nchange. In particular, we observed that: (1) images with incomplete attention\nregions are more vulnerable to adversarial attacks; and (2) successful\nadversarial attacks lead to deviated and scattered attention map. Accordingly,\nan attention-based adversarial defense framework is designed to simultaneously\nrectify the attention map for prediction and preserve the attention area\nbetween adversarial and original images. The problem of adding iteratively\nattacked samples is also discussed in the context of visual attention change.\nWe hope the attention-related data analysis and defense solution in this study\nwill shed some light on the mechanism behind the adversarial attack and also\nfacilitate future adversarial defense/attack model design.\n"], ["2018-11-23", "http://arxiv.org/abs/1811.09716", "Robustness via curvature regularization, and vice versa.", ["Seyed-Mohsen Moosavi-Dezfooli", " Alhussein Fawzi", " Jonathan Uesato", " Pascal Frossard"], "  State-of-the-art classifiers have been shown to be largely vulnerable to\nadversarial perturbations. One of the most effective strategies to improve\nrobustness is adversarial training. In this paper, we investigate the effect of\nadversarial training on the geometry of the classification landscape and\ndecision boundaries. We show in particular that adversarial training leads to a\nsignificant decrease in the curvature of the loss surface with respect to\ninputs, leading to a drastically more \"linear\" behaviour of the network. Using\na locally quadratic approximation, we provide theoretical evidence on the\nexistence of a strong relation between large robustness and small curvature. To\nfurther show the importance of reduced curvature for improving the robustness,\nwe propose a new regularizer that directly minimizes curvature of the loss\nsurface, and leads to adversarial robustness that is on par with adversarial\ntraining. Besides being a more efficient and principled alternative to\nadversarial training, the proposed regularizer confirms our claims on the\nimportance of exhibiting quasi-linear behavior in the vicinity of data points\nin order to achieve robustness.\n"], ["2018-11-23", "http://arxiv.org/abs/1811.09600", "Decoupling Direction and Norm for Efficient Gradient-Based L2 Adversarial Attacks and Defenses.", ["J\u00e9r\u00f4me Rony", " Luiz G. Hafemann", " Luiz S. Oliveira", " Ismail Ben Ayed", " Robert Sabourin", " Eric Granger"], "  Research on adversarial examples in computer vision tasks has shown that\nsmall, often imperceptible changes to an image can induce misclassification,\nwhich has security implications for a wide range of image processing systems.\nConsidering $L_2$ norm distortions, the Carlini and Wagner attack is presently\nthe most effective white-box attack in the literature. However, this method is\nslow since it performs a line-search for one of the optimization terms, and\noften requires thousands of iterations. In this paper, an efficient approach is\nproposed to generate gradient-based attacks that induce misclassifications with\nlow $L_2$ norm, by decoupling the direction and the norm of the adversarial\nperturbation that is added to the image. Experiments conducted on the MNIST,\nCIFAR-10 and ImageNet datasets indicate that our attack achieves comparable\nresults to the state-of-the-art (in terms of $L_2$ norm) with considerably\nfewer iterations (as few as 100 iterations), which opens the possibility of\nusing these attacks for adversarial training. Models trained with our attack\nachieve state-of-the-art robustness against white-box gradient-based $L_2$\nattacks on the MNIST and CIFAR-10 datasets, outperforming the Madry defense\nwhen the attacks are limited to a maximum norm.\n"], ["2018-11-22", "http://arxiv.org/abs/1811.09310", "Parametric Noise Injection: Trainable Randomness to Improve Deep Neural Network Robustness against Adversarial Attack.", ["Adnan Siraj Rakin", " Zhezhi He", " Deliang Fan"], "  Recent development in the field of Deep Learning have exposed the underlying\nvulnerability of Deep Neural Network (DNN) against adversarial examples. In\nimage classification, an adversarial example is a carefully modified image that\nis visually imperceptible to the original image but can cause DNN model to\nmisclassify it. Training the network with Gaussian noise is an effective\ntechnique to perform model regularization, thus improving model robustness\nagainst input variation. Inspired by this classical method, we explore to\nutilize the regularization characteristic of noise injection to improve DNN's\nrobustness against adversarial attack. In this work, we propose\nParametric-Noise-Injection (PNI) which involves trainable Gaussian noise\ninjection at each layer on either activation or weights through solving the\nmin-max optimization problem, embedded with adversarial training. These\nparameters are trained explicitly to achieve improved robustness. To the best\nof our knowledge, this is the first work that uses trainable noise injection to\nimprove network robustness against adversarial attacks, rather than manually\nconfiguring the injected noise level through cross-validation. The extensive\nresults show that our proposed PNI technique effectively improves the\nrobustness against a variety of powerful white-box and black-box attacks such\nas PGD, C & W, FGSM, transferable attack and ZOO attack. Last but not the\nleast, PNI method improves both clean- and perturbed-data accuracy in\ncomparison to the state-of-the-art defense methods, which outperforms current\nunbroken PGD defense by 1.1 % and 6.8 % on clean test data and perturbed test\ndata respectively using Resnet-20 architecture.\n"], ["2018-11-22", "http://arxiv.org/abs/1811.09300", "Strength in Numbers: Trading-off Robustness and Computation via Adversarially-Trained Ensembles.", ["Edward Grefenstette", " Robert Stanforth", " Brendan O'Donoghue", " Jonathan Uesato", " Grzegorz Swirszcz", " Pushmeet Kohli"], "  While deep learning has led to remarkable results on a number of challenging\nproblems, researchers have discovered a vulnerability of neural networks in\nadversarial settings, where small but carefully chosen perturbations to the\ninput can make the models produce extremely inaccurate outputs. This makes\nthese models particularly unsuitable for safety-critical application domains\n(e.g. self-driving cars) where robustness is extremely important. Recent work\nhas shown that augmenting training with adversarially generated data provides\nsome degree of robustness against test-time attacks. In this paper we\ninvestigate how this approach scales as we increase the computational budget\ngiven to the defender. We show that increasing the number of parameters in\nadversarially-trained models increases their robustness, and in particular that\nensembling smaller models while adversarially training the entire ensemble as a\nsingle model is a more efficient way of spending said budget than simply using\na larger single model. Crucially, we show that it is the adversarial training\nof the ensemble, rather than the ensembling of adversarially trained models,\nwhich provides robustness.\n"], ["2018-11-22", "http://arxiv.org/abs/1811.09043", "Detecting Adversarial Perturbations Through Spatial Behavior in Activation Spaces.", ["Ziv Katzir", " Yuval Elovici"], "  Neural network based classifiers are still prone to manipulation through\nadversarial perturbations. State of the art attacks can overcome most of the\ndefense or detection mechanisms suggested so far, and adversaries have the\nupper hand in this arms race. Adversarial examples are designed to resemble the\nnormal input from which they were constructed, while triggering an incorrect\nclassification. This basic design goal leads to a characteristic spatial\nbehavior within the context of Activation Spaces, a term coined by the authors\nto refer to the hyperspaces formed by the activation values of the network's\nlayers. Within the output of the first layers of the network, an adversarial\nexample is likely to resemble normal instances of the source class, while in\nthe final layers such examples will diverge towards the adversary's target\nclass. The steps below enable us to leverage this inherent shift from one class\nto another in order to form a novel adversarial example detector. We construct\nEuclidian spaces out of the activation values of each of the deep neural\nnetwork layers. Then, we induce a set of k-nearest neighbor classifiers (k-NN),\none per activation space of each neural network layer, using the\nnon-adversarial examples. We leverage those classifiers to produce a sequence\nof class labels for each nonperturbed input sample and estimate the a priori\nprobability for a class label change between one activation space and another.\nDuring the detection phase we compute a sequence of classification labels for\neach input using the trained classifiers. We then estimate the likelihood of\nthose classification sequences and show that adversarial sequences are far less\nlikely than normal ones. We evaluated our detection method against the state of\nthe art C&W attack method, using two image classification datasets (MNIST,\nCIFAR-10) reaching an AUC 0f 0.95 for the CIFAR-10 dataset.\n"], ["2018-11-21", "http://arxiv.org/abs/1811.09020", "Task-generalizable Adversarial Attack based on Perceptual Metric.", ["Muzammal Naseer", " Salman H. Khan", " Shafin Rahman", " Fatih Porikli"], "  Deep neural networks (DNNs) can be easily fooled by adding human\nimperceptible perturbations to the images. These perturbed images are known as\n`adversarial examples' and pose a serious threat to security and safety\ncritical systems. A litmus test for the strength of adversarial examples is\ntheir transferability across different DNN models in a black box setting (i.e.\nwhen the target model's architecture and parameters are not known to attacker).\nCurrent attack algorithms that seek to enhance adversarial transferability work\non the decision level i.e. generate perturbations that alter the network\ndecisions. This leads to two key limitations: (a) An attack is dependent on the\ntask-specific loss function (e.g. softmax cross-entropy for object recognition)\nand therefore does not generalize beyond its original task. (b) The adversarial\nexamples are specific to the network architecture and demonstrate poor\ntransferability to other network architectures. We propose a novel approach to\ncreate adversarial examples that can broadly fool different networks on\nmultiple tasks. Our approach is based on the following intuition: \"Perpetual\nmetrics based on neural network features are highly generalizable and show\nexcellent performance in measuring and stabilizing input distortions. Therefore\nan ideal attack that creates maximum distortions in the network feature space\nshould realize highly transferable examples\". We report extensive experiments\nto show how adversarial examples generalize across multiple networks for\nclassification, object detection and segmentation tasks.\n"], ["2018-11-21", "http://arxiv.org/abs/1811.09008", "Towards Robust Neural Networks with Lipschitz Continuity.", ["Muhammad Usama", " Dong Eui Chang"], "  Deep neural networks have shown remarkable performance across a wide range of\nvision-based tasks, particularly due to the availability of large-scale\ndatasets for training and better architectures. However, data seen in the real\nworld are often affected by distortions that not accounted for by the training\ndatasets. In this paper, we address the challenge of robustness and stability\nof neural networks and propose a general training method that can be used to\nmake the existing neural network architectures more robust and stable to input\nvisual perturbations while using only available datasets for training. Proposed\ntraining method is convenient to use as it does not require data augmentation\nor changes in the network architecture. We provide theoretical proof as well as\nempirical evidence for the efficiency of the proposed training method by\nperforming experiments with existing neural network architectures and\ndemonstrate that same architecture when trained with the proposed training\nmethod perform better than when trained with conventional training approach in\nthe presence of noisy datasets.\n"], ["2018-11-20", "http://arxiv.org/abs/1811.08577", "How the Softmax Output is Misleading for Evaluating the Strength of Adversarial Examples.", ["Utku Ozbulak", " Neve Wesley De", " Messem Arnout Van"], "  Even before deep learning architectures became the de facto models for\ncomplex computer vision tasks, the softmax function was, given its elegant\nproperties, already used to analyze the predictions of feedforward neural\nnetworks. Nowadays, the output of the softmax function is also commonly used to\nassess the strength of adversarial examples: malicious data points designed to\nfail machine learning models during the testing phase. However, in this paper,\nwe show that it is possible to generate adversarial examples that take\nadvantage of some properties of the softmax function, leading to undesired\noutcomes when interpreting the strength of the adversarial examples at hand.\nSpecifically, we argue that the output of the softmax function is a poor\nindicator when the strength of an adversarial example is analyzed and that this\nindicator can be easily tricked by already existing methods for adversarial\nexample generation.\n"], ["2018-11-20", "http://arxiv.org/abs/1811.08484", "MimicGAN: Corruption-Mimicking for Blind Image Recovery & Adversarial Defense.", ["Rushil Anirudh", " Jayaraman J. Thiagarajan", " Bhavya Kailkhura", " Timo Bremer"], "  Solving inverse problems continues to be a central challenge in computer\nvision. Existing techniques either explicitly construct an inverse mapping\nusing prior knowledge about the corruption, or learn the inverse directly using\na large collection of examples. However, in practice, the nature of corruption\nmay be unknown, and thus it is challenging to regularize the problem of\ninferring a plausible solution. On the other hand, collecting task-specific\ntraining data is tedious for known corruptions and impossible for unknown ones.\nWe present MimicGAN, an unsupervised technique to solve general inverse\nproblems based on image priors in the form of generative adversarial networks\n(GANs). Using a GAN prior, we show that one can reliably recover solutions to\nunderdetermined inverse problems through a surrogate network that learns to\nmimic the corruption at test time. Our system successively estimates the\ncorruption and the clean image without the need for supervisory training, while\noutperforming existing baselines in blind image recovery. We also demonstrate\nthat MimicGAN improves upon recent GAN-based defenses against adversarial\nattacks and represents one of the strongest test-time defenses available today.\n"], ["2018-11-20", "http://arxiv.org/abs/1811.08458", "Intermediate Level Adversarial Attack for Enhanced Transferability.", ["Qian Huang", " Zeqi Gu", " Isay Katsman", " Horace He", " Pian Pawakapan", " Zhiqiu Lin", " Serge Belongie", " Ser-Nam Lim"], "  Neural networks are vulnerable to adversarial examples, malicious inputs\ncrafted to fool trained models. Adversarial examples often exhibit black-box\ntransfer, meaning that adversarial examples for one model can fool another\nmodel. However, adversarial examples may be overfit to exploit the particular\narchitecture and feature representation of a source model, resulting in\nsub-optimal black-box transfer attacks to other target models. This leads us to\nintroduce the Intermediate Level Attack (ILA), which attempts to fine-tune an\nexisting adversarial example for greater black-box transferability by\nincreasing its perturbation on a pre-specified layer of the source model. We\nshow that our method can effectively achieve this goal and that we can decide a\nnearly-optimal layer of the source model to perturb without any knowledge of\nthe target models.\n"], ["2018-11-20", "http://arxiv.org/abs/1811.08080", "Lightweight Lipschitz Margin Training for Certified Defense against Adversarial Examples.", ["Hajime Ono", " Tsubasa Takahashi", " Kazuya Kakizaki"], "  How can we make machine learning provably robust against adversarial examples\nin a scalable way? Since certified defense methods, which ensure\n$\\epsilon$-robust, consume huge resources, they can only achieve small degree\nof robustness in practice. Lipschitz margin training (LMT) is a scalable\ncertified defense, but it can also only achieve small robustness due to\nover-regularization. How can we make certified defense more efficiently? We\npresent LC-LMT, a light weight Lipschitz margin training which solves the above\nproblem. Our method has the following properties; (a) efficient: it can achieve\n$\\epsilon$-robustness at early epoch, and (b) robust: it has a potential to get\nhigher robustness than LMT. In the evaluation, we demonstrate the benefits of\nthe proposed method. LC-LMT can achieve required robustness more than 30 epoch\nearlier than LMT in MNIST, and shows more than 90 $\\%$ accuracy against both\nlegitimate and adversarial inputs.\n"], ["2018-11-20", "http://arxiv.org/abs/1812.02622", "Convolutional Neural Networks with Transformed Input based on Robust Tensor Network Decomposition.", ["Jenn-Bing Ong", " Wee-Keong Ng", " C. -C. Jay Kuo"], "  Tensor network decomposition, originated from quantum physics to model\nentangled many-particle quantum systems, turns out to be a promising\nmathematical technique to efficiently represent and process big data in\nparsimonious manner. In this study, we show that tensor networks can\nsystematically partition structured data, e.g. color images, for distributed\nstorage and communication in privacy-preserving manner. Leveraging the sea of\nbig data and metadata privacy, empirical results show that neighbouring\nsubtensors with implicit information stored in tensor network formats cannot be\nidentified for data reconstruction. This technique complements the existing\nencryption and randomization techniques which store explicit data\nrepresentation at one place and highly susceptible to adversarial attacks such\nas side-channel attacks and de-anonymization. Furthermore, we propose a theory\nfor adversarial examples that mislead convolutional neural networks to\nmisclassification using subspace analysis based on singular value decomposition\n(SVD). The theory is extended to analyze higher-order tensors using\ntensor-train SVD (TT-SVD); it helps to explain the level of susceptibility of\ndifferent datasets to adversarial attacks, the structural similarity of\ndifferent adversarial attacks including global and localized attacks, and the\nefficacy of different adversarial defenses based on input transformation. An\nefficient and adaptive algorithm based on robust TT-SVD is then developed to\ndetect strong and static adversarial attacks.\n"], ["2018-11-19", "http://arxiv.org/abs/1811.07950", "Optimal Transport Classifier: Defending Against Adversarial Attacks by Regularized Deep Embedding.", ["Yao Li", " Martin Renqiang Min", " Wenchao Yu", " Cho-Jui Hsieh", " Thomas C. M. Lee", " Erik Kruus"], "  Recent studies have demonstrated the vulnerability of deep convolutional\nneural networks against adversarial examples. Inspired by the observation that\nthe intrinsic dimension of image data is much smaller than its pixel space\ndimension and the vulnerability of neural networks grows with the input\ndimension, we propose to embed high-dimensional input images into a\nlow-dimensional space to perform classification. However, arbitrarily\nprojecting the input images to a low-dimensional space without regularization\nwill not improve the robustness of deep neural networks. Leveraging optimal\ntransport theory, we propose a new framework, Optimal Transport Classifier\n(OT-Classifier), and derive an objective that minimizes the discrepancy between\nthe distribution of the true label and the distribution of the OT-Classifier\noutput. Experimental results on several benchmark datasets show that, our\nproposed framework achieves state-of-the-art performance against strong\nadversarial attack methods.\n"], ["2018-11-18", "http://arxiv.org/abs/1811.07457", "Generalizable Adversarial Training via Spectral Normalization.", ["Farzan Farnia", " Jesse M. Zhang", " David Tse"], "  Deep neural networks (DNNs) have set benchmarks on a wide array of supervised\nlearning tasks. Trained DNNs, however, often lack robustness to minor\nadversarial perturbations to the input, which undermines their true\npracticality. Recent works have increased the robustness of DNNs by fitting\nnetworks using adversarially-perturbed training samples, but the improved\nperformance can still be far below the performance seen in non-adversarial\nsettings. A significant portion of this gap can be attributed to the decrease\nin generalization performance due to adversarial training. In this work, we\nextend the notion of margin loss to adversarial settings and bound the\ngeneralization error for DNNs trained under several well-known gradient-based\nattack schemes, motivating an effective regularization scheme based on spectral\nnormalization of the DNN's weight matrices. We also provide a\ncomputationally-efficient method for normalizing the spectral norm of\nconvolutional layers with arbitrary stride and padding schemes in deep\nconvolutional networks. We evaluate the power of spectral normalization\nextensively on combinations of datasets, network architectures, and adversarial\ntraining schemes. The code is available at\nhttps://github.com/jessemzhang/dl_spectral_normalization.\n"], ["2018-11-18", "http://arxiv.org/abs/1811.07375", "The Taboo Trap: Behavioural Detection of Adversarial Samples.", ["Ilia Shumailov", " Yiren Zhao", " Robert Mullins", " Ross Anderson"], "  Deep Neural Networks (DNNs) have become a powerful tool for a wide range of\nproblems. Yet recent work has shown an increasing variety of adversarial\nsamples that can fool them. Most existing detection mechanisms impose\nsignificant costs, either by using additional classifiers to spot adversarial\nsamples, or by requiring the DNN to be restructured. In this paper, we\nintroduce a novel defence. We train our DNN so that, as long as it is working\nas intended on the kind of inputs we expect, its behavior is constrained, in\nthat a set of behaviors are taboo. If it is exposed to adversarial samples,\nthey will often cause a taboo behavior, which we can detect. As an analogy, we\ncan imagine that we are teaching our robot good manners; if it's ever rude, we\nknow it's come under some bad influence. This defence mechanism is very simple\nand, although it involves a modest increase in training, has almost zero\ncomputation overhead at runtime -- making it particularly suitable for use in\nembedded systems. Taboos can be both subtle and diverse. Just as humans' choice\nof language can convey a lot of information about location, affiliation, class\nand much else that can be opaque to outsiders but that enables members of the\nsame group to recognise each other, so also taboo choice can encode and hide\ninformation. We can use this to make adversarial attacks much harder. It is a\nwell-established design principle that the security of a system should not\ndepend on the obscurity of its design, but of some variable (the key) which can\ndiffer between implementations and be changed as necessary. We explain how\ntaboos can be used to equip a classifier with just such a key, and to tune the\nkeying mechanism to adversaries of various capabilities. We evaluate the\nperformance of a prototype against a wide range of attacks and show how our\nsimple defense can work well in practice.\n"], ["2018-11-18", "http://arxiv.org/abs/1811.07311", "Regularized adversarial examples for model interpretability.", ["Yoel Shoshan", " Vadim Ratner"], "  As machine learning algorithms continue to improve, there is an increasing\nneed for explaining why a model produces a certain prediction for a certain\ninput. In recent years, several methods for model interpretability have been\ndeveloped, aiming to provide explanation of which subset regions of the model\ninput is the main reason for the model prediction. In parallel, a significant\nresearch community effort is occurring in recent years for developing\nadversarial example generation methods for fooling models, while not altering\nthe true label of the input,as it would have been classified by a human\nannotator. In this paper, we bridge the gap between adversarial example\ngeneration and model interpretability, and introduce a modification to the\nadversarial example generation process which encourages better\ninterpretability. We analyze the proposed method on a public medical imaging\ndataset, both quantitatively and qualitatively, and show that it significantly\noutperforms the leading known alternative method. Our suggested method is\nsimple to implement, and can be easily plugged into most common adversarial\nexample generation frameworks. Additionally, we propose an explanation quality\nmetric - $APE$ - \"Adversarial Perturbative Explanation\", which measures how\nwell an explanation describes model decisions.\n"], ["2018-11-17", "http://arxiv.org/abs/1811.07266", "DeepConsensus: using the consensus of features from multiple layers to attain robust image classification.", ["Yuchen Li", " Safwan Hossain", " Kiarash Jamali", " Frank Rudzicz"], "  We consider a classifier whose test set is exposed to various perturbations\nthat are not present in the training set. These test samples still contain\nenough features to map them to the same class as their unperturbed counterpart.\nCurrent architectures exhibit rapid degradation of accuracy when trained on\nstandard datasets but then used to classify perturbed samples of that data. To\naddress this, we present a novel architecture named DeepConsensus that\nsignificantly improves generalization to these test-time perturbations. Our key\ninsight is that deep neural networks should directly consider summaries of low\nand high level features when making classifications. Existing convolutional\nneural networks can be augmented with DeepConsensus, leading to improved\nresistance against large and small perturbations on MNIST, EMNIST,\nFashionMNIST, CIFAR10 and SVHN datasets.\n"], ["2018-11-17", "http://arxiv.org/abs/1811.07211", "Classifiers Based on Deep Sparse Coding Architectures are Robust to Deep Learning Transferable Examples.", ["Jacob M. Springer", " Charles S. Strauss", " Austin M. Thresher", " Edward Kim", " Garrett T. Kenyon"], "  Although deep learning has shown great success in recent years, researchers\nhave discovered a critical flaw where small, imperceptible changes in the input\nto the system can drastically change the output classification. These attacks\nare exploitable in nearly all of the existing deep learning classification\nframeworks. However, the susceptibility of deep sparse coding models to\nadversarial examples has not been examined. Here, we show that classifiers\nbased on a deep sparse coding model whose classification accuracy is\ncompetitive with a variety of deep neural network models are robust to\nadversarial examples that effectively fool those same deep learning models. We\ndemonstrate both quantitatively and qualitatively that the robustness of deep\nsparse coding models to adversarial examples arises from two key properties.\nFirst, because deep sparse coding models learn general features corresponding\nto generators of the dataset as a whole, rather than highly discriminative\nfeatures for distinguishing specific classes, the resulting classifiers are\nless dependent on idiosyncratic features that might be more easily exploited.\nSecond, because deep sparse coding models utilize fixed point attractor\ndynamics with top-down feedback, it is more difficult to find small changes to\nthe input that drive the resulting representations out of the correct attractor\nbasin.\n"], ["2018-11-17", "http://arxiv.org/abs/1811.07108", "Boosting the Robustness Verification of DNN by Identifying the Achilles's Heel.", ["Chengdong Feng", " Zhenbang Chen", " Weijiang Hong", " Hengbiao Yu", " Wei Dong", " Ji Wang"], "  Deep Neural Network (DNN) is a widely used deep learning technique. How to\nensure the safety of DNN-based system is a critical problem for the research\nand application of DNN. Robustness is an important safety property of DNN.\nHowever, existing work of verifying DNN's robustness is time-consuming and hard\nto scale to large-scale DNNs. In this paper, we propose a boosting method for\nDNN robustness verification, aiming to find counter-examples earlier. Our\nobservation is DNN's different inputs have different possibilities of existing\ncounter-examples around them, and the input with a small difference between the\nlargest output value and the second largest output value tends to be the\nachilles's heel of the DNN. We have implemented our method and applied it on\nReluplex, a state-of-the-art DNN verification tool, and four DNN attacking\nmethods. The results of the extensive experiments on two benchmarks indicate\nthe effectiveness of our boosting method.\n"], ["2018-11-16", "http://arxiv.org/abs/1811.07018", "Protecting Voice Controlled Systems Using Sound Source Identification Based on Acoustic Cues.", ["Yuan Gong", " Christian Poellabauer"], "  Over the last few years, a rapidly increasing number of Internet-of-Things\n(IoT) systems that adopt voice as the primary user input have emerged. These\nsystems have been shown to be vulnerable to various types of voice spoofing\nattacks. Existing defense techniques can usually only protect from a specific\ntype of attack or require an additional authentication step that involves\nanother device. Such defense strategies are either not strong enough or lower\nthe usability of the system. Based on the fact that legitimate voice commands\nshould only come from humans rather than a playback device, we propose a novel\ndefense strategy that is able to detect the sound source of a voice command\nbased on its acoustic features. The proposed defense strategy does not require\nany information other than the voice command itself and can protect a system\nfrom multiple types of spoofing attacks. Our proof-of-concept experiments\nverify the feasibility and effectiveness of this defense strategy.\n"], ["2018-11-16", "http://arxiv.org/abs/1811.06969", "DARCCC: Detecting Adversaries by Reconstruction from Class Conditional Capsules.", ["Nicholas Frosst", " Sara Sabour", " Geoffrey Hinton"], "  We present a simple technique that allows capsule models to detect\nadversarial images. In addition to being trained to classify images, the\ncapsule model is trained to reconstruct the images from the pose parameters and\nidentity of the correct top-level capsule. Adversarial images do not look like\na typical member of the predicted class and they have much larger\nreconstruction errors when the reconstruction is produced from the top-level\ncapsule for that class. We show that setting a threshold on the $l2$ distance\nbetween the input image and its reconstruction from the winning capsule is very\neffective at detecting adversarial images for three different datasets. The\nsame technique works quite well for CNNs that have been trained to reconstruct\nthe image from all or part of the last hidden layer before the softmax. We then\nexplore a stronger, white-box attack that takes the reconstruction error into\naccount. This attack is able to fool our detection technique but in order to\nmake the model change its prediction to another class, the attack must\ntypically make the \"adversarial\" image resemble images of the other class.\n"], ["2018-11-15", "http://arxiv.org/abs/1811.06609", "A Spectral View of Adversarially Robust Features.", ["Shivam Garg", " Vatsal Sharan", " Brian Hu Zhang", " Gregory Valiant"], "  Given the apparent difficulty of learning models that are robust to\nadversarial perturbations, we propose tackling the simpler problem of\ndeveloping adversarially robust features. Specifically, given a dataset and\nmetric of interest, the goal is to return a function (or multiple functions)\nthat 1) is robust to adversarial perturbations, and 2) has significant\nvariation across the datapoints. We establish strong connections between\nadversarially robust features and a natural spectral property of the geometry\nof the dataset and metric of interest. This connection can be leveraged to\nprovide both robust features, and a lower bound on the robustness of any\nfunction that has significant variance across the dataset. Finally, we provide\nempirical evidence that the adversarially robust features given by this\nspectral approach can be fruitfully leveraged to learn a robust (and accurate)\nmodel.\n"], ["2018-11-15", "http://arxiv.org/abs/1811.06539", "A note on hyperparameters in black-box adversarial examples.", ["Jamie Hayes"], "  Since Biggio et al. (2013) and Szegedy et al. (2013) first drew attention to\nadversarial examples, there has been a flood of research into defending and\nattacking machine learning models. However, almost all proposed attacks assume\nwhite-box access to a model. In other words, the attacker is assumed to have\nperfect knowledge of the models weights and architecture. With this insider\nknowledge, a white-box attack can leverage gradient information to craft\nadversarial examples. Black-box attacks assume no knowledge of the model\nweights or architecture. These attacks craft adversarial examples using\ninformation only contained in the logits or hard classification label. Here, we\nassume the attacker can use the logits in order to find an adversarial example.\nEmpirically, we show that 2-sided stochastic gradient estimation techniques are\nnot sensitive to scaling parameters, and can be used to mount powerful\nblack-box attacks requiring relatively few model queries.\n"], ["2018-11-15", "http://arxiv.org/abs/1811.06492", "Mathematical Analysis of Adversarial Attacks.", ["Zehao Dou", " Stanley J. Osher", " Bao Wang"], "  In this paper, we analyze efficacy of the fast gradient sign method (FGSM)\nand the Carlini-Wagner's L2 (CW-L2) attack. We prove that, within a certain\nregime, the untargeted FGSM can fool any convolutional neural nets (CNNs) with\nReLU activation; the targeted FGSM can mislead any CNNs with ReLU activation to\nclassify any given image into any prescribed class. For a special two-layer\nneural network: a linear layer followed by the softmax output activation, we\nshow that the CW-L2 attack increases the ratio of the classification\nprobability between the target and ground truth classes. Moreover, we provide\nnumerical results to verify all our theoretical results.\n"], ["2018-11-15", "http://arxiv.org/abs/1811.06418", "Adversarial Examples from Cryptographic Pseudo-Random Generators.", ["S\u00e9bastien Bubeck", " Yin Tat Lee", " Eric Price", " Ilya Razenshteyn"], "  In our recent work (Bubeck, Price, Razenshteyn, arXiv:1805.10204) we argued\nthat adversarial examples in machine learning might be due to an inherent\ncomputational hardness of the problem. More precisely, we constructed a binary\nclassification task for which (i) a robust classifier exists; yet no\nnon-trivial accuracy can be obtained with an efficient algorithm in (ii) the\nstatistical query model. In the present paper we significantly strengthen both\n(i) and (ii): we now construct a task which admits (i') a maximally robust\nclassifier (that is it can tolerate perturbations of size comparable to the\nsize of the examples themselves); and moreover we prove computational hardness\nof learning this task under (ii') a standard cryptographic assumption.\n"], ["2018-11-14", "http://arxiv.org/abs/1811.06029", "Verification of Recurrent Neural Networks Through Rule Extraction.", ["Qinglong Wang", " Kaixuan Zhang", " Xue Liu", " C. Lee Giles"], "  The verification problem for neural networks is verifying whether a neural\nnetwork will suffer from adversarial samples, or approximating the maximal\nallowed scale of adversarial perturbation that can be endured. While most prior\nwork contributes to verifying feed-forward networks, little has been explored\nfor verifying recurrent networks. This is due to the existence of a more\nrigorous constraint on the perturbation space for sequential data, and the lack\nof a proper metric for measuring the perturbation. In this work, we address\nthese challenges by proposing a metric which measures the distance between\nstrings, and use deterministic finite automata (DFA) to represent a rigorous\noracle which examines if the generated adversarial samples violate certain\nconstraints on a perturbation. More specifically, we empirically show that\ncertain recurrent networks allow relatively stable DFA extraction. As such,\nDFAs extracted from these recurrent networks can serve as a surrogate oracle\nfor when the ground truth DFA is unknown. We apply our verification mechanism\nto several widely used recurrent networks on a set of the Tomita grammars. The\nresults demonstrate that only a few models remain robust against adversarial\nsamples. In addition, we show that for grammars with different levels of\ncomplexity, there is also a difference in the difficulty of robust learning of\nthese grammars.\n"], ["2018-11-14", "http://arxiv.org/abs/1811.05808", "Robustness of spectral methods for community detection.", ["Ludovic Stephan", " Laurent Massouli\u00e9"], "  The present work is concerned with community detection. Specifically, we\nconsider a random graph drawn according to the stochastic block model~: its\nvertex set is partitioned into blocks, or communities, and edges are placed\nrandomly and independently of each other with probability depending only on the\ncommunities of their two endpoints. In this context, our aim is to recover the\ncommunity labels better than by random guess, based only on the observation of\nthe graph.\n  In the sparse case, where edge probabilities are in $O(1/n)$, we introduce a\nnew spectral method based on the distance matrix $D^{(l)}$, where $D^{(l)}_{ij}\n= 1$ iff the graph distance between $i$ and $j$, noted $d(i, j)$ is equal to\n$\\ell$. We show that when $\\ell \\sim c\\log(n)$ for carefully chosen $c$, the\neigenvectors associated to the largest eigenvalues of $D^{(l)}$ provide enough\ninformation to perform non-trivial community recovery with high probability,\nprovided we are above the so-called Kesten-Stigum threshold. This yields an\nefficient algorithm for community detection, since computation of the matrix\n$D^{(l)}$ can be done in $O(n^{1+\\kappa})$ operations for a small constant\n$\\kappa$.\n  We then study the sensitivity of the eigendecomposition of $D^{(l)}$ when we\nallow an adversarial perturbation of the edges of $G$. We show that when the\nconsidered perturbation does not affect more than $O(n^\\varepsilon)$ vertices\nfor some small $\\varepsilon > 0$, the highest eigenvalues and their\ncorresponding eigenvectors incur negligible perturbations, which allows us to\nstill perform efficient recovery.\n"], ["2018-11-13", "http://arxiv.org/abs/1811.05521", "Deep Q learning for fooling neural networks.", ["Mandar Kulkarni"], "  Deep learning models are vulnerable to external attacks. In this paper, we\npropose a Reinforcement Learning (RL) based approach to generate adversarial\nexamples for the pre-trained (target) models. We assume a semi black-box\nsetting where the only access an adversary has to the target model is the class\nprobabilities obtained for the input queries. We train a Deep Q Network (DQN)\nagent which, with experience, learns to attack only a small portion of image\npixels to generate non-targeted adversarial images. Initially, an agent\nexplores an environment by sequentially modifying random sets of image pixels\nand observes its effect on the class probabilities. At the end of an episode,\nit receives a positive (negative) reward if it succeeds (fails) to alter the\nlabel of the image. Experimental results with MNIST, CIFAR-10 and Imagenet\ndatasets demonstrate that our RL framework is able to learn an effective attack\npolicy.\n"], ["2018-11-08", "http://arxiv.org/abs/1811.03733", "Universal Decision-Based Black-Box Perturbations: Breaking Security-Through-Obscurity Defenses.", ["Thomas A. Hogan", " Bhavya Kailkhura"], "  We study the problem of finding a universal (image-agnostic) perturbation to\nfool machine learning (ML) classifiers (e.g., neural nets, decision tress) in\nthe hard-label black-box setting. Recent work in adversarial ML in the\nwhite-box setting (model parameters are known) has shown that many\nstate-of-the-art image classifiers are vulnerable to universal adversarial\nperturbations: a fixed human-imperceptible perturbation that, when added to any\nimage, causes it to be misclassified with high probability Kurakin et al.\n[2016], Szegedy et al. [2013], Chen et al. [2017a], Carlini and Wagner [2017].\nThis paper considers a more practical and challenging problem of finding such\nuniversal perturbations in an obscure (or black-box) setting. More\nspecifically, we use zeroth order optimization algorithms to find such a\nuniversal adversarial perturbation when no model information is revealed-except\nthat the attacker can make queries to probe the classifier. We further relax\nthe assumption that the output of a query is continuous valued confidence\nscores for all the classes and consider the case where the output is a\nhard-label decision. Surprisingly, we found that even in these extremely\nobscure regimes, state-of-the-art ML classifiers can be fooled with a very high\nprobability just by adding a single human-imperceptible image perturbation to\nany natural image. The surprising existence of universal perturbations in a\nhard-label black-box setting raises serious security concerns with the\nexistence of a universal noise vector that adversaries can possibly exploit to\nbreak a classifier on most natural images.\n"], ["2018-11-08", "http://arxiv.org/abs/1811.03685", "New CleverHans Feature: Better Adversarial Robustness Evaluations with Attack Bundling.", ["Ian Goodfellow"], "  This technical report describes a new feature of the CleverHans library\ncalled \"attack bundling\". Many papers about adversarial examples present lists\nof error rates corresponding to different attack algorithms. A common approach\nis to take the maximum across this list and compare defenses against that error\nrate. We argue that a better approach is to use attack bundling: the max should\nbe taken across many examples at the level of individual examples, then the\nerror rate should be calculated by averaging after this maximization operation.\nReporting the bundled attacker error rate provides a lower bound on the true\nworst-case error rate. The traditional approach of reporting the maximum error\nrate across attacks can underestimate the true worst-case error rate by an\namount approaching 100\\% as the number of attacks approaches infinity. Attack\nbundling can be used with different prioritization schemes to optimize\nquantities such as error rate on adversarial examples, perturbation size needed\nto cause misclassification, or failure rate when using a specific confidence\nthreshold.\n"], ["2018-11-08", "http://arxiv.org/abs/1811.03531", "A Geometric Perspective on the Transferability of Adversarial Directions.", ["Zachary Charles", " Harrison Rosenberg", " Dimitris Papailiopoulos"], "  State-of-the-art machine learning models frequently misclassify inputs that\nhave been perturbed in an adversarial manner. Adversarial perturbations\ngenerated for a given input and a specific classifier often seem to be\neffective on other inputs and even different classifiers. In other words,\nadversarial perturbations seem to transfer between different inputs, models,\nand even different neural network architectures. In this work, we show that in\nthe context of linear classifiers and two-layer ReLU networks, there provably\nexist directions that give rise to adversarial perturbations for many\nclassifiers and data points simultaneously. We show that these \"transferable\nadversarial directions\" are guaranteed to exist for linear separators of a\ngiven set, and will exist with high probability for linear classifiers trained\non independent sets drawn from the same distribution. We extend our results to\nlarge classes of two-layer ReLU networks. We further show that adversarial\ndirections for ReLU networks transfer to linear classifiers while the reverse\nneed not hold, suggesting that adversarial perturbations for more complex\nmodels are more likely to transfer to other classifiers. We validate our\nfindings empirically, even for deeper ReLU networks.\n"], ["2018-11-07", "http://arxiv.org/abs/1811.03456", "CAAD 2018: Iterative Ensemble Adversarial Attack.", ["Jiayang Liu", " Weiming Zhang", " Nenghai Yu"], "  Deep Neural Networks (DNNs) have recently led to significant improvements in\nmany fields. However, DNNs are vulnerable to adversarial examples which are\nsamples with imperceptible perturbations while dramatically misleading the\nDNNs. Adversarial attacks can be used to evaluate the robustness of deep\nlearning models before they are deployed. Unfortunately, most of existing\nadversarial attacks can only fool a black-box model with a low success rate. To\nimprove the success rates for black-box adversarial attacks, we proposed an\niterated adversarial attack against an ensemble of image classifiers. With this\nmethod, we won the 5th place in CAAD 2018 Targeted Adversarial Attack\ncompetition.\n"], ["2018-11-07", "http://arxiv.org/abs/1811.03194", "AdVersarial: Perceptual Ad Blocking meets Adversarial Machine Learning.", ["Florian Tram\u00e8r", " Pascal Dupr\u00e9", " Gili Rusak", " Giancarlo Pellegrino", " Dan Boneh"], "  Perceptual ad-blocking is a novel approach that detects online advertisements\nbased on their visual content. Compared to traditional filter lists, the use of\nperceptual signals is believed to be less prone to an arms race with web\npublishers and ad networks. We demonstrate that this may not be the case. We\ndescribe attacks on multiple perceptual ad-blocking techniques, and unveil a\nnew arms race that likely disfavors ad-blockers. Unexpectedly, perceptual\nad-blocking can also introduce new vulnerabilities that let an attacker bypass\nweb security boundaries and mount DDoS attacks.\n  We first analyze the design space of perceptual ad-blockers and present a\nunified architecture that incorporates prior academic and commercial work. We\nthen explore a variety of attacks on the ad-blocker's detection pipeline, that\nenable publishers or ad networks to evade or detect ad-blocking, and at times\neven abuse its high privilege level to bypass web security boundaries.\n  On one hand, we show that perceptual ad-blocking must visually classify\nrendered web content to escape an arms race centered on obfuscation of page\nmarkup. On the other, we present a concrete set of attacks on visual\nad-blockers by constructing adversarial examples in a real web page context.\nFor seven ad-detectors, we create perturbed ads, ad-disclosure logos, and\nnative web content that misleads perceptual ad-blocking with 100% success\nrates. In one of our attacks, we demonstrate how a malicious user can upload\nadversarial content, such as a perturbed image in a Facebook post, that fools\nthe ad-blocker into removing another users' non-ad content.\n  Moving beyond the Web and visual domain, we also build adversarial examples\nfor AdblockRadio, an open source radio client that uses machine learning to\ndetects ads in raw audio streams.\n"], ["2018-11-06", "http://arxiv.org/abs/1811.02625", "MixTrain: Scalable Training of Verifiably Robust Neural Networks.", ["Shiqi Wang", " Yizheng Chen", " Ahmed Abdou", " Suman Jana"], "  Making neural networks robust against adversarial inputs has resulted in an\narms race between new defenses and attacks. The most promising defenses,\nadversarially robust training and verifiably robust training, have limitations\nthat restrict their practical applications. The adversarially robust training\nonly makes the networks robust against a subclass of attackers and we reveal\nsuch weaknesses by developing a new attack based on interval gradients. By\ncontrast, verifiably robust training provides protection against any L-p\nnorm-bounded attacker but incurs orders of magnitude more computational and\nmemory overhead than adversarially robust training.\n  We propose two novel techniques, stochastic robust approximation and dynamic\nmixed training, to drastically improve the efficiency of verifiably robust\ntraining without sacrificing verified robustness. We leverage two critical\ninsights: (1) instead of over the entire training set, sound\nover-approximations over randomly subsampled training data points are\nsufficient for efficiently guiding the robust training process; and (2) We\nobserve that the test accuracy and verifiable robustness often conflict after\ncertain training epochs. Therefore, we use a dynamic loss function to\nadaptively balance them for each epoch.\n  We designed and implemented our techniques as part of MixTrain and evaluated\nit on six networks trained on three popular datasets including MNIST, CIFAR,\nand ImageNet-200. Our evaluations show that MixTrain can achieve up to $95.2\\%$\nverified robust accuracy against $L_\\infty$ norm-bounded attackers while taking\n$15$ and $3$ times less training time than state-of-the-art verifiably robust\ntraining and adversarially robust training schemes, respectively. Furthermore,\nMixTrain easily scales to larger networks like the one trained on ImageNet-200,\nsignificantly outperforming the existing verifiably robust training methods.\n"], ["2018-11-06", "http://arxiv.org/abs/1811.02248", "SparseFool: a few pixels make a big difference.", ["Apostolos Modas", " Seyed-Mohsen Moosavi-Dezfooli", " Pascal Frossard"], "  Deep Neural Networks have achieved extraordinary results on image\nclassification tasks, but have been shown to be vulnerable to attacks with\ncarefully crafted perturbations of the input data. Although most attacks\nusually change values of many image's pixels, it has been shown that deep\nnetworks are also vulnerable to sparse alterations of the input. However, no\ncomputationally efficient method has been proposed to compute sparse\nperturbations. In this paper, we exploit the low mean curvature of the decision\nboundary, and propose SparseFool, a geometry inspired sparse attack that\ncontrols the sparsity of the perturbations. Extensive evaluations show that our\napproach computes sparse perturbations very fast, and scales efficiently to\nhigh dimensional data. We further analyze the transferability and the visual\neffects of the perturbations, and show the existence of shared semantic\ninformation across the images and the networks. Finally, we show that\nadversarial training can only slightly improve the robustness against sparse\nadditive perturbations computed with SparseFool.\n"], ["2018-11-05", "http://arxiv.org/abs/1811.01811", "Active Deep Learning Attacks under Strict Rate Limitations for Online API Calls.", ["Yi Shi", " Yalin E. Sagduyu", " Kemal Davaslioglu", " Jason H. Li"], "  Machine learning has been applied to a broad range of applications and some\nof them are available online as application programming interfaces (APIs) with\neither free (trial) or paid subscriptions. In this paper, we study adversarial\nmachine learning in the form of back-box attacks on online classifier APIs. We\nstart with a deep learning based exploratory (inference) attack, which aims to\nbuild a classifier that can provide similar classification results (labels) as\nthe target classifier. To minimize the difference between the labels returned\nby the inferred classifier and the target classifier, we show that the deep\nlearning based exploratory attack requires a large number of labeled training\ndata samples. These labels can be collected by calling the online API, but\nusually there is some strict rate limitation on the number of allowed API\ncalls. To mitigate the impact of limited training data, we develop an active\nlearning approach that first builds a classifier based on a small number of API\ncalls and uses this classifier to select samples to further collect their\nlabels. Then, a new classifier is built using more training data samples. This\nupdating process can be repeated multiple times. We show that this active\nlearning approach can build an adversarial classifier with a small statistical\ndifference from the target classifier using only a limited number of training\ndata samples. We further consider evasion and causative (poisoning) attacks\nbased on the inferred classifier that is built by the exploratory attack.\nEvasion attack determines samples that the target classifier is likely to\nmisclassify, whereas causative attack provides erroneous training data samples\nto reduce the reliability of the re-trained classifier. The success of these\nattacks show that adversarial machine learning emerges as a feasible threat in\nthe realistic case with limited training data.\n"], ["2018-11-05", "http://arxiv.org/abs/1811.01749", "FUNN: Flexible Unsupervised Neural Network.", ["David Vigouroux", " Sylvain Picard"], "  Deep neural networks have demonstrated high accuracy in image classification\ntasks. However, they were shown to be weak against adversarial examples: a\nsmall perturbation in the image which changes the classification output\ndramatically. In recent years, several defenses have been proposed to solve\nthis issue in supervised classification tasks. We propose a method to obtain\nrobust features in unsupervised learning tasks against adversarial attacks. Our\nmethod differs from existing solutions by directly learning the robust features\nwithout the need to project the adversarial examples in the original examples\ndistribution space. A first auto-encoder A1 is in charge of perturbing the\ninput image to fool another auto-encoder A2 which is in charge of regenerating\nthe original image. A1 tries to find the less perturbed image under the\nconstraint that the error in the output of A2 should be at least equal to a\nthreshold. Thanks to this training, the encoder of A2 will be robust against\nadversarial attacks and could be used in different tasks like classification.\nUsing state-of-art network architectures, we demonstrate the robustness of the\nfeatures obtained thanks to this method in classification tasks.\n"], ["2018-11-05", "http://arxiv.org/abs/1811.01629", "On the Transferability of Adversarial Examples Against CNN-Based Image Forensics.", ["Mauro Barni", " Kassem Kallas", " Ehsan Nowroozi", " Benedetta Tondi"], "  Recent studies have shown that Convolutional Neural Networks (CNN) are\nrelatively easy to attack through the generation of so-called adversarial\nexamples. Such vulnerability also affects CNN-based image forensic tools.\nResearch in deep learning has shown that adversarial examples exhibit a certain\ndegree of transferability, i.e., they maintain part of their effectiveness even\nagainst CNN models other than the one targeted by the attack. This is a very\nstrong property undermining the usability of CNN's in security-oriented\napplications. In this paper, we investigate if attack transferability also\nholds in image forensics applications. With specific reference to the case of\nmanipulation detection, we analyse the results of several experiments\nconsidering different sources of mismatch between the CNN used to build the\nadversarial examples and the one adopted by the forensic analyst. The analysis\nranges from cases in which the mismatch involves only the training dataset, to\ncases in which the attacker and the forensic analyst adopt different\narchitectures. The results of our experiments show that, in the majority of the\ncases, the attacks are not transferable, thus easing the design of proper\ncountermeasures at least when the attacker does not have a perfect knowledge of\nthe target detector.\n"], ["2018-11-04", "http://arxiv.org/abs/1811.01444", "FAdeML: Understanding the Impact of Pre-Processing Noise Filtering on Adversarial Machine Learning.", ["Faiq Khalid", " Muhammmad Abdullah Hanif", " Semeen Rehman", " Junaid Qadir", " Muhammad Shafique"], "  Deep neural networks (DNN)-based machine learning (ML) algorithms have\nrecently emerged as the leading ML paradigm particularly for the task of\nclassification due to their superior capability of learning efficiently from\nlarge datasets. The discovery of a number of well-known attacks such as dataset\npoisoning, adversarial examples, and network manipulation (through the addition\nof malicious nodes) has, however, put the spotlight squarely on the lack of\nsecurity in DNN-based ML systems. In particular, malicious actors can use these\nwell-known attacks to cause random/targeted misclassification, or cause a\nchange in the prediction confidence, by only slightly but systematically\nmanipulating the environmental parameters, inference data, or the data\nacquisition block. Most of the prior adversarial attacks have, however, not\naccounted for the pre-processing noise filters commonly integrated with the\nML-inference module. Our contribution in this work is to show that this is a\nmajor omission since these noise filters can render ineffective the majority of\nthe existing attacks, which rely essentially on introducing adversarial noise.\nApart from this, we also extend the state of the art by proposing a novel\npre-processing noise Filter-aware Adversarial ML attack called FAdeML. To\ndemonstrate the effectiveness of the proposed methodology, we generate an\nadversarial attack image by exploiting the \"VGGNet\" DNN trained for the \"German\nTraffic Sign Recognition Benchmarks (GTSRB\" dataset, which despite having no\nvisual noise, can cause a classifier to misclassify even in the presence of\npre-processing noise filters.\n"], ["2018-11-04", "http://arxiv.org/abs/1811.01443", "SSCNets: A Selective Sobel Convolution-based Technique to Enhance the Robustness of Deep Neural Networks against Security Attacks.", ["Hammad Tariq", " Hassan Ali", " Muhammad Abdullah Hanif", " Faiq Khalid", " Semeen Rehman", " Rehan Ahmed", " Muhammad Shafique"], "  Recent studies have shown that slight perturbations in the input data can\nsignificantly affect the robustness of Deep Neural Networks (DNNs), leading to\nmisclassification and confidence reduction. In this paper, we introduce a novel\ntechnique based on the Selective Sobel Convolution (SSC) operation in the\ntraining loop, that increases the robustness of a given DNN by allowing it to\nlearn important edges in the input in a controlled fashion. This is achieved by\nintroducing a trainable parameter, which acts as a threshold for eliminating\nthe weaker edges. We validate our technique against the attacks of Cleverhans\nlibrary on Convolutional DNNs against adversarial attacks. Our experimental\nresults on the MNIST and CIFAR10 datasets illustrate that this controlled\nlearning considerably increases the accuracy of the DNNs by 1.53% even when\nsubjected to adversarial attacks.\n"], ["2018-11-04", "http://arxiv.org/abs/1811.01437", "QuSecNets: Quantization-based Defense Mechanism for Securing Deep Neural Network against Adversarial Attacks.", ["Hassan Ali", " Hammad Tariq", " Muhammad Abdullah Hanif", " Faiq Khalid", " Semeen Rehman", " Rehan Ahmed", " Muhammad Shafique"], "  Deep Neural Networks (DNNs) have recently been shown vulnerable to\nadversarial attacks in which the input examples are perturbed to fool these\nDNNs towards confidence reduction and (targeted or random) misclassification.\nIn this paper, we demonstrate that how an efficient quantization technique can\nbe leveraged to increase the robustness of a given DNN against adversarial\nattacks. We present two quantization-based defense mechanisms, namely Constant\nQuantization (CQ) and Variable Quantization (VQ), applied at the input to\nincrease the robustness of DNNs. In CQ, the intensity of the input pixel is\nquantized according to the number of quantization levels. While in VQ, the\nquantization levels are recursively updated during the training phase, thereby\nproviding a stronger defense mechanism. We apply our techniques on the\nConvolutional Neural Networks (CNNs, a particular type of DNN which is heavily\nused in vision-based applications) against adversarial attacks from the\nopen-source Cleverhans library. Our experimental results show 1%-5% increase in\nthe adversarial accuracy for MNIST and 0%-2.4% increase in the adversarial\naccuracy for CIFAR10.\n"], ["2018-11-03", "http://arxiv.org/abs/1811.01302", "Adversarial Gain.", ["Peter Henderson", " Koustuv Sinha", " Rosemary Nan Ke", " Joelle Pineau"], "  Adversarial examples can be defined as inputs to a model which induce a\nmistake - where the model output is different than that of an oracle, perhaps\nin surprising or malicious ways. Original models of adversarial attacks are\nprimarily studied in the context of classification and computer vision tasks.\nWhile several attacks have been proposed in natural language processing (NLP)\nsettings, they often vary in defining the parameters of an attack and what a\nsuccessful attack would look like. The goal of this work is to propose a\nunifying model of adversarial examples suitable for NLP tasks in both\ngenerative and classification settings. We define the notion of adversarial\ngain: based in control theory, it is a measure of the change in the output of a\nsystem relative to the perturbation of the input (caused by the so-called\nadversary) presented to the learner. This definition, as we show, can be used\nunder different feature spaces and distance conditions to determine attack or\ndefense effectiveness across different intuitive manifolds. This notion of\nadversarial gain not only provides a useful way for evaluating adversaries and\ndefenses, but can act as a building block for future work in robustness under\nadversaries due to its rooted nature in stability and manifold theory.\n"], ["2018-11-03", "http://arxiv.org/abs/1811.01225", "CAAD 2018: Powerful None-Access Black-Box Attack Based on Adversarial Transformation Network.", ["Xiaoyi Dong", " Weiming Zhang", " Nenghai Yu"], "  In this paper, we propose an improvement of Adversarial Transformation\nNetworks(ATN) to generate adversarial examples, which can fool white-box models\nand black-box models with a state of the art performance and won the 2rd place\nin the non-target task in CAAD 2018.\n"], ["2018-11-03", "http://arxiv.org/abs/1811.01213", "Learning to Defense by Learning to Attack.", ["Haoming Jiang", " Zhehui Chen", " Yuyang Shi", " Bo Dai", " Tuo Zhao"], "  Adversarial training provides a principled approach for training robust\nneural networks. From an optimization perspective, adversarial training is\nessentially solving a minimax robust optimization problem. The outer\nminimization is trying to learn a robust classifier, while the inner\nmaximization is trying to generate adversarial samples. Unfortunately, such a\nminimax problem is challenging to solve due to the lack of convex-concave\nstructure. This work proposes a new adversarial training method based on a\ngeneral learning-to-learn framework. Specifically, instead of applying the\nexisting hand-design algorithms for the inner problem, we learn an optimizer,\nwhich is parametrized as a convolutional neural network. At the same time, a\nrobust classifier is learned to defend the adversarial attack generated by the\nlearned optimizer. Our experiments demonstrate that our proposed method\nsignificantly outperforms existing adversarial training methods on CIFAR-10 and\nCIFAR-100 datasets.\n"], ["2018-11-03", "http://arxiv.org/abs/1811.01312", "Adversarial Black-Box Attacks on Automatic Speech Recognition Systems using Multi-Objective Evolutionary Optimization.", ["Shreya Khare", " Rahul Aralikatte", " Senthil Mani"], "  Fooling deep neural networks with adversarial input have exposed a\nsignificant vulnerability in the current state-of-the-art systems in multiple\ndomains. Both black-box and white-box approaches have been used to either\nreplicate the model itself or to craft examples which cause the model to fail.\nIn this work, we propose a framework which uses multi-objective evolutionary\noptimization to perform both targeted and un-targeted black-box attacks on\nAutomatic Speech Recognition (ASR) systems. We apply this framework on two ASR\nsystems: Deepspeech and Kaldi-ASR, which increases the Word Error Rates (WER)\nof these systems by upto 980%, indicating the potency of our approach. During\nboth un-targeted and targeted attacks, the adversarial samples maintain a high\nacoustic similarity of 0.98 and 0.97 with the original audio.\n"], ["2018-11-02", "http://arxiv.org/abs/1811.01134", "A Marauder's Map of Security and Privacy in Machine Learning.", ["Nicolas Papernot"], "  There is growing recognition that machine learning (ML) exposes new security\nand privacy vulnerabilities in software systems, yet the technical community's\nunderstanding of the nature and extent of these vulnerabilities remains limited\nbut expanding. In this talk, we explore the threat model space of ML algorithms\nthrough the lens of Saltzer and Schroeder's principles for the design of secure\ncomputer systems. This characterization of the threat space prompts an\ninvestigation of current and future research directions. We structure our\ndiscussion around three of these directions, which we believe are likely to\nlead to significant progress. The first encompasses a spectrum of approaches to\nverification and admission control, which is a prerequisite to enable fail-safe\ndefaults in machine learning systems. The second seeks to design mechanisms for\nassembling reliable records of compromise that would help understand the degree\nto which vulnerabilities are exploited by adversaries, as well as favor\npsychological acceptability of machine learning applications. The third pursues\nformal frameworks for security and privacy in machine learning, which we argue\nshould strive to align machine learning goals such as generalization with\nsecurity and privacy desiderata like robustness or privacy. Key insights\nresulting from these three directions pursued both in the ML and security\ncommunities are identified and the effectiveness of approaches are related to\nstructural elements of ML algorithms and the data used to train them. We\nconclude by systematizing best practices in our community.\n"], ["2018-11-02", "http://arxiv.org/abs/1811.01057", "Semidefinite relaxations for certifying robustness to adversarial examples.", ["Aditi Raghunathan", " Jacob Steinhardt", " Percy Liang"], "  Despite their impressive performance on diverse tasks, neural networks fail\ncatastrophically in the presence of adversarial inputs---imperceptibly but\nadversarially perturbed versions of natural inputs. We have witnessed an arms\nrace between defenders who attempt to train robust networks and attackers who\ntry to construct adversarial examples. One promise of ending the arms race is\ndeveloping certified defenses, ones which are provably robust against all\nattackers in some family. These certified defenses are based on convex\nrelaxations which construct an upper bound on the worst case loss over all\nattackers in the family. Previous relaxations are loose on networks that are\nnot trained against the respective relaxation. In this paper, we propose a new\nsemidefinite relaxation for certifying robustness that applies to arbitrary\nReLU networks. We show that our proposed relaxation is tighter than previous\nrelaxations and produces meaningful robustness guarantees on three different\n\"foreign networks\" whose training objectives are agnostic to our proposed\nrelaxation.\n"], ["2018-11-02", "http://arxiv.org/abs/1811.01031", "TrISec: Training Data-Unaware Imperceptible Security Attacks on Deep Neural Networks.", ["Faiq Khalid", " Muhammad Abdullah Hanif", " Semeen Rehman", " Rehan Ahmed", " Muhammad Shafique"], "  Most of the data manipulation attacks on deep neural networks (DNNs) during\nthe training stage introduce a perceptible noise that can be catered by\npreprocessing during inference. Therefore, data poisoning attacks during\ninference (e.g., adversarial attacks) are becoming more popular. However, they\ndo not consider the imperceptibility factor in their optimization algorithms,\nand can be detected by correlation and structural testing. Therefore, in this\npaper, we propose a novel methodology which automatically generates\nimperceptible attack images by using the back-propagation algorithm on\npre-trained DNNs. We present a case study on traffic sign detection using the\nVGGNet and the German Traffic Sign Recognition Benchmarks dataset in an\nautonomous driving use case. Our results demonstrate that the generated attack\nimages successfully perform misclassification while remaining imperceptible in\nboth `subjective' and `objective' quality tests\n"], ["2018-11-02", "http://arxiv.org/abs/1811.00866", "Efficient Neural Network Robustness Certification with General Activation Functions.", ["Huan Zhang", " Tsui-Wei Weng", " Pin-Yu Chen", " Cho-Jui Hsieh", " Luca Daniel"], "  Finding minimum distortion of adversarial examples and thus certifying\nrobustness in neural network classifiers for given data points is known to be a\nchallenging problem. Nevertheless, recently it has been shown to be possible to\ngive a non-trivial certified lower bound of minimum adversarial distortion, and\nsome recent progress has been made towards this direction by exploiting the\npiece-wise linear nature of ReLU activations. However, a generic robustness\ncertification for general activation functions still remains largely\nunexplored. To address this issue, in this paper we introduce CROWN, a general\nframework to certify robustness of neural networks with general activation\nfunctions for given input data points. The novelty in our algorithm consists of\nbounding a given activation function with linear and quadratic functions, hence\nallowing it to tackle general activation functions including but not limited to\nfour popular choices: ReLU, tanh, sigmoid and arctan. In addition, we\nfacilitate the search for a tighter certified lower bound by adaptively\nselecting appropriate surrogates for each neuron activation. Experimental\nresults show that CROWN on ReLU networks can notably improve the certified\nlower bounds compared to the current state-of-the-art algorithm Fast-Lin, while\nhaving comparable computational efficiency. Furthermore, CROWN also\ndemonstrates its effectiveness and flexibility on networks with general\nactivation functions, including tanh, sigmoid and arctan.\n"], ["2018-11-02", "http://arxiv.org/abs/1811.00830", "Towards Adversarial Malware Detection: Lessons Learned from PDF-based Attacks.", ["Davide Maiorca", " Battista Biggio", " Giorgio Giacinto"], "  Malware still constitutes a major threat in the cybersecurity landscape, also\ndue to the widespread use of infection vectors such as documents. These\ninfection vectors hide embedded malicious code to the victim users,\nfacilitating the use of social engineering techniques to infect their machines.\nResearch showed that machine-learning algorithms provide effective detection\nmechanisms against such threats, but the existence of an arms race in\nadversarial settings has recently challenged such systems. In this work, we\nfocus on malware embedded in PDF files as a representative case of such an arms\nrace. We start by providing a comprehensive taxonomy of the different\napproaches used to generate PDF malware, and of the corresponding\nlearning-based detection systems. We then categorize threats specifically\ntargeted against learning-based PDF malware detectors, using a well-established\nframework in the field of adversarial machine learning. This framework allows\nus to categorize known vulnerabilities of learning-based PDF malware detectors\nand to identify novel attacks that may threaten such systems, along with the\npotential defense mechanisms that can mitigate the impact of such threats. We\nconclude the paper by discussing how such findings highlight promising research\ndirections towards tackling the more general challenge of designing robust\nmalware detectors in adversarial settings.\n"], ["2018-11-01", "http://arxiv.org/abs/1811.00621", "Improving Adversarial Robustness by Encouraging Discriminative Features.", ["Chirag Agarwal", " Anh Nguyen", " Dan Schonfeld"], "  Deep neural networks (DNNs) have achieved state-of-the-art results in various\npattern recognition tasks. However, they perform poorly on out-of-distribution\nadversarial examples i.e. inputs that are specifically crafted by an adversary\nto cause DNNs to misbehave, questioning the security and reliability of\napplications. In this paper, we encourage DNN classifiers to learn more\ndiscriminative features by imposing a center loss in addition to the regular\nsoftmax cross-entropy loss. Intuitively, the center loss encourages DNNs to\nsimultaneously learns a center for the deep features of each class, and\nminimize the distances between the intra-class deep features and their\ncorresponding class centers. We hypothesize that minimizing distances between\nintra-class features and maximizing the distances between inter-class features\nat the same time would improve a classifier's robustness to adversarial\nexamples. Our results on state-of-the-art architectures on MNIST, CIFAR-10, and\nCIFAR-100 confirmed that intuition and highlight the importance of\ndiscriminative features.\n"], ["2018-11-01", "http://arxiv.org/abs/1811.00525", "On the Geometry of Adversarial Examples.", ["Marc Khoury", " Dylan Hadfield-Menell"], "  Adversarial examples are a pervasive phenomenon of machine learning models\nwhere seemingly imperceptible perturbations to the input lead to\nmisclassifications for otherwise statistically accurate models. We propose a\ngeometric framework, drawing on tools from the manifold reconstruction\nliterature, to analyze the high-dimensional geometry of adversarial examples.\nIn particular, we highlight the importance of codimension: for low-dimensional\ndata manifolds embedded in high-dimensional space there are many directions off\nthe manifold in which to construct adversarial examples. Adversarial examples\nare a natural consequence of learning a decision boundary that classifies the\nlow-dimensional data manifold well, but classifies points near the manifold\nincorrectly. Using our geometric framework we prove (1) a tradeoff between\nrobustness under different norms, (2) that adversarial training in balls around\nthe data is sample inefficient, and (3) sufficient sampling conditions under\nwhich nearest neighbor classifiers and ball-based adversarial training are\nrobust.\n"], ["2018-11-01", "http://arxiv.org/abs/1811.00401", "Excessive Invariance Causes Adversarial Vulnerability.", ["J\u00f6rn-Henrik Jacobsen", " Jens Behrmann", " Richard Zemel", " Matthias Bethge"], "  Despite their impressive performance, deep neural networks exhibit striking\nfailures on out-of-distribution inputs. One core idea of adversarial example\nresearch is to reveal neural network errors under such distribution shifts. We\ndecompose these errors into two complementary sources: sensitivity and\ninvariance. We show deep networks are not only too sensitive to task-irrelevant\nchanges of their input, as is well-known from epsilon-adversarial examples, but\nare also too invariant to a wide range of task-relevant changes, thus making\nvast regions in input space vulnerable to adversarial attacks. We show such\nexcessive invariance occurs across various tasks and architecture types. On\nMNIST and ImageNet one can manipulate the class-specific content of almost any\nimage without changing the hidden activations. We identify an insufficiency of\nthe standard cross-entropy loss as a reason for these failures. Further, we\nextend this objective based on an information-theoretic analysis so it\nencourages the model to consider all task-dependent features in its decision.\nThis provides the first approach tailored explicitly to overcome excessive\ninvariance and resulting vulnerabilities.\n"], ["2018-10-31", "http://arxiv.org/abs/1811.02658", "When Not to Classify: Detection of Reverse Engineering Attacks on DNN Image Classifiers.", ["Yujia Wang", " David J. Miller", " George Kesidis"], "  This paper addresses detection of a reverse engineering (RE) attack targeting\na deep neural network (DNN) image classifier; by querying, RE's aim is to\ndiscover the classifier's decision rule. RE can enable test-time evasion\nattacks, which require knowledge of the classifier. Recently, we proposed a\nquite effective approach (ADA) to detect test-time evasion attacks. In this\npaper, we extend ADA to detect RE attacks (ADA-RE). We demonstrate our method\nis successful in detecting \"stealthy\" RE attacks before they learn enough to\nlaunch effective test-time evasion attacks.\n"], ["2018-10-31", "http://arxiv.org/abs/1811.00189", "Reversible Adversarial Examples.", ["Jiayang Liu", " Dongdong Hou", " Weiming Zhang", " Nenghai Yu"], "  Deep Neural Networks have recently led to significant improvement in many\nfields such as image classification and speech recognition. However, these\nmachine learning models are vulnerable to adversarial examples which can\nmislead machine learning classifiers to give incorrect classifications. In this\npaper, we take advantage of reversible data hiding to construct reversible\nadversarial examples which are still misclassified by Deep Neural Networks.\nFurthermore, the proposed method can recover original images from reversible\nadversarial examples with no distortion.\n"], ["2018-10-30", "http://arxiv.org/abs/1810.12576", "Improved Network Robustness with Adversary Critic.", ["Alexander Matyasko", " Lap-Pui Chau"], "  Ideally, what confuses neural network should be confusing to humans. However,\nrecent experiments have shown that small, imperceptible perturbations can\nchange the network prediction. To address this gap in perception, we propose a\nnovel approach for learning robust classifier. Our main idea is: adversarial\nexamples for the robust classifier should be indistinguishable from the regular\ndata of the adversarial target. We formulate a problem of learning robust\nclassifier in the framework of Generative Adversarial Networks (GAN), where the\nadversarial attack on classifier acts as a generator, and the critic network\nlearns to distinguish between regular and adversarial images. The classifier\ncost is augmented with the objective that its adversarial examples should\nconfuse the adversary critic. To improve the stability of the adversarial\nmapping, we introduce adversarial cycle-consistency constraint which ensures\nthat the adversarial mapping of the adversarial examples is close to the\noriginal. In the experiments, we show the effectiveness of our defense. Our\nmethod surpasses in terms of robustness networks trained with adversarial\ntraining. Additionally, we verify in the experiments with human annotators on\nMTurk that adversarial examples are indeed visually confusing. Codes for the\nproject are available at https://github.com/aam-at/adversary_critic.\n"], ["2018-10-30", "http://arxiv.org/abs/1810.12715", "On the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models.", ["Sven Gowal", " Krishnamurthy Dvijotham", " Robert Stanforth", " Rudy Bunel", " Chongli Qin", " Jonathan Uesato", " Relja Arandjelovic", " Timothy Mann", " Pushmeet Kohli"], "  Recent work has shown that it is possible to train deep neural networks that\nare provably robust to norm-bounded adversarial perturbations. Most of these\nmethods are based on minimizing an upper bound on the worst-case loss over all\npossible adversarial perturbations. While these techniques show promise, they\noften result in difficult optimization procedures that remain hard to scale to\nlarger networks. Through a comprehensive analysis, we show how a simple\nbounding technique, interval bound propagation (IBP), can be exploited to train\nlarge provably robust neural networks that beat the state-of-the-art in\nverified accuracy. While the upper bound computed by IBP can be quite weak for\ngeneral networks, we demonstrate that an appropriate loss and clever\nhyper-parameter schedule allow the network to adapt such that the IBP bound is\ntight. This results in a fast and stable learning algorithm that outperforms\nmore sophisticated methods and achieves state-of-the-art results on MNIST,\nCIFAR-10 and SVHN. It also allows us to train the largest model to be verified\nbeyond vacuous bounds on a downscaled version of ImageNet.\n"], ["2018-10-29", "http://arxiv.org/abs/1810.12272", "Adversarial Risk and Robustness: General Definitions and Implications for the Uniform Distribution.", ["Dimitrios I. Diochnos", " Saeed Mahloujifar", " Mohammad Mahmoody"], "  We study adversarial perturbations when the instances are uniformly\ndistributed over $\\{0,1\\}^n$. We study both \"inherent\" bounds that apply to any\nproblem and any classifier for such a problem as well as bounds that apply to\nspecific problems and specific hypothesis classes.\n  As the current literature contains multiple definitions of adversarial risk\nand robustness, we start by giving a taxonomy for these definitions based on\ntheir goals, we identify one of them as the one guaranteeing misclassification\nby pushing the instances to the error region. We then study some classic\nalgorithms for learning monotone conjunctions and compare their adversarial\nrisk and robustness under different definitions by attacking the hypotheses\nusing instances drawn from the uniform distribution. We observe that sometimes\nthese definitions lead to significantly different bounds. Thus, this study\nadvocates for the use of the error-region definition, even though other\ndefinitions, in other contexts, may coincide with the error-region definition.\n  Using the error-region definition of adversarial perturbations, we then study\ninherent bounds on risk and robustness of any classifier for any classification\nproblem whose instances are uniformly distributed over $\\{0,1\\}^n$. Using the\nisoperimetric inequality for the Boolean hypercube, we show that for initial\nerror $0.01$, there always exists an adversarial perturbation that changes\n$O(\\sqrt{n})$ bits of the instances to increase the risk to $0.5$, making\nclassifier's decisions meaningless. Furthermore, by also using the central\nlimit theorem we show that when $n\\to \\infty$, at most $c \\cdot \\sqrt{n}$ bits\nof perturbations, for a universal constant $c< 1.17$, suffice for increasing\nthe risk to $0.5$, and the same $c \\cdot \\sqrt{n} $ bits of perturbations on\naverage suffice to increase the risk to $1$, hence bounding the robustness by\n$c \\cdot \\sqrt{n}$.\n"], ["2018-10-29", "http://arxiv.org/abs/1810.12042", "Logit Pairing Methods Can Fool Gradient-Based Attacks.", ["Marius Mosbach", " Maksym Andriushchenko", " Thomas Trost", " Matthias Hein", " Dietrich Klakow"], "  Recently, Kannan et al. [2018] proposed several logit regularization methods\nto improve the adversarial robustness of classifiers. We show that the\ncomputationally fast methods they propose - Clean Logit Pairing (CLP) and Logit\nSqueezing (LSQ) - just make the gradient-based optimization problem of crafting\nadversarial examples harder without providing actual robustness. We find that\nAdversarial Logit Pairing (ALP) may indeed provide robustness against\nadversarial examples, especially when combined with adversarial training, and\nwe examine it in a variety of settings. However, the increase in adversarial\naccuracy is much smaller than previously claimed. Finally, our results suggest\nthat the evaluation against an iterative PGD attack relies heavily on the\nparameters used and may result in false conclusions regarding robustness of a\nmodel.\n"], ["2018-10-28", "http://arxiv.org/abs/1810.11914", "Rademacher Complexity for Adversarially Robust Generalization.", ["Dong Yin", " Kannan Ramchandran", " Peter Bartlett"], "  Many machine learning models are vulnerable to adversarial attacks; for\nexample, adding adversarial perturbations that are imperceptible to humans can\noften make machine learning models produce wrong predictions with high\nconfidence. Moreover, although we may obtain robust models on the training\ndataset via adversarial training, in some problems the learned models cannot\ngeneralize well to the test data. In this paper, we focus on $\\ell_\\infty$\nattacks, and study the adversarially robust generalization problem through the\nlens of Rademacher complexity. For binary linear classifiers, we prove tight\nbounds for the adversarial Rademacher complexity, and show that the adversarial\nRademacher complexity is never smaller than its natural counterpart, and it has\nan unavoidable dimension dependence, unless the weight vector has bounded\n$\\ell_1$ norm. The results also extend to multi-class linear classifiers. For\n(nonlinear) neural networks, we show that the dimension dependence in the\nadversarial Rademacher complexity also exists. We further consider a surrogate\nadversarial loss for one-hidden layer ReLU network and prove margin bounds for\nthis setting. Our results indicate that having $\\ell_1$ norm constraints on the\nweight matrices might be a potential way to improve generalization in the\nadversarial setting. We demonstrate experimental results that validate our\ntheoretical findings.\n"], ["2018-10-28", "http://arxiv.org/abs/1810.11783", "RecurJac: An Efficient Recursive Algorithm for Bounding Jacobian Matrix of Neural Networks and Its Applications.", ["Huan Zhang", " Pengchuan Zhang", " Cho-Jui Hsieh"], "  The Jacobian matrix (or the gradient for single-output networks) is directly\nrelated to many important properties of neural networks, such as the function\nlandscape, stationary points, (local) Lipschitz constants and robustness to\nadversarial attacks. In this paper, we propose a recursive algorithm, RecurJac,\nto compute both upper and lower bounds for each element in the Jacobian matrix\nof a neural network with respect to network's input, and the network can\ncontain a wide range of activation functions. As a byproduct, we can\nefficiently obtain a (local) Lipschitz constant, which plays a crucial role in\nneural network robustness verification, as well as the training stability of\nGANs. Experiments show that (local) Lipschitz constants produced by our method\nis of better quality than previous approaches, thus providing better robustness\nverification results. Our algorithm has polynomial time complexity, and its\ncomputation time is reasonable even for relatively large networks.\nAdditionally, we use our bounds of Jacobian matrix to characterize the\nlandscape of the neural network, for example, to determine whether there exist\nstationary points in a local neighborhood. Source code available at\n\\url{http://github.com/huanzhang12/RecurJac-Jacobian-bounds}.\n"], ["2018-10-28", "http://arxiv.org/abs/1810.11793", "Robust Audio Adversarial Example for a Physical Attack.", ["Hiromu Yakura", " Jun Sakuma"], "  We propose a method to generate audio adversarial examples that can attack a\nstate-of-the-art speech recognition model in the physical world. Previous work\nassumes that generated adversarial examples are directly fed to the recognition\nmodel, and is not able to perform such a physical attack because of\nreverberation and noise from playback environments. In contrast, our method\nobtains robust adversarial examples by simulating transformations caused by\nplayback or recording in the physical world and incorporating the\ntransformations into the generation process. Evaluation and a listening\nexperiment demonstrated that our adversarial examples are able to attack\nwithout being noticed by humans. This result suggests that audio adversarial\nexamples generated by the proposed method may become a real threat.\n"], ["2018-10-27", "http://arxiv.org/abs/1810.11726", "Towards Robust Deep Neural Networks.", ["Timothy E. Wang", " Yiming Gu", " Dhagash Mehta", " Xiaojun Zhao", " Edgar A. Bernal"], "  We investigate the topics of sensitivity and robustness in feedforward and\nconvolutional neural networks. Combining energy landscape techniques developed\nin computational chemistry with tools drawn from formal methods, we produce\nempirical evidence indicating that networks corresponding to lower-lying minima\nin the optimization landscape of the learning objective tend to be more robust.\nThe robustness estimate used is the inverse of a proposed sensitivity measure,\nwhich we define as the volume of an over-approximation of the reachable set of\nnetwork outputs under all additive $l_{\\infty}$-bounded perturbations on the\ninput data. We present a novel loss function which includes a sensitivity term\nin addition to the traditional task-oriented and regularization terms. In our\nexperiments on standard machine learning and computer vision datasets, we show\nthat the proposed loss function leads to networks which reliably optimize the\nrobustness measure as well as other related metrics of adversarial robustness\nwithout significant degradation in the classification error. Experimental\nresults indicate that the proposed method outperforms state-of-the-art\nsensitivity-based learning approaches with regards to robustness to adversarial\nattacks. We also show that although the introduced framework does not\nexplicitly enforce an adversarial loss, it achieves competitive overall\nperformance relative to methods that do.\n"], ["2018-10-27", "http://arxiv.org/abs/1810.11711", "Regularization Effect of Fast Gradient Sign Method and its Generalization.", ["Chandler Zuo"], "  Fast Gradient Sign Method (FGSM) is a popular method to generate adversarial\nexamples that make neural network models robust against perturbations. Despite\nits empirical success, its theoretical property is not well understood. This\npaper develops theory to explain the regularization effect of Generalized FGSM,\na class of methods to generate adversarial examples. Motivated from the\nrelationship between FGSM and LASSO penalty, the asymptotic properties of\nGeneralized FGSM are derived in the Generalized Linear Model setting, which is\nessentially the 1-layer neural network setting with certain activation\nfunctions. In such simple neural network models, I prove that Generalized FGSM\nestimation is root n-consistent and weakly oracle under proper conditions. The\nasymptotic results are also highly similar to penalized likelihood estimation.\nNevertheless, Generalized FGSM introduces additional bias when data sampling is\nnot sign neutral, a concept I introduce to describe the balance-ness of the\nnoise signs. Although the theory in this paper is developed under simple neural\nnetwork settings, I argue that it may give insights and justification for FGSM\nin deep neural network settings as well.\n"], ["2018-10-26", "http://arxiv.org/abs/1810.11580", "Attacks Meet Interpretability: Attribute-steered Detection of Adversarial Samples.", ["Guanhong Tao", " Shiqing Ma", " Yingqi Liu", " Xiangyu Zhang"], "  Adversarial sample attacks perturb benign inputs to induce DNN misbehaviors.\nRecent research has demonstrated the widespread presence and the devastating\nconsequences of such attacks. Existing defense techniques either assume prior\nknowledge of specific attacks or may not work well on complex models due to\ntheir underlying assumptions. We argue that adversarial sample attacks are\ndeeply entangled with interpretability of DNN models: while classification\nresults on benign inputs can be reasoned based on the human perceptible\nfeatures/attributes, results on adversarial samples can hardly be explained.\nTherefore, we propose a novel adversarial sample detection technique for face\nrecognition models, based on interpretability. It features a novel\nbi-directional correspondence inference between attributes and internal neurons\nto identify neurons critical for individual attributes. The activation values\nof critical neurons are enhanced to amplify the reasoning part of the\ncomputation and the values of other neurons are weakened to suppress the\nuninterpretable part. The classification results after such transformation are\ncompared with those of the original model to detect adversaries. Results show\nthat our technique can achieve 94% detection accuracy for 7 different kinds of\nattacks with 9.91% false positives on benign inputs. In contrast, a\nstate-of-the-art feature squeezing technique can only achieve 55% accuracy with\n23.3% false positives.\n"], ["2018-10-25", "http://arxiv.org/abs/1810.10751", "Attack Graph Convolutional Networks by Adding Fake Nodes.", ["Xiaoyun Wang", " Joe Eaton", " Cho-Jui Hsieh", " Felix Wu"], "  Graph convolutional networks (GCNs) have been widely used for classifying\ngraph nodes in the semi-supervised setting. Previous work have shown that GCNs\nare vulnerable to the perturbation on adjacency and feature matrices of\nexisting nodes. However, it is unrealistic to change existing nodes in many\napplications, such as existing users in social networks. In this paper, we\ndesign algorithms to attack GCNs by adding fake nodes. A greedy algorithm is\nproposed to generate adjacency and feature matrices of fake nodes, aiming to\nminimize the classification accuracy on the existing nodes. In additional, we\nintroduce a discriminator to classify fake nodes from real nodes, and propose a\nGreedy-GAN attack to simultaneously update the discriminator and the attacker,\nto make fake nodes indistinguishable to the real ones. Our non-targeted attack\ndecreases the accuracy of GCN down to 0.10, and our targeted attack reaches a\nsuccess rate of 99% on the whole datasets, and 94% on average for attacking a\nsingle target node.\n"], ["2018-10-25", "http://arxiv.org/abs/1810.10731", "Law and Adversarial Machine Learning.", ["Ram Shankar Siva Kumar", " David R. O'Brien", " Kendra Albert", " Salome Vilojen"], "  When machine learning systems fail because of adversarial manipulation, how\nshould society expect the law to respond? Through scenarios grounded in\nadversarial ML literature, we explore how some aspects of computer crime,\ncopyright, and tort law interface with perturbation, poisoning, model stealing\nand model inversion attacks to show how some attacks are more likely to result\nin liability than others. We end with a call for action to ML researchers to\ninvest in transparent benchmarks of attacks and defenses; architect ML systems\nwith forensics in mind and finally, think more about adversarial machine\nlearning in the context of civil liberties. The paper is targeted towards ML\nresearchers who have no legal background.\n"], ["2018-10-25", "http://arxiv.org/abs/1810.10939", "Evading classifiers in discrete domains with provable optimality guarantees.", ["Bogdan Kulynych", " Jamie Hayes", " Nikita Samarin", " Carmela Troncoso"], "  Machine-learning models for security-critical applications such as bot,\nmalware, or spam detection, operate in constrained discrete domains. These\napplications would benefit from having provable guarantees against adversarial\nexamples. The existing literature on provable adversarial robustness of models,\nhowever, exclusively focuses on robustness to gradient-based attacks in domains\nsuch as images. These attacks model the adversarial cost, e.g., amount of\ndistortion applied to an image, as a $p$-norm. We argue that this approach is\nnot well-suited to model adversarial costs in constrained domains where not all\nexamples are feasible.\n  We introduce a graphical framework that (1) generalizes existing attacks in\ndiscrete domains, (2) can accommodate complex cost functions beyond $p$-norms,\nincluding financial cost incurred when attacking a classifier, and (3)\nefficiently produces valid adversarial examples with guarantees of minimal\nadversarial cost. These guarantees directly translate into a notion of\nadversarial robustness that takes into account domain constraints and the\nadversary's capabilities. We show how our framework can be used to evaluate\nsecurity by crafting adversarial examples that evade a Twitter-bot detection\nclassifier with provably minimal number of changes; and to build privacy\ndefenses by crafting adversarial examples that evade a privacy-invasive\nwebsite-fingerprinting classifier.\n"], ["2018-10-24", "http://arxiv.org/abs/1810.10625", "Robust Adversarial Learning via Sparsifying Front Ends.", ["Soorya Gopalakrishnan", " Zhinus Marzi", " Upamanyu Madhow", " Ramtin Pedarsani"], "  It is by now well-known that small adversarial perturbations can induce\nclassification errors in deep neural networks. In this paper, we take a\nbottom-up signal processing perspective to this problem and show that a\nsystematic exploitation of sparsity in natural data is a promising tool for\ndefense. For linear classifiers, we show that a sparsifying front end is\nprovably effective against $\\ell_{\\infty}$-bounded attacks, reducing output\ndistortion due to the attack by a factor of roughly $K/N$ where $N$ is the data\ndimension and $K$ is the sparsity level. We then extend this concept to deep\nnetworks, showing that a \"locally linear\" model can be used to develop a\ntheoretical foundation for crafting attacks and defenses. We also devise\nattacks based on the locally linear model that outperform the well-known FGSM\nattack. We supplement our theoretical results with experiments on the MNIST\nhandwritten digit database, showing the efficacy of the proposed sparsity-based\ndefense schemes.\n"], ["2018-10-23", "http://arxiv.org/abs/1810.10031", "Stochastic Substitute Training: A Gray-box Approach to Craft Adversarial Examples Against Gradient Obfuscation Defenses.", ["Mohammad Hashemi", " Greg Cusack", " Eric Keller"], "  It has been shown that adversaries can craft example inputs to neural\nnetworks which are similar to legitimate inputs but have been created to\npurposely cause the neural network to misclassify the input. These adversarial\nexamples are crafted, for example, by calculating gradients of a carefully\ndefined loss function with respect to the input. As a countermeasure, some\nresearchers have tried to design robust models by blocking or obfuscating\ngradients, even in white-box settings. Another line of research proposes\nintroducing a separate detector to attempt to detect adversarial examples. This\napproach also makes use of gradient obfuscation techniques, for example, to\nprevent the adversary from trying to fool the detector. In this paper, we\nintroduce stochastic substitute training, a gray-box approach that can craft\nadversarial examples for defenses which obfuscate gradients. For those defenses\nthat have tried to make models more robust, with our technique, an adversary\ncan craft adversarial examples with no knowledge of the defense. For defenses\nthat attempt to detect the adversarial examples, with our technique, an\nadversary only needs very limited information about the defense to craft\nadversarial examples. We demonstrate our technique by applying it against two\ndefenses which make models more robust and two defenses which detect\nadversarial examples.\n"], ["2018-10-23", "http://arxiv.org/abs/1810.09650", "One Bit Matters: Understanding Adversarial Examples as the Abuse of Redundancy.", ["Jingkang Wang", " Ruoxi Jia", " Gerald Friedland", " Bo Li", " Costas Spanos"], "  Despite the great success achieved in machine learning (ML), adversarial\nexamples have caused concerns with regards to its trustworthiness: A small\nperturbation of an input results in an arbitrary failure of an otherwise\nseemingly well-trained ML model. While studies are being conducted to discover\nthe intrinsic properties of adversarial examples, such as their transferability\nand universality, there is insufficient theoretic analysis to help understand\nthe phenomenon in a way that can influence the design process of ML\nexperiments. In this paper, we deduce an information-theoretic model which\nexplains adversarial attacks as the abuse of feature redundancies in ML\nalgorithms. We prove that feature redundancy is a necessary condition for the\nexistence of adversarial examples. Our model helps to explain some major\nquestions raised in many anecdotal studies on adversarial examples. Our theory\nis backed up by empirical measurements of the information content of benign and\nadversarial examples on both image and text datasets. Our measurements show\nthat typical adversarial examples introduce just enough redundancy to overflow\nthe decision making of an ML model trained on corresponding benign examples. We\nconclude with actionable recommendations to improve the robustness of machine\nlearners against adversarial examples.\n"], ["2018-10-23", "http://arxiv.org/abs/1810.10109", "Et Tu Alexa? When Commodity WiFi Devices Turn into Adversarial Motion Sensors.", ["Yanzi Zhu", " Zhujun Xiao", " Yuxin Chen", " Zhijing Li", " Max Liu", " Ben Y. Zhao", " Haitao Zheng"], "  Our work demonstrates a new set of silent reconnaissance attacks, which\nleverages the presence of commodity WiFi devices to track users inside private\nhomes and offices, without compromising any WiFi network, data packets, or\ndevices. We show that just by sniffing existing WiFi signals, an adversary can\naccurately detect and track movements of users inside a building. This is made\npossible by our new signal model that links together human motion near WiFi\ntransmitters and variance of multipath signal propagation seen by the attacker\nsniffer outside of the property. The resulting attacks are cheap, highly\neffective, and yet difficult to detect. We implement the attack using a single\ncommodity smartphone, deploy it in 11 real-world offices and residential\napartments, and show it is highly effective. Finally, we evaluate potential\ndefenses, and propose a practical and effective defense based on AP signal\nobfuscation.\n"], ["2018-10-22", "http://arxiv.org/abs/1810.09619", "Sparse DNNs with Improved Adversarial Robustness.", ["Yiwen Guo", " Chao Zhang", " Changshui Zhang", " Yurong Chen"], "  Deep neural networks (DNNs) are computationally/memory-intensive and\nvulnerable to adversarial attacks, making them prohibitive in some real-world\napplications. By converting dense models into sparse ones, pruning appears to\nbe a promising solution to reducing the computation/memory cost. This paper\nstudies classification models, especially DNN-based ones, to demonstrate that\nthere exists intrinsic relationships between their sparsity and adversarial\nrobustness. Our analyses reveal, both theoretically and empirically, that\nnonlinear DNN-based classifiers behave differently under $l_2$ attacks from\nsome linear ones. We further demonstrate that an appropriately higher model\nsparsity implies better robustness of nonlinear DNNs, whereas over-sparsified\nmodels can be more difficult to resist adversarial examples.\n"], ["2018-10-22", "http://arxiv.org/abs/1810.09519", "Adversarial Risk Bounds via Function Transformation.", ["Justin Khim", " Po-Ling Loh"], "  We derive bounds for a notion of adversarial risk, designed to characterize\nthe robustness of linear and neural network classifiers to adversarial\nperturbations. Specifically, we introduce a new class of function\ntransformations with the property that the risk of the transformed functions\nupper-bounds the adversarial risk of the original functions. This reduces the\nproblem of deriving bounds on the adversarial risk to the problem of deriving\nrisk bounds using standard learning-theoretic techniques. We then derive bounds\non the Rademacher complexities of the transformed function classes, obtaining\nerror rates on the same order as the generalization error of the original\nfunction classes. We also discuss extensions of our theory to multiclass\nclassification and regression. Finally, we provide two algorithms for\noptimizing the adversarial risk bounds in the linear case, and discuss\nconnections to regularization and distributional robustness.\n"], ["2018-10-22", "http://arxiv.org/abs/1810.09225", "Cost-Sensitive Robustness against Adversarial Examples.", ["Xiao Zhang", " David Evans"], "  Several recent works have developed methods for training classifiers that are\ncertifiably robust against norm-bounded adversarial perturbations. These\nmethods assume that all the adversarial transformations are equally important,\nwhich is seldom the case in real-world applications. We advocate for\ncost-sensitive robustness as the criteria for measuring the classifier's\nperformance for tasks where some adversarial transformation are more important\nthan others. We encode the potential harm of each adversarial transformation in\na cost matrix, and propose a general objective function to adapt the robust\ntraining method of Wong & Kolter (2018) to optimize for cost-sensitive\nrobustness. Our experiments on simple MNIST and CIFAR10 models with a variety\nof cost matrices show that the proposed approach can produce models with\nsubstantially reduced cost-sensitive robust error, while maintaining\nclassification accuracy.\n"], ["2018-10-19", "http://arxiv.org/abs/1810.08640", "On Extensions of CLEVER: A Neural Network Robustness Evaluation Algorithm.", ["Tsui-Wei Weng", " Huan Zhang", " Pin-Yu Chen", " Aurelie Lozano", " Cho-Jui Hsieh", " Luca Daniel"], "  CLEVER (Cross-Lipschitz Extreme Value for nEtwork Robustness) is an Extreme\nValue Theory (EVT) based robustness score for large-scale deep neural networks\n(DNNs). In this paper, we propose two extensions on this robustness score.\nFirst, we provide a new formal robustness guarantee for classifier functions\nthat are twice differentiable. We apply extreme value theory on the new formal\nrobustness guarantee and the estimated robustness is called second-order CLEVER\nscore. Second, we discuss how to handle gradient masking, a common defensive\ntechnique, using CLEVER with Backward Pass Differentiable Approximation (BPDA).\nWith BPDA applied, CLEVER can evaluate the intrinsic robustness of neural\nnetworks of a broader class -- networks with non-differentiable input\ntransformations. We demonstrate the effectiveness of CLEVER with BPDA in\nexperiments on a 121-layer Densenet model trained on the ImageNet dataset.\n"], ["2018-10-18", "http://arxiv.org/abs/1810.08280", "Exploring Adversarial Examples in Malware Detection.", ["Octavian Suciu", " Scott E. Coull", " Jeffrey Johns"], "  The convolutional neural network (CNN) architecture is increasingly being\napplied to new domains, such as malware detection, where it is able to learn\nmalicious behavior from raw bytes extracted from executables. These\narchitectures reach impressive performance with no feature engineering effort\ninvolved, but their robustness against active attackers is yet to be\nunderstood. Such malware detectors could face a new attack vector in the form\nof adversarial interference with the classification model. Existing evasion\nattacks intended to cause misclassification on test-time instances, which have\nbeen extensively studied for image classifiers, are not applicable because of\nthe input semantics that prevents arbitrary changes to the binaries. This paper\nexplores the area of adversarial examples for malware detection. By training an\nexisting model on a production-scale dataset, we show that some previous\nattacks are less effective than initially reported, while simultaneously\nhighlighting architectural weaknesses that facilitate new attack strategies for\nmalware classification. Finally, we explore how generalizable different attack\nstrategies are, the trade-offs when aiming to increase their effectiveness, and\nthe transferability of single-step attacks.\n"], ["2018-10-18", "http://arxiv.org/abs/1810.08070", "A Training-based Identification Approach to VIN Adversarial Examples.", ["Yingdi Wang", " Wenjia Niu", " Tong Chen", " Yingxiao Xiang", " Jingjing Liu", " Gang Li", " Jiqiang Liu"], "  With the rapid development of Artificial Intelligence (AI), the problem of AI\nsecurity has gradually emerged. Most existing machine learning algorithms may\nbe attacked by adversarial examples. An adversarial example is a slightly\nmodified input sample that can lead to a false result of machine learning\nalgorithms. The adversarial examples pose a potential security threat for many\nAI application areas, especially in the domain of robot path planning. In this\nfield, the adversarial examples obstruct the algorithm by adding obstacles to\nthe normal maps, resulting in multiple effects on the predicted path. However,\nthere is no suitable approach to automatically identify them. To our knowledge,\nall previous work uses manual observation method to estimate the attack results\nof adversarial maps, which is time-consuming. Aiming at the existing problem,\nthis paper explores a method to automatically identify the adversarial examples\nin Value Iteration Networks (VIN), which has a strong generalization ability.\nWe analyze the possible scenarios caused by the adversarial maps. We propose a\ntraining-based identification approach to VIN adversarial examples by combing\nthe path feature comparison and path image classification. We evaluate our\nmethod using the adversarial maps dataset, show that our method can achieve a\nhigh-accuracy and faster identification than manual observation method.\n"], ["2018-10-17", "http://arxiv.org/abs/1810.07481", "Provable Robustness of ReLU networks via Maximization of Linear Regions.", ["Francesco University of T\u00fcbingen Croce", " Maksym Saarland University Andriushchenko", " Matthias University of T\u00fcbingen Hein"], "  It has been shown that neural network classifiers are not robust. This raises\nconcerns about their usage in safety-critical systems. We propose in this paper\na regularization scheme for ReLU networks which provably improves the\nrobustness of the classifier by maximizing the linear regions of the classifier\nas well as the distance to the decision boundary. Our techniques allow even to\nfind the minimal adversarial perturbation for a fraction of test points for\nlarge networks. In the experiments we show that our approach improves upon\nadversarial training both in terms of lower and upper bounds on the robustness\nand is comparable or better than the state-of-the-art in terms of test error\nand robustness.\n"], ["2018-10-16", "http://arxiv.org/abs/1810.10337", "Projecting Trouble: Light Based Adversarial Attacks on Deep Learning Classifiers.", ["Nicole Nichols", " Robert Jasper"], "  This work demonstrates a physical attack on a deep learning image\nclassification system using projected light onto a physical scene. Prior work\nis dominated by techniques for creating adversarial examples which directly\nmanipulate the digital input of the classifier. Such an attack is limited to\nscenarios where the adversary can directly update the inputs to the classifier.\nThis could happen by intercepting and modifying the inputs to an online API\nsuch as Clarifai or Cloud Vision. Such limitations have led to a vein of\nresearch around physical attacks where objects are constructed to be inherently\nadversarial or adversarial modifications are added to cause misclassification.\nOur work differs from other physical attacks in that we can cause\nmisclassification dynamically without altering physical objects in a permanent\nway.\n  We construct an experimental setup which includes a light projection source,\nan object for classification, and a camera to capture the scene. Experiments\nare conducted against 2D and 3D objects from CIFAR-10. Initial tests show\nprojected light patterns selected via differential evolution could degrade\nclassification from 98% to 22% and 89% to 43% probability for 2D and 3D targets\nrespectively. Subsequent experiments explore sensitivity to physical setup and\ncompare two additional baseline conditions for all 10 CIFAR classes. Some\nphysical targets are more susceptible to perturbation. Simple attacks show near\nequivalent success, and 6 of the 10 classes were disrupted by light.\n"], ["2018-10-16", "http://arxiv.org/abs/1810.07339", "Security Matters: A Survey on Adversarial Machine Learning.", ["Guofu Li", " Pengjia Zhu", " Jin Li", " Zhemin Yang", " Ning Cao", " Zhiyi Chen"], "  Adversarial machine learning is a fast growing research area, which considers\nthe scenarios when machine learning systems may face potential adversarial\nattackers, who intentionally synthesize input data to make a well-trained model\nto make mistake. It always involves a defending side, usually a classifier, and\nan attacking side that aims to cause incorrect output. The earliest studies on\nthe adversarial examples for machine learning algorithms start from the\ninformation security area, which considers a much wider varieties of attacking\nmethods. But recent research focus that popularized by the deep learning\ncommunity places strong emphasis on how the \"imperceivable\" perturbations on\nthe normal inputs may cause dramatic mistakes by the deep learning with\nsupposed super-human accuracy. This paper serves to give a comprehensive\nintroduction to a range of aspects of the adversarial deep learning topic,\nincluding its foundations, typical attacking and defending strategies, and some\nextended studies.\n"], ["2018-10-15", "http://arxiv.org/abs/1810.06583", "Concise Explanations for Neural Networks using Adversarial Training.", ["Prasad Chalasani", " Jiefeng Chen", " Somesh Jha", " Xi Wu"], "  We show new connections between adversarial learning and explainability in\nDeep Neural Networks (DNNs). One form of explanation of the output of a neural\nnetwork model in terms of its input features, is a vector of\nfeature-attributions using the Integrated Gradient (IG) method. Two desirable\ncharacteristics of an attribution-based explanation are: (1)\n$\\textit{sparseness}$: the attributions of irrelevant or weakly relevant\nfeatures should be negligible, thus resulting in \\textit{concise} explanations\nin terms of the significant features, and (2) $\\textit{stability}$: it should\nnot vary significantly within a small local neighborhood of the input.\n  Our first contribution is a theoretical exploration of these connections for\na class of 1-layer networks (which includes models such as logistic\nregression), for which we show that (a) adversarial training using an\n$\\ell_\\infty$-bounded adversary produces models with sparse IG vectors, and (b)\nnatural model-training while encouraging stable IG-explanations (via an extra\nterm in the loss function), is equivalent to adversarial training. Our second\ncontribution is an empirical verification of phenomenon (a), which we show,\nsomewhat surprisingly, occurs $\\textit{not only in 1-layer networks,}$\n$\\textit{but also DNNs trained on standard image datasets:}$ adversarial\ntraining with $\\ell_\\infty$-bounded perturbations, yields significantly sparser\nIG vectors, with little degradation in performance on natural test data,\ncompared to natural training. Moreover, the sparseness of the IG vectors is\nsignificantly better than that achievable via $\\ell_1$-regularized natural\ntraining.\n"], ["2018-10-11", "http://arxiv.org/abs/1810.05162", "Characterizing Adversarial Examples Based on Spatial Consistency Information for Semantic Segmentation.", ["Chaowei Xiao", " Ruizhi Deng", " Bo Li", " Fisher Yu", " Mingyan Liu", " Dawn Song"], "  Deep Neural Networks (DNNs) have been widely applied in various recognition\ntasks. However, recently DNNs have been shown to be vulnerable against\nadversarial examples, which can mislead DNNs to make arbitrary incorrect\npredictions. While adversarial examples are well studied in classification\ntasks, other learning problems may have different properties. For instance,\nsemantic segmentation requires additional components such as dilated\nconvolutions and multiscale processing. In this paper, we aim to characterize\nadversarial examples based on spatial context information in semantic\nsegmentation. We observe that spatial consistency information can be\npotentially leveraged to detect adversarial examples robustly even when a\nstrong adaptive attacker has access to the model and detection strategies. We\nalso show that adversarial examples based on attacks considered within the\npaper barely transfer among models, even though transferability is common in\nclassification. Our observations shed new light on developing adversarial\nattacks and defenses to better understand the vulnerabilities of DNNs.\n"], ["2018-10-11", "http://arxiv.org/abs/1810.05206", "MeshAdv: Adversarial Meshes for Visual Recognition.", ["Chaowei Xiao", " Dawei Yang", " Bo Li", " Jia Deng", " Mingyan Liu"], "  Highly expressive models such as deep neural networks (DNNs) have been widely\napplied to various applications. However, recent studies show that DNNs are\nvulnerable to adversarial examples, which are carefully crafted inputs aiming\nto mislead the predictions. Currently, the majority of these studies have\nfocused on perturbation added to image pixels, while such manipulation is not\nphysically realistic. Some works have tried to overcome this limitation by\nattaching printable 2D patches or painting patterns onto surfaces, but can be\npotentially defended because 3D shape features are intact. In this paper, we\npropose meshAdv to generate \"adversarial 3D meshes\" from objects that have rich\nshape features but minimal textural variation. To manipulate the shape or\ntexture of the objects, we make use of a differentiable renderer to compute\naccurate shading on the shape and propagate the gradient. Extensive experiments\nshow that the generated 3D meshes are effective in attacking both classifiers\nand object detectors. We evaluate the attack under different viewpoints. In\naddition, we design a pipeline to perform black-box attack on a photorealistic\nrenderer with unknown rendering parameters.\n"], ["2018-10-09", "http://arxiv.org/abs/1810.05665", "Is PGD-Adversarial Training Necessary? Alternative Training via a Soft-Quantization Network with Noisy-Natural Samples Only.", ["Tianhang Zheng", " Changyou Chen", " Kui Ren"], "  Recent work on adversarial attack and defense suggests that PGD is a\nuniversal $l_\\infty$ first-order attack, and PGD adversarial training can\nsignificantly improve network robustness against a wide range of first-order\n$l_\\infty$-bounded attacks, represented as the state-of-the-art defense method.\nHowever, an obvious weakness of PGD adversarial training is its\nhighly-computational cost in generating adversarial samples, making it\ncomputationally infeasible for large and high-resolution real datasets such as\nthe ImageNet dataset. In addition, recent work also has suggested a simple\n\"close-form\" solution to a robust model on MNIST. Therefore, a natural question\nraised is that is PGD adversarial training really necessary for robust defense?\nIn this paper, we give a negative answer by proposing a training paradigm that\nis comparable to PGD adversarial training on several standard datasets, while\nonly using noisy-natural samples. Specifically, we reformulate the min-max\nobjective in PGD adversarial training by a problem to minimize the original\nnetwork loss plus $l_1$ norms of its gradients w.r.t. the inputs. For the\n$l_1$-norm loss, we propose a computationally-feasible solution by embedding a\ndifferentiable soft-quantization layer after the network input layer. We show\nformally that the soft-quantization layer trained with noisy-natural samples is\nan alternative approach to minimizing the $l_1$-gradient norms as in PGD\nadversarial training. Extensive empirical evaluations on standard datasets show\nthat our proposed models are comparable to PGD-adversarially-trained models\nunder PGD and BPDA attacks. Remarkably, our method achieves a 24X speed-up on\nMNIST while maintaining a comparable defensive ability, and for the first time\nfine-tunes a robust Imagenet model within only two days. Code is provided on\n\\url{https://github.com/tianzheng4/Noisy-Training-Soft-Quantization}\n"], ["2018-10-09", "http://arxiv.org/abs/1810.03913", "Analyzing the Noise Robustness of Deep Neural Networks.", ["Mengchen Liu", " Shixia Liu", " Hang Su", " Kelei Cao", " Jun Zhu"], "  Deep neural networks (DNNs) are vulnerable to maliciously generated\nadversarial examples. These examples are intentionally designed by making\nimperceptible perturbations and often mislead a DNN into making an incorrect\nprediction. This phenomenon means that there is significant risk in applying\nDNNs to safety-critical applications, such as driverless cars. To address this\nissue, we present a visual analytics approach to explain the primary cause of\nthe wrong predictions introduced by adversarial examples. The key is to analyze\nthe datapaths of the adversarial examples and compare them with those of the\nnormal examples. A datapath is a group of critical neurons and their\nconnections. To this end, we formulate the datapath extraction as a subset\nselection problem and approximately solve it based on back-propagation. A\nmulti-level visualization consisting of a segmented DAG (layer level), an Euler\ndiagram (feature map level), and a heat map (neuron level), has been designed\nto help experts investigate datapaths from the high-level layers to the\ndetailed neuron activations. Two case studies are conducted that demonstrate\nthe promise of our approach in support of explaining the working mechanism of\nadversarial examples.\n"], ["2018-10-09", "http://arxiv.org/abs/1810.03806", "The Adversarial Attack and Detection under the Fisher Information Metric.", ["Chenxiao Zhao", " P. Thomas Fletcher", " Mixue Yu", " Yaxin Peng", " Guixu Zhang", " Chaomin Shen"], "  Many deep learning models are vulnerable to the adversarial attack, i.e.,\nimperceptible but intentionally-designed perturbations to the input can cause\nincorrect output of the networks. In this paper, using information geometry, we\nprovide a reasonable explanation for the vulnerability of deep learning models.\nBy considering the data space as a non-linear space with the Fisher information\nmetric induced from a neural network, we first propose an adversarial attack\nalgorithm termed one-step spectral attack (OSSA). The method is described by a\nconstrained quadratic form of the Fisher information matrix, where the optimal\nadversarial perturbation is given by the first eigenvector, and the model\nvulnerability is reflected by the eigenvalues. The larger an eigenvalue is, the\nmore vulnerable the model is to be attacked by the corresponding eigenvector.\nTaking advantage of the property, we also propose an adversarial detection\nmethod with the eigenvalues serving as characteristics. Both our attack and\ndetection algorithms are numerically optimized to work efficiently on large\ndatasets. Our evaluations show superior performance compared with other\nmethods, implying that the Fisher information is a promising approach to\ninvestigate the adversarial attacks and defenses.\n"], ["2018-10-08", "http://arxiv.org/abs/1810.04065", "Limitations of adversarial robustness: strong No Free Lunch Theorem.", ["Elvis Dohmatob"], "  This manuscript presents some new impossibility results on adversarial\nrobustness in machine learning, a very important yet largely open problem. We\nshow that if conditioned on a class label the data distribution satisfies the\n$W_2$ Talagrand transportation-cost inequality (for example, this condition is\nsatisfied if the conditional distribution has density which is log-concave; is\nthe uniform measure on a compact Riemannian manifold with positive Ricci\ncurvature, any classifier can be adversarially fooled with high probability\nonce the perturbations are slightly greater than the natural noise level in the\nproblem. We call this result The Strong \"No Free Lunch\" Theorem as some recent\nresults (Tsipras et al. 2018, Fawzi et al. 2018, etc.) on the subject can be\nimmediately recovered as very particular cases. Our theoretical bounds are\ndemonstrated on both simulated and real data (MNIST). We conclude the\nmanuscript with some speculation on possible future research directions.\n"], ["2018-10-08", "http://arxiv.org/abs/1810.03773", "Average Margin Regularization for Classifiers.", ["Matt Olfat", " Anil Aswani"], "  Adversarial robustness has become an important research topic given empirical\ndemonstrations on the lack of robustness of deep neural networks.\nUnfortunately, recent theoretical results suggest that adversarial training\ninduces a strict tradeoff between classification accuracy and adversarial\nrobustness. In this paper, we propose and then study a new regularization for\nany margin classifier or deep neural network. We motivate this regularization\nby a novel generalization bound that shows a tradeoff in classifier accuracy\nbetween maximizing its margin and average margin. We thus call our approach an\naverage margin (AM) regularization, and it consists of a linear term added to\nthe objective. We theoretically show that for certain distributions AM\nregularization can both improve classifier accuracy and robustness to\nadversarial attacks. We conclude by using both synthetic and real data to\nempirically show that AM regularization can strictly improve both accuracy and\nrobustness for support vector machine's (SVM's) and deep neural networks,\nrelative to unregularized classifiers and adversarially trained classifiers.\n"], ["2018-10-08", "http://arxiv.org/abs/1810.03739", "Efficient Two-Step Adversarial Defense for Deep Neural Networks.", ["Ting-Jui Chang", " Yukun He", " Peng Li"], "  In recent years, deep neural networks have demonstrated outstanding\nperformance in many machine learning tasks. However, researchers have\ndiscovered that these state-of-the-art models are vulnerable to adversarial\nexamples: legitimate examples added by small perturbations which are\nunnoticeable to human eyes. Adversarial training, which augments the training\ndata with adversarial examples during the training process, is a well known\ndefense to improve the robustness of the model against adversarial attacks.\nHowever, this robustness is only effective to the same attack method used for\nadversarial training. Madry et al.(2017) suggest that effectiveness of\niterative multi-step adversarial attacks and particularly that projected\ngradient descent (PGD) may be considered the universal first order adversary\nand applying the adversarial training with PGD implies resistance against many\nother first order attacks. However, the computational cost of the adversarial\ntraining with PGD and other multi-step adversarial examples is much higher than\nthat of the adversarial training with other simpler attack techniques. In this\npaper, we show how strong adversarial examples can be generated only at a cost\nsimilar to that of two runs of the fast gradient sign method (FGSM), allowing\ndefense against adversarial attacks with a robustness level comparable to that\nof the adversarial training with multi-step adversarial examples. We\nempirically demonstrate the effectiveness of the proposed two-step defense\napproach against different attack methods and its improvements over existing\ndefense strategies.\n"], ["2018-10-08", "http://arxiv.org/abs/1810.03538", "Combinatorial Attacks on Binarized Neural Networks.", ["Elias B. Khalil", " Amrita Gupta", " Bistra Dilkina"], "  Binarized Neural Networks (BNNs) have recently attracted significant interest\ndue to their computational efficiency. Concurrently, it has been shown that\nneural networks may be overly sensitive to \"attacks\" - tiny adversarial changes\nin the input - which may be detrimental to their use in safety-critical\ndomains. Designing attack algorithms that effectively fool trained models is a\nkey step towards learning robust neural networks. The discrete,\nnon-differentiable nature of BNNs, which distinguishes them from their\nfull-precision counterparts, poses a challenge to gradient-based attacks. In\nthis work, we study the problem of attacking a BNN through the lens of\ncombinatorial and integer optimization. We propose a Mixed Integer Linear\nProgramming (MILP) formulation of the problem. While exact and flexible, the\nMILP quickly becomes intractable as the network and perturbation space grow. To\naddress this issue, we propose IProp, a decomposition-based algorithm that\nsolves a sequence of much smaller MILP problems. Experimentally, we evaluate\nboth proposed methods against the standard gradient-based attack (FGSM) on\nMNIST and Fashion-MNIST, and show that IProp performs favorably compared to\nFGSM, while scaling beyond the limits of the MILP.\n"], ["2018-10-04", "http://arxiv.org/abs/1810.02180", "Improved Generalization Bounds for Robust Learning.", ["Idan Attias", " Aryeh Kontorovich", " Yishay Mansour"], "  We consider a model of robust learning in an adversarial environment. The\nlearner gets uncorrupted training data with access to possible corruptions that\nmay be affected by the adversary during testing. The learner's goal is to build\na robust classifier that would be tested on future adversarial examples. We use\na zero-sum game between the learner and the adversary as our game theoretic\nframework. The adversary is limited to $k$ possible corruptions for each input.\nOur model is closely related to the adversarial examples model of Schmidt et\nal. (2018); Madry et al. (2017).\n  Our main results consist of generalization bounds for the binary and\nmulti-class classification, as well as the real-valued case (regression). For\nthe binary classification setting, we both tighten the generalization bound of\nFeige, Mansour, and Schapire (2015), and also are able to handle an infinite\nhypothesis class $H$. The sample complexity is improved from\n$O(\\frac{1}{\\epsilon^4}\\log(\\frac{|H|}{\\delta}))$ to\n$O(\\frac{1}{\\epsilon^2}(k\\log(k)VC(H)+\\log\\frac{1}{\\delta}))$. Additionally, we\nextend the algorithm and generalization bound from the binary to the multiclass\nand real-valued cases. Along the way, we obtain results on fat-shattering\ndimension and Rademacher complexity of $k$-fold maxima over function classes;\nthese may be of independent interest.\n  For binary classification, the algorithm of Feige et al. (2015) uses a regret\nminimization algorithm and an ERM oracle as a blackbox; we adapt it for the\nmulti-class and regression settings. The algorithm provides us with\nnear-optimal policies for the players on a given training sample.\n"], ["2018-10-04", "http://arxiv.org/abs/1810.02424", "Feature Prioritization and Regularization Improve Standard Accuracy and Adversarial Robustness.", ["Chihuang Liu", " Joseph JaJa"], "  Adversarial training has been successfully applied to build robust models at\na certain cost. While the robustness of a model increases, the standard\nclassification accuracy declines. This phenomenon is suggested to be an\ninherent trade-off. We propose a model that employs feature prioritization by a\nnonlinear attention module and $L_2$ feature regularization to improve the\nadversarial robustness and the standard accuracy relative to adversarial\ntraining. The attention module encourages the model to rely heavily on robust\nfeatures by assigning larger weights to them while suppressing non-robust\nfeatures. The regularizer encourages the model to extract similar features for\nthe natural and adversarial images, effectively ignoring the added\nperturbation. In addition to evaluating the robustness of our model, we provide\njustification for the attention module and propose a novel experimental\nstrategy that quantitatively demonstrates that our model is almost ideally\naligned with salient data characteristics. Additional experimental results\nillustrate the power of our model relative to the state of the art methods.\n"], ["2018-10-02", "http://arxiv.org/abs/1810.01407", "Can Adversarially Robust Learning Leverage Computational Hardness?.", ["Saeed Mahloujifar", " Mohammad Mahmoody"], "  Making learners robust to adversarial perturbation at test time (i.e.,\nevasion attacks) or training time (i.e., poisoning attacks) has emerged as a\nchallenging task. It is known that for some natural settings, sublinear\nperturbations in the training phase or the testing phase can drastically\ndecrease the quality of the predictions. These negative results, however, are\ninformation theoretic and only prove the existence of such successful\nadversarial perturbations. A natural question for these settings is whether or\nnot we can make classifiers computationally robust to polynomial-time attacks.\n  In this work, we prove strong barriers against achieving such envisioned\ncomputational robustness both for evasion and poisoning attacks. In particular,\nwe show that if the test instances come from a product distribution (e.g.,\nuniform over $\\{0,1\\}^n$ or $[0,1]^n$, or isotropic $n$-variate Gaussian) and\nthat there is an initial constant error, then there exists a polynomial-time\nattack that finds adversarial examples of Hamming distance $O(\\sqrt n)$. For\npoisoning attacks, we prove that for any learning algorithm with sample\ncomplexity $m$ and any efficiently computable \"predicate\" defining some \"bad\"\nproperty $B$ for the produced hypothesis (e.g., failing on a particular test)\nthat happens with an initial constant probability, there exist polynomial-time\nonline poisoning attacks that tamper with $O (\\sqrt m)$ many examples, replace\nthem with other correctly labeled examples, and increases the probability of\nthe bad event $B$ to $\\approx 1$.\n  Both of our poisoning and evasion attacks are black-box in how they access\ntheir corresponding components of the system (i.e., the hypothesis, the\nconcept, and the learning algorithm) and make no further assumptions about the\nclassifier or the learning algorithm producing the classifier.\n"], ["2018-10-02", "http://arxiv.org/abs/1810.01185", "Adversarial Examples - A Complete Characterisation of the Phenomenon.", ["Alexandru Constantin Serban", " Erik Poll", " Joost Visser"], "  We provide a complete characterisation of the phenomenon of adversarial\nexamples - inputs intentionally crafted to fool machine learning models. We aim\nto cover all the important concerns in this field of study: (1) the conjectures\non the existence of adversarial examples, (2) the security, safety and\nrobustness implications, (3) the methods used to generate and (4) protect\nagainst adversarial examples and (5) the ability of adversarial examples to\ntransfer between different machine learning models. We provide ample background\ninformation in an effort to make this document self-contained. Therefore, this\ndocument can be used as survey, tutorial or as a catalog of attacks and\ndefences using adversarial examples.\n"], ["2018-10-02", "http://arxiv.org/abs/1810.01110", "Link Prediction Adversarial Attack.", ["Jinyin Chen", " Ziqiang Shi", " Yangyang Wu", " Xuanheng Xu", " Haibin Zheng"], "  Deep neural network has shown remarkable performance in solving computer\nvision and some graph evolved tasks, such as node classification and link\nprediction. However, the vulnerability of deep model has also been revealed by\ncarefully designed adversarial examples generated by various adversarial attack\nmethods. With the wider application of deep model in complex network analysis,\nin this paper we define and formulate the link prediction adversarial attack\nproblem and put forward a novel iterative gradient attack (IGA) based on the\ngradient information in trained graph auto-encoder (GAE). To our best\nknowledge, it is the first time link prediction adversarial attack problem is\ndefined and attack method is brought up. Not surprisingly, GAE was easily\nfooled by adversarial network with only a few links perturbed on the clean\nnetwork. By conducting comprehensive experiments on different real-world data\nsets, we can conclude that most deep model based and other state-of-art link\nprediction algorithms cannot escape the adversarial attack just like GAE. We\ncan benefit the attack as an efficient privacy protection tool from link\nprediction unknown violation, on the other hand, link prediction attack can be\na robustness evaluation metric for current link prediction algorithm in attack\ndefensibility.\n"], ["2018-10-01", "http://arxiv.org/abs/1810.01279", "Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network.", ["Xuanqing Liu", " Yao Li", " Chongruo Wu", " Cho-Jui Hsieh"], "  We present a new algorithm to train a robust neural network against\nadversarial attacks. Our algorithm is motivated by the following two ideas.\nFirst, although recent work has demonstrated that fusing randomness can improve\nthe robustness of neural networks (Liu 2017), we noticed that adding noise\nblindly to all the layers is not the optimal way to incorporate randomness.\nInstead, we model randomness under the framework of Bayesian Neural Network\n(BNN) to formally learn the posterior distribution of models in a scalable way.\nSecond, we formulate the mini-max problem in BNN to learn the best model\ndistribution under adversarial attacks, leading to an adversarial-trained\nBayesian neural net. Experiment results demonstrate that the proposed algorithm\nachieves state-of-the-art performance under strong attacks. On CIFAR-10 with\nVGG network, our model leads to 14\\% accuracy improvement compared with\nadversarial training (Madry 2017) and random self-ensemble (Liu 2017) under PGD\nattack with $0.035$ distortion, and the gap becomes even larger on a subset of\nImageNet.\n"], ["2018-10-01", "http://arxiv.org/abs/1810.01021", "Large batch size training of neural networks with adversarial training and second-order information.", ["Zhewei Yao", " Amir Gholami", " Kurt Keutzer", " Michael Mahoney"], "  The most straightforward method to accelerate Stochastic Gradient Descent\n(SGD) is to distribute the randomly selected batch of inputs over multiple\nprocessors. To keep the distributed processors fully utilized requires\ncommensurately growing the batch size; however, large batch training usually\nleads to poor generalization. Existing solutions for large batch training\neither significantly degrade accuracy or require massive hyper-parameter\ntuning. To address this issue, we propose a novel large batch training method\nwhich combines recent results in adversarial training and second order\ninformation. We extensively evaluate our method on Cifar-10/100, SVHN,\nTinyImageNet, and ImageNet datasets, using multiple NNs, including residual\nnetworks as well as smaller networks such as SqueezeNext. Our new approach\nexceeds the performance of the existing solutions in terms of both accuracy and\nthe number of SGD iterations (up to 1\\% and $5\\times$, respectively). We\nemphasize that this is achieved without any additional hyper-parameter tuning\nto tailor our proposed method in any of these experiments. With slight\nhyper-parameter tuning, our method can reduce the number of SGD iterations of\nResNet18 on Cifar-10/ImageNet to $44.8\\times$ and $28.8\\times$, respectively.\nWe have open sourced the method including tools for computing Hessian spectrum.\n"], ["2018-10-01", "http://arxiv.org/abs/1810.00740", "Improving the Generalization of Adversarial Training with Domain Adaptation.", ["Chuanbiao Song", " Kun He", " Liwei Wang", " John E. Hopcroft"], "  By injecting adversarial examples into training data, adversarial training is\npromising for improving the robustness of deep learning models. However, most\nexisting adversarial training approaches are based on a specific type of\nadversarial attack. It may not provide sufficiently representative samples from\nthe adversarial domain, leading to a weak generalization ability on adversarial\nexamples from other attacks. Moreover, during the adversarial training,\nadversarial perturbations on inputs are usually crafted by fast single-step\nadversaries so as to scale to large datasets. This work is mainly focused on\nthe adversarial training yet efficient FGSM adversary. In this scenario, it is\ndifficult to train a model with great generalization due to the lack of\nrepresentative adversarial samples, aka the samples are unable to accurately\nreflect the adversarial domain. To alleviate this problem, we propose a novel\nAdversarial Training with Domain Adaptation (ATDA) method. Our intuition is to\nregard the adversarial training on FGSM adversary as a domain adaption task\nwith limited number of target domain samples. The main idea is to learn a\nrepresentation that is semantically meaningful and domain invariant on the\nclean domain as well as the adversarial domain. Empirical evaluations on\nFashion-MNIST, SVHN, CIFAR-10 and CIFAR-100 demonstrate that ATDA can greatly\nimprove the generalization of adversarial training and the smoothness of the\nlearned models, and outperforms state-of-the-art methods on standard benchmark\ndatasets. To show the transfer ability of our method, we also extend ATDA to\nthe adversarial training on iterative attacks such as PGD-Adversial Training\n(PAT) and the defense performance is improved considerably.\n"], ["2018-10-01", "http://arxiv.org/abs/1810.00953", "Improved robustness to adversarial examples using Lipschitz regularization of the loss.", ["Chris Finlay", " Adam Oberman", " Bilal Abbasi"], "  We augment adversarial training (AT) with worst case adversarial training\n(WCAT) which improves adversarial robustness by 11% over the current\nstate-of-the-art result in the $\\ell_2$ norm on CIFAR-10. We obtain verifiable\naverage case and worst case robustness guarantees, based on the expected and\nmaximum values of the norm of the gradient of the loss. We interpret\nadversarial training as Total Variation Regularization, which is a fundamental\ntool in mathematical image processing, and WCAT as Lipschitz regularization.\n"], ["2018-09-30", "http://arxiv.org/abs/1810.00470", "Procedural Noise Adversarial Examples for Black-Box Attacks on Deep Neural Networks.", ["Kenneth T. Co", " Luis Mu\u00f1oz-Gonz\u00e1lez", " Emil C. Lupu"], "  Deep neural networks have been shown to be vulnerable to adversarial\nexamples, perturbed inputs that are designed specifically to produce\nintentional errors in the learning algorithms. However, existing attacks are\neither computationally expensive or require extensive knowledge of the target\nmodel and its dataset to succeed. Hence, these methods are not practical in a\ndeployed adversarial setting. In this paper we introduce an exploratory\napproach for generating adversarial examples using procedural noise. We show\nthat it is possible to construct practical black-box attacks with low\ncomputational cost against robust neural network architectures such as\nInception v3 and Inception ResNet v2 on the ImageNet dataset. We show that\nthese attacks successfully cause misclassification with a low number of\nqueries, significantly outperforming state-of-the-art black box attacks. Our\nattack demonstrates the fragility of these neural networks to Perlin noise, a\ntype of procedural noise used for generating realistic textures. Perlin noise\nattacks achieve at least 90% top 1 error across all classifiers. More\nworryingly, we show that most Perlin noise perturbations are \"universal\" in\nthat they generalize, as adversarial examples, across large portions of the\ndataset, with up to 73% of images misclassified using a single perturbation.\nThese findings suggest a systemic fragility of DNNs that needs to be explored\nfurther. We also show the limitations of adversarial training, a technique used\nto enhance the robustness against adversarial examples. Thus, the attacker just\nneeds to change the perspective to generate the adversarial examples to craft\nsuccessful attacks and, for the defender, it is difficult to foresee a priori\nall possible types of adversarial perturbations.\n"], ["2018-09-29", "http://arxiv.org/abs/1810.01268", "CAAD 2018: Generating Transferable Adversarial Examples.", ["Yash Sharma", " Tien-Dung Le", " Moustafa Alzantot"], "  Deep neural networks (DNNs) are vulnerable to adversarial examples,\nperturbations carefully crafted to fool the targeted DNN, in both the\nnon-targeted and targeted case. In the non-targeted case, the attacker simply\naims to induce misclassification. In the targeted case, the attacker aims to\ninduce classification to a specified target class. In addition, it has been\nobserved that strong adversarial examples can transfer to unknown models,\nyielding a serious security concern. The NIPS 2017 competition was organized to\naccelerate research in adversarial attacks and defenses, taking place in the\nrealistic setting where submitted adversarial attacks attempt to transfer to\nsubmitted defenses. The CAAD 2018 competition took place with nearly identical\nrules to the NIPS 2017 one. Given the requirement that the NIPS 2017\nsubmissions were to be open-sourced, participants in the CAAD 2018 competition\nwere able to directly build upon previous solutions, and thus improve the\nstate-of-the-art in this setting. Our team participated in the CAAD 2018\ncompetition, and won 1st place in both attack subtracks, non-targeted and\ntargeted adversarial attacks, and 3rd place in defense. We outline our\nsolutions and development results in this article. We hope our results can\ninform researchers in both generating and defending against adversarial\nexamples.\n"], ["2018-09-29", "http://arxiv.org/abs/1810.00208", "To compress or not to compress: Understanding the Interactions between Adversarial Attacks and Neural Network Compression.", ["Yiren Zhao", " Ilia Shumailov", " Robert Mullins", " Ross Anderson"], "  As deep neural networks (DNNs) become widely used, pruned and quantised\nmodels are becoming ubiquitous on edge devices; such compressed DNNs are\npopular for lowering computational requirements. Meanwhile, recent studies show\nthat adversarial samples can be effective at making DNNs misclassify. We,\ntherefore, investigate the extent to which adversarial samples are transferable\nbetween uncompressed and compressed DNNs. We find that adversarial samples\nremain transferable for both pruned and quantised models. For pruning, the\nadversarial samples generated from heavily pruned models remain effective on\nuncompressed models. For quantisation, we find the transferability of\nadversarial samples is highly sensitive to integer precision.\n"], ["2018-09-29", "http://arxiv.org/abs/1810.00144", "Interpreting Adversarial Robustness: A View from Decision Surface in Input Space.", ["Fuxun Yu", " Chenchen Liu", " Yanzhi Wang", " Liang Zhao", " Xiang Chen"], "  One popular hypothesis of neural network generalization is that the flat\nlocal minima of loss surface in parameter space leads to good generalization.\nHowever, we demonstrate that loss surface in parameter space has no obvious\nrelationship with generalization, especially under adversarial settings.\nThrough visualizing decision surfaces in both parameter space and input space,\nwe instead show that the geometry property of decision surface in input space\ncorrelates well with the adversarial robustness. We then propose an adversarial\nrobustness indicator, which can evaluate a neural network's intrinsic\nrobustness property without testing its accuracy under adversarial attacks.\nGuided by it, we further propose our robust training method. Without involving\nadversarial training, our method could enhance network's intrinsic adversarial\nrobustness against various adversarial attacks.\n"], ["2018-09-28", "http://arxiv.org/abs/1809.10875", "Characterizing Audio Adversarial Examples Using Temporal Dependency.", ["Zhuolin Yang", " Bo Li", " Pin-Yu Chen", " Dawn Song"], "  Recent studies have highlighted adversarial examples as a ubiquitous threat\nto different neural network models and many downstream applications.\nNonetheless, as unique data properties have inspired distinct and powerful\nlearning principles, this paper aims to explore their potentials towards\nmitigating adversarial inputs. In particular, our results reveal the importance\nof using the temporal dependency in audio data to gain discriminate power\nagainst adversarial examples. Tested on the automatic speech recognition (ASR)\ntasks and three recent audio adversarial attacks, we find that (i) input\ntransformation developed from image adversarial defense provides limited\nrobustness improvement and is subtle to advanced attacks; (ii) temporal\ndependency can be exploited to gain discriminative power against audio\nadversarial examples and is resistant to adaptive attacks considered in our\nexperiments. Our results not only show promising means of improving the\nrobustness of ASR systems, but also offer novel insights in exploiting\ndomain-specific data properties to mitigate negative effects of adversarial\nexamples.\n"], ["2018-09-28", "http://arxiv.org/abs/1810.00069", "Adversarial Attacks and Defences: A Survey.", ["Anirban Chakraborty", " Manaar Alam", " Vishal Dey", " Anupam Chattopadhyay", " Debdeep Mukhopadhyay"], "  Deep learning has emerged as a strong and efficient framework that can be\napplied to a broad spectrum of complex learning problems which were difficult\nto solve using the traditional machine learning techniques in the past. In the\nlast few years, deep learning has advanced radically in such a way that it can\nsurpass human-level performance on a number of tasks. As a consequence, deep\nlearning is being extensively used in most of the recent day-to-day\napplications. However, security of deep learning systems are vulnerable to\ncrafted adversarial examples, which may be imperceptible to the human eye, but\ncan lead the model to misclassify the output. In recent times, different types\nof adversaries based on their threat model leverage these vulnerabilities to\ncompromise a deep learning system where adversaries have high incentives.\nHence, it is extremely important to provide robustness to deep learning\nalgorithms against these adversaries. However, there are only a few strong\ncountermeasures which can be used in all types of attack scenarios to design a\nrobust deep learning system. In this paper, we attempt to provide a detailed\ndiscussion on different types of adversarial attacks with various threat models\nand also elaborate the efficiency and challenges of recent countermeasures\nagainst them.\n"], ["2018-09-28", "http://arxiv.org/abs/1810.00024", "Explainable Black-Box Attacks Against Model-based Authentication.", ["Washington Garcia", " Joseph I. Choi", " Suman K. Adari", " Somesh Jha", " Kevin R. B. Butler"], "  Establishing unique identities for both humans and end systems has been an\nactive research problem in the security community, giving rise to innovative\nmachine learning-based authentication techniques. Although such techniques\noffer an automated method to establish identity, they have not been vetted\nagainst sophisticated attacks that target their core machine learning\ntechnique. This paper demonstrates that mimicking the unique signatures\ngenerated by host fingerprinting and biometric authentication systems is\npossible. We expose the ineffectiveness of underlying machine learning\nclassification models by constructing a blind attack based around the query\nsynthesis framework and utilizing Explainable-AI (XAI) techniques. We launch an\nattack in under 130 queries on a state-of-the-art face authentication system,\nand under 100 queries on a host authentication system. We examine how these\nattacks can be defended against and explore their limitations. XAI provides an\neffective means for adversaries to infer decision boundaries and provides a new\nway forward in constructing attacks against systems using machine learning\nmodels for authentication.\n"], ["2018-09-26", "http://arxiv.org/abs/1810.07242", "Adversarial Attacks on Cognitive Self-Organizing Networks: The Challenge and the Way Forward.", ["Muhammad Usama", " Junaid Qadir", " Ala Al-Fuqaha"], "  Future communications and data networks are expected to be largely cognitive\nself-organizing networks (CSON). Such networks will have the essential property\nof cognitive self-organization, which can be achieved using machine learning\ntechniques (e.g., deep learning). Despite the potential of these techniques,\nthese techniques in their current form are vulnerable to adversarial attacks\nthat can cause cascaded damages with detrimental consequences for the whole\nnetwork. In this paper, we explore the effect of adversarial attacks on CSON.\nOur experiments highlight the level of threat that CSON have to deal with in\norder to meet the challenges of next-generation networks and point out\npromising directions for future work.\n"], ["2018-09-24", "http://arxiv.org/abs/1809.09262", "Neural Networks with Structural Resistance to Adversarial Attacks.", ["Alfaro Luca de"], "  In adversarial attacks to machine-learning classifiers, small perturbations\nare added to input that is correctly classified. The perturbations yield\nadversarial examples, which are virtually indistinguishable from the\nunperturbed input, and yet are misclassified. In standard neural networks used\nfor deep learning, attackers can craft adversarial examples from most input to\ncause a misclassification of their choice.\n  We introduce a new type of network units, called RBFI units, whose non-linear\nstructure makes them inherently resistant to adversarial attacks. On\npermutation-invariant MNIST, in absence of adversarial attacks, networks using\nRBFI units match the performance of networks using sigmoid units, and are\nslightly below the accuracy of networks with ReLU units. When subjected to\nadversarial attacks, networks with RBFI units retain accuracies above 90% for\nattacks that degrade the accuracy of networks with ReLU or sigmoid units to\nbelow 2%. RBFI networks trained with regular input are superior in their\nresistance to adversarial attacks even to ReLU and sigmoid networks trained\nwith the help of adversarial examples.\n  The non-linear structure of RBFI units makes them difficult to train using\nstandard gradient descent. We show that networks of RBFI units can be\nefficiently trained to high accuracies using pseudogradients, computed using\nfunctions especially crafted to facilitate learning instead of their true\nderivatives. We show that the use of pseudogradients makes training deep RBFI\nnetworks practical, and we compare several structural alternatives of RBFI\nnetworks for their accuracy.\n"], ["2018-09-24", "http://arxiv.org/abs/1809.08999", "Fast Geometrically-Perturbed Adversarial Faces.", ["Ali Dabouei", " Sobhan Soleymani", " Jeremy Dawson", " Nasser M. Nasrabadi"], "  The state-of-the-art performance of deep learning algorithms has led to a\nconsiderable increase in the utilization of machine learning in\nsecurity-sensitive and critical applications. However, it has recently been\nshown that a small and carefully crafted perturbation in the input space can\ncompletely fool a deep model. In this study, we explore the extent to which\nface recognition systems are vulnerable to geometrically-perturbed adversarial\nfaces. We propose a fast landmark manipulation method for generating\nadversarial faces, which is approximately 200 times faster than the previous\ngeometric attacks and obtains 99.86% success rate on the state-of-the-art face\nrecognition models. To further force the generated samples to be natural, we\nintroduce a second attack constrained on the semantic structure of the face\nwhich has the half speed of the first attack with the success rate of 99.96%.\nBoth attacks are extremely robust against the state-of-the-art defense methods\nwith the success rate of equal or greater than 53.59%. Code is available at\nhttps://github.com/alldbi/FLM\n"], ["2018-09-24", "http://arxiv.org/abs/1809.08986", "On The Utility of Conditional Generation Based Mutual Information for Characterizing Adversarial Subspaces.", ["Chia-Yi Hsu", " Pei-Hsuan Lu", " Pin-Yu Chen", " Chia-Mu Yu"], "  Recent studies have found that deep learning systems are vulnerable to\nadversarial examples; e.g., visually unrecognizable adversarial images can\neasily be crafted to result in misclassification. The robustness of neural\nnetworks has been studied extensively in the context of adversary detection,\nwhich compares a metric that exhibits strong discriminate power between natural\nand adversarial examples. In this paper, we propose to characterize the\nadversarial subspaces through the lens of mutual information (MI) approximated\nby conditional generation methods. We use MI as an information-theoretic metric\nto strengthen existing defenses and improve the performance of adversary\ndetection. Experimental results on MagNet defense demonstrate that our proposed\nMI detector can strengthen its robustness against powerful adversarial attacks.\n"], ["2018-09-24", "http://arxiv.org/abs/1809.08758", "Low Frequency Adversarial Perturbation.", ["Chuan Guo", " Jared S. Frank", " Kilian Q. Weinberger"], "  Adversarial images aim to change a target model's decision by minimally\nperturbing a target image. In the black-box setting, the absence of gradient\ninformation often renders this search problem costly in terms of query\ncomplexity. In this paper we propose to restrict the search for adversarial\nimages to a low frequency domain. This approach is readily compatible with many\nexisting black-box attack frameworks and consistently reduces their query cost\nby 2 to 4 times. Further, we can circumvent image transformation defenses even\nwhen both the model and the defense strategy are unknown. Finally, we\ndemonstrate the efficacy of this technique by fooling the Google Cloud Vision\nplatform with an unprecedented low number of model queries.\n"], ["2018-09-23", "http://arxiv.org/abs/1809.08706", "Is Ordered Weighted $\\ell_1$ Regularized Regression Robust to Adversarial Perturbation? A Case Study on OSCAR.", ["Pin-Yu Chen", " Bhanukiran Vinzamuri", " Sijia Liu"], "  Many state-of-the-art machine learning models such as deep neural networks\nhave recently shown to be vulnerable to adversarial perturbations, especially\nin classification tasks. Motivated by adversarial machine learning, in this\npaper we investigate the robustness of sparse regression models with strongly\ncorrelated covariates to adversarially designed measurement noises.\nSpecifically, we consider the family of ordered weighted $\\ell_1$ (OWL)\nregularized regression methods and study the case of OSCAR (octagonal shrinkage\nclustering algorithm for regression) in the adversarial setting. Under a\nnorm-bounded threat model, we formulate the process of finding a maximally\ndisruptive noise for OWL-regularized regression as an optimization problem and\nillustrate the steps towards finding such a noise in the case of OSCAR.\nExperimental results demonstrate that the regression performance of grouping\nstrongly correlated features can be severely degraded under our adversarial\nsetting, even when the noise budget is significantly smaller than the\nground-truth signals.\n"], ["2018-09-22", "http://arxiv.org/abs/1809.08516", "Adversarial Defense via Data Dependent Activation Function and Total Variation Minimization.", ["Bao Wang", " Alex T. Lin", " Zuoqiang Shi", " Wei Zhu", " Penghang Yin", " Andrea L. Bertozzi", " Stanley J. Osher"], "  We improve the robustness of deep neural nets to adversarial attacks by using\nan interpolating function as the output activation. This data-dependent\nactivation remarkably improves both generalization and robustness towards\nadversarial attacks. In the CIFAR10 benchmark, we raise the accuracy of the\nProjected Gradient Descent adversarial training from $\\sim 46\\%$ to $\\sim 69\\%$\nfor ResNet20. When we combine this data-dependent activation with total\nvariation minimization on adversarial images and training data augmentation, we\nachieve an improvement in accuracy by 38.9$\\%$ for ResNet56 under the strongest\nattack of the Iterative Fast Gradient Sign Method. We further provide an\nintuitive explanation of our defense by analyzing the geometry of the feature\nspace. For reproducibility, the code is made available at\n\\url{https://github.com/BaoWangMath/DNN-DataDependentActivation}.\n"], ["2018-09-21", "http://arxiv.org/abs/1809.08352", "Unrestricted Adversarial Examples.", ["Tom B. Brown", " Nicholas Carlini", " Chiyuan Zhang", " Catherine Olsson", " Paul Christiano", " Ian Goodfellow"], "  We introduce a two-player contest for evaluating the safety and robustness of\nmachine learning systems, with a large prize pool. Unlike most prior work in ML\nrobustness, which studies norm-constrained adversaries, we shift our focus to\nunconstrained adversaries. Defenders submit machine learning models, and try to\nachieve high accuracy and coverage on non-adversarial data while making no\nconfident mistakes on adversarial inputs. Attackers try to subvert defenses by\nfinding arbitrary unambiguous inputs where the model assigns an incorrect label\nwith high confidence. We propose a simple unambiguous dataset (\"bird-or-\nbicycle\") to use as part of this contest. We hope this contest will help to\nmore comprehensively evaluate the worst-case adversarial risk of machine\nlearning models.\n"], ["2018-09-21", "http://arxiv.org/abs/1809.08316", "Adversarial Binaries for Authorship Identification.", ["Xiaozhu Meng", " Barton P. Miller", " Somesh Jha"], "  Binary code authorship identification determines authors of a binary program.\nExisting techniques have used supervised machine learning for this task. In\nthis paper, we look this problem from an attacker's perspective. We aim to\nmodify a test binary, such that it not only causes misprediction but also\nmaintains the functionality of the original input binary. Attacks against\nbinary code are intrinsically more difficult than attacks against domains such\nas computer vision, where attackers can change each pixel of the input image\nindependently and still maintain a valid image. For binary code, even flipping\none bit of a binary may cause the binary to be invalid, to crash at the\nrun-time, or to lose the original functionality. We investigate two types of\nattacks: untargeted attacks, causing misprediction to any of the incorrect\nauthors, and targeted attacks, causing misprediction to a specific one among\nthe incorrect authors. We develop two key attack capabilities: feature vector\nmodification, generating an adversarial feature vector that both corresponds to\na real binary and causes the required misprediction, and input binary\nmodification, modifying the input binary to match the adversarial feature\nvector while maintaining the functionality of the input binary. We evaluated\nour attack against classifiers trained with a state-of-the-art method for\nauthorship attribution. The classifiers for authorship identification have 91%\naccuracy on average. Our untargeted attack has a 96% success rate on average,\nshowing that we can effectively suppress authorship signal. Our targeted attack\nhas a 46% success rate on average, showing that it is possible, but\nsignificantly more difficult to impersonate a specific programmer's style. Our\nattack reveals that existing binary code authorship identification techniques\nrely on code features that are easy to modify, and thus are vulnerable to\nattacks.\n"], ["2018-09-20", "http://arxiv.org/abs/1809.07802", "Playing the Game of Universal Adversarial Perturbations.", ["Julien Perolat", " Mateusz Malinowski", " Bilal Piot", " Olivier Pietquin"], "  We study the problem of learning classifiers robust to universal adversarial\nperturbations. While prior work approaches this problem via robust\noptimization, adversarial training, or input transformation, we instead phrase\nit as a two-player zero-sum game. In this new formulation, both players\nsimultaneously play the same game, where one player chooses a classifier that\nminimizes a classification loss whilst the other player creates an adversarial\nperturbation that increases the same loss when applied to every sample in the\ntraining set. By observing that performing a classification (respectively\ncreating adversarial samples) is the best response to the other player, we\npropose a novel extension of a game-theoretic algorithm, namely fictitious\nplay, to the domain of training robust classifiers. Finally, we empirically\nshow the robustness and versatility of our approach in two defence scenarios\nwhere universal attacks are performed on several image classification datasets\n-- CIFAR10, CIFAR100 and ImageNet.\n"], ["2018-09-19", "http://arxiv.org/abs/1809.08098", "Efficient Formal Safety Analysis of Neural Networks.", ["Shiqi Wang", " Kexin Pei", " Justin Whitehouse", " Junfeng Yang", " Suman Jana"], "  Neural networks are increasingly deployed in real-world safety-critical\ndomains such as autonomous driving, aircraft collision avoidance, and malware\ndetection. However, these networks have been shown to often mispredict on\ninputs with minor adversarial or even accidental perturbations. Consequences of\nsuch errors can be disastrous and even potentially fatal as shown by the recent\nTesla autopilot crash. Thus, there is an urgent need for formal analysis\nsystems that can rigorously check neural networks for violations of different\nsafety properties such as robustness against adversarial perturbations within a\ncertain $L$-norm of a given image. An effective safety analysis system for a\nneural network must be able to either ensure that a safety property is\nsatisfied by the network or find a counterexample, i.e., an input for which the\nnetwork will violate the property. Unfortunately, most existing techniques for\nperforming such analysis struggle to scale beyond very small networks and the\nones that can scale to larger networks suffer from high false positives and\ncannot produce concrete counterexamples in case of a property violation. In\nthis paper, we present a new efficient approach for rigorously checking\ndifferent safety properties of neural networks that significantly outperforms\nexisting approaches by multiple orders of magnitude. Our approach can check\ndifferent safety properties and find concrete counterexamples for networks that\nare 10$\\times$ larger than the ones supported by existing analysis techniques.\nWe believe that our approach to estimating tight output bounds of a network for\na given input range can also help improve the explainability of neural networks\nand guide the training process of more robust neural networks.\n"], ["2018-09-19", "http://arxiv.org/abs/1809.07062", "Adversarial Training Towards Robust Multimedia Recommender System.", ["Jinhui Tang", " Xiaoyu Du", " Xiangnan He", " Fajie Yuan", " Qi Tian", " Tat-Seng Chua"], "  With the prevalence of multimedia content on the Web, developing recommender\nsolutions that can effectively leverage the rich signal in multimedia data is\nin urgent need. Owing to the success of deep neural networks in representation\nlearning, recent advance on multimedia recommendation has largely focused on\nexploring deep learning methods to improve the recommendation accuracy. To\ndate, however, there has been little effort to investigate the robustness of\nmultimedia representation and its impact on the performance of multimedia\nrecommendation.\n  In this paper, we shed light on the robustness of multimedia recommender\nsystem. Using the state-of-the-art recommendation framework and deep image\nfeatures, we demonstrate that the overall system is not robust, such that a\nsmall (but purposeful) perturbation on the input image will severely decrease\nthe recommendation accuracy. This implies the possible weakness of multimedia\nrecommender system in predicting user preference, and more importantly, the\npotential of improvement by enhancing its robustness. To this end, we propose a\nnovel solution named Adversarial Multimedia Recommendation (AMR), which can\nlead to a more robust multimedia recommender model by using adversarial\nlearning. The idea is to train the model to defend an adversary, which adds\nperturbations to the target image with the purpose of decreasing the model's\naccuracy. We conduct experiments on two representative multimedia\nrecommendation tasks, namely, image recommendation and visually-aware product\nrecommendation. Extensive results verify the positive effect of adversarial\nlearning and demonstrate the effectiveness of our AMR method. Source codes are\navailable in https://github.com/duxy-me/AMR.\n"], ["2018-09-19", "http://arxiv.org/abs/1809.07016", "Generating 3D Adversarial Point Clouds.", ["Chong Xiang", " Charles R. Qi", " Bo Li"], "  Deep neural networks are known to be vulnerable to adversarial examples which\nare carefully crafted instances to cause the models to make wrong predictions.\nWhile adversarial examples for 2D images and CNNs have been extensively\nstudied, less attention has been paid to 3D data such as point clouds. Given\nmany safety-critical 3D applications such as autonomous driving, it is\nimportant to study how adversarial point clouds could affect current deep 3D\nmodels. In this work, we propose several novel algorithms to craft adversarial\npoint clouds against PointNet, a widely used deep neural network for point\ncloud processing. Our algorithms work in two ways: adversarial point\nperturbation and adversarial point generation. For point perturbation, we shift\nexisting points negligibly. For point generation, we generate either a set of\nindependent and scattered points or a small number (1-3) of point clusters with\nmeaningful shapes such as balls and airplanes which could be hidden in the\nhuman psyche. In addition, we formulate six perturbation measurement metrics\ntailored to the attacks in point clouds and conduct extensive experiments to\nevaluate the proposed algorithms on the ModelNet40 3D shape classification\ndataset. Overall, our attack algorithms achieve a success rate higher than 99%\nfor all targeted attacks\n"], ["2018-09-17", "http://arxiv.org/abs/1809.06498", "HashTran-DNN: A Framework for Enhancing Robustness of Deep Neural Networks against Adversarial Malware Samples.", ["Deqiang Li", " Ramesh Baral", " Tao Li", " Han Wang", " Qianmu Li", " Shouhuai Xu"], "  Adversarial machine learning in the context of image processing and related\napplications has received a large amount of attention. However, adversarial\nmachine learning, especially adversarial deep learning, in the context of\nmalware detection has received much less attention despite its apparent\nimportance. In this paper, we present a framework for enhancing the robustness\nof Deep Neural Networks (DNNs) against adversarial malware samples, dubbed\nHashing Transformation Deep Neural Networks} (HashTran-DNN). The core idea is\nto use hash functions with a certain locality-preserving property to transform\nsamples to enhance the robustness of DNNs in malware classification. The\nframework further uses a Denoising Auto-Encoder (DAE) regularizer to\nreconstruct the hash representations of samples, making the resulting DNN\nclassifiers capable of attaining the locality information in the latent space.\nWe experiment with two concrete instantiations of the HashTran-DNN framework to\nclassify Android malware. Experimental results show that four known attacks can\nrender standard DNNs useless in classifying Android malware, that known\ndefenses can at most defend three of the four attacks, and that HashTran-DNN\ncan effectively defend against all of the four attacks.\n"], ["2018-09-17", "http://arxiv.org/abs/1809.06452", "Robustness Guarantees for Bayesian Inference with Gaussian Processes.", ["Luca Cardelli", " Marta Kwiatkowska", " Luca Laurenti", " Andrea Patane"], "  Bayesian inference and Gaussian processes are widely used in applications\nranging from robotics and control to biological systems. Many of these\napplications are safety-critical and require a characterization of the\nuncertainty associated with the learning model and formal guarantees on its\npredictions. In this paper we define a robustness measure for Bayesian\ninference against input perturbations, given by the probability that, for a\ntest point and a compact set in the input space containing the test point, the\nprediction of the learning model will remain $\\delta-$close for all the points\nin the set, for $\\delta>0.$ Such measures can be used to provide formal\nguarantees for the absence of adversarial examples. By employing the theory of\nGaussian processes, we derive tight upper bounds on the resulting robustness by\nutilising the Borell-TIS inequality, and propose algorithms for their\ncomputation. We evaluate our techniques on two examples, a GP regression\nproblem and a fully-connected deep neural network, where we rely on weak\nconvergence to GPs to study adversarial examples on the MNIST dataset.\n"], ["2018-09-16", "http://arxiv.org/abs/1809.05962", "Robust Adversarial Perturbation on Deep Proposal-based Models.", ["Yuezun Li", " Daniel Tian", " Mingching-Chang", " Xiao Bian", " Siwei Lyu"], "  Adversarial noises are useful tools to probe the weakness of deep learning\nbased computer vision algorithms. In this paper, we describe a robust\nadversarial perturbation (R-AP) method to attack deep proposal-based object\ndetectors and instance segmentation algorithms. Our method focuses on attacking\nthe common component in these algorithms, namely Region Proposal Network (RPN),\nto universally degrade their performance in a black-box fashion. To do so, we\ndesign a loss function that combines a label loss and a novel shape loss, and\noptimize it with respect to image using a gradient based iterative algorithm.\nEvaluations are performed on the MS COCO 2014 dataset for the adversarial\nattacking of 6 state-of-the-art object detectors and 2 instance segmentation\nalgorithms. Experimental results demonstrate the efficacy of the proposed\nmethod.\n"], ["2018-09-16", "http://arxiv.org/abs/1809.05966", "Exploring the Vulnerability of Single Shot Module in Object Detectors via Imperceptible Background Patches.", ["Yuezun Li", " Xiao Bian", " Ming-ching Chang", " Siwei Lyu"], "  Recent works succeeded to generate adversarial perturbations on the entire\nimage or the object of interests to corrupt CNN based object detectors. In this\npaper, we focus on exploring the vulnerability of the Single Shot Module (SSM)\ncommonly used in recent object detectors, by adding small perturbations to\npatches in the background outside the object. The SSM is referred to the Region\nProposal Network used in a two-stage object detector or the single-stage object\ndetector itself. The SSM is typically a fully convolutional neural network\nwhich generates output in a single forward pass. Due to the excessive\nconvolutions used in SSM, the actual receptive field is larger than the object\nitself. As such, we propose a novel method to corrupt object detectors by\ngenerating imperceptible patches only in the background. Our method can find a\nfew background patches for perturbation, which can effectively decrease true\npositives and dramatically increase false positives. Efficacy is demonstrated\non 5 two-stage object detectors and 8 single-stage object detectors on the MS\nCOCO 2014 dataset. Results indicate that perturbations with small distortions\noutside the bounding box of object region can still severely damage the\ndetection performance.\n"], ["2018-09-13", "http://arxiv.org/abs/1809.05165", "Defensive Dropout for Hardening Deep Neural Networks under Adversarial Attacks.", ["Siyue Wang", " Xiao Wang", " Pu Zhao", " Wujie Wen", " David Kaeli", " Peter Chin", " Xue Lin"], "  Deep neural networks (DNNs) are known vulnerable to adversarial attacks. That\nis, adversarial examples, obtained by adding delicately crafted distortions\nonto original legal inputs, can mislead a DNN to classify them as any target\nlabels. This work provides a solution to hardening DNNs under adversarial\nattacks through defensive dropout. Besides using dropout during training for\nthe best test accuracy, we propose to use dropout also at test time to achieve\nstrong defense effects. We consider the problem of building robust DNNs as an\nattacker-defender two-player game, where the attacker and the defender know\neach others' strategies and try to optimize their own strategies towards an\nequilibrium. Based on the observations of the effect of test dropout rate on\ntest accuracy and attack success rate, we propose a defensive dropout algorithm\nto determine an optimal test dropout rate given the neural network model and\nthe attacker's strategy for generating adversarial examples.We also investigate\nthe mechanism behind the outstanding defense effects achieved by the proposed\ndefensive dropout. Comparing with stochastic activation pruning (SAP), another\ndefense method through introducing randomness into the DNN model, we find that\nour defensive dropout achieves much larger variances of the gradients, which is\nthe key for the improved defense effects (much lower attack success rate). For\nexample, our defensive dropout can reduce the attack success rate from 100% to\n13.89% under the currently strongest attack i.e., C&W attack on MNIST dataset.\n"], ["2018-09-13", "http://arxiv.org/abs/1809.04913", "Query-Efficient Black-Box Attack by Active Learning.", ["Pengcheng Li", " Jinfeng Yi", " Lijun Zhang"], "  Deep neural network (DNN) as a popular machine learning model is found to be\nvulnerable to adversarial attack. This attack constructs adversarial examples\nby adding small perturbations to the raw input, while appearing unmodified to\nhuman eyes but will be misclassified by a well-trained classifier. In this\npaper, we focus on the black-box attack setting where attackers have almost no\naccess to the underlying models. To conduct black-box attack, a popular\napproach aims to train a substitute model based on the information queried from\nthe target DNN. The substitute model can then be attacked using existing\nwhite-box attack approaches, and the generated adversarial examples will be\nused to attack the target DNN. Despite its encouraging results, this approach\nsuffers from poor query efficiency, i.e., attackers usually needs to query a\nhuge amount of times to collect enough information for training an accurate\nsubstitute model. To this end, we first utilize state-of-the-art white-box\nattack methods to generate samples for querying, and then introduce an active\nlearning strategy to significantly reduce the number of queries needed.\nBesides, we also propose a diversity criterion to avoid the sampling bias. Our\nextensive experimental results on MNIST and CIFAR-10 show that the proposed\nmethod can reduce more than $90\\%$ of queries while preserve attacking success\nrates and obtain an accurate substitute model which is more than $85\\%$ similar\nwith the target oracle.\n"], ["2018-09-13", "http://arxiv.org/abs/1809.04790", "Adversarial Examples: Opportunities and Challenges.", ["Jiliang Zhang", " Chen Li"], "  Deep neural networks (DNNs) have shown huge superiority over humans in image\nrecognition, speech processing, autonomous vehicles and medical diagnosis.\nHowever, recent studies indicate that DNNs are vulnerable to adversarial\nexamples (AEs) which are designed by attackers to fool deep learning models.\nDifferent from real examples, AEs can mislead the model to predict incorrect\noutputs while hardly be distinguished by human eyes, therefore threaten\nsecurity-critical deep-learning applications. In recent years, the generation\nand defense of AEs have become a research hotspot in the field of artificial\nintelligence (AI) security. This article reviews the latest research progress\nof AEs. First, we introduce the concept, cause, characteristics and evaluation\nmetrics of AEs, then give a survey on the state-of-the-art AE generation\nmethods with the discussion of advantages and disadvantages. After that, we\nreview the existing defenses and discuss their limitations. Finally, future\nresearch opportunities and challenges on AEs are prospected.\n"], ["2018-09-11", "http://arxiv.org/abs/1809.04098", "On the Structural Sensitivity of Deep Convolutional Networks to the Directions of Fourier Basis Functions.", ["Yusuke Tsuzuku", " Issei Sato"], "  Data-agnostic quasi-imperceptible perturbations on inputs are known to\ndegrade recognition accuracy of deep convolutional networks severely. This\nphenomenon is considered to be a potential security issue. Moreover, some\nresults on statistical generalization guarantees indicate that the phenomenon\ncan be a key to improve the networks' generalization. However, the\ncharacteristics of the shared directions of such harmful perturbations remain\nunknown. Our primal finding is that convolutional networks are sensitive to the\ndirections of Fourier basis functions. We derived the property by specializing\na hypothesis of the cause of the sensitivity, known as the linearity of neural\nnetworks, to convolutional networks and empirically validated it. As a\nby-product of the analysis, we propose an algorithm to create shift-invariant\nuniversal adversarial perturbations available in black-box settings.\n"], ["2018-09-11", "http://arxiv.org/abs/1809.04397", "Isolated and Ensemble Audio Preprocessing Methods for Detecting Adversarial Examples against Automatic Speech Recognition.", ["Krishan Rajaratnam", " Kunal Shah", " Jugal Kalita"], "  An adversarial attack is an exploitative process in which minute alterations\nare made to natural inputs, causing the inputs to be misclassified by neural\nmodels. In the field of speech recognition, this has become an issue of\nincreasing significance. Although adversarial attacks were originally\nintroduced in computer vision, they have since infiltrated the realm of speech\nrecognition. In 2017, a genetic attack was shown to be quite potent against the\nSpeech Commands Model. Limited-vocabulary speech classifiers, such as the\nSpeech Commands Model, are used in a variety of applications, particularly in\ntelephony; as such, adversarial examples produced by this attack pose as a\nmajor security threat. This paper explores various methods of detecting these\nadversarial examples with combinations of audio preprocessing. One particular\ncombined defense incorporating compressions, speech coding, filtering, and\naudio panning was shown to be quite effective against the attack on the Speech\nCommands Model, detecting audio adversarial examples with 93.5% precision and\n91.2% recall.\n"], ["2018-09-11", "http://arxiv.org/abs/1809.04120", "Humans can decipher adversarial images.", ["Zhenglong Zhou", " Chaz Firestone"], "  How similar is the human mind to the sophisticated machine-learning systems\nthat mirror its performance? Models of object categorization based on\nconvolutional neural networks (CNNs) have achieved human-level benchmarks in\nassigning known labels to novel images. These advances promise to support\ntransformative technologies such as autonomous vehicles and machine diagnosis;\nbeyond this, they also serve as candidate models for the visual system itself\n-- not only in their output but perhaps even in their underlying mechanisms and\nprinciples. However, unlike human vision, CNNs can be \"fooled\" by adversarial\nexamples -- carefully crafted images that appear as nonsense patterns to humans\nbut are recognized as familiar objects by machines, or that appear as one\nobject to humans and a different object to machines. This seemingly extreme\ndivergence between human and machine classification challenges the promise of\nthese new advances, both as applied image-recognition systems and also as\nmodels of the human mind. Surprisingly, however, little work has empirically\ninvestigated human classification of such adversarial stimuli: Does human and\nmachine performance fundamentally diverge? Or could humans decipher such images\nand predict the machine's preferred labels? Here, we show that human and\nmachine classification of adversarial stimuli are robustly related: In eight\nexperiments on five prominent and diverse adversarial imagesets, human subjects\nreliably identified the machine's chosen label over relevant foils. This\npattern persisted for images with strong antecedent identities, and even for\nimages described as \"totally unrecognizable to human eyes\". We suggest that\nhuman intuition may be a more reliable guide to machine (mis)classification\nthan has typically been imagined, and we explore the consequences of this\nresult for minds and machines alike.\n"], ["2018-09-09", "http://arxiv.org/abs/1809.03113", "Second-Order Adversarial Attack and Certifiable Robustness.", ["Bai Li", " Changyou Chen", " Wenlin Wang", " Lawrence Carin"], "  We propose a powerful second-order attack method that outperforms existing\nattack methods on reducing the accuracy of state-of-the-art defense models\nbased on adversarial training. The effectiveness of our attack method motivates\nan investigation of provable robustness of a defense model. To this end, we\nintroduce a framework that allows one to obtain a certifiable lower bound on\nthe prediction accuracy against adversarial examples. We conduct experiments to\nshow the effectiveness of our attack method. At the same time, our defense\nmodels obtain higher accuracies compared to previous works under our proposed\nattack.\n"], ["2018-09-09", "http://arxiv.org/abs/1809.03063", "The Curse of Concentration in Robust Learning: Evasion and Poisoning Attacks from Concentration of Measure.", ["Saeed Mahloujifar", " Dimitrios I. Diochnos", " Mohammad Mahmoody"], "  Many modern machine learning classifiers are shown to be vulnerable to\nadversarial perturbations of the instances. Despite a massive amount of work\nfocusing on making classifiers robust, the task seems quite challenging. In\nthis work, through a theoretical study, we investigate the adversarial risk and\nrobustness of classifiers and draw a connection to the well-known phenomenon of\nconcentration of measure in metric measure spaces. We show that if the metric\nprobability space of the test instance is concentrated, any classifier with\nsome initial constant error is inherently vulnerable to adversarial\nperturbations.\n  One class of concentrated metric probability spaces are the so-called Levy\nfamilies that include many natural distributions. In this special case, our\nattacks only need to perturb the test instance by at most $O(\\sqrt n)$ to make\nit misclassified, where $n$ is the data dimension. Using our general result\nabout Levy instance spaces, we first recover as special case some of the\npreviously proved results about the existence of adversarial examples. However,\nmany more Levy families are known (e.g., product distribution under the Hamming\ndistance) for which we immediately obtain new attacks that find adversarial\nexamples of distance $O(\\sqrt n)$.\n  Finally, we show that concentration of measure for product spaces implies the\nexistence of forms of \"poisoning\" attacks in which the adversary tampers with\nthe training data with the goal of degrading the classifier. In particular, we\nshow that for any learning algorithm that uses $m$ training examples, there is\nan adversary who can increase the probability of any \"bad property\" (e.g.,\nfailing on a particular test instance) that initially happens with\nnon-negligible probability to $\\approx 1$ by substituting only $\\tilde{O}(\\sqrt\nm)$ of the examples with other (still correctly labeled) examples.\n"], ["2018-09-09", "http://arxiv.org/abs/1809.03008", "Training for Faster Adversarial Robustness Verification via Inducing ReLU Stability.", ["Kai Y. Xiao", " Vincent Tjeng", " Nur Muhammad Shafiullah", " Aleksander Madry"], "  We explore the concept of co-design in the context of neural network\nverification. Specifically, we aim to train deep neural networks that not only\nare robust to adversarial perturbations but also whose robustness can be\nverified more easily. To this end, we identify two properties of network models\n- weight sparsity and so-called ReLU stability - that turn out to significantly\nimpact the complexity of the corresponding verification task. We demonstrate\nthat improving weight sparsity alone already enables us to turn computationally\nintractable verification problems into tractable ones. Then, improving ReLU\nstability leads to an additional 4-13x speedup in verification times. An\nimportant feature of our methodology is its \"universality,\" in the sense that\nit can be used with a broad range of training procedures and verification\napproaches.\n"], ["2018-09-08", "http://arxiv.org/abs/1809.02918", "Towards Query Efficient Black-box Attacks: An Input-free Perspective.", ["Yali Du", " Meng Fang", " Jinfeng Yi", " Jun Cheng", " Dacheng Tao"], "  Recent studies have highlighted that deep neural networks (DNNs) are\nvulnerable to adversarial attacks, even in a black-box scenario. However, most\nof the existing black-box attack algorithms need to make a huge amount of\nqueries to perform attacks, which is not practical in the real world. We note\none of the main reasons for the massive queries is that the adversarial example\nis required to be visually similar to the original image, but in many cases,\nhow adversarial examples look like does not matter much. It inspires us to\nintroduce a new attack called \\emph{input-free} attack, under which an\nadversary can choose an arbitrary image to start with and is allowed to add\nperceptible perturbations on it. Following this approach, we propose two\ntechniques to significantly reduce the query complexity. First, we initialize\nan adversarial example with a gray color image on which every pixel has roughly\nthe same importance for the target model. Then we shrink the dimension of the\nattack space by perturbing a small region and tiling it to cover the input\nimage. To make our algorithm more effective, we stabilize a projected gradient\nascent algorithm with momentum, and also propose a heuristic approach for\nregion size selection. Through extensive experiments, we show that with only\n1,701 queries on average, we can perturb a gray image to any target class of\nImageNet with a 100\\% success rate on InceptionV3. Besides, our algorithm has\nsuccessfully defeated two real-world systems, the Clarifai food detection API\nand the Baidu Animal Identification API.\n"], ["2018-09-08", "http://arxiv.org/abs/1809.02797", "Fast Gradient Attack on Network Embedding.", ["Jinyin Chen", " Yangyang Wu", " Xuanheng Xu", " Yixian Chen", " Haibin Zheng", " Qi Xuan"], "  Network embedding maps a network into a low-dimensional Euclidean space, and\nthus facilitate many network analysis tasks, such as node classification, link\nprediction and community detection etc, by utilizing machine learning methods.\nIn social networks, we may pay special attention to user privacy, and would\nlike to prevent some target nodes from being identified by such network\nanalysis methods in certain cases. Inspired by successful adversarial attack on\ndeep learning models, we propose a framework to generate adversarial networks\nbased on the gradient information in Graph Convolutional Network (GCN). In\nparticular, we extract the gradient of pairwise nodes based on the adversarial\nnetwork, and select the pair of nodes with maximum absolute gradient to realize\nthe Fast Gradient Attack (FGA) and update the adversarial network. This process\nis implemented iteratively and terminated until certain condition is satisfied,\ni.e., the number of modified links reaches certain predefined value.\nComprehensive attacks, including unlimited attack, direct attack and indirect\nattack, are performed on six well-known network embedding methods. The\nexperiments on real-world networks suggest that our proposed FGA behaves better\nthan some baseline methods, i.e., the network embedding can be easily disturbed\nusing FGA by only rewiring few links, achieving state-of-the-art attack\nperformance.\n"], ["2018-09-08", "http://arxiv.org/abs/1809.02786", "Structure-Preserving Transformation: Generating Diverse and Transferable Adversarial Examples.", ["Dan Peng", " Zizhan Zheng", " Xiaofeng Zhang"], "  Adversarial examples are perturbed inputs designed to fool machine learning\nmodels. Most recent works on adversarial examples for image classification\nfocus on directly modifying pixels with minor perturbations. A common\nrequirement in all these works is that the malicious perturbations should be\nsmall enough (measured by an L_p norm for some p) so that they are\nimperceptible to humans. However, small perturbations can be unnecessarily\nrestrictive and limit the diversity of adversarial examples generated. Further,\nan L_p norm based distance metric ignores important structure patterns hidden\nin images that are important to human perception. Consequently, even the minor\nperturbation introduced in recent works often makes the adversarial examples\nless natural to humans. More importantly, they often do not transfer well and\nare therefore less effective when attacking black-box models especially for\nthose protected by a defense mechanism. In this paper, we propose a\nstructure-preserving transformation (SPT) for generating natural and diverse\nadversarial examples with extremely high transferability. The key idea of our\napproach is to allow perceptible deviation in adversarial examples while\nkeeping structure patterns that are central to a human classifier. Empirical\nresults on the MNIST and the fashion-MNIST datasets show that adversarial\nexamples generated by our approach can easily bypass strong adversarial\ntraining. Further, they transfer well to other target models with no loss or\nlittle loss of successful attack rate.\n"], ["2018-09-08", "http://arxiv.org/abs/1809.02861", "Why Do Adversarial Attacks Transfer? Explaining Transferability of Evasion and Poisoning Attacks.", ["Ambra Demontis", " Marco Melis", " Maura Pintor", " Matthew Jagielski", " Battista Biggio", " Alina Oprea", " Cristina Nita-Rotaru", " Fabio Roli"], "  Transferability captures the ability of an attack against a machine-learning\nmodel to be effective against a different, potentially unknown, model.\nEmpirical evidence for transferability has been shown in previous work, but the\nunderlying reasons why an attack transfers or not are not yet well understood.\nIn this paper, we present a comprehensive analysis aimed to investigate the\ntransferability of both test-time evasion and training-time poisoning attacks.\nWe provide a unifying optimization framework for evasion and poisoning attacks,\nand a formal definition of transferability of such attacks. We highlight two\nmain factors contributing to attack transferability: the intrinsic adversarial\nvulnerability of the target model, and the complexity of the surrogate model\nused to optimize the attack. Based on these insights, we define three metrics\nthat impact an attack's transferability. Interestingly, our results derived\nfrom theoretical analysis hold for both evasion and poisoning attacks, and are\nconfirmed experimentally using a wide range of linear and non-linear\nclassifiers and datasets.\n"], ["2018-09-07", "http://arxiv.org/abs/1809.02681", "Open Set Adversarial Examples.", ["Zhedong Zheng", " Liang Zheng", " Zhilan Hu", " Yi Yang"], "  Adversarial examples in recent works target at closed set recognition\nsystems, in which the training and testing classes are identical. In real-world\nscenarios, however, the testing classes may have limited, if any, overlap with\nthe training classes, a problem named open set recognition. To our knowledge,\nthe community does not have a specific design of adversarial examples targeting\nat this practical setting. Arguably, the new setting compromises traditional\nclosed set attack methods in two aspects. First, closed set attack methods are\nbased on classification and target at classification as well, but the open set\nproblem suggests a different task, \\emph{i.e.,} retrieval. It is undesirable\nthat the generation mechanism of closed set recognition is different from the\naim of open set recognition. Second, given that the query image is usually of\nan unseen class, predicting its category from the training classes is not\nreasonable, which leads to an inferior adversarial gradient. In this work, we\nview open set recognition as a retrieval task and propose a new approach,\nOpposite-Direction Feature Attack (ODFA), to generate adversarial examples /\nqueries. When using an attacked example as query, we aim that the true matches\nbe ranked as low as possible. In addressing the two limitations of closed set\nattack methods, ODFA directly works on the features for retrieval. The idea is\nto push away the feature of the adversarial query in the opposite direction of\nthe original feature. Albeit simple, ODFA leads to a larger drop in Recall@K\nand mAP than the close-set attack methods on two open set recognition datasets,\n\\emph{i.e.,} Market-1501 and CUB-200-2011. We also demonstrate that the attack\nperformance of ODFA is not evidently superior to the state-of-the-art methods\nunder closed set recognition (Cifar-10), suggesting its specificity for open\nset problems.\n"], ["2018-09-07", "http://arxiv.org/abs/1809.02560", "A Deeper Look at 3D Shape Classifiers.", ["Jong-Chyi Su", " Matheus Gadelha", " Rui Wang", " Subhransu Maji"], "  We investigate the role of representations and architectures for classifying\n3D shapes in terms of their computational efficiency, generalization, and\nrobustness to adversarial transformations. By varying the number of training\nexamples and employing cross-modal transfer learning we study the role of\ninitialization of existing deep architectures for 3D shape classification. Our\nanalysis shows that multiview methods continue to offer the best generalization\neven without pretraining on large labeled image datasets, and even when trained\non simplified inputs such as binary silhouettes. Furthermore, the performance\nof voxel-based 3D convolutional networks and point-based architectures can be\nimproved via cross-modal transfer from image representations. Finally, we\nanalyze the robustness of 3D shape classifiers to adversarial transformations\nand present a novel approach for generating adversarial perturbations of a 3D\nshape for multiview classifiers using a differentiable renderer. We find that\npoint-based networks are more robust to point position perturbations while\nvoxel-based and multiview networks are easily fooled with the addition of\nimperceptible noise to the input.\n"], ["2018-09-07", "http://arxiv.org/abs/1809.02444", "Metamorphic Relation Based Adversarial Attacks on Differentiable Neural Computer.", ["Alvin Chan", " Lei Ma", " Felix Juefei-Xu", " Xiaofei Xie", " Yang Liu", " Yew Soon Ong"], "  Deep neural networks (DNN), while becoming the driving force of many novel\ntechnology and achieving tremendous success in many cutting-edge applications,\nare still vulnerable to adversarial attacks. Differentiable neural computer\n(DNC) is a novel computing machine with DNN as its central controller operating\non an external memory module for data processing. The unique architecture of\nDNC contributes to its state-of-the-art performance in tasks which requires the\nability to represent variables and data structure as well as to store data over\nlong timescales. However, there still lacks a comprehensive study on how\nadversarial examples affect DNC in terms of robustness. In this paper, we\npropose metamorphic relation based adversarial techniques for a range of tasks\ndescribed in the natural processing language domain. We show that the\nnear-perfect performance of the DNC in bAbI logical question answering tasks\ncan be degraded by adversarially injected sentences. We further perform\nin-depth study on the role of DNC's memory size in its robustness and analyze\nthe potential reason causing why DNC fails. Our study demonstrates the current\nchallenges and potential opportunities towards constructing more robust DNCs.\n"], ["2018-09-07", "http://arxiv.org/abs/1809.02701", "Trick Me If You Can: Human-in-the-loop Generation of Adversarial Examples for Question Answering.", ["Eric Wallace", " Pedro Rodriguez", " Shi Feng", " Ikuya Yamada", " Jordan Boyd-Graber"], "  Adversarial evaluation stress tests a model's understanding of natural\nlanguage. While past approaches expose superficial patterns, the resulting\nadversarial examples are limited in complexity and diversity. We propose\nhuman-in-the-loop adversarial generation, where human authors are guided to\nbreak models. We aid the authors with interpretations of model predictions\nthrough an interactive user interface. We apply this generation framework to a\nquestion answering task called Quizbowl, where trivia enthusiasts craft\nadversarial questions. The resulting questions are validated via live\nhuman--computer matches: although the questions appear ordinary to humans, they\nsystematically stump neural and information retrieval models. The adversarial\nquestions cover diverse phenomena from multi-hop reasoning to entity type\ndistractors, exposing open challenges in robust question answering.\n"], ["2018-09-06", "http://arxiv.org/abs/1809.02104", "Are adversarial examples inevitable?.", ["Ali Shafahi", " W. Ronny Huang", " Christoph Studer", " Soheil Feizi", " Tom Goldstein"], "  A wide range of defenses have been proposed to harden neural networks against\nadversarial attacks. However, a pattern has emerged in which the majority of\nadversarial defenses are quickly broken by new attacks. Given the lack of\nsuccess at generating robust defenses, we are led to ask a fundamental\nquestion: Are adversarial attacks inevitable? This paper analyzes adversarial\nexamples from a theoretical perspective, and identifies fundamental bounds on\nthe susceptibility of a classifier to adversarial attacks. We show that, for\ncertain classes of problems, adversarial examples are inescapable. Using\nexperiments, we explore the implications of theoretical guarantees for\nreal-world problems and discuss how factors such as dimensionality and image\ncomplexity limit a classifier's robustness against adversarial examples.\n"], ["2018-09-06", "http://arxiv.org/abs/1809.02079", "Adversarial Over-Sensitivity and Over-Stability Strategies for Dialogue Models.", ["Tong Niu", " Mohit Bansal"], "  We present two categories of model-agnostic adversarial strategies that\nreveal the weaknesses of several generative, task-oriented dialogue models:\nShould-Not-Change strategies that evaluate over-sensitivity to small and\nsemantics-preserving edits, as well as Should-Change strategies that test if a\nmodel is over-stable against subtle yet semantics-changing modifications. We\nnext perform adversarial training with each strategy, employing a max-margin\napproach for negative generative examples. This not only makes the target\ndialogue model more robust to the adversarial inputs, but also helps it perform\nsignificantly better on the original inputs. Moreover, training on all\nstrategies combined achieves further improvements, achieving a new\nstate-of-the-art performance on the original task (also verified via human\nevaluation). In addition to adversarial training, we also address the\nrobustness task at the model-level, by feeding it subword units as both inputs\nand outputs, and show that the resulting model is equally competitive, requires\nonly 1/4 of the original vocabulary size, and is robust to one of the\nadversarial strategies (to which the original model is vulnerable) even without\nadversarial training.\n"], ["2018-09-06", "http://arxiv.org/abs/1809.02077", "IDSGAN: Generative Adversarial Networks for Attack Generation against Intrusion Detection.", ["Zilong Lin", " Yong Shi", " Zhi Xue"], "  As an important tool in security, the intrusion detection system bears the\nresponsibility of the defense to network attacks performed by malicious\ntraffic. Nowadays, with the help of machine learning algorithms, the intrusion\ndetection system develops rapidly. However, the robustness of this system is\nquestionable when it faces the adversarial attacks. To improve the detection\nsystem, more potential attack approaches should be researched. In this paper, a\nframework of the generative adversarial networks, IDSGAN, is proposed to\ngenerate the adversarial attacks, which can deceive and evade the intrusion\ndetection system. Considering that the internal structure of the detection\nsystem is unknown to attackers, adversarial attack examples perform the\nblack-box attacks against the detection system. IDSGAN leverages a generator to\ntransform original malicious traffic into adversarial malicious traffic. A\ndiscriminator classifies traffic examples and simulates the black-box detection\nsystem. More significantly, we only modify part of the attacks' nonfunctional\nfeatures to guarantee the validity of the intrusion. Based on the dataset\nNSL-KDD, the feasibility of the model is demonstrated to attack many detection\nsystems with different attacks and the excellent results are achieved.\nMoreover, the robustness of IDSGAN is verified by changing the amount of the\nunmodified features.\n"], ["2018-09-06", "http://arxiv.org/abs/1809.01829", "Adversarial Reprogramming of Text Classification Neural Networks.", ["Paarth Neekhara", " Shehzeen Hussain", " Shlomo Dubnov", " Farinaz Koushanfar"], "  Adversarial Reprogramming has demonstrated success in utilizing pre-trained\nneural network classifiers for alternative classification tasks without\nmodification to the original network. An adversary in such an attack scenario\ntrains an additive contribution to the inputs to repurpose the neural network\nfor the new classification task. While this reprogramming approach works for\nneural networks with a continuous input space such as that of images, it is not\ndirectly applicable to neural networks trained for tasks such as text\nclassification, where the input space is discrete. Repurposing such\nclassification networks would require the attacker to learn an adversarial\nprogram that maps inputs from one discrete space to the other. In this work, we\nintroduce a context-based vocabulary remapping model to reprogram neural\nnetworks trained on a specific sequence classification task, for a new sequence\nclassification task desired by the adversary. We propose training procedures\nfor this adversarial program in both white-box and black-box settings. We\ndemonstrate the application of our model by adversarially repurposing various\ntext-classification models including LSTM, bi-directional LSTM and CNN for\nalternate classification tasks.\n"], ["2018-09-05", "http://arxiv.org/abs/1809.01715", "Bridging machine learning and cryptography in defence against adversarial attacks.", ["Olga Taran", " Shideh Rezaeifar", " Slava Voloshynovskiy"], "  In the last decade, deep learning algorithms have become very popular thanks\nto the achieved performance in many machine learning and computer vision tasks.\nHowever, most of the deep learning architectures are vulnerable to so called\nadversarial examples. This questions the security of deep neural networks (DNN)\nfor many security- and trust-sensitive domains. The majority of the proposed\nexisting adversarial attacks are based on the differentiability of the DNN cost\nfunction.Defence strategies are mostly based on machine learning and signal\nprocessing principles that either try to detect-reject or filter out the\nadversarial perturbations and completely neglect the classical cryptographic\ncomponent in the defence. In this work, we propose a new defence mechanism\nbased on the second Kerckhoffs's cryptographic principle which states that the\ndefence and classification algorithm are supposed to be known, but not the key.\nTo be compliant with the assumption that the attacker does not have access to\nthe secret key, we will primarily focus on a gray-box scenario and do not\naddress a white-box one. More particularly, we assume that the attacker does\nnot have direct access to the secret block, but (a) he completely knows the\nsystem architecture, (b) he has access to the data used for training and\ntesting and (c) he can observe the output of the classifier for each given\ninput. We show empirically that our system is efficient against most famous\nstate-of-the-art attacks in black-box and gray-box scenarios.\n"], ["2018-09-04", "http://arxiv.org/abs/1809.01093", "Adversarial Attacks on Node Embeddings.", ["Aleksandar Bojchevski", " Stephan G\u00fcnnemann"], "  The goal of network representation learning is to learn low-dimensional node\nembeddings that capture the graph structure and are useful for solving\ndownstream tasks. However, despite the proliferation of such methods there is\ncurrently no study of their robustness to adversarial attacks. We provide the\nfirst adversarial vulnerability analysis on the widely used family of methods\nbased on random walks. We derive efficient adversarial perturbations that\npoison the network structure and have a negative effect on both the quality of\nthe embeddings and the downstream tasks. We further show that our attacks are\ntransferable -- they generalize to many models -- and are successful even when\nthe attacker has restricted actions.\n"], ["2018-09-03", "http://arxiv.org/abs/1809.01697", "HASP: A High-Performance Adaptive Mobile Security Enhancement Against Malicious Speech Recognition.", ["Zirui Xu", " Fuxun Yu", " Chenchen Liu", " Xiang Chen"], "  Nowadays, machine learning based Automatic Speech Recognition (ASR) technique\nhas widely spread in smartphones, home devices, and public facilities. As\nconvenient as this technology can be, a considerable security issue also raises\n-- the users' speech content might be exposed to malicious ASR monitoring and\ncause severe privacy leakage. In this work, we propose HASP -- a\nhigh-performance security enhancement approach to solve this security issue on\nmobile devices. Leveraging ASR systems' vulnerability to the adversarial\nexamples, HASP is designed to cast human imperceptible adversarial noises to\nreal-time speech and effectively perturb malicious ASR monitoring by increasing\nthe Word Error Rate (WER). To enhance the practical performance on mobile\ndevices, HASP is also optimized for effective adaptation to the human speech\ncharacteristics, environmental noises, and mobile computation scenarios. The\nexperiments show that HASP can achieve optimal real-time security enhancement:\nit can lead an average WER of 84.55% for perturbing the malicious ASR\nmonitoring, and the data processing speed is 15x to 40x faster compared to the\nstate-of-the-art methods. Moreover, HASP can effectively perturb various ASR\nsystems, demonstrating a strong transferability.\n"], ["2018-09-03", "http://arxiv.org/abs/1809.00594", "Adversarial Attack Type I: Cheat Classifiers by Significant Changes.", ["Sanli Tang", " Xiaolin Huang", " Mingjian Chen", " Chengjin Sun", " Jie Yang"], "  Despite the great success of deep neural networks, the adversarial attack can\ncheat some well-trained classifiers by small permutations. In this paper, we\npropose another type of adversarial attack that can cheat classifiers by\nsignificant changes. For example, we can significantly change a face but\nwell-trained neural networks still recognize the adversarial and the original\nexample as the same person. Statistically, the existing adversarial attack\nincreases Type II error and the proposed one aims at Type I error, which are\nhence named as Type II and Type I adversarial attack, respectively. The two\ntypes of attack are equally important but are essentially different, which are\nintuitively explained and numerically evaluated. To implement the proposed\nattack, a supervised variation autoencoder is designed and then the classifier\nis attacked by updating the latent variables using gradient information.\n{Besides, with pre-trained generative models, Type I attack on latent spaces is\ninvestigated as well.} Experimental results show that our method is practical\nand effective to generate Type I adversarial examples on large-scale image\ndatasets. Most of these generated examples can pass detectors designed for\ndefending Type II attack and the strengthening strategy is only efficient with\na specific type attack, both implying that the underlying reasons for Type I\nand Type II attack are different.\n"], ["2018-08-31", "http://arxiv.org/abs/1809.00065", "MULDEF: Multi-model-based Defense Against Adversarial Examples for Neural Networks.", ["Siwakorn Srisakaokul", " Yuhao Zhang", " Zexuan Zhong", " Wei Yang", " Tao Xie", " Bo Li"], "  Despite being popularly used in many applications, neural network models have\nbeen found to be vulnerable to adversarial examples, i.e., carefully crafted\nexamples aiming to mislead machine learning models. Adversarial examples can\npose potential risks on safety and security critical applications. However,\nexisting defense approaches are still vulnerable to attacks, especially in a\nwhite-box attack scenario. To address this issue, we propose a new defense\napproach, named MulDef, based on robustness diversity. Our approach consists of\n(1) a general defense framework based on multiple models and (2) a technique\nfor generating these multiple models to achieve high defense capability. In\nparticular, given a target model, our framework includes multiple models\n(constructed from the target model) to form a model family. The model family is\ndesigned to achieve robustness diversity (i.e., an adversarial example\nsuccessfully attacking one model cannot succeed in attacking other models in\nthe family). At runtime, a model is randomly selected from the family to be\napplied on each input example. Our general framework can inspire rich future\nresearch to construct a desirable model family achieving higher robustness\ndiversity. Our evaluation results show that MulDef (with only up to 5 models in\nthe family) can substantially improve the target model's accuracy on\nadversarial examples by 22-74% in a white-box attack scenario, while\nmaintaining similar accuracy on legitimate examples.\n"], ["2018-08-28", "http://arxiv.org/abs/1808.09413", "DLFuzz: Differential Fuzzing Testing of Deep Learning Systems.", ["Jianmin Guo", " Yu Jiang", " Yue Zhao", " Quan Chen", " Jiaguang Sun"], "  Deep learning (DL) systems are increasingly applied to safety-critical\ndomains such as autonomous driving cars. It is of significant importance to\nensure the reliability and robustness of DL systems. Existing testing\nmethodologies always fail to include rare inputs in the testing dataset and\nexhibit low neuron coverage. In this paper, we propose DLFuzz, the frst\ndifferential fuzzing testing framework to guide DL systems exposing incorrect\nbehaviors. DLFuzz keeps minutely mutating the input to maximize the neuron\ncoverage and the prediction difference between the original input and the\nmutated input, without manual labeling effort or cross-referencing oracles from\nother DL systems with the same functionality. We present empirical evaluations\non two well-known datasets to demonstrate its efficiency. Compared with\nDeepXplore, the state-of-the-art DL whitebox testing framework, DLFuzz does not\nrequire extra efforts to find similar functional DL systems for\ncross-referencing check, but could generate 338.59% more adversarial inputs\nwith 89.82% smaller perturbations, averagely obtain 2.86% higher neuron\ncoverage, and save 20.11% time consumption.\n"], ["2018-08-28", "http://arxiv.org/abs/1808.09115", "All You Need is \"Love\": Evading Hate-speech Detection.", ["Tommi Gr\u00f6ndahl", " Luca Pajola", " Mika Juuti", " Mauro Conti", " N. Asokan"], "  With the spread of social networks and their unfortunate use for hate speech,\nautomatic detection of the latter has become a pressing problem. In this paper,\nwe reproduce seven state-of-the-art hate speech detection models from prior\nwork, and show that they perform well only when tested on the same type of data\nthey were trained on. Based on these results, we argue that for successful hate\nspeech detection, model architecture is less important than the type of data\nand labeling criteria. We further show that all proposed detection techniques\nare brittle against adversaries who can (automatically) insert typos, change\nword boundaries or add innocuous words to the original hate speech. A\ncombination of these methods is also effective against Google Perspective -- a\ncutting-edge solution from industry. Our experiments demonstrate that\nadversarial training does not completely mitigate the attacks, and using\ncharacter-level features makes the models systematically more attack-resistant\nthan using word-level features.\n"], ["2018-08-28", "http://arxiv.org/abs/1808.09540", "Lipschitz regularized Deep Neural Networks generalize and are adversarially robust.", ["Chris Finlay", " Jeff Calder", " Bilal Abbasi", " Adam Oberman"], "  In this work we study input gradient regularization of deep neural networks,\nand demonstrate that such regularization leads to generalization proofs and\nimproved adversarial robustness. The proof of generalization does not overcome\nthe curse of dimensionality, but it is independent of the number of layers in\nthe networks. The adversarial robustness regularization combines adversarial\ntraining, which we show to be equivalent to Total Variation regularization,\nwith Lipschitz regularization. We demonstrate empirically that the regularized\nmodels are more robust, and that gradient norms of images can be used for\nattack detection.\n"], ["2018-08-27", "http://arxiv.org/abs/1809.00958", "Targeted Nonlinear Adversarial Perturbations in Images and Videos.", ["Roberto Rey-de-Castro", " Herschel Rabitz"], "  We introduce a method for learning adversarial perturbations targeted to\nindividual images or videos. The learned perturbations are found to be sparse\nwhile at the same time containing a high level of feature detail. Thus, the\nextracted perturbations allow a form of object or action recognition and\nprovide insights into what features the studied deep neural network models\nconsider important when reaching their classification decisions. From an\nadversarial point of view, the sparse perturbations successfully confused the\nmodels into misclassifying, although the perturbed samples still belonged to\nthe same original class by visual examination. This is discussed in terms of a\nprospective data augmentation scheme. The sparse yet high-quality perturbations\nmay also be leveraged for image or video compression.\n"], ["2018-08-27", "http://arxiv.org/abs/1808.08750", "Generalisation in humans and deep neural networks.", ["Robert Geirhos", " Carlos R. Medina Temme", " Jonas Rauber", " Heiko H. Sch\u00fctt", " Matthias Bethge", " Felix A. Wichmann"], "  We compare the robustness of humans and current convolutional deep neural\nnetworks (DNNs) on object recognition under twelve different types of image\ndegradations. First, using three well known DNNs (ResNet-152, VGG-19,\nGoogLeNet) we find the human visual system to be more robust to nearly all of\nthe tested image manipulations, and we observe progressively diverging\nclassification error-patterns between humans and DNNs when the signal gets\nweaker. Secondly, we show that DNNs trained directly on distorted images\nconsistently surpass human performance on the exact distortion types they were\ntrained on, yet they display extremely poor generalisation abilities when\ntested on other distortion types. For example, training on salt-and-pepper\nnoise does not imply robustness on uniform white noise and vice versa. Thus,\nchanges in the noise distribution between training and testing constitutes a\ncrucial challenge to deep learning vision systems that can be systematically\naddressed in a lifelong machine learning approach. Our new dataset consisting\nof 83K carefully measured human psychophysical trials provide a useful\nreference for lifelong robustness against image degradations set by the human\nvisual system.\n"], ["2018-08-26", "http://arxiv.org/abs/1808.08609", "Adversarially Regularising Neural NLI Models to Integrate Logical Background Knowledge.", ["Pasquale Minervini", " Sebastian Riedel"], "  Adversarial examples are inputs to machine learning models designed to cause\nthe model to make a mistake. They are useful for understanding the shortcomings\nof machine learning models, interpreting their results, and for regularisation.\nIn NLP, however, most example generation strategies produce input text by using\nknown, pre-specified semantic transformations, requiring significant manual\neffort and in-depth understanding of the problem and domain. In this paper, we\ninvestigate the problem of automatically generating adversarial examples that\nviolate a set of given First-Order Logic constraints in Natural Language\nInference (NLI). We reduce the problem of identifying such adversarial examples\nto a combinatorial optimisation problem, by maximising a quantity measuring the\ndegree of violation of such constraints and by using a language model for\ngenerating linguistically-plausible examples. Furthermore, we propose a method\nfor adversarially regularising neural NLI models for incorporating background\nknowledge. Our results show that, while the proposed method does not always\nimprove results on the SNLI and MultiNLI datasets, it significantly and\nconsistently increases the predictive accuracy on adversarially-crafted\ndatasets -- up to a 79.6% relative improvement -- while drastically reducing\nthe number of background knowledge violations. Furthermore, we show that\nadversarial examples transfer among model architectures, and that the proposed\nadversarial training procedure improves the robustness of NLI models to\nadversarial examples.\n"], ["2018-08-25", "http://arxiv.org/abs/1808.08444", "Guiding Deep Learning System Testing using Surprise Adequacy.", ["Jinhan Kim", " Robert Feldt", " Shin Yoo"], "  Deep Learning (DL) systems are rapidly being adopted in safety and security\ncritical domains, urgently calling for ways to test their correctness and\nrobustness. Testing of DL systems has traditionally relied on manual collection\nand labelling of data. Recently, a number of coverage criteria based on neuron\nactivation values have been proposed. These criteria essentially count the\nnumber of neurons whose activation during the execution of a DL system\nsatisfied certain properties, such as being above predefined thresholds.\nHowever, existing coverage criteria are not sufficiently fine grained to\ncapture subtle behaviours exhibited by DL systems. Moreover, evaluations have\nfocused on showing correlation between adversarial examples and proposed\ncriteria rather than evaluating and guiding their use for actual testing of DL\nsystems. We propose a novel test adequacy criterion for testing of DL systems,\ncalled Surprise Adequacy for Deep Learning Systems (SADL), which is based on\nthe behaviour of DL systems with respect to their training data. We measure the\nsurprise of an input as the difference in DL system's behaviour between the\ninput and the training data (i.e., what was learnt during training), and\nsubsequently develop this as an adequacy criterion: a good test input should be\nsufficiently but not overtly surprising compared to training data. Empirical\nevaluation using a range of DL systems from simple image classifiers to\nautonomous driving car platforms shows that systematic sampling of inputs based\non their surprise can improve classification accuracy of DL systems against\nadversarial examples by up to 77.5% via retraining.\n"], ["2018-08-25", "http://arxiv.org/abs/1808.08426", "Analysis of adversarial attacks against CNN-based image forgery detectors.", ["Diego Gragnaniello", " Francesco Marra", " Giovanni Poggi", " Luisa Verdoliva"], "  With the ubiquitous diffusion of social networks, images are becoming a\ndominant and powerful communication channel. Not surprisingly, they are also\nincreasingly subject to manipulations aimed at distorting information and\nspreading fake news. In recent years, the scientific community has devoted\nmajor efforts to contrast this menace, and many image forgery detectors have\nbeen proposed. Currently, due to the success of deep learning in many\nmultimedia processing tasks, there is high interest towards CNN-based\ndetectors, and early results are already very promising. Recent studies in\ncomputer vision, however, have shown CNNs to be highly vulnerable to\nadversarial attacks, small perturbations of the input data which drive the\nnetwork towards erroneous classification. In this paper we analyze the\nvulnerability of CNN-based image forensics methods to adversarial attacks,\nconsidering several detectors and several types of attack, and testing\nperformance on a wide range of common manipulations, both easily and hardly\ndetectable.\n"], ["2018-08-24", "http://arxiv.org/abs/1808.08197", "Is Machine Learning in Power Systems Vulnerable?.", ["Yize Chen", " Yushi Tan", " Deepjyoti Deka"], "  Recent advances in Machine Learning(ML) have led to its broad adoption in a\nseries of power system applications, ranging from meter data analytics,\nrenewable/load/price forecasting to grid security assessment. Although these\ndata-driven methods yield state-of-the-art performances in many tasks, the\nrobustness and security of applying such algorithms in modern power grids have\nnot been discussed. In this paper, we attempt to address the issues regarding\nthe security of ML applications in power systems. We first show that most of\nthe current ML algorithms proposed in power systems are vulnerable to\nadversarial examples, which are maliciously crafted input data. We then adopt\nand extend a simple yet efficient algorithm for finding subtle perturbations,\nwhich could be used for generating adversaries for both categorical(e.g., user\nload profile classification) and sequential applications(e.g., renewables\ngeneration forecasting). Case studies on classification of power quality\ndisturbances and forecast of building loads demonstrate the vulnerabilities of\ncurrent ML algorithms in power networks under our adversarial designs. These\nvulnerabilities call for design of robust and secure ML algorithms for real\nworld applications.\n"], ["2018-08-23", "http://arxiv.org/abs/1808.07945", "Maximal Jacobian-based Saliency Map Attack.", ["Rey Wiyatno", " Anqi Xu"], "  The Jacobian-based Saliency Map Attack is a family of adversarial attack\nmethods for fooling classification models, such as deep neural networks for\nimage classification tasks. By saturating a few pixels in a given image to\ntheir maximum or minimum values, JSMA can cause the model to misclassify the\nresulting adversarial image as a specified erroneous target class. We propose\ntwo variants of JSMA, one which removes the requirement to specify a target\nclass, and another that additionally does not need to specify whether to only\nincrease or decrease pixel intensities. Our experiments highlight the\ncompetitive speeds and qualities of these variants when applied to datasets of\nhand-written digits and natural scenes.\n"], ["2018-08-23", "http://arxiv.org/abs/1808.07713", "Adversarial Attacks on Deep-Learning Based Radio Signal Classification.", ["Meysam Sadeghi", " Erik G. Larsson"], "  Deep learning (DL), despite its enormous success in many computer vision and\nlanguage processing applications, is exceedingly vulnerable to adversarial\nattacks. We consider the use of DL for radio signal (modulation) classification\ntasks, and present practical methods for the crafting of white-box and\nuniversal black-box adversarial attacks in that application. We show that these\nattacks can considerably reduce the classification performance, with extremely\nsmall perturbations of the input. In particular, these attacks are\nsignificantly more powerful than classical jamming attacks, which raises\nsignificant security and robustness concerns in the use of DL-based algorithms\nfor the wireless physical layer.\n"], ["2018-08-20", "http://arxiv.org/abs/1808.08282", "Controlling Over-generalization and its Effect on Adversarial Examples Generation and Detection.", ["Mahdieh Abbasi", " Arezoo Rajabi", " Azadeh Sadat Mozafari", " Rakesh B. Bobba", " Christian Gagne"], "  Convolutional Neural Networks (CNNs) significantly improve the\nstate-of-the-art for many applications, especially in computer vision. However,\nCNNs still suffer from a tendency to confidently classify out-distribution\nsamples from unknown classes into pre-defined known classes. Further, they are\nalso vulnerable to adversarial examples. We are relating these two issues\nthrough the tendency of CNNs to over-generalize for areas of the input space\nnot covered well by the training set. We show that a CNN augmented with an\nextra output class can act as a simple yet effective end-to-end model for\ncontrolling over-generalization. As an appropriate training set for the extra\nclass, we introduce two resources that are computationally efficient to obtain:\na representative natural out-distribution set and interpolated in-distribution\nsamples. To help select a representative natural out-distribution set among\navailable ones, we propose a simple measurement to assess an out-distribution\nset's fitness. We also demonstrate that training such an augmented CNN with\nrepresentative out-distribution natural datasets and some interpolated samples\nallows it to better handle a wide range of unseen out-distribution samples and\nblack-box adversarial examples without training it on any adversaries. Finally,\nwe show that generation of white-box adversarial attacks using our proposed\naugmented CNN can become harder, as the attack algorithms have to get around\nthe rejection regions when generating actual adversaries.\n"], ["2018-08-20", "http://arxiv.org/abs/1808.06645", "Stochastic Combinatorial Ensembles for Defending Against Adversarial Examples.", ["George A. Adam", " Petr Smirnov", " David Duvenaud", " Benjamin Haibe-Kains", " Anna Goldenberg"], "  Many deep learning algorithms can be easily fooled with simple adversarial\nexamples. To address the limitations of existing defenses, we devised a\nprobabilistic framework that can generate an exponentially large ensemble of\nmodels from a single model with just a linear cost. This framework takes\nadvantage of neural network depth and stochastically decides whether or not to\ninsert noise removal operators such as VAEs between layers. We show empirically\nthe important role that model gradients have when it comes to determining\ntransferability of adversarial examples, and take advantage of this result to\ndemonstrate that it is possible to train models with limited adversarial attack\ntransferability. Additionally, we propose a detection method based on metric\nlearning in order to detect adversarial examples that have no hope of being\ncleaned of maliciously engineered noise.\n"], ["2018-08-17", "http://arxiv.org/abs/1808.05770", "Reinforcement Learning for Autonomous Defence in Software-Defined Networking.", ["Yi Han", " Benjamin I. P. Rubinstein", " Tamas Abraham", " Tansu Alpcan", " Vel Olivier De", " Sarah Erfani", " David Hubczenko", " Christopher Leckie", " Paul Montague"], "  Despite the successful application of machine learning (ML) in a wide range\nof domains, adaptability---the very property that makes machine learning\ndesirable---can be exploited by adversaries to contaminate training and evade\nclassification. In this paper, we investigate the feasibility of applying a\nspecific class of machine learning algorithms, namely, reinforcement learning\n(RL) algorithms, for autonomous cyber defence in software-defined networking\n(SDN). In particular, we focus on how an RL agent reacts towards different\nforms of causative attacks that poison its training process, including\nindiscriminate and targeted, white-box and black-box attacks. In addition, we\nalso study the impact of the attack timing, and explore potential\ncountermeasures such as adversarial training.\n"], ["2018-08-16", "http://arxiv.org/abs/1808.05705", "Mitigation of Adversarial Attacks through Embedded Feature Selection.", ["Ziyi Bao", " Luis Mu\u00f1oz-Gonz\u00e1lez", " Emil C. Lupu"], "  Machine learning has become one of the main components for task automation in\nmany application domains. Despite the advancements and impressive achievements\nof machine learning, it has been shown that learning algorithms can be\ncompromised by attackers both at training and test time. Machine learning\nsystems are especially vulnerable to adversarial examples where small\nperturbations added to the original data points can produce incorrect or\nunexpected outputs in the learning algorithms at test time. Mitigation of these\nattacks is hard as adversarial examples are difficult to detect. Existing\nrelated work states that the security of machine learning systems against\nadversarial examples can be weakened when feature selection is applied to\nreduce the systems' complexity. In this paper, we empirically disprove this\nidea, showing that the relative distortion that the attacker has to introduce\nto succeed in the attack is greater when the target is using a reduced set of\nfeatures. We also show that the minimal adversarial examples differ\nstatistically more strongly from genuine examples with a lower number of\nfeatures. However, reducing the feature count can negatively impact the\nsystem's performance. We illustrate the trade-off between security and accuracy\nwith specific examples. We propose a design methodology to evaluate the\nsecurity of machine learning classifiers with embedded feature selection\nagainst adversarial examples crafted using different attack strategies.\n"], ["2018-08-16", "http://arxiv.org/abs/1808.05665", "Adversarial Attacks Against Automatic Speech Recognition Systems via Psychoacoustic Hiding.", ["Lea Sch\u00f6nherr", " Katharina Kohls", " Steffen Zeiler", " Thorsten Holz", " Dorothea Kolossa"], "  Voice interfaces are becoming accepted widely as input methods for a diverse\nset of devices. This development is driven by rapid improvements in automatic\nspeech recognition (ASR), which now performs on par with human listening in\nmany tasks. These improvements base on an ongoing evolution of DNNs as the\ncomputational core of ASR. However, recent research results show that DNNs are\nvulnerable to adversarial perturbations, which allow attackers to force the\ntranscription into a malicious output.\n  In this paper, we introduce a new type of adversarial examples based on\npsychoacoustic hiding. Our attack exploits the characteristics of DNN-based ASR\nsystems, where we extend the original analysis procedure by an additional\nbackpropagation step. We use this backpropagation to learn the degrees of\nfreedom for the adversarial perturbation of the input signal, i.e., we apply a\npsychoacoustic model and manipulate the acoustic signal below the thresholds of\nhuman perception. To further minimize the perceptibility of the perturbations,\nwe use forced alignment to find the best fitting temporal alignment between the\noriginal audio sample and the malicious target transcription. These extensions\nallow us to embed an arbitrary audio input with a malicious voice command that\nis then transcribed by the ASR system, with the audio signal remaining barely\ndistinguishable from the original signal. In an experimental evaluation, we\nattack the state-of-the-art speech recognition system Kaldi and determine the\nbest performing parameter and analysis setup for different types of input. Our\nresults show that we are successful in up to 98% of cases with a computational\neffort of fewer than two minutes for a ten-second audio file. Based on user\nstudies, we found that none of our target transcriptions were audible to human\nlisteners, who still understand the original speech content with unchanged\naccuracy.\n"], ["2018-08-16", "http://arxiv.org/abs/1808.05537", "Distributionally Adversarial Attack.", ["Tianhang Zheng", " Changyou Chen", " Kui Ren"], "  Recent work on adversarial attack has shown that Projected Gradient Descent\n(PGD) Adversary is a universal first-order adversary, and the classifier\nadversarially trained by PGD is robust against a wide range of first-order\nattacks. It is worth noting that the original objective of an attack/defense\nmodel relies on a data distribution $p(\\mathbf{x})$, typically in the form of\nrisk maximization/minimization, e.g.,\n$\\max/\\min\\mathbb{E}_{p(\\mathbf(x))}\\mathcal{L}(\\mathbf{x})$ with\n$p(\\mathbf{x})$ some unknown data distribution and $\\mathcal{L}(\\cdot)$ a loss\nfunction. However, since PGD generates attack samples independently for each\ndata sample based on $\\mathcal{L}(\\cdot)$, the procedure does not necessarily\nlead to good generalization in terms of risk optimization. In this paper, we\nachieve the goal by proposing distributionally adversarial attack (DAA), a\nframework to solve an optimal {\\em adversarial-data distribution}, a perturbed\ndistribution that satisfies the $L_\\infty$ constraint but deviates from the\noriginal data distribution to increase the generalization risk maximally.\nAlgorithmically, DAA performs optimization on the space of potential data\ndistributions, which introduces direct dependency between all data points when\ngenerating adversarial samples. DAA is evaluated by attacking state-of-the-art\ndefense models, including the adversarially-trained models provided by {\\em MIT\nMadryLab}. Notably, DAA ranks {\\em the first place} on MadryLab's white-box\nleaderboards, reducing the accuracy of their secret MNIST model to $88.79\\%$\n(with $l_\\infty$ perturbations of $\\epsilon = 0.3$) and the accuracy of their\nsecret CIFAR model to $44.71\\%$ (with $l_\\infty$ perturbations of $\\epsilon =\n8.0$). Code for the experiments is released on\n\\url{https://github.com/tianzheng4/Distributionally-Adversarial-Attack}.\n"], ["2018-08-10", "http://arxiv.org/abs/1808.04218", "Android HIV: A Study of Repackaging Malware for Evading Machine-Learning Detection.", ["Xiao Chen", " Chaoran Li", " Derui Wang", " Sheng Wen", " Jun Zhang", " Surya Nepal", " Yang Xiang", " Kui Ren"], "  Machine learning based solutions have been successfully employed for\nautomatic detection of malware in Android applications. However, machine\nlearning models are known to lack robustness against inputs crafted by an\nadversary. So far, the adversarial examples can only deceive Android malware\ndetectors that rely on syntactic features, and the perturbations can only be\nimplemented by simply modifying Android manifest. While recent Android malware\ndetectors rely more on semantic features from Dalvik bytecode rather than\nmanifest, existing attacking/defending methods are no longer effective. In this\npaper, we introduce a new highly-effective attack that generates adversarial\nexamples of Android malware and evades being detected by the current models. To\nthis end, we propose a method of applying optimal perturbations onto Android\nAPK using a substitute model. Based on the transferability concept, the\nperturbations that successfully deceive the substitute model are likely to\ndeceive the original models as well. We develop an automated tool to generate\nthe adversarial examples without human intervention to apply the attacks. In\ncontrast to existing works, the adversarial examples crafted by our method can\nalso deceive recent machine learning based detectors that rely on semantic\nfeatures such as control-flow-graph. The perturbations can also be implemented\ndirectly onto APK's Dalvik bytecode rather than Android manifest to evade from\nrecent detectors. We evaluated the proposed manipulation methods for\nadversarial examples by using the same datasets that Drebin and MaMadroid (5879\nmalware samples) used. Our results show that, the malware detection rates\ndecreased from 96% to 1% in MaMaDroid, and from 97% to 1% in Drebin, with just\na small distortion generated by our adversarial examples manipulation method.\n"], ["2018-08-10", "http://arxiv.org/abs/1808.03601", "Using Randomness to Improve Robustness of Machine-Learning Models Against Evasion Attacks.", ["Fan Yang", " Zhiyuan Chen"], "  Machine learning models have been widely used in security applications such\nas intrusion detection, spam filtering, and virus or malware detection.\nHowever, it is well-known that adversaries are always trying to adapt their\nattacks to evade detection. For example, an email spammer may guess what\nfeatures spam detection models use and modify or remove those features to avoid\ndetection. There has been some work on making machine learning models more\nrobust to such attacks. However, one simple but promising approach called {\\em\nrandomization} is underexplored. This paper proposes a novel\nrandomization-based approach to improve robustness of machine learning models\nagainst evasion attacks. The proposed approach incorporates randomization into\nboth model training time and model application time (meaning when the model is\nused to detect attacks). We also apply this approach to random forest, an\nexisting ML method which already has some degree of randomness. Experiments on\nintrusion detection and spam filtering data show that our approach further\nimproves robustness of random-forest method. We also discuss how this approach\ncan be applied to other ML models.\n"], ["2018-08-08", "http://arxiv.org/abs/1808.02651", "Beyond Pixel Norm-Balls: Parametric Adversaries using an Analytically Differentiable Renderer.", ["Hsueh-Ti Derek Liu", " Michael Tao", " Chun-Liang Li", " Derek Nowrouzezahrai", " Alec Jacobson"], "  Many machine learning image classifiers are vulnerable to adversarial\nattacks, inputs with perturbations designed to intentionally trigger\nmisclassification. Current adversarial methods directly alter pixel colors and\nevaluate against pixel norm-balls: pixel perturbations smaller than a specified\nmagnitude, according to a measurement norm. This evaluation, however, has\nlimited practical utility since perturbations in the pixel space do not\ncorrespond to underlying real-world phenomena of image formation that lead to\nthem and has no security motivation attached. Pixels in natural images are\nmeasurements of light that has interacted with the geometry of a physical\nscene. As such, we propose the direct perturbation of physical parameters that\nunderly image formation: lighting and geometry. As such, we propose a novel\nevaluation measure, parametric norm-balls, by directly perturbing physical\nparameters that underly image formation. One enabling contribution we present\nis a physically-based differentiable renderer that allows us to propagate pixel\ngradients to the parametric space of lighting and geometry. Our approach\nenables physically-based adversarial attacks, and our differentiable renderer\nleverages models from the interactive rendering literature to balance the\nperformance and accuracy trade-offs necessary for a memory-efficient and\nscalable adversarial data augmentation workflow.\n"], ["2018-08-07", "http://arxiv.org/abs/1808.02455", "Data augmentation using synthetic data for time series classification with deep residual networks.", ["Hassan Ismail Fawaz", " Germain Forestier", " Jonathan Weber", " Lhassane Idoumghar", " Pierre-Alain Muller"], "  Data augmentation in deep neural networks is the process of generating\nartificial data in order to reduce the variance of the classifier with the goal\nto reduce the number of errors. This idea has been shown to improve deep neural\nnetwork's generalization capabilities in many computer vision tasks such as\nimage recognition and object localization. Apart from these applications, deep\nConvolutional Neural Networks (CNNs) have also recently gained popularity in\nthe Time Series Classification (TSC) community. However, unlike in image\nrecognition problems, data augmentation techniques have not yet been\ninvestigated thoroughly for the TSC task. This is surprising as the accuracy of\ndeep learning models for TSC could potentially be improved, especially for\nsmall datasets that exhibit overfitting, when a data augmentation method is\nadopted. In this paper, we fill this gap by investigating the application of a\nrecently proposed data augmentation technique based on the Dynamic Time Warping\ndistance, for a deep learning model for TSC. To evaluate the potential of\naugmenting the training set, we performed extensive experiments using the UCR\nTSC benchmark. Our preliminary experiments reveal that data augmentation can\ndrastically increase deep CNN's accuracy on some datasets and significantly\nimprove the deep model's accuracy when the method is used in an ensemble\napproach.\n"], ["2018-08-06", "http://arxiv.org/abs/1808.01976", "Adversarial Vision Challenge.", ["Wieland Brendel", " Jonas Rauber", " Alexey Kurakin", " Nicolas Papernot", " Behar Veliqi", " Marcel Salath\u00e9", " Sharada P. Mohanty", " Matthias Bethge"], "  The NIPS 2018 Adversarial Vision Challenge is a competition to facilitate\nmeasurable progress towards robust machine vision models and more generally\napplicable adversarial attacks. This document is an updated version of our\ncompetition proposal that was accepted in the competition track of 32nd\nConference on Neural Information Processing Systems (NIPS 2018).\n"], ["2018-08-06", "http://arxiv.org/abs/1808.01785", "Defense Against Adversarial Attacks with Saak Transform.", ["Sibo Song", " Yueru Chen", " Ngai-Man Cheung", " C. -C. Jay Kuo"], "  Deep neural networks (DNNs) are known to be vulnerable to adversarial\nperturbations, which imposes a serious threat to DNN-based decision systems. In\nthis paper, we propose to apply the lossy Saak transform to adversarially\nperturbed images as a preprocessing tool to defend against adversarial attacks.\nSaak transform is a recently-proposed state-of-the-art for computing the\nspatial-spectral representations of input images. Empirically, we observe that\noutputs of the Saak transform are very discriminative in differentiating\nadversarial examples from clean ones. Therefore, we propose a Saak transform\nbased preprocessing method with three steps: 1) transforming an input image to\na joint spatial-spectral representation via the forward Saak transform, 2)\napply filtering to its high-frequency components, and, 3) reconstructing the\nimage via the inverse Saak transform. The processed image is found to be robust\nagainst adversarial perturbations. We conduct extensive experiments to\ninvestigate various settings of the Saak transform and filtering functions.\nWithout harming the decision performance on clean images, our method\noutperforms state-of-the-art adversarial defense methods by a substantial\nmargin on both the CIFAR-10 and ImageNet datasets. Importantly, our results\nsuggest that adversarial perturbations can be effectively and efficiently\ndefended using state-of-the-art frequency analysis.\n"], ["2018-08-06", "http://arxiv.org/abs/1808.01753", "Gray-box Adversarial Training.", ["Vivek B. S.", " Konda Reddy Mopuri", " R. Venkatesh Babu"], "  Adversarial samples are perturbed inputs crafted to mislead the machine\nlearning systems. A training mechanism, called adversarial training, which\npresents adversarial samples along with clean samples has been introduced to\nlearn robust models. In order to scale adversarial training for large datasets,\nthese perturbations can only be crafted using fast and simple methods (e.g.,\ngradient ascent). However, it is shown that adversarial training converges to a\ndegenerate minimum, where the model appears to be robust by generating weaker\nadversaries. As a result, the models are vulnerable to simple black-box\nattacks. In this paper we, (i) demonstrate the shortcomings of existing\nevaluation policy, (ii) introduce novel variants of white-box and black-box\nattacks, dubbed gray-box adversarial attacks\" based on which we propose novel\nevaluation method to assess the robustness of the learned models, and (iii)\npropose a novel variant of adversarial training, named Graybox Adversarial\nTraining\" that uses intermediate versions of the models to seed the\nadversaries. Experimental evaluation demonstrates that the models trained using\nour method exhibit better robustness compared to both undefended and\nadversarially trained model\n"], ["2018-08-05", "http://arxiv.org/abs/1808.01688", "Is Robustness the Cost of Accuracy? -- A Comprehensive Study on the Robustness of 18 Deep Image Classification Models.", ["Dong Su", " Huan Zhang", " Hongge Chen", " Jinfeng Yi", " Pin-Yu Chen", " Yupeng Gao"], "  The prediction accuracy has been the long-lasting and sole standard for\ncomparing the performance of different image classification models, including\nthe ImageNet competition. However, recent studies have highlighted the lack of\nrobustness in well-trained deep neural networks to adversarial examples.\nVisually imperceptible perturbations to natural images can easily be crafted\nand mislead the image classifiers towards misclassification. To demystify the\ntrade-offs between robustness and accuracy, in this paper we thoroughly\nbenchmark 18 ImageNet models using multiple robustness metrics, including the\ndistortion, success rate and transferability of adversarial examples between\n306 pairs of models. Our extensive experimental results reveal several new\ninsights: (1) linear scaling law - the empirical $\\ell_2$ and $\\ell_\\infty$\ndistortion metrics scale linearly with the logarithm of classification error;\n(2) model architecture is a more critical factor to robustness than model size,\nand the disclosed accuracy-robustness Pareto frontier can be used as an\nevaluation criterion for ImageNet model designers; (3) for a similar network\narchitecture, increasing network depth slightly improves robustness in\n$\\ell_\\infty$ distortion; (4) there exist models (in VGG family) that exhibit\nhigh adversarial transferability, while most adversarial examples crafted from\none model can only be transferred within the same family. Experiment code is\npublicly available at \\url{https://github.com/huanzhang12/Adversarial_Survey}.\n"], ["2018-08-05", "http://arxiv.org/abs/1808.01664", "Structured Adversarial Attack: Towards General Implementation and Better Interpretability.", ["Kaidi Xu", " Sijia Liu", " Pu Zhao", " Pin-Yu Chen", " Huan Zhang", " Quanfu Fan", " Deniz Erdogmus", " Yanzhi Wang", " Xue Lin"], "  When generating adversarial examples to attack deep neural networks (DNNs),\nLp norm of the added perturbation is usually used to measure the similarity\nbetween original image and adversarial example. However, such adversarial\nattacks perturbing the raw input spaces may fail to capture structural\ninformation hidden in the input. This work develops a more general attack\nmodel, i.e., the structured attack (StrAttack), which explores group sparsity\nin adversarial perturbations by sliding a mask through images aiming for\nextracting key spatial structures. An ADMM (alternating direction method of\nmultipliers)-based framework is proposed that can split the original problem\ninto a sequence of analytically solvable subproblems and can be generalized to\nimplement other attacking methods. Strong group sparsity is achieved in\nadversarial perturbations even with the same level of Lp norm distortion as the\nstate-of-the-art attacks. We demonstrate the effectiveness of StrAttack by\nextensive experimental results onMNIST, CIFAR-10, and ImageNet. We also show\nthat StrAttack provides better interpretability (i.e., better correspondence\nwith discriminative image regions)through adversarial saliency map (Papernot et\nal., 2016b) and class activation map(Zhou et al., 2016).\n"], ["2018-08-04", "http://arxiv.org/abs/1808.01452", "Traits & Transferability of Adversarial Examples against Instance Segmentation & Object Detection.", ["Raghav Gurbaxani", " Shivank Mishra"], "  Despite the recent advancements in deploying neural networks for image\nclassification, it has been found that adversarial examples are able to fool\nthese models leading them to misclassify the images. Since these models are now\nbeing widely deployed, we provide an insight on the threat of these adversarial\nexamples by evaluating their characteristics and transferability to more\ncomplex models that utilize Image Classification as a subtask.\n  We demonstrate the ineffectiveness of adversarial examples when applied to\nInstance Segmentation & Object Detection models. We show that this\nineffectiveness arises from the inability of adversarial examples to withstand\ntransformations such as scaling or a change in lighting conditions. Moreover,\nwe show that there exists a small threshold below which the adversarial\nproperty is retained while applying these input transformations.\n  Additionally, these attacks demonstrate weak cross-network transferability\nacross neural network architectures, e.g. VGG16 and ResNet50, however, the\nattack may fool both the networks if passed sequentially through networks\nduring its formation.\n  The lack of scalability and transferability challenges the question of how\nadversarial images would be effective in the real world.\n"], ["2018-08-04", "http://arxiv.org/abs/1808.01546", "ATMPA: Attacking Machine Learning-based Malware Visualization Detection Methods via Adversarial Examples.", ["Xinbo Liu", " Jiliang Zhang", " Yapin Lin", " He Li"], "  Since the threat of malicious software (malware) has become increasingly\nserious, automatic malware detection techniques have received increasing\nattention, where machine learning (ML)-based visualization detection methods\nbecome more and more popular. In this paper, we demonstrate that the\nstate-of-the-art ML-based visualization detection methods are vulnerable to\nAdversarial Example (AE) attacks. We develop a novel Adversarial Texture\nMalware Perturbation Attack (ATMPA) method based on the gradient descent and\nL-norm optimization method, where attackers can introduce some tiny\nperturbations on the transformed dataset such that ML-based malware detection\nmethods will completely fail. The experimental results on the MS BIG malware\ndataset show that a small interference can reduce the accuracy rate down to 0%\nfor several ML-based detection methods, and the rate of transferability is\n74.1% on average.\n"], ["2018-08-03", "http://arxiv.org/abs/1808.01352", "DeepCloak: Adversarial Crafting As a Defensive Measure to Cloak Processes.", ["Mehmet Sinan Inci", " Thomas Eisenbarth", " Berk Sunar"], "  Over the past decade, side-channels have proven to be significant and\npractical threats to modern computing systems. Recent attacks have all\nexploited the underlying shared hardware. While practical, mounting such a\ncomplicated attack is still akin to listening on a private conversation in a\ncrowded train station. The attacker has to either perform significant manual\nlabor or use AI systems to automate the process. The recent academic literature\npoints to the latter option. With the abundance of cheap computing power and\nthe improvements made in AI, it is quite advantageous to automate such tasks.\nBy using AI systems however, malicious parties also inherit their weaknesses.\nOne such weakness is undoubtedly the vulnerability to adversarial samples.\n  In contrast to the previous literature, for the first time, we propose the\nuse of adversarial learning as a defensive tool to obfuscate and mask private\ninformation. We demonstrate the viability of this approach by first training\nCNNs and other machine learning classifiers on leakage trace of different\nprocesses. After training highly accurate models (99+% accuracy), we\ninvestigate their resolve against adversarial learning methods. By applying\nminimal perturbations to input traces, the adversarial traffic by the defender\ncan run as an attachment to the original process and cloak it against a\nmalicious classifier.\n  Finally, we investigate whether an attacker can protect her classifier model\nby employing adversarial defense methods, namely adversarial re-training and\ndefensive distillation. Our results show that even in the presence of an\nintelligent adversary that employs such techniques, all 10 of the tested\nadversarial learning methods still manage to successfully craft adversarial\nperturbations and the proposed cloaking methodology succeeds.\n"], ["2018-08-03", "http://arxiv.org/abs/1808.01153", "Ask, Acquire, and Attack: Data-free UAP Generation using Class Impressions.", ["Konda Reddy Mopuri", " Phani Krishna Uppala", " R. Venkatesh Babu"], "  Deep learning models are susceptible to input specific noise, called\nadversarial perturbations. Moreover, there exist input-agnostic noise, called\nUniversal Adversarial Perturbations (UAP) that can affect inference of the\nmodels over most input samples. Given a model, there exist broadly two\napproaches to craft UAPs: (i) data-driven: that require data, and (ii)\ndata-free: that do not require data samples. Data-driven approaches require\nactual samples from the underlying data distribution and craft UAPs with high\nsuccess (fooling) rate. However, data-free approaches craft UAPs without\nutilizing any data samples and therefore result in lesser success rates. In\nthis paper, for data-free scenarios, we propose a novel approach that emulates\nthe effect of data samples with class impressions in order to craft UAPs using\ndata-driven objectives. Class impression for a given pair of category and model\nis a generic representation (in the input space) of the samples belonging to\nthat category. Further, we present a neural network based generative model that\nutilizes the acquired class impressions to learn crafting UAPs. Experimental\nevaluation demonstrates that the learned generative model, (i) readily crafts\nUAPs via simple feed-forwarding through neural network layers, and (ii)\nachieves state-of-the-art success rates for data-free scenario and closer to\nthat for data-driven setting without actually utilizing any data samples.\n"], ["2018-07-31", "http://arxiv.org/abs/1808.00123", "EagleEye: Attack-Agnostic Defense against Adversarial Inputs (Technical Report).", ["Yujie Ji", " Xinyang Zhang", " Ting Wang"], "  Deep neural networks (DNNs) are inherently vulnerable to adversarial inputs:\nsuch maliciously crafted samples trigger DNNs to misbehave, leading to\ndetrimental consequences for DNN-powered systems. The fundamental challenges of\nmitigating adversarial inputs stem from their adaptive and variable nature.\nExisting solutions attempt to improve DNN resilience against specific attacks;\nyet, such static defenses can often be circumvented by adaptively engineered\ninputs or by new attack variants.\n  Here, we present EagleEye, an attack-agnostic adversarial tampering analysis\nengine for DNN-powered systems. Our design exploits the {\\em minimality\nprinciple} underlying many attacks: to maximize the attack's evasiveness, the\nadversary often seeks the minimum possible distortion to convert genuine inputs\nto adversarial ones. We show that this practice entails the distinct\ndistributional properties of adversarial inputs in the input space. By\nleveraging such properties in a principled manner, EagleEye effectively\ndiscriminates adversarial inputs and even uncovers their correct classification\noutputs. Through extensive empirical evaluation using a range of benchmark\ndatasets and DNN models, we validate EagleEye's efficacy. We further\ninvestigate the adversary's possible countermeasures, which implies a difficult\ndilemma for her: to evade EagleEye's detection, excessive distortion is\nnecessary, thereby significantly reducing the attack's evasiveness regarding\nother detection mechanisms.\n"], ["2018-07-27", "http://arxiv.org/abs/1807.10454", "Rob-GAN: Generator, Discriminator, and Adversarial Attacker.", ["Xuanqing Liu", " Cho-Jui Hsieh"], "  We study two important concepts in adversarial deep learning---adversarial\ntraining and generative adversarial network (GAN). Adversarial training is the\ntechnique used to improve the robustness of discriminator by combining\nadversarial attacker and discriminator in the training phase. GAN is commonly\nused for image generation by jointly optimizing discriminator and generator. We\nshow these two concepts are indeed closely related and can be used to\nstrengthen each other---adding a generator to the adversarial training\nprocedure can improve the robustness of discriminators, and adding an\nadversarial attack to GAN training can improve the convergence speed and lead\nto better generators. Combining these two insights, we develop a framework\ncalled Rob-GAN to jointly optimize generator and discriminator in the presence\nof adversarial attacks---the generator generates fake images to fool\ndiscriminator; the adversarial attacker perturbs real images to fool the\ndiscriminator, and the discriminator wants to minimize loss under fake and\nadversarial images. Through this end-to-end training procedure, we are able to\nsimultaneously improve the convergence speed of GAN training, the quality of\nsynthetic images, and the robustness of discriminator under strong adversarial\nattacks. Experimental results demonstrate that the obtained classifier is more\nrobust than the state-of-the-art adversarial training approach, and the\ngenerator outperforms SN-GAN on ImageNet-143.\n"], ["2018-07-26", "http://arxiv.org/abs/1807.10335", "A general metric for identifying adversarial images.", ["Siddharth Krishna Kumar"], "  It is well known that a determined adversary can fool a neural network by\nmaking imperceptible adversarial perturbations to an image. Recent studies have\nshown that these perturbations can be detected even without information about\nthe neural network if the strategy taken by the adversary is known beforehand.\nUnfortunately, these studies suffer from the generalization limitation -- the\ndetection method has to be recalibrated every time the adversary changes his\nstrategy. In this study, we attempt to overcome the generalization limitation\nby deriving a metric which reliably identifies adversarial images even when the\napproach taken by the adversary is unknown. Our metric leverages key\ndifferences between the spectra of clean and adversarial images when an image\nis treated as a matrix. Our metric is able to detect adversarial images across\ndifferent datasets and attack strategies without any additional re-calibration.\nIn addition, our approach provides geometric insights into several unanswered\nquestions about adversarial perturbations.\n"], ["2018-07-26", "http://arxiv.org/abs/1807.10272", "Evaluating and Understanding the Robustness of Adversarial Logit Pairing.", ["Logan Engstrom", " Andrew Ilyas", " Anish Athalye"], "  We evaluate the robustness of Adversarial Logit Pairing, a recently proposed\ndefense against adversarial examples. We find that a network trained with\nAdversarial Logit Pairing achieves 0.6% accuracy in the threat model in which\nthe defense is considered. We provide a brief overview of the defense and the\nthreat models/claims considered, as well as a discussion of the methodology and\nresults of our attack, which may offer insights into the reasons underlying the\nvulnerability of ALP to adversarial attack.\n"], ["2018-07-25", "http://arxiv.org/abs/1807.09937", "HiDDeN: Hiding Data With Deep Networks.", ["Jiren Zhu", " Russell Kaplan", " Justin Johnson", " Li Fei-Fei"], "  Recent work has shown that deep neural networks are highly sensitive to tiny\nperturbations of input images, giving rise to adversarial examples. Though this\nproperty is usually considered a weakness of learned models, we explore whether\nit can be beneficial. We find that neural networks can learn to use invisible\nperturbations to encode a rich amount of useful information. In fact, one can\nexploit this capability for the task of data hiding. We jointly train encoder\nand decoder networks, where given an input message and cover image, the encoder\nproduces a visually indistinguishable encoded image, from which the decoder can\nrecover the original message. We show that these encodings are competitive with\nexisting data hiding algorithms, and further that they can be made robust to\nnoise: our models learn to reconstruct hidden information in an encoded image\ndespite the presence of Gaussian blurring, pixel-wise dropout, cropping, and\nJPEG compression. Even though JPEG is non-differentiable, we show that a robust\nmodel can be trained using differentiable approximations. Finally, we\ndemonstrate that adversarial training improves the visual quality of encoded\nimages.\n"], ["2018-07-25", "http://arxiv.org/abs/1807.09705", "Limitations of the Lipschitz constant as a defense against adversarial examples.", ["Todd Huster", " Cho-Yu Jason Chiang", " Ritu Chadha"], "  Several recent papers have discussed utilizing Lipschitz constants to limit\nthe susceptibility of neural networks to adversarial examples. We analyze\nrecently proposed methods for computing the Lipschitz constant. We show that\nthe Lipschitz constant may indeed enable adversarially robust neural networks.\nHowever, the methods currently employed for computing it suffer from\ntheoretical and practical limitations. We argue that addressing this\nshortcoming is a promising direction for future research into certified\nadversarial defenses.\n"], ["2018-07-25", "http://arxiv.org/abs/1807.09443", "Unbounded Output Networks for Classification.", ["Stefan Elfwing", " Eiji Uchibe", " Kenji Doya"], "  We proposed the expected energy-based restricted Boltzmann machine (EE-RBM)\nas a discriminative RBM method for classification. Two characteristics of the\nEE-RBM are that the output is unbounded and that the target value of correct\nclassification is set to a value much greater than one. In this study, by\nadopting features of the EE-RBM approach to feed-forward neural networks, we\npropose the UnBounded output network (UBnet) which is characterized by three\nfeatures: (1) unbounded output units; (2) the target value of correct\nclassification is set to a value much greater than one; and (3) the models are\ntrained by a modified mean-squared error objective. We evaluate our approach\nusing the MNIST, CIFAR-10, and CIFAR-100 benchmark datasets. We first\ndemonstrate, for shallow UBnets on MNIST, that a setting of the target value\nequal to the number of hidden units significantly outperforms a setting of the\ntarget value equal to one, and it also outperforms standard neural networks by\nabout 25\\%. We then validate our approach by achieving high-level\nclassification performance on the three datasets using unbounded output\nresidual networks. We finally use MNIST to analyze the learned features and\nweights, and we demonstrate that UBnets are much more robust against\nadversarial examples than the standard approach of using a softmax output layer\nand training the networks by a cross-entropy objective.\n"], ["2018-07-24", "http://arxiv.org/abs/1807.09380", "Learning Discriminative Video Representations Using Adversarial Perturbations.", ["Jue Wang", " Anoop Cherian"], "  Adversarial perturbations are noise-like patterns that can subtly change the\ndata, while failing an otherwise accurate classifier. In this paper, we propose\nto use such perturbations for improving the robustness of video\nrepresentations. To this end, given a well-trained deep model for per-frame\nvideo recognition, we first generate adversarial noise adapted to this model.\nUsing the original data features from the full video sequence and their\nperturbed counterparts, as two separate bags, we develop a binary\nclassification problem that learns a set of discriminative hyperplanes -- as a\nsubspace -- that will separate the two bags from each other. This subspace is\nthen used as a descriptor for the video, dubbed discriminative subspace\npooling. As the perturbed features belong to data classes that are likely to be\nconfused with the original features, the discriminative subspace will\ncharacterize parts of the feature space that are more representative of the\noriginal data, and thus may provide robust video representations. To learn such\ndescriptors, we formulate a subspace learning objective on the Stiefel manifold\nand resort to Riemannian optimization methods for solving it efficiently. We\nprovide experiments on several video datasets and demonstrate state-of-the-art\nresults.\n"], ["2018-07-21", "http://arxiv.org/abs/1807.08108", "Simultaneous Adversarial Training - Learn from Others Mistakes.", ["Zukang Liao"], "  Adversarial examples are maliciously tweaked images that can easily fool\nmachine learning techniques, such as neural networks, but they are normally not\nvisually distinguishable for human beings. One of the main approaches to solve\nthis problem is to retrain the networks using those adversarial examples,\nnamely adversarial training. However, standard adversarial training might not\nactually change the decision boundaries but cause the problem of gradient\nmasking, resulting in a weaker ability to generate adversarial examples.\nTherefore, it cannot alleviate the problem of black-box attacks, where\nadversarial examples generated from other networks can transfer to the targeted\none. In order to reduce the problem of black-box attacks, we propose a novel\nmethod that allows two networks to learn from each others' adversarial examples\nand become resilient to black-box attacks. We also combine this method with a\nsimple domain adaptation to further improve the performance.\n"], ["2018-07-20", "http://arxiv.org/abs/1807.07978", "Prior Convictions: Black-Box Adversarial Attacks with Bandits and Priors.", ["Andrew Ilyas", " Logan Engstrom", " Aleksander Madry"], "  We study the problem of generating adversarial examples in a black-box\nsetting in which only loss-oracle access to a model is available. We introduce\na framework that conceptually unifies much of the existing work on black-box\nattacks, and we demonstrate that the current state-of-the-art methods are\noptimal in a natural sense. Despite this optimality, we show how to improve\nblack-box attacks by bringing a new element into the problem: gradient priors.\nWe give a bandit optimization-based algorithm that allows us to seamlessly\nintegrate any such priors, and we explicitly identify and incorporate two\nexamples. The resulting methods use two to four times fewer queries and fail\ntwo to five times less often than the current state-of-the-art.\n"], ["2018-07-20", "http://arxiv.org/abs/1807.07769", "Physical Adversarial Examples for Object Detectors.", ["Kevin Eykholt", " Ivan Evtimov", " Earlence Fernandes", " Bo Li", " Amir Rahmati", " Florian Tramer", " Atul Prakash", " Tadayoshi Kohno", " Dawn Song"], "  Deep neural networks (DNNs) are vulnerable to adversarial\nexamples-maliciously crafted inputs that cause DNNs to make incorrect\npredictions. Recent work has shown that these attacks generalize to the\nphysical domain, to create perturbations on physical objects that fool image\nclassifiers under a variety of real-world conditions. Such attacks pose a risk\nto deep learning models used in safety-critical cyber-physical systems. In this\nwork, we extend physical attacks to more challenging object detection models, a\nbroader class of deep learning algorithms widely used to detect and label\nmultiple objects within a scene. Improving upon a previous physical attack on\nimage classifiers, we create perturbed physical objects that are either ignored\nor mislabeled by object detection models. We implement a Disappearance Attack,\nin which we cause a Stop sign to \"disappear\" according to the detector-either\nby covering thesign with an adversarial Stop sign poster, or by adding\nadversarial stickers onto the sign. In a video recorded in a controlled lab\nenvironment, the state-of-the-art YOLOv2 detector failed to recognize these\nadversarial Stop signs in over 85% of the video frames. In an outdoor\nexperiment, YOLO was fooled by the poster and sticker attacks in 72.5% and\n63.5% of the video frames respectively. We also use Faster R-CNN, a different\nobject detection model, to demonstrate the transferability of our adversarial\nperturbations. The created poster perturbation is able to fool Faster R-CNN in\n85.9% of the video frames in a controlled lab environment, and 40.2% of the\nvideo frames in an outdoor environment. Finally, we present preliminary results\nwith a new Creation Attack, where in innocuous physical stickers fool a model\ninto detecting nonexistent objects.\n"], ["2018-07-18", "http://arxiv.org/abs/1807.10590", "Harmonic Adversarial Attack Method.", ["Wen Heng", " Shuchang Zhou", " Tingting Jiang"], "  Adversarial attacks find perturbations that can fool models into\nmisclassifying images. Previous works had successes in generating\nnoisy/edge-rich adversarial perturbations, at the cost of degradation of image\nquality. Such perturbations, even when they are small in scale, are usually\neasily spottable by human vision. In contrast, we propose Harmonic Adversar-\nial Attack Methods (HAAM), that generates edge-free perturbations by using\nharmonic functions. The property of edge-free guarantees that the generated\nadversarial images can still preserve visual quality, even when perturbations\nare of large magnitudes. Experiments also show that adversaries generated by\nHAAM often have higher rates of success when transferring between models. In\naddition, we find harmonic perturbations can simulate natural phenomena like\nnatural lighting and shadows. It would then be possible to help find corner\ncases for given models, as a first step to improving them.\n"], ["2018-07-17", "http://arxiv.org/abs/1807.06752", "Gradient Band-based Adversarial Training for Generalized Attack Immunity of A3C Path Finding.", ["Tong Chen", " Wenjia Niu", " Yingxiao Xiang", " Xiaoxuan Bai", " Jiqiang Liu", " Zhen Han", " Gang Li"], "  As adversarial attacks pose a serious threat to the security of AI system in\npractice, such attacks have been extensively studied in the context of computer\nvision applications. However, few attentions have been paid to the adversarial\nresearch on automatic path finding. In this paper, we show dominant adversarial\nexamples are effective when targeting A3C path finding, and design a Common\nDominant Adversarial Examples Generation Method (CDG) to generate dominant\nadversarial examples against any given map. In addition, we propose Gradient\nBand-based Adversarial Training, which trained with a single randomly choose\ndominant adversarial example without taking any modification, to realize the\n\"1:N\" attack immunity for generalized dominant adversarial examples. Extensive\nexperimental results show that, the lowest generation precision for CDG\nalgorithm is 91.91%, and the lowest immune precision for Gradient Band-based\nAdversarial Training is 93.89%, which can prove that our method can realize the\ngeneralized attack immunity of A3C path finding with a high confidence.\n"], ["2018-07-17", "http://arxiv.org/abs/1807.06732", "Motivating the Rules of the Game for Adversarial Example Research.", ["Justin Gilmer", " Ryan P. Adams", " Ian Goodfellow", " David Andersen", " George E. Dahl"], "  Advances in machine learning have led to broad deployment of systems with\nimpressive performance on important problems. Nonetheless, these systems can be\ninduced to make errors on data that are surprisingly similar to examples the\nlearned system handles correctly. The existence of these errors raises a\nvariety of questions about out-of-sample generalization and whether bad actors\nmight use such examples to abuse deployed systems. As a result of these\nsecurity concerns, there has been a flurry of recent papers proposing\nalgorithms to defend against such malicious perturbations of correctly handled\nexamples. It is unclear how such misclassifications represent a different kind\nof security problem than other errors, or even other attacker-produced examples\nthat have no specific relationship to an uncorrupted input. In this paper, we\nargue that adversarial example defense papers have, to date, mostly considered\nabstract, toy games that do not relate to any specific security concern.\nFurthermore, defense papers have not yet precisely described all the abilities\nand limitations of attackers that would be relevant in practical security.\nTowards this end, we establish a taxonomy of motivations, constraints, and\nabilities for more plausible adversaries. Finally, we provide a series of\nrecommendations outlining a path forward for future work to more clearly\narticulate the threat model and perform more meaningful evaluation.\n"], ["2018-07-17", "http://arxiv.org/abs/1807.06714", "Defend Deep Neural Networks Against Adversarial Examples via Fixed andDynamic Quantized Activation Functions.", ["Adnan Siraj Rakin", " Jinfeng Yi", " Boqing Gong", " Deliang Fan"], "  Recent studies have shown that deep neural networks (DNNs) are vulnerable to\nadversarial attacks. To this end, many defense approaches that attempt to\nimprove the robustness of DNNs have been proposed. In a separate and yet\nrelated area, recent works have explored to quantize neural network weights and\nactivation functions into low bit-width to compress model size and reduce\ncomputational complexity. In this work,we find that these two different tracks,\nnamely the pursuit of network compactness and robustness, can bemerged into one\nand give rise to networks of both advantages. To the best of our knowledge,\nthis is the first work that uses quantization of activation functions to defend\nagainst adversarial examples. We also propose to train robust neural networks\nby using adaptive quantization techniques for the activation functions. Our\nproposed Dynamic Quantized Activation (DQA) is verified through a wide range of\nexperiments with the MNIST and CIFAR-10 datasets under different white-box\nattack methods, including FGSM, PGD, andC&W attacks. Furthermore, Zeroth Order\nOptimization and substitute model based black-box attacks are also considered\nin this work. The experimental results clearly show that the robustness of DNNs\ncould be greatly improved using the proposed DQA.\n"], ["2018-07-16", "http://arxiv.org/abs/1807.06064", "Online Robust Policy Learning in the Presence of Unknown Adversaries.", ["Aaron J. Havens", " Zhanhong Jiang", " Soumik Sarkar"], "  The growing prospect of deep reinforcement learning (DRL) being used in\ncyber-physical systems has raised concerns around safety and robustness of\nautonomous agents. Recent work on generating adversarial attacks have shown\nthat it is computationally feasible for a bad actor to fool a DRL policy into\nbehaving sub optimally. Although certain adversarial attacks with specific\nattack models have been addressed, most studies are only interested in off-line\noptimization in the data space (e.g., example fitting, distillation). This\npaper introduces a Meta-Learned Advantage Hierarchy (MLAH) framework that is\nattack model-agnostic and more suited to reinforcement learning, via handling\nthe attacks in the decision space (as opposed to data space) and directly\nmitigating learned bias introduced by the adversary. In MLAH, we learn separate\nsub-policies (nominal and adversarial) in an online manner, as guided by a\nsupervisory master agent that detects the presence of the adversary by\nleveraging the advantage function for the sub-policies. We demonstrate that the\nproposed algorithm enables policy learning with significantly lower bias as\ncompared to the state-of-the-art policy learning approaches even in the\npresence of heavy state information attacks. We present algorithm analysis and\nsimulation results using popular OpenAI Gym environments.\n"], ["2018-07-16", "http://arxiv.org/abs/1807.05832", "Manifold Adversarial Learning.", ["Shufei Zhang", " Kaizhu Huang", " Jianke Zhu", " Yang Liu"], "  The recently proposed adversarial training methods show the robustness to\nboth adversarial and original examples and achieve state-of-the-art results in\nsupervised and semi-supervised learning. All the existing adversarial training\nmethods con- sider only how the worst perturbed examples (i.e., adversarial\nexamples) could affect the model output. Despite their success, we argue that\nsuch setting may be in lack of generalization, since the output space (or label\nspace) is apparently less informative. In this paper, we propose a novel\nmethod, called Manifold Adver- sarial Training (MAT). MAT manages to build an\nadversarial framework based on how the worst perturbation could affect the\ndistributional manifold rather than the output space. Particularly, a latent\ndata space with the Gaussian Mixture Model (GMM) will be first derived. On one\nhand, MAT tries to perturb the input samples in the way that would rough the\ndistributional manifold the worst. On the other hand, the deep learning model\nis trained trying to promote in the latent space the manifold smoothness,\nmeasured by the variation of Gaussian mixtures (given the local perturbation\naround the data point). Importantly, since the latent space is more informative\nthan the output space, the proposed MAT can learn better a ro- bust and compact\ndata representation, leading to further performance improvemen- t. The proposed\nMAT is important in that it can be considered as a superset of one\nrecently-proposed discriminative feature learning approach called center loss.\nWe conducted a series of experiments in both supervised and semi-supervised\nlearn- ing on three benchmark data sets, showing that the proposed MAT can\nachieve remarkable performance, much better than those of the state-of-the-art\nadversarial approaches.\n"], ["2018-07-12", "http://arxiv.org/abs/1807.04457", "Query-Efficient Hard-label Black-box Attack:An Optimization-based Approach.", ["Minhao Cheng", " Thong Le", " Pin-Yu Chen", " Jinfeng Yi", " Huan Zhang", " Cho-Jui Hsieh"], "  We study the problem of attacking a machine learning model in the hard-label\nblack-box setting, where no model information is revealed except that the\nattacker can make queries to probe the corresponding hard-label decisions. This\nis a very challenging problem since the direct extension of state-of-the-art\nwhite-box attacks (e.g., CW or PGD) to the hard-label black-box setting will\nrequire minimizing a non-continuous step function, which is combinatorial and\ncannot be solved by a gradient-based optimizer. The only current approach is\nbased on random walk on the boundary, which requires lots of queries and lacks\nconvergence guarantees. We propose a novel way to formulate the hard-label\nblack-box attack as a real-valued optimization problem which is usually\ncontinuous and can be solved by any zeroth order optimization algorithm. For\nexample, using the Randomized Gradient-Free method, we are able to bound the\nnumber of iterations needed for our algorithm to achieve stationary points. We\ndemonstrate that our proposed method outperforms the previous random walk\napproach to attacking convolutional neural networks on MNIST, CIFAR, and\nImageNet datasets. More interestingly, we show that the proposed algorithm can\nalso be used to attack other discrete and non-continuous machine learning\nmodels, such as Gradient Boosting Decision Trees (GBDT).\n"], ["2018-07-11", "http://arxiv.org/abs/1807.04200", "With Friends Like These, Who Needs Adversaries?.", ["Saumya Jetley", " Nicholas A. Lord", " Philip H. S. Torr"], "  The vulnerability of deep image classification networks to adversarial attack\nis now well known, but less well understood. Via a novel experimental analysis,\nwe illustrate some facts about deep convolutional networks for image\nclassification that shed new light on their behaviour and how it connects to\nthe problem of adversaries. In short, the celebrated performance of these\nnetworks and their vulnerability to adversarial attack are simply two sides of\nthe same coin: the input image-space directions along which the networks are\nmost vulnerable to attack are the same directions which they use to achieve\ntheir classification performance in the first place. We develop this result in\ntwo main steps. The first uncovers the fact that classes tend to be associated\nwith specific image-space directions. This is shown by an examination of the\nclass-score outputs of nets as functions of 1D movements along these\ndirections. This provides a novel perspective on the existence of universal\nadversarial perturbations. The second is a clear demonstration of the tight\ncoupling between classification performance and vulnerability to adversarial\nattack within the spaces spanned by these directions. Thus, our analysis\nresolves the apparent contradiction between accuracy and vulnerability. It\nprovides a new perspective on much of the prior art and reveals profound\nimplications for efforts to construct neural nets that are both accurate and\nrobust to adversarial attack.\n"], ["2018-07-10", "http://arxiv.org/abs/1807.03888", "A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks.", ["Kimin Lee", " Kibok Lee", " Honglak Lee", " Jinwoo Shin"], "  Detecting test samples drawn sufficiently far away from the training\ndistribution statistically or adversarially is a fundamental requirement for\ndeploying a good classifier in many real-world machine learning applications.\nHowever, deep neural networks with the softmax classifier are known to produce\nhighly overconfident posterior distributions even for such abnormal samples. In\nthis paper, we propose a simple yet effective method for detecting any abnormal\nsamples, which is applicable to any pre-trained softmax neural classifier. We\nobtain the class conditional Gaussian distributions with respect to (low- and\nupper-level) features of the deep models under Gaussian discriminant analysis,\nwhich result in a confidence score based on the Mahalanobis distance. While\nmost prior methods have been evaluated for detecting either out-of-distribution\nor adversarial samples, but not both, the proposed method achieves the\nstate-of-the-art performances for both cases in our experiments. Moreover, we\nfound that our proposed method is more robust in harsh cases, e.g., when the\ntraining dataset has noisy labels or small number of samples. Finally, we show\nthat the proposed method enjoys broader usage by applying it to\nclass-incremental learning: whenever out-of-distribution samples are detected,\nour classification rule can incorporate new classes well without further\ntraining deep models.\n"], ["2018-07-10", "http://arxiv.org/abs/1807.03571", "A Game-Based Approximate Verification of Deep Neural Networks with Provable Guarantees.", ["Min Wu", " Matthew Wicker", " Wenjie Ruan", " Xiaowei Huang", " Marta Kwiatkowska"], "  Despite the improved accuracy of deep neural networks, the discovery of\nadversarial examples has raised serious safety concerns. In this paper, we\nstudy two variants of pointwise robustness, the maximum safe radius problem,\nwhich for a given input sample computes the minimum distance to an adversarial\nexample, and the feature robustness problem, which aims to quantify the\nrobustness of individual features to adversarial perturbations. We demonstrate\nthat, under the assumption of Lipschitz continuity, both problems can be\napproximated using finite optimisation by discretising the input space, and the\napproximation has provable guarantees, i.e., the error is bounded. We then show\nthat the resulting optimisation problems can be reduced to the solution of\ntwo-player turn-based games, where the first player selects features and the\nsecond perturbs the image within the feature. While the second player aims to\nminimise the distance to an adversarial example, depending on the optimisation\nobjective the first player can be cooperative or competitive. We employ an\nanytime approach to solve the games, in the sense of approximating the value of\na game by monotonically improving its upper and lower bounds. The Monte Carlo\ntree search algorithm is applied to compute upper bounds for both games, and\nthe Admissible A* and the Alpha-Beta Pruning algorithms are, respectively, used\nto compute lower bounds for the maximum safety radius and feature robustness\ngames. When working on the upper bound of the maximum safe radius problem, our\ntool demonstrates competitive performance against existing adversarial example\ncrafting algorithms. Furthermore, we show how our framework can be deployed to\nevaluate pointwise robustness of neural networks in safety-critical\napplications such as traffic sign recognition in self-driving cars.\n"], ["2018-07-10", "http://arxiv.org/abs/1807.04270", "Attack and defence in cellular decision-making: lessons from machine learning.", ["Thomas J. Rademaker", " Emmanuel Bengio", " Paul Fran\u00e7ois"], "  Machine learning algorithms can be fooled by small well-designed adversarial\nperturbations. This is reminiscent of cellular decision-making where ligands\n(called antagonists) prevent correct signalling, like in early immune\nrecognition. We draw a formal analogy between neural networks used in machine\nlearning and models of cellular decision-making (adaptive proofreading). We\napply attacks from machine learning to simple decision-making models, and show\nexplicitly the correspondence to antagonism by weakly bound ligands. Such\nantagonism is absent in more nonlinear models, which inspired us to implement a\nbiomimetic defence in neural networks filtering out adversarial perturbations.\nWe then apply a gradient-descent approach from machine learning to different\ncellular decision-making models, and we reveal the existence of two regimes\ncharacterized by the presence or absence of a critical point for the gradient.\nThis critical point causes the strongest antagonists to lie close to the\ndecision boundary. This is validated in the loss landscapes of robust neural\nnetworks and cellular decision-making models, and observed experimentally for\nimmune cells. For both regimes, we explain how associated defence mechanisms\nshape the geometry of the loss landscape, and why different adversarial attacks\nare effective in different regimes. Our work connects evolved cellular\ndecision-making to machine learning, and motivates the design of a general\ntheory of adversarial perturbations, both for in vivo and in silico systems.\n"], ["2018-07-09", "http://arxiv.org/abs/1807.03326", "Adaptive Adversarial Attack on Scene Text Recognition.", ["Xiaoyong Yuan", " Pan He", " Xiaolin Andy Li"], "  Recent studies have shown that state-of-the-art deep learning models are\nvulnerable to the inputs with small perturbations (adversarial examples). We\nobserve two critical obstacles in adversarial examples: (i) Strong adversarial\nattacks (e.g., C&W attack) require manually tuning hyper-parameters and take a\nlong time to construct an adversarial example, making it impractical to attack\nreal-time systems; (ii) Most of the studies focus on non-sequential tasks, such\nas image classification, yet only a few consider sequential tasks. In this\nwork, we speed up adversarial attacks, especially on sequential learning tasks.\nBy leveraging the uncertainty of each task, we directly learn the adaptive\nmulti-task weightings, without manually searching hyper-parameters. A unified\narchitecture is developed and evaluated for both non-sequential tasks and\nsequential ones. To validate the effectiveness, we take the scene text\nrecognition task as a case study. To our best knowledge, our proposed method is\nthe first attempt to adversarial attack for scene text recognition. Adaptive\nAttack achieves over 99.9\\% success rate with 3-6X speedup compared to\nstate-of-the-art adversarial attacks.\n"], ["2018-07-08", "http://arxiv.org/abs/1807.02905", "Vulnerability Analysis of Chest X-Ray Image Classification Against Adversarial Attacks.", ["Saeid Asgari Taghanaki", " Arkadeep Das", " Ghassan Hamarneh"], "  Recently, there have been several successful deep learning approaches for\nautomatically classifying chest X-ray images into different disease categories.\nHowever, there is not yet a comprehensive vulnerability analysis of these\nmodels against the so-called adversarial perturbations/attacks, which makes\ndeep models more trustful in clinical practices. In this paper, we extensively\nanalyzed the performance of two state-of-the-art classification deep networks\non chest X-ray images. These two networks were attacked by three different\ncategories (ten methods in total) of adversarial methods (both white- and\nblack-box), namely gradient-based, score-based, and decision-based attacks.\nFurthermore, we modified the pooling operations in the two classification\nnetworks to measure their sensitivities against different attacks, on the\nspecific task of chest X-ray classification.\n"], ["2018-07-05", "http://arxiv.org/abs/1807.02188", "Implicit Generative Modeling of Random Noise during Training for Adversarial Robustness.", ["Priyadarshini Panda", " Kaushik Roy"], "  We introduce a Noise-based prior Learning (NoL) approach for training neural\nnetworks that are intrinsically robust to adversarial attacks. We find that the\nimplicit generative modeling of random noise with the same loss function used\nduring posterior maximization, improves a model's understanding of the data\nmanifold furthering adversarial robustness. We evaluate our approach's efficacy\nand provide a simplistic visualization tool for understanding adversarial data,\nusing Principal Component Analysis. Our analysis reveals that adversarial\nrobustness, in general, manifests in models with higher variance along the\nhigh-ranked principal components. We show that models learnt with our approach\nperform remarkably well against a wide-range of attacks. Furthermore, combining\nNoL with state-of-the-art adversarial training extends the robustness of a\nmodel, even beyond what it is adversarially trained for, in both white-box and\nblack-box attack scenarios.\n"], ["2018-07-04", "http://arxiv.org/abs/1807.01697", "Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations.", ["Dan Hendrycks", " Thomas G. Dietterich"], "  In this paper we establish rigorous benchmarks for image classifier\nrobustness. Our first benchmark, ImageNet-C, standardizes and expands the\ncorruption robustness topic, while showing which classifiers are preferable in\nsafety-critical applications. Unlike recent robustness research, this benchmark\nevaluates performance on commonplace corruptions not worst-case adversarial\ncorruptions. We find that there are negligible changes in relative corruption\nrobustness from AlexNet to ResNet classifiers, and we discover ways to enhance\ncorruption robustness. Then we propose a new dataset called Icons-50 which\nopens research on a new kind of robustness, surface variation robustness. With\nthis dataset we evaluate the frailty of classifiers on new styles of known\nobjects and unexpected instances of known classes. We also demonstrate two\nmethods that improve surface variation robustness. Together our benchmarks may\naid future work toward networks that learn fundamental class structure and also\nrobustly generalize.\n"], ["2018-07-03", "http://arxiv.org/abs/1807.01216", "Local Gradients Smoothing: Defense against localized adversarial attacks.", ["Muzammal Naseer", " Salman H. Khan", " Fatih Porikli"], "  Deep neural networks (DNNs) have shown vulnerability to adversarial attacks,\ni.e., carefully perturbed inputs designed to mislead the network at inference\ntime. Recently introduced localized attacks, Localized and Visible Adversarial\nNoise (LaVAN) and Adversarial patch, pose a new challenge to deep learning\nsecurity by adding adversarial noise only within a specific region without\naffecting the salient objects in an image. Driven by the observation that such\nattacks introduce concentrated high-frequency changes at a particular image\nlocation, we have developed an effective method to estimate noise location in\ngradient domain and transform those high activation regions caused by\nadversarial noise in image domain while having minimal effect on the salient\nobject that is important for correct classification. Our proposed Local\nGradients Smoothing (LGS) scheme achieves this by regularizing gradients in the\nestimated noisy region before feeding the image to DNN for inference. We have\nshown the effectiveness of our method in comparison to other defense methods\nincluding Digital Watermarking, JPEG compression, Total Variance Minimization\n(TVM) and Feature squeezing on ImageNet dataset. In addition, we systematically\nstudy the robustness of the proposed defense mechanism against Back Pass\nDifferentiable Approximation (BPDA), a state of the art attack recently\ndeveloped to break defenses that transform an input sample to minimize the\nadversarial effect. Compared to other defense mechanisms, LGS is by far the\nmost resistant to BPDA in localized adversarial attack setting.\n"], ["2018-07-03", "http://arxiv.org/abs/1807.01069", "Adversarial Robustness Toolbox v0.4.0.", ["Maria-Irina Nicolae", " Mathieu Sinn", " Minh Ngoc Tran", " Ambrish Rawat", " Martin Wistuba", " Valentina Zantedeschi", " Nathalie Baracaldo", " Bryant Chen", " Heiko Ludwig", " Ian M. Molloy", " Ben Edwards"], "  Adversarial examples have become an indisputable threat to the security of\nmodern AI systems based on deep neural networks (DNNs). The Adversarial\nRobustness Toolbox (ART) is a Python library designed to support researchers\nand developers in creating novel defence techniques, as well as in deploying\npractical defences of real-world AI systems. Researchers can use ART to\nbenchmark novel defences against the state-of-the-art. For developers, the\nlibrary provides interfaces which support the composition of comprehensive\ndefence systems using individual methods as building blocks. The Adversarial\nRobustness Toolbox supports machine learning models (and deep neural networks\n(DNNs) specifically) implemented in any of the most popular deep learning\nframeworks (TensorFlow, Keras, PyTorch and MXNet). Currently, the library is\nprimarily intended to improve the adversarial robustness of visual recognition\nsystems, however, future releases that will comprise adaptations to other data\nmodes (such as speech, text or time series) are envisioned. The ART source code\nis released (https://github.com/IBM/adversarial-robustness-toolbox) under an\nMIT license. The release includes code examples and extensive documentation\n(http://adversarial-robustness-toolbox.readthedocs.io) to help researchers and\ndevelopers get quickly started.\n"], ["2018-07-02", "http://arxiv.org/abs/1807.00458", "Adversarial Perturbations Against Real-Time Video Classification Systems.", ["Shasha Li", " Ajaya Neupane", " Sujoy Paul", " Chengyu Song", " Srikanth V. Krishnamurthy", " Amit K. Roy Chowdhury", " Ananthram Swami"], "  Recent research has demonstrated the brittleness of machine learning systems\nto adversarial perturbations. However, the studies have been mostly limited to\nperturbations on images and more generally, classification that does not deal\nwith temporally varying inputs. In this paper we ask \"Are adversarial\nperturbations possible in real-time video classification systems and if so,\nwhat properties must they satisfy?\" Such systems find application in\nsurveillance applications, smart vehicles, and smart elderly care and thus,\nmisclassification could be particularly harmful (e.g., a mishap at an elderly\ncare facility may be missed). We show that accounting for temporal structure is\nkey to generating adversarial examples in such systems. We exploit recent\nadvances in generative adversarial network (GAN) architectures to account for\ntemporal correlations and generate adversarial samples that can cause\nmisclassification rates of over 80% for targeted activities. More importantly,\nthe samples also leave other activities largely unaffected making them\nextremely stealthy. Finally, we also surprisingly find that in many scenarios,\nthe same perturbation can be applied to every frame in a video clip that makes\nthe adversary's ability to achieve misclassification relatively easy.\n"], ["2018-07-01", "http://arxiv.org/abs/1807.00340", "Towards Adversarial Training with Moderate Performance Improvement for Neural Network Classification.", ["Xinhan Di", " Pengqian Yu", " Meng Tian"], "  It has been demonstrated that deep neural networks are prone to noisy\nexamples particular adversarial samples during inference process. The gap\nbetween robust deep learning systems in real world applications and vulnerable\nneural networks is still large. Current adversarial training strategies improve\nthe robustness against adversarial samples. However, these methods lead to\naccuracy reduction when the input examples are clean thus hinders the\npracticability. In this paper, we investigate an approach that protects the\nneural network classification from the adversarial samples and improves its\naccuracy when the input examples are clean. We demonstrate the versatility and\neffectiveness of our proposed approach on a variety of different networks and\ndatasets.\n"], ["2018-06-29", "http://arxiv.org/abs/1807.00051", "Adversarial Examples in Deep Learning: Characterization and Divergence.", ["Wenqi Wei", " Ling Liu", " Margaret Loper", " Stacey Truex", " Lei Yu", " Mehmet Emre Gursoy", " Yanzhao Wu"], "  The burgeoning success of deep learning has raised the security and privacy\nconcerns as more and more tasks are accompanied with sensitive data.\nAdversarial attacks in deep learning have emerged as one of the dominating\nsecurity threat to a range of mission-critical deep learning systems and\napplications. This paper takes a holistic and principled approach to perform\nstatistical characterization of adversarial examples in deep learning. We\nprovide a general formulation of adversarial examples and elaborate on the\nbasic principle for adversarial attack algorithm design. We introduce easy and\nhard categorization of adversarial attacks to analyze the effectiveness of\nadversarial examples in terms of attack success rate, degree of change in\nadversarial perturbation, average entropy of prediction qualities, and fraction\nof adversarial examples that lead to successful attacks. We conduct extensive\nexperimental study on adversarial behavior in easy and hard attacks under deep\nlearning models with different hyperparameters and different deep learning\nframeworks. We show that the same adversarial attack behaves differently under\ndifferent hyperparameters and across different frameworks due to the different\nfeatures learned under different deep learning model training process. Our\nstatistical characterization with strong empirical evidence provides a\ntransformative enlightenment on mitigation strategies towards effective\ncountermeasures against present and future adversarial attacks.\n"], ["2018-06-28", "http://arxiv.org/abs/1806.11146", "Adversarial Reprogramming of Neural Networks.", ["Gamaleldin F. Elsayed", " Ian Goodfellow", " Jascha Sohl-Dickstein"], "  Deep neural networks are susceptible to \\emph{adversarial} attacks. In\ncomputer vision, well-crafted perturbations to images can cause neural networks\nto make mistakes such as confusing a cat with a computer. Previous adversarial\nattacks have been designed to degrade performance of models or cause machine\nlearning models to produce specific outputs chosen ahead of time by the\nattacker. We introduce attacks that instead {\\em reprogram} the target model to\nperform a task chosen by the attacker---without the attacker needing to specify\nor compute the desired output for each test-time input. This attack finds a\nsingle adversarial perturbation, that can be added to all test-time inputs to a\nmachine learning model in order to cause the model to perform a task chosen by\nthe adversary---even if the model was not trained to do this task. These\nperturbations can thus be considered a program for the new task. We demonstrate\nadversarial reprogramming on six ImageNet classification models, repurposing\nthese models to perform a counting task, as well as classification tasks:\nclassification of MNIST and CIFAR-10 examples presented as inputs to the\nImageNet model.\n"], ["2018-06-27", "http://arxiv.org/abs/1806.10707", "Gradient Similarity: An Explainable Approach to Detect Adversarial Attacks against Deep Learning.", ["Jasjeet Dhaliwal", " Saurabh Shintre"], "  Deep neural networks are susceptible to small-but-specific adversarial\nperturbations capable of deceiving the network. This vulnerability can lead to\npotentially harmful consequences in security-critical applications. To address\nthis vulnerability, we propose a novel metric called \\emph{Gradient Similarity}\nthat allows us to capture the influence of training data on test inputs. We\nshow that \\emph{Gradient Similarity} behaves differently for normal and\nadversarial inputs, and enables us to detect a variety of adversarial attacks\nwith a near perfect ROC-AUC of 95-100\\%. Even white-box adversaries equipped\nwith perfect knowledge of the system cannot bypass our detector easily. On the\nMNIST dataset, white-box attacks are either detected with a high ROC-AUC of\n87-96\\%, or require very high distortion to bypass our detector.\n"], ["2018-06-27", "http://arxiv.org/abs/1806.10496", "Customizing an Adversarial Example Generator with Class-Conditional GANs.", ["Shih-hong Tsai"], "  Adversarial examples are intentionally crafted data with the purpose of\ndeceiving neural networks into misclassification. When we talk about strategies\nto create such examples, we usually refer to perturbation-based methods that\nfabricate adversarial examples by applying invisible perturbations onto normal\ndata. The resulting data reserve their visual appearance to human observers,\nyet can be totally unrecognizable to DNN models, which in turn leads to\ncompletely misleading predictions. In this paper, however, we consider crafting\nadversarial examples from existing data as a limitation to example diversity.\nWe propose a non-perturbation-based framework that generates native adversarial\nexamples from class-conditional generative adversarial networks.As such, the\ngenerated data will not resemble any existing data and thus expand example\ndiversity, raising the difficulty in adversarial defense. We then extend this\nframework to pre-trained conditional GANs, in which we turn an existing\ngenerator into an \"adversarial-example generator\". We conduct experiments on\nour approach for MNIST and CIFAR10 datasets and have satisfactory results,\nshowing that this approach can be a potential alternative to previous attack\nstrategies.\n"], ["2018-06-25", "http://arxiv.org/abs/1806.09410", "Exploring Adversarial Examples: Patterns of One-Pixel Attacks.", ["David K\u00fcgler", " Alexander Distergoft", " Arjan Kuijper", " Anirban Mukhopadhyay"], "  Failure cases of black-box deep learning, e.g. adversarial examples, might\nhave severe consequences in healthcare. Yet such failures are mostly studied in\nthe context of real-world images with calibrated attacks. To demystify the\nadversarial examples, rigorous studies need to be designed. Unfortunately,\ncomplexity of the medical images hinders such study design directly from the\nmedical images. We hypothesize that adversarial examples might result from the\nincorrect mapping of image space to the low dimensional generation manifold by\ndeep networks. To test the hypothesis, we simplify a complex medical problem\nnamely pose estimation of surgical tools into its barest form. An analytical\ndecision boundary and exhaustive search of the one-pixel attack across multiple\nimage dimensions let us localize the regions of frequent successful one-pixel\nattacks at the image space.\n"], ["2018-06-23", "http://arxiv.org/abs/1806.09035", "Defending Malware Classification Networks Against Adversarial Perturbations with Non-Negative Weight Restrictions.", ["Alex Kouzemtchenko"], "  There is a growing body of literature showing that deep neural networks are\nvulnerable to adversarial input modification. Recently this work has been\nextended from image classification to malware classification over boolean\nfeatures. In this paper we present several new methods for training restricted\nnetworks in this specific domain that are highly effective at preventing\nadversarial perturbations. We start with a fully adversarially resistant neural\nnetwork that has hard non-negative weight restrictions and is equivalent to\nlearning a monotonic boolean function and then attempt to relax the constraints\nto improve classifier accuracy.\n"], ["2018-06-23", "http://arxiv.org/abs/1806.09030", "On Adversarial Examples for Character-Level Neural Machine Translation.", ["Javid Ebrahimi", " Daniel Lowd", " Dejing Dou"], "  Evaluating on adversarial examples has become a standard procedure to measure\nrobustness of deep learning models. Due to the difficulty of creating white-box\nadversarial examples for discrete text input, most analyses of the robustness\nof NLP models have been done through black-box adversarial examples. We\ninvestigate adversarial examples for character-level neural machine translation\n(NMT), and contrast black-box adversaries with a novel white-box adversary,\nwhich employs differentiable string-edit operations to rank adversarial\nchanges. We propose two novel types of attacks which aim to remove or change a\nword in a translation, rather than simply break the NMT. We demonstrate that\nwhite-box adversarial examples are significantly stronger than their black-box\ncounterparts in different attack scenarios, which show more serious\nvulnerabilities than previously known. In addition, after performing\nadversarial training, which takes only 3 times longer than regular training, we\ncan improve the model's robustness significantly.\n"], ["2018-06-23", "http://arxiv.org/abs/1806.08970", "Evaluation of Momentum Diverse Input Iterative Fast Gradient Sign Method (M-DI2-FGSM) Based Attack Method on MCS 2018 Adversarial Attacks on Black Box Face Recognition System.", ["Md Ashraful Alam Milton"], "  The convolutional neural network is the crucial tool for the recent success\nof deep learning based methods on various computer vision tasks like\nclassification, segmentation, and detection. Convolutional neural networks\nachieved state-of-the-art performance in these tasks and every day pushing the\nlimit of computer vision and AI. However, adversarial attack on computer vision\nsystems is threatening their application in the real life and in\nsafety-critical applications. Necessarily, Finding adversarial examples are\nimportant to detect susceptible models to attack and take safeguard measures to\novercome the adversarial attacks. In this regard, MCS 2018 Adversarial Attacks\non Black Box Face Recognition challenge aims to facilitate the research of\nfinding new adversarial attack techniques and their effectiveness in generating\nadversarial examples. In this challenge, the attack\"s nature is targeted-attack\non the black-box neural network where we have no knowledge about black-block\"s\ninner structure. The attacker must modify a set of five images of a single\nperson so that the neural network miss-classify them as target image which is a\nset of five images of another person. In this competition, we applied Momentum\nDiverse Input Iterative Fast Gradient Sign Method (M-DI2-FGSM) to make an\nadversarial attack on black-box face recognition system. We tested our method\non MCS 2018 Adversarial Attacks on Black Box Face Recognition challenge and\nfound competitive result. Our solution got validation score 1.404 which better\nthan baseline score 1.407 and stood 14 place among 132 teams in the\nleader-board. Further improvement can be achieved by finding improved feature\nextraction from source image, carefully chosen hyper-parameters, finding\nimproved substitute model of the black-box and better optimization method.\n"], ["2018-06-21", "http://arxiv.org/abs/1806.09186", "Detection based Defense against Adversarial Examples from the Steganalysis Point of View.", ["Jiayang Liu", " Weiming Zhang", " Yiwei Zhang", " Dongdong Hou", " Yujia Liu", " Hongyue Zha", " Nenghai Yu"], "  Deep Neural Networks (DNNs) have recently led to significant improvements in\nmany fields. However, DNNs are vulnerable to adversarial examples which are\nsamples with imperceptible perturbations while dramatically misleading the\nDNNs. Moreover, adversarial examples can be used to perform an attack on\nvarious kinds of DNN based systems, even if the adversary has no access to the\nunderlying model. Many defense methods have been proposed, such as obfuscating\ngradients of the networks or detecting adversarial examples. However it is\nproved out that these defense methods are not effective or cannot resist\nsecondary adversarial attacks. In this paper, we point out that steganalysis\ncan be applied to adversarial examples detection, and propose a method to\nenhance steganalysis features by estimating the probability of modifications\ncaused by adversarial attacks. Experimental results show that the proposed\nmethod can accurately detect adversarial examples. Moreover, secondary\nadversarial attacks cannot be directly performed to our method because our\nmethod is not based on a neural network but based on high-dimensional\nartificial features and FLD (Fisher Linear Discriminant) ensemble.\n"], ["2018-06-20", "http://arxiv.org/abs/1806.08028", "Gradient Adversarial Training of Neural Networks.", ["Ayan Sinha", " Zhao Chen", " Vijay Badrinarayanan", " Andrew Rabinovich"], "  We propose gradient adversarial training, an auxiliary deep learning\nframework applicable to different machine learning problems. In gradient\nadversarial training, we leverage a prior belief that in many contexts,\nsimultaneous gradient updates should be statistically indistinguishable from\neach other. We enforce this consistency using an auxiliary network that\nclassifies the origin of the gradient tensor, and the main network serves as an\nadversary to the auxiliary network in addition to performing standard\ntask-based training. We demonstrate gradient adversarial training for three\ndifferent scenarios: (1) as a defense to adversarial examples we classify\ngradient tensors and tune them to be agnostic to the class of their\ncorresponding example, (2) for knowledge distillation, we do binary\nclassification of gradient tensors derived from the student or teacher network\nand tune the student gradient tensor to mimic the teacher's gradient tensor;\nand (3) for multi-task learning we classify the gradient tensors derived from\ndifferent task loss functions and tune them to be statistically\nindistinguishable. For each of the three scenarios we show the potential of\ngradient adversarial training procedure. Specifically, gradient adversarial\ntraining increases the robustness of a network to adversarial attacks, is able\nto better distill the knowledge from a teacher network to a student network\ncompared to soft targets, and boosts multi-task learning by aligning the\ngradient tensors derived from the task specific loss functions. Overall, our\nexperiments demonstrate that gradient tensors contain latent information about\nwhatever tasks are being trained, and can support diverse machine learning\nproblems when intelligently guided through adversarialization using a auxiliary\nnetwork.\n"], ["2018-06-20", "http://arxiv.org/abs/1806.07723", "Combinatorial Testing for Deep Learning Systems.", ["Lei Ma", " Fuyuan Zhang", " Minhui Xue", " Bo Li", " Yang Liu", " Jianjun Zhao", " Yadong Wang"], "  Deep learning (DL) has achieved remarkable progress over the past decade and\nbeen widely applied to many safety-critical applications. However, the\nrobustness of DL systems recently receives great concerns, such as adversarial\nexamples against computer vision systems, which could potentially result in\nsevere consequences. Adopting testing techniques could help to evaluate the\nrobustness of a DL system and therefore detect vulnerabilities at an early\nstage. The main challenge of testing such systems is that its runtime state\nspace is too large: if we view each neuron as a runtime state for DL, then a DL\nsystem often contains massive states, rendering testing each state almost\nimpossible. For traditional software, combinatorial testing (CT) is an\neffective testing technique to reduce the testing space while obtaining\nrelatively high defect detection abilities. In this paper, we perform an\nexploratory study of CT on DL systems. We adapt the concept in CT and propose a\nset of coverage criteria for DL systems, as well as a CT coverage guided test\ngeneration technique. Our evaluation demonstrates that CT provides a promising\navenue for testing DL systems. We further pose several open questions and\ninteresting directions for combinatorial testing of DL systems.\n"], ["2018-06-19", "http://arxiv.org/abs/1806.07492", "On the Learning of Deep Local Features for Robust Face Spoofing Detection.", ["Souza Gustavo Botelho de", " Jo\u00e3o Paulo Papa", " Aparecido Nilceu Marana"], "  Biometrics emerged as a robust solution for security systems. However, given\nthe dissemination of biometric applications, criminals are developing\ntechniques to circumvent them by simulating physical or behavioral traits of\nlegal users (spoofing attacks). Despite face being a promising characteristic\ndue to its universality, acceptability and presence of cameras almost\neverywhere, face recognition systems are extremely vulnerable to such frauds\nsince they can be easily fooled with common printed facial photographs.\nState-of-the-art approaches, based on Convolutional Neural Networks (CNNs),\npresent good results in face spoofing detection. However, these methods do not\nconsider the importance of learning deep local features from each facial\nregion, even though it is known from face recognition that each facial region\npresents different visual aspects, which can also be exploited for face\nspoofing detection. In this work we propose a novel CNN architecture trained in\ntwo steps for such task. Initially, each part of the neural network learns\nfeatures from a given facial region. Afterwards, the whole model is fine-tuned\non the whole facial images. Results show that such pre-training step allows the\nCNN to learn different local spoofing cues, improving the performance and the\nconvergence speed of the final model, outperforming the state-of-the-art\napproaches.\n"], ["2018-06-19", "http://arxiv.org/abs/1806.07409", "Built-in Vulnerabilities to Imperceptible Adversarial Perturbations.", ["Thomas Tanay", " Jerone T. A. Andrews", " Lewis D. Griffin"], "  Designing models that are robust to small adversarial perturbations of their\ninputs has proven remarkably difficult. In this work we show that the reverse\nproblem---making models more vulnerable---is surprisingly easy. After\npresenting some proofs of concept on MNIST, we introduce a generic tilting\nattack that injects vulnerabilities into the linear layers of pre-trained\nnetworks by increasing their sensitivity to components of low variance in the\ntraining data without affecting their performance on test data. We illustrate\nthis attack on a multilayer perceptron trained on SVHN and use it to design a\nstand-alone adversarial module which we call a steganogram decoder. Finally, we\nshow on CIFAR-10 that a poisoning attack with a poisoning rate as low as 0.1%\ncan induce vulnerabilities to chosen imperceptible backdoor signals in\nstate-of-the-art networks. Beyond their practical implications, these different\nresults shed new light on the nature of the adversarial example phenomenon.\n"], ["2018-06-15", "http://arxiv.org/abs/1806.06108", "Non-Negative Networks Against Adversarial Attacks.", ["William Fleshman", " Edward Raff", " Jared Sylvester", " Steven Forsyth", " Mark McLean"], "  Adversarial attacks against neural networks are a problem of considerable\nimportance, for which effective defenses are not yet readily available. We make\nprogress toward this problem by showing that non-negative weight constraints\ncan be used to improve resistance in specific scenarios. In particular, we show\nthat they can provide an effective defense for binary classification problems\nwith asymmetric cost, such as malware or spam detection. We also show the\npotential for non-negativity to be helpful to non-binary problems by applying\nit to image classification.\n"], ["2018-06-14", "http://arxiv.org/abs/1806.05476", "Copycat CNN: Stealing Knowledge by Persuading Confession with Random Non-Labeled Data.", ["Jacson Rodrigues Correia-Silva", " Rodrigo F. Berriel", " Claudine Badue", " Souza Alberto F. de", " Thiago Oliveira-Santos"], "  In the past few years, Convolutional Neural Networks (CNNs) have been\nachieving state-of-the-art performance on a variety of problems. Many companies\nemploy resources and money to generate these models and provide them as an API,\ntherefore it is in their best interest to protect them, i.e., to avoid that\nsomeone else copies them. Recent studies revealed that state-of-the-art CNNs\nare vulnerable to adversarial examples attacks, and this weakness indicates\nthat CNNs do not need to operate in the problem domain (PD). Therefore, we\nhypothesize that they also do not need to be trained with examples of the PD in\norder to operate in it.\n  Given these facts, in this paper, we investigate if a target black-box CNN\ncan be copied by persuading it to confess its knowledge through random\nnon-labeled data. The copy is two-fold: i) the target network is queried with\nrandom data and its predictions are used to create a fake dataset with the\nknowledge of the network; and ii) a copycat network is trained with the fake\ndataset and should be able to achieve similar performance as the target\nnetwork.\n  This hypothesis was evaluated locally in three problems (facial expression,\nobject, and crosswalk classification) and against a cloud-based API. In the\ncopy attacks, images from both non-problem domain and PD were used. All copycat\nnetworks achieved at least 93.7% of the performance of the original models with\nnon-problem domain data, and at least 98.6% using additional data from the PD.\nAdditionally, the copycat CNN successfully copied at least 97.3% of the\nperformance of the Microsoft Azure Emotion API. Our results show that it is\npossible to create a copycat CNN by simply querying a target network as\nblack-box with random non-labeled data.\n"], ["2018-06-13", "http://arxiv.org/abs/1806.05337", "Hierarchical interpretations for neural network predictions.", ["Chandan Singh", " W. James Murdoch", " Bin Yu"], "  Deep neural networks (DNNs) have achieved impressive predictive performance\ndue to their ability to learn complex, non-linear relationships between\nvariables. However, the inability to effectively visualize these relationships\nhas led to DNNs being characterized as black boxes and consequently limited\ntheir applications. To ameliorate this problem, we introduce the use of\nhierarchical interpretations to explain DNN predictions through our proposed\nmethod, agglomerative contextual decomposition (ACD). Given a prediction from a\ntrained DNN, ACD produces a hierarchical clustering of the input features,\nalong with the contribution of each cluster to the final prediction. This\nhierarchy is optimized to identify clusters of features that the DNN learned\nare predictive. Using examples from Stanford Sentiment Treebank and ImageNet,\nwe show that ACD is effective at diagnosing incorrect predictions and\nidentifying dataset bias. Through human experiments, we demonstrate that ACD\nenables users both to identify the more accurate of two DNNs and to better\ntrust a DNN's outputs. We also find that ACD's hierarchy is largely robust to\nadversarial perturbations, implying that it captures fundamental aspects of the\ninput and ignores spurious noise.\n"], ["2018-06-13", "http://arxiv.org/abs/1806.05236", "Manifold Mixup: Better Representations by Interpolating Hidden States.", ["Vikas Verma", " Alex Lamb", " Christopher Beckham", " Amir Najafi", " Ioannis Mitliagkas", " Aaron Courville", " David Lopez-Paz", " Yoshua Bengio"], "  Deep neural networks excel at learning the training data, but often provide\nincorrect and confident predictions when evaluated on slightly different test\nexamples. This includes distribution shifts, outliers, and adversarial\nexamples. To address these issues, we propose Manifold Mixup, a simple\nregularizer that encourages neural networks to predict less confidently on\ninterpolations of hidden representations. Manifold Mixup leverages semantic\ninterpolations as additional training signal, obtaining neural networks with\nsmoother decision boundaries at multiple levels of representation. As a result,\nneural networks trained with Manifold Mixup learn class-representations with\nfewer directions of variance. We prove theory on why this flattening happens\nunder ideal conditions, validate it on practical situations, and connect it to\nprevious works on information theory and generalization. In spite of incurring\nno significant computation and being implemented in a few lines of code,\nManifold Mixup improves strong baselines in supervised learning, robustness to\nsingle-step adversarial attacks, and test log-likelihood.\n"], ["2018-06-12", "http://arxiv.org/abs/1806.04646", "Adversarial Attacks on Variational Autoencoders.", ["George Gondim-Ribeiro", " Pedro Tabacof", " Eduardo Valle"], "  Adversarial attacks are malicious inputs that derail machine-learning models.\nWe propose a scheme to attack autoencoders, as well as a quantitative\nevaluation framework that correlates well with the qualitative assessment of\nthe attacks. We assess --- with statistically validated experiments --- the\nresistance to attacks of three variational autoencoders (simple, convolutional,\nand DRAW) in three datasets (MNIST, SVHN, CelebA), showing that both DRAW's\nrecurrence and attention mechanism lead to better resistance. As autoencoders\nare proposed for compressing data --- a scenario in which their safety is\nparamount --- we expect more attention will be given to adversarial attacks on\nthem.\n"], ["2018-06-12", "http://arxiv.org/abs/1806.04425", "Ranking Robustness Under Adversarial Document Manipulations.", ["Gregory Goren", " Oren Kurland", " Moshe Tennenholtz", " Fiana Raiber"], "  For many queries in the Web retrieval setting there is an on-going ranking\ncompetition: authors manipulate their documents so as to promote them in\nrankings. Such competitions can have unwarranted effects not only in terms of\nretrieval effectiveness, but also in terms of ranking robustness. A case in\npoint, rankings can (rapidly) change due to small indiscernible perturbations\nof documents. While there has been a recent growing interest in analyzing the\nrobustness of classifiers to adversarial manipulations, there has not yet been\na study of the robustness of relevance-ranking functions. We address this\nchallenge by formally analyzing different definitions and aspects of the\nrobustness of learning-to-rank-based ranking functions. For example, we\nformally show that increased regularization of linear ranking functions\nincreases ranking robustness. This finding leads us to conjecture that\ndecreased variance of any ranking function results in increased robustness. We\npropose several measures for quantifying ranking robustness and use them to\nanalyze ranking competitions between documents' authors. The empirical findings\nsupport our formal analysis and conjecture for both RankSVM and LambdaMART.\n"], ["2018-06-11", "http://arxiv.org/abs/1806.04169", "Defense Against the Dark Arts: An overview of adversarial example security research and future research directions.", ["Ian Goodfellow"], "  This article presents a summary of a keynote lecture at the Deep Learning\nSecurity workshop at IEEE Security and Privacy 2018. This lecture summarizes\nthe state of the art in defenses against adversarial examples and provides\nrecommendations for future research directions on this topic.\n"], ["2018-06-08", "http://arxiv.org/abs/1806.02977", "Monge blunts Bayes: Hardness Results for Adversarial Training.", ["Zac Cranko", " Aditya Krishna Menon", " Richard Nock", " Cheng Soon Ong", " Zhan Shi", " Christian Walder"], "  The last few years have seen a staggering number of empirical studies of the\nrobustness of neural networks in a model of adversarial perturbations of their\ninputs. Most rely on an adversary which carries out local modifications within\nprescribed balls. None however has so far questioned the broader picture: how\nto frame a resource-bounded adversary so that it can be severely detrimental to\nlearning, a non-trivial problem which entails at a minimum the choice of loss\nand classifiers.\n  We suggest a formal answer for losses that satisfy the minimal statistical\nrequirement of being proper. We pin down a simple sufficient property for any\ngiven class of adversaries to be detrimental to learning, involving a central\nmeasure of \"harmfulness\" which generalizes the well-known class of integral\nprobability metrics. A key feature of our result is that it holds for all\nproper losses, and for a popular subset of these, the optimisation of this\ncentral measure appears to be independent of the loss. When classifiers are\nLipschitz -- a now popular approach in adversarial training --, this\noptimisation resorts to optimal transport to make a low-budget compression of\nclass marginals. Toy experiments reveal a finding recently separately observed:\ntraining against a sufficiently budgeted adversary of this kind improves\ngeneralization.\n"], ["2018-06-07", "http://arxiv.org/abs/1806.02924", "Revisiting Adversarial Risk.", ["Arun Sai Suggala", " Adarsh Prasad", " Vaishnavh Nagarajan", " Pradeep Ravikumar"], "  Recent works on adversarial perturbations show that there is an inherent\ntrade-off between standard test accuracy and adversarial accuracy.\nSpecifically, they show that no classifier can simultaneously be robust to\nadversarial perturbations and achieve high standard test accuracy. However,\nthis is contrary to the standard notion that on tasks such as image\nclassification, humans are robust classifiers with low error rate. In this\nwork, we show that the main reason behind this confusion is the inexact\ndefinition of adversarial perturbation that is used in the literature. To fix\nthis issue, we propose a slight, yet important modification to the existing\ndefinition of adversarial perturbation. Based on the modified definition, we\nshow that there is no trade-off between adversarial and standard accuracies;\nthere exist classifiers that are robust and achieve high standard accuracy. We\nfurther study several properties of this new definition of adversarial risk and\nits relation to the existing definition.\n"], ["2018-06-07", "http://arxiv.org/abs/1806.02782", "Training Augmentation with Adversarial Examples for Robust Speech Recognition.", ["Sining Sun", " Ching-Feng Yeh", " Mari Ostendorf", " Mei-Yuh Hwang", " Lei Xie"], "  This paper explores the use of adversarial examples in training speech\nrecognition systems to increase robustness of deep neural network acoustic\nmodels. During training, the fast gradient sign method is used to generate\nadversarial examples augmenting the original training data. Different from\nconventional data augmentation based on data transformations, the examples are\ndynamically generated based on current acoustic model parameters. We assess the\nimpact of adversarial data augmentation in experiments on the Aurora-4 and\nCHiME-4 single-channel tasks, showing improved robustness against noise and\nchannel variation. Further improvement is obtained when combining adversarial\nexamples with teacher/student training, leading to a 23% relative word error\nrate reduction on Aurora-4.\n"], ["2018-06-06", "http://arxiv.org/abs/1806.02371", "Adversarial Attack on Graph Structured Data.", ["Hanjun Dai", " Hui Li", " Tian Tian", " Xin Huang", " Lin Wang", " Jun Zhu", " Le Song"], "  Deep learning on graph structures has shown exciting results in various\napplications. However, few attentions have been paid to the robustness of such\nmodels, in contrast to numerous research work for image or text adversarial\nattack and defense. In this paper, we focus on the adversarial attacks that\nfool the model by modifying the combinatorial structure of data. We first\npropose a reinforcement learning based attack method that learns the\ngeneralizable attack policy, while only requiring prediction labels from the\ntarget classifier. Also, variants of genetic algorithms and gradient methods\nare presented in the scenario where prediction confidence or gradients are\navailable. We use both synthetic and real-world data to show that, a family of\nGraph Neural Network models are vulnerable to these attacks, in both\ngraph-level and node-level classification tasks. We also show such attacks can\nbe used to diagnose the learned classifiers.\n"], ["2018-06-06", "http://arxiv.org/abs/1806.02256", "Adversarial Regression with Multiple Learners.", ["Liang Tong", " Sixie Yu", " Scott Alfeld", " Yevgeniy Vorobeychik"], "  Despite the considerable success enjoyed by machine learning techniques in\npractice, numerous studies demonstrated that many approaches are vulnerable to\nattacks. An important class of such attacks involves adversaries changing\nfeatures at test time to cause incorrect predictions. Previous investigations\nof this problem pit a single learner against an adversary. However, in many\nsituations an adversary's decision is aimed at a collection of learners, rather\nthan specifically targeted at each independently. We study the problem of\nadversarial linear regression with multiple learners. We approximate the\nresulting game by exhibiting an upper bound on learner loss functions, and show\nthat the resulting game has a unique symmetric equilibrium. We present an\nalgorithm for computing this equilibrium, and show through extensive\nexperiments that equilibrium models are significantly more robust than\nconventional regularized linear regression.\n"], ["2018-06-06", "http://arxiv.org/abs/1806.02032", "Killing Four Birds with one Gaussian Process: Analyzing Test-Time Attack Vectors on Classification.", ["Kathrin Grosse", " Michael T. Smith", " Michael Backes"], "  The wide usage of Machine Learning (ML) leads to direct security threats, as\nML algorithms are vulnerable to a plethora of attacks themselves. Different\nattack vectors are known, and target for example the training phase using\nmanipulated data. Alternatively, they take place at test time and aim for\nmiss-classification, the leakage of the training data or extraction of the\nmodel. Previous works studied different test time attacks individually. We show\nthat using an ML model enabling formal analysis and allowing control over the\ndecision surface curvature, interesting insights can be gained when attack\nvectors are not studied in isolation but in relation to each pother. We show\nfor example how we can secure Gaussian Process Classification against empirical\nmembership inference by properly configuring the algorithm. In this\nconfiguration, however, the model's parameters are leaked. This allows an\nanalytic computation of the the training data, which is thus leaked, against\nthe original intention of protecting the data. We extend our study to evasion\nattacks, and find that analogously, hardening the model against one attack\nboils down to enabling a different attacker.\n"], ["2018-06-05", "http://arxiv.org/abs/1806.02299", "DPatch: An Adversarial Patch Attack on Object Detectors.", ["Xin Liu", " Huanrui Yang", " Ziwei Liu", " Linghao Song", " Hai Li", " Yiran Chen"], "  Object detectors have emerged as an indispensable module in modern computer\nvision systems. In this work, we propose DPatch -- a black-box\nadversarial-patch-based attack towards mainstream object detectors (i.e. Faster\nR-CNN and YOLO). Unlike the original adversarial patch that only manipulates\nimage-level classifier, our DPatch simultaneously attacks the bounding box\nregression and object classification so as to disable their predictions.\nCompared to prior works, DPatch has several appealing properties: (1) DPatch\ncan perform both untargeted and targeted effective attacks, degrading the mAP\nof Faster R-CNN and YOLO from 75.10% and 65.7% down to below 1%, respectively.\n(2) DPatch is small in size and its attacking effect is location-independent,\nmaking it very practical to implement real-world attacks. (3) DPatch\ndemonstrates great transferability among different detectors as well as\ntraining datasets. For example, DPatch that is trained on Faster R-CNN can\neffectively attack YOLO, and vice versa. Extensive evaluations imply that\nDPatch can perform effective attacks under black-box setup, i.e., even without\nthe knowledge of the attacked network's architectures and parameters.\nSuccessful realization of DPatch also illustrates the intrinsic vulnerability\nof the modern detector architectures to such patch-based adversarial attacks.\n"], ["2018-06-04", "http://arxiv.org/abs/1806.02190", "Mitigation of Policy Manipulation Attacks on Deep Q-Networks with Parameter-Space Noise.", ["Vahid Behzadan", " Arslan Munir"], "  Recent developments have established the vulnerability of deep reinforcement\nlearning to policy manipulation attacks via intentionally perturbed inputs,\nknown as adversarial examples. In this work, we propose a technique for\nmitigation of such attacks based on addition of noise to the parameter space of\ndeep reinforcement learners during training. We experimentally verify the\neffect of parameter-space noise in reducing the transferability of adversarial\nexamples, and demonstrate the promising performance of this technique in\nmitigating the impact of whitebox and blackbox attacks at both test and\ntraining times.\n"], ["2018-06-04", "http://arxiv.org/abs/1806.01477", "An Explainable Adversarial Robustness Metric for Deep Learning Neural Networks.", ["Chirag Agarwal", " Bo Dong", " Dan Schonfeld", " Anthony Hoogs"], "  Deep Neural Networks(DNN) have excessively advanced the field of computer\nvision by achieving state of the art performance in various vision tasks. These\nresults are not limited to the field of vision but can also be seen in speech\nrecognition and machine translation tasks. Recently, DNNs are found to poorly\nfail when tested with samples that are crafted by making imperceptible changes\nto the original input images. This causes a gap between the validation and\nadversarial performance of a DNN. An effective and generalizable robustness\nmetric for evaluating the performance of DNN on these adversarial inputs is\nstill missing from the literature. In this paper, we propose Noise Sensitivity\nScore (NSS), a metric that quantifies the performance of a DNN on a specific\ninput under different forms of fix-directional attacks. An insightful\nmathematical explanation is provided for deeply understanding the proposed\nmetric. By leveraging the NSS, we also proposed a skewness based dataset\nrobustness metric for evaluating a DNN's adversarial performance on a given\ndataset. Extensive experiments using widely used state of the art architectures\nalong with popular classification datasets, such as MNIST, CIFAR-10, CIFAR-100,\nand ImageNet, are used to validate the effectiveness and generalization of our\nproposed metrics. Instead of simply measuring a DNN's adversarial robustness in\nthe input domain, as previous works, the proposed NSS is built on top of\ninsightful mathematical understanding of the adversarial attack and gives a\nmore explicit explanation of the robustness.\n"], ["2018-06-04", "http://arxiv.org/abs/1806.01471", "PAC-learning in the presence of evasion adversaries.", ["Daniel Cullina", " Arjun Nitin Bhagoji", " Prateek Mittal"], "  The existence of evasion attacks during the test phase of machine learning\nalgorithms represents a significant challenge to both their deployment and\nunderstanding. These attacks can be carried out by adding imperceptible\nperturbations to inputs to generate adversarial examples and finding effective\ndefenses and detectors has proven to be difficult. In this paper, we step away\nfrom the attack-defense arms race and seek to understand the limits of what can\nbe learned in the presence of an evasion adversary. In particular, we extend\nthe Probably Approximately Correct (PAC)-learning framework to account for the\npresence of an adversary. We first define corrupted hypothesis classes which\narise from standard binary hypothesis classes in the presence of an evasion\nadversary and derive the Vapnik-Chervonenkis (VC)-dimension for these, denoted\nas the adversarial VC-dimension. We then show that sample complexity upper\nbounds from the Fundamental Theorem of Statistical learning can be extended to\nthe case of evasion adversaries, where the sample complexity is controlled by\nthe adversarial VC-dimension. We then explicitly derive the adversarial\nVC-dimension for halfspace classifiers in the presence of a sample-wise\nnorm-constrained adversary of the type commonly studied for evasion attacks and\nshow that it is the same as the standard VC-dimension, closing an open\nquestion. Finally, we prove that the adversarial VC-dimension can be either\nlarger or smaller than the standard VC-dimension depending on the hypothesis\nclass and adversary, making it an interesting object of study in its own right.\n"], ["2018-06-02", "http://arxiv.org/abs/1806.00667", "Sufficient Conditions for Idealised Models to Have No Adversarial Examples: a Theoretical and Empirical Study with Bayesian Neural Networks.", ["Yarin Gal", " Lewis Smith"], "  We prove, under two sufficient conditions, that idealised models can have no\nadversarial examples. We discuss which idealised models satisfy our conditions,\nand show that idealised Bayesian neural networks (BNNs) satisfy these. We\ncontinue by studying near-idealised BNNs using HMC inference, demonstrating the\ntheoretical ideas in practice. We experiment with HMC on synthetic data derived\nfrom MNIST for which we know the ground-truth image density, showing that\nnear-perfect epistemic uncertainty correlates to density under image manifold,\nand that adversarial images lie off the manifold in our setting. This suggests\nwhy MC dropout, which can be seen as performing approximate inference, has been\nobserved to be an effective defence against adversarial examples in practice;\nWe highlight failure-cases of non-idealised BNNs relying on dropout, suggesting\na new attack for dropout models and a new defence as well. Lastly, we\ndemonstrate the defence on a cats-vs-dogs image classification task with a\nVGG13 variant.\n"], ["2018-06-02", "http://arxiv.org/abs/1806.00580", "Detecting Adversarial Examples via Key-based Network.", ["Pinlong Zhao", " Zhouyu Fu", " Ou wu", " Qinghua Hu", " Jun Wang"], "  Though deep neural networks have achieved state-of-the-art performance in\nvisual classification, recent studies have shown that they are all vulnerable\nto the attack of adversarial examples. Small and often imperceptible\nperturbations to the input images are sufficient to fool the most powerful deep\nneural networks. Various defense methods have been proposed to address this\nissue. However, they either require knowledge on the process of generating\nadversarial examples, or are not robust against new attacks specifically\ndesigned to penetrate the existing defense. In this work, we introduce\nkey-based network, a new detection-based defense mechanism to distinguish\nadversarial examples from normal ones based on error correcting output codes,\nusing the binary code vectors produced by multiple binary classifiers applied\nto randomly chosen label-sets as signatures to match normal images and reject\nadversarial examples. In contrast to existing defense methods, the proposed\nmethod does not require knowledge of the process for generating adversarial\nexamples and can be applied to defend against different types of attacks. For\nthe practical black-box and gray-box scenarios, where the attacker does not\nknow the encoding scheme, we show empirically that key-based network can\neffectively detect adversarial examples generated by several state-of-the-art\nattacks.\n"], ["2018-05-31", "http://arxiv.org/abs/1806.00088", "PeerNets: Exploiting Peer Wisdom Against Adversarial Attacks.", ["Jan Svoboda", " Jonathan Masci", " Federico Monti", " Michael M. Bronstein", " Leonidas Guibas"], "  Deep learning systems have become ubiquitous in many aspects of our lives.\nUnfortunately, it has been shown that such systems are vulnerable to\nadversarial attacks, making them prone to potential unlawful uses. Designing\ndeep neural networks that are robust to adversarial attacks is a fundamental\nstep in making such systems safer and deployable in a broader variety of\napplications (e.g. autonomous driving), but more importantly is a necessary\nstep to design novel and more advanced architectures built on new computational\nparadigms rather than marginally building on the existing ones. In this paper\nwe introduce PeerNets, a novel family of convolutional networks alternating\nclassical Euclidean convolutions with graph convolutions to harness information\nfrom a graph of peer samples. This results in a form of non-local forward\npropagation in the model, where latent features are conditioned on the global\nstructure induced by the graph, that is up to 3 times more robust to a variety\nof white- and black-box adversarial attacks compared to conventional\narchitectures with almost no drop in accuracy.\n"], ["2018-05-31", "http://arxiv.org/abs/1806.00081", "Resisting Adversarial Attacks using Gaussian Mixture Variational Autoencoders.", ["Partha Ghosh", " Arpan Losalka", " Michael J Black"], "  Susceptibility of deep neural networks to adversarial attacks poses a major\ntheoretical and practical challenge. All efforts to harden classifiers against\nsuch attacks have seen limited success. Two distinct categories of samples to\nwhich deep networks are vulnerable, \"adversarial samples\" and \"fooling\nsamples\", have been tackled separately so far due to the difficulty posed when\nconsidered together. In this work, we show how one can address them both under\none unified framework. We tie a discriminative model with a generative model,\nrendering the adversarial objective to entail a conflict. Our model has the\nform of a variational autoencoder, with a Gaussian mixture prior on the latent\nvector. Each mixture component of the prior distribution corresponds to one of\nthe classes in the data. This enables us to perform selective classification,\nleading to the rejection of adversarial samples instead of misclassification.\nOur method inherently provides a way of learning a selective classifier in a\nsemi-supervised scenario as well, which can resist adversarial attacks. We also\nshow how one can reclassify the rejected adversarial samples.\n"], ["2018-05-31", "http://arxiv.org/abs/1805.12514", "Scaling provable adversarial defenses.", ["Eric Wong", " Frank R. Schmidt", " Jan Hendrik Metzen", " J. Zico Kolter"], "  Recent work has developed methods for learning deep network classifiers that\nare provably robust to norm-bounded adversarial perturbation; however, these\nmethods are currently only possible for relatively small feedforward networks.\nIn this paper, in an effort to scale these approaches to substantially larger\nmodels, we extend previous work in three main directions. First, we present a\ntechnique for extending these training procedures to much more general\nnetworks, with skip connections (such as ResNets) and general nonlinearities;\nthe approach is fully modular, and can be implemented automatically (analogous\nto automatic differentiation). Second, in the specific case of $\\ell_\\infty$\nadversarial perturbations and networks with ReLU nonlinearities, we adopt a\nnonlinear random projection for training, which scales linearly in the number\nof hidden units (previous approaches scaled quadratically). Third, we show how\nto further improve robust error through cascade models. On both MNIST and CIFAR\ndata sets, we train classifiers that improve substantially on the state of the\nart in provable robust adversarial error bounds: from 5.8% to 3.1% on MNIST\n(with $\\ell_\\infty$ perturbations of $\\epsilon=0.1$), and from 80% to 36.4% on\nCIFAR (with $\\ell_\\infty$ perturbations of $\\epsilon=2/255$). Code for all\nexperiments in the paper is available at\nhttps://github.com/locuslab/convex_adversarial/.\n"], ["2018-05-31", "http://arxiv.org/abs/1805.12487", "Sequential Attacks on Agents for Long-Term Adversarial Goals.", ["Edgar Tretschk", " Seong Joon Oh", " Mario Fritz"], "  Reinforcement learning (RL) has advanced greatly in the past few years with\nthe employment of effective deep neural networks (DNNs) on the policy networks.\nWith the great effectiveness came serious vulnerability issues with DNNs that\nsmall adversarial perturbations on the input can change the output of the\nnetwork. Several works have pointed out that learned agents with a DNN policy\nnetwork can be manipulated against achieving the original task through a\nsequence of small perturbations on the input states. In this paper, we\ndemonstrate furthermore that it is also possible to impose an arbitrary\nadversarial reward on the victim policy network through a sequence of attacks.\nOur method involves the latest adversarial attack technique, Adversarial\nTransformer Network (ATN), that learns to generate the attack and is easy to\nintegrate into the policy network. As a result of our attack, the victim agent\nis misguided to optimise for the adversarial reward over time. Our results\nexpose serious security threats for RL applications in safety-critical systems\nincluding drones, medical analysis, and self-driving cars.\n"], ["2018-05-31", "http://arxiv.org/abs/1805.12316", "Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data.", ["Puyudi Yang", " Jianbo Chen", " Cho-Jui Hsieh", " Jane-Ling Wang", " Michael I. Jordan"], "  We present a probabilistic framework for studying adversarial attacks on\ndiscrete data. Based on this framework, we derive a perturbation-based method,\nGreedy Attack, and a scalable learning-based method, Gumbel Attack, that\nillustrate various tradeoffs in the design of attacks. We demonstrate the\neffectiveness of these methods using both quantitative metrics and human\nevaluation on various state-of-the-art models for text classification,\nincluding a word-based CNN, a character-based CNN and an LSTM. As as example of\nour results, we show that the accuracy of character-based convolutional\nnetworks drops to the level of random selection by modifying only five\ncharacters through Greedy Attack.\n"], ["2018-05-30", "http://arxiv.org/abs/1805.12302", "Adversarial Attacks on Face Detectors using Neural Net based Constrained Optimization.", ["Avishek Joey Bose", " Parham Aarabi"], "  Adversarial attacks involve adding, small, often imperceptible, perturbations\nto inputs with the goal of getting a machine learning model to misclassifying\nthem. While many different adversarial attack strategies have been proposed on\nimage classification models, object detection pipelines have been much harder\nto break. In this paper, we propose a novel strategy to craft adversarial\nexamples by solving a constrained optimization problem using an adversarial\ngenerator network. Our approach is fast and scalable, requiring only a forward\npass through our trained generator network to craft an adversarial sample.\nUnlike in many attack strategies, we show that the same trained generator is\ncapable of attacking new images without explicitly optimizing on them. We\nevaluate our attack on a trained Faster R-CNN face detector on the cropped\n300-W face dataset where we manage to reduce the number of detected faces to\n$0.5\\%$ of all originally detected faces. In a different experiment, also on\n300-W, we demonstrate the robustness of our attack to a JPEG compression based\ndefense typical JPEG compression level of $75\\%$ reduces the effectiveness of\nour attack from only $0.5\\%$ of detected faces to a modest $5.0\\%$.\n"], ["2018-05-30", "http://arxiv.org/abs/1805.11852", "ADAGIO: Interactive Experimentation with Adversarial Attack and Defense for Audio.", ["Nilaksh Das", " Madhuri Shanbhogue", " Shang-Tse Chen", " Li Chen", " Michael E. Kounavis", " Duen Horng Chau"], "  Adversarial machine learning research has recently demonstrated the\nfeasibility to confuse automatic speech recognition (ASR) models by introducing\nacoustically imperceptible perturbations to audio samples. To help researchers\nand practitioners gain better understanding of the impact of such attacks, and\nto provide them with tools to help them more easily evaluate and craft strong\ndefenses for their models, we present ADAGIO, the first tool designed to allow\ninteractive experimentation with adversarial attacks and defenses on an ASR\nmodel in real time, both visually and aurally. ADAGIO incorporates AMR and MP3\naudio compression techniques as defenses, which users can interactively apply\nto attacked audio samples. We show that these techniques, which are based on\npsychoacoustic principles, effectively eliminate targeted attacks, reducing the\nattack success rate from 92.5% to 0%. We will demonstrate ADAGIO and invite the\naudience to try it on the Mozilla Common Voice dataset.\n"], ["2018-05-30", "http://arxiv.org/abs/1805.12017", "Robustifying Models Against Adversarial Attacks by Langevin Dynamics.", ["Vignesh Srinivasan", " Arturo Marban", " Klaus-Robert M\u00fcller", " Wojciech Samek", " Shinichi Nakajima"], "  Adversarial attacks on deep learning models have compromised their\nperformance considerably. As remedies, a lot of defense methods were proposed,\nwhich however, have been circumvented by newer attacking strategies. In the\nmidst of this ensuing arms race, the problem of robustness against adversarial\nattacks still remains unsolved. This paper proposes a novel, simple yet\neffective defense strategy where adversarial samples are relaxed onto the\nunderlying manifold of the (unknown) target class distribution. Specifically,\nour algorithm drives off-manifold adversarial samples towards high density\nregions of the data generating distribution of the target class by the\nMetroplis-adjusted Langevin algorithm (MALA) with perceptual boundary taken\ninto account. Although the motivation is similar to projection methods, e.g.,\nDefense-GAN, our algorithm, called MALA for DEfense (MALADE), is equipped with\nsignificant dispersion - projection is distributed broadly, and therefore any\nwhitebox attack cannot accurately align the input so that the MALADE moves it\nto a targeted untrained spot where the model predicts a wrong label. In our\nexperiments, MALADE exhibited state-of-the-art performance against various\nelaborate attacking strategies.\n"], ["2018-05-30", "http://arxiv.org/abs/1805.12152", "Robustness May Be at Odds with Accuracy.", ["Dimitris Tsipras", " Shibani Santurkar", " Logan Engstrom", " Alexander Turner", " Aleksander Madry"], "  We show that there may exist an inherent tension between the goal of\nadversarial robustness and that of standard generalization. Specifically,\ntraining robust models may not only be more resource-consuming, but also lead\nto a reduction of standard accuracy. We demonstrate that this trade-off between\nthe standard accuracy of a model and its robustness to adversarial\nperturbations provably exists in a fairly simple and natural setting. These\nfindings also corroborate a similar phenomenon observed empirically in more\ncomplex settings. Further, we argue that this phenomenon is a consequence of\nrobust classifiers learning fundamentally different feature representations\nthan standard classifiers. These differences, in particular, seem to result in\nunexpected benefits: the representations learned by robust models tend to align\nbetter with salient data characteristics and human perception.\n"], ["2018-05-29", "http://arxiv.org/abs/1805.11770", "AutoZOOM: Autoencoder-based Zeroth Order Optimization Method for Attacking Black-box Neural Networks.", ["Chun-Chen Tu", " Paishun Ting", " Pin-Yu Chen", " Sijia Liu", " Huan Zhang", " Jinfeng Yi", " Cho-Jui Hsieh", " Shin-Ming Cheng"], "  Recent studies have shown that adversarial examples in state-of-the-art image\nclassifiers trained by deep neural networks (DNN) can be easily generated when\nthe target model is transparent to an attacker, known as the white-box setting.\nHowever, when attacking a deployed machine learning service, one can only\nacquire the input-output correspondences of the target model; this is the\nso-called black-box attack setting. The major drawback of existing black-box\nattacks is the need for excessive model queries, which may give a false sense\nof model robustness due to inefficient query designs. To bridge this gap, we\npropose a generic framework for query-efficient black-box attacks. Our\nframework, AutoZOOM, which is short for Autoencoder-based Zeroth Order\nOptimization Method, has two novel building blocks towards efficient black-box\nattacks: (i) an adaptive random gradient estimation strategy to balance query\ncounts and distortion, and (ii) an autoencoder that is either trained offline\nwith unlabeled data or a bilinear resizing operation for attack acceleration.\nExperimental results suggest that, by applying AutoZOOM to a state-of-the-art\nblack-box attack (ZOO), a significant reduction in model queries can be\nachieved without sacrificing the attack success rate and the visual quality of\nthe resulting adversarial examples. In particular, when compared to the\nstandard ZOO method, AutoZOOM can consistently reduce the mean query counts in\nfinding successful adversarial examples (or reaching the same distortion level)\nby at least 93% on MNIST, CIFAR-10 and ImageNet datasets, leading to novel\ninsights on adversarial robustness.\n"], ["2018-05-29", "http://arxiv.org/abs/1805.11596", "Adversarial Noise Attacks of Deep Learning Architectures -- Stability Analysis via Sparse Modeled Signals.", ["Yaniv Romano", " Aviad Aberdam", " Jeremias Sulam", " Michael Elad"], "  Despite their impressive performance, deep convolutional neural networks\n(CNNs) have been shown to be sensitive to small adversarial perturbations.\nThese nuisances, which one can barely notice, are powerful enough to fool\nsophisticated and well performing classifiers, leading to ridiculous\nmisclassification results. In this paper we analyze the stability of\nstate-of-the-art deep-learning classification machines to adversarial\nperturbations, where we assume that the signals belong to the (possibly\nmulti-layer) sparse representation model. We start with convolutional sparsity\nand then proceed to its multi-layered version, which is tightly connected to\nCNNs. Our analysis links between the stability of the classification to noise\nand the underlying structure of the signal, quantified by the sparsity of its\nrepresentation under a fixed dictionary. In addition, we offer similar\nstability theorems for two practical pursuit algorithms, which are posed as two\ndifferent deep-learning architectures - the layered Thresholding and the\nlayered Basis Pursuit. Our analysis establishes the better robustness of the\nlater to adversarial attacks. We corroborate these theoretical results by\nnumerical experiments on three datasets: MNIST, CIFAR-10 and CIFAR-100.\n"], ["2018-05-29", "http://arxiv.org/abs/1805.11666", "Why Botnets Work: Distributed Brute-Force Attacks Need No Synchronization.", ["Salman Salamatian", " Wasim Huleihel", " Ahmad Beirami", " Asaf Cohen", " Muriel M\u00e9dard"], "  In September 2017, McAffee Labs quarterly report estimated that brute force\nattacks represent 20\\% of total network attacks, making them the most prevalent\ntype of attack ex-aequo with browser based vulnerabilities. These attacks have\nsometimes catastrophic consequences, and understanding their fundamental limits\nmay play an important role in the risk assessment of password-secured systems,\nand in the design of better security protocols. While some solutions exist to\nprevent online brute-force attacks that arise from one single IP address,\nattacks performed by botnets are more challenging. In this paper, we analyze\nthese distributed attacks by using a simplified model. Our aim is to understand\nthe impact of distribution and asynchronization on the overall computational\neffort necessary to breach a system. Our result is based on Guesswork, a\nmeasure of the number of queries (guesses) required of an adversary before a\ncorrect sequence, such as a password, is found in an optimal attack. Guesswork\nis a direct surrogate for time and computational effort of guessing a sequence\nfrom a set of sequences with associated likelihoods. We model the lack of\nsynchronization by a worst-case optimization in which the queries made by\nmultiple adversarial agents are received in the worst possible order for the\nadversary, resulting in a min-max formulation. We show that, even without\nsynchronization, and for sequences of growing length, the asymptotic optimal\nperformance is achievable by using randomized guesses drawn from an appropriate\ndistribution. Therefore, randomization is key for distributed asynchronous\nattacks. In other words, asynchronous guessers can asymptotically perform\nbrute-force attacks as efficiently as synchronized guessers.\n"], ["2018-05-28", "http://arxiv.org/abs/1805.10997", "Adversarial Examples in Remote Sensing.", ["Wojciech Czaja", " Neil Fendley", " Michael Pekala", " Christopher Ratto", " I-Jeng Wang"], "  This paper considers attacks against machine learning algorithms used in\nremote sensing applications, a domain that presents a suite of challenges that\nare not fully addressed by current research focused on natural image data such\nas ImageNet. In particular, we present a new study of adversarial examples in\nthe context of satellite image classification problems. Using a recently\ncurated data set and associated classifier, we provide a preliminary analysis\nof adversarial examples in settings where the targeted classifier is permitted\nmultiple observations of the same location over time. While our experiments to\ndate are purely digital, our problem setup explicitly incorporates a number of\npractical considerations that a real-world attacker would need to take into\naccount when mounting a physical attack. We hope this work provides a useful\nstarting point for future studies of potential vulnerabilities in this setting.\n"], ["2018-05-28", "http://arxiv.org/abs/1805.11090", "GenAttack: Practical Black-box Attacks with Gradient-Free Optimization.", ["Moustafa Alzantot", " Yash Sharma", " Supriyo Chakraborty", " Huan Zhang", " Cho-Jui Hsieh", " Mani Srivastava"], "  Deep neural networks are vulnerable to adversarial examples, even in the\nblack-box setting, where the attacker is restricted solely to query access.\nExisting black-box approaches to generating adversarial examples typically\nrequire a significant number of queries, either for training a substitute\nnetwork or performing gradient estimation. We introduce GenAttack, a\ngradient-free optimization technique that uses genetic algorithms for\nsynthesizing adversarial examples in the black-box setting. Our experiments on\ndifferent datasets (MNIST, CIFAR-10, and ImageNet) show that GenAttack can\nsuccessfully generate visually imperceptible adversarial examples against\nstate-of-the-art image recognition models with orders of magnitude fewer\nqueries than previous approaches. Against MNIST and CIFAR-10 models, GenAttack\nrequired roughly 2,126 and 2,568 times fewer queries respectively, than ZOO,\nthe prior state-of-the-art black-box attack. In order to scale up the attack to\nlarge-scale high-dimensional ImageNet models, we perform a series of\noptimizations that further improve the query efficiency of our attack leading\nto 237 times fewer queries against the Inception-v3 model than ZOO.\nFurthermore, we show that GenAttack can successfully attack some\nstate-of-the-art ImageNet defenses, including ensemble adversarial training and\nnon-differentiable or randomized input transformations. Our results suggest\nthat evolutionary algorithms open up a promising area of research into\neffective black-box attacks.\n"], ["2018-05-27", "http://arxiv.org/abs/1805.10652", "Defending Against Adversarial Attacks by Leveraging an Entire GAN.", ["Gokula Krishnan Santhanam", " Paulina Grnarova"], "  Recent work has shown that state-of-the-art models are highly vulnerable to\nadversarial perturbations of the input. We propose cowboy, an approach to\ndetecting and defending against adversarial attacks by using both the\ndiscriminator and generator of a GAN trained on the same dataset. We show that\nthe discriminator consistently scores the adversarial samples lower than the\nreal samples across multiple attacks and datasets. We provide empirical\nevidence that adversarial samples lie outside of the data manifold learned by\nthe GAN. Based on this, we propose a cleaning method which uses both the\ndiscriminator and generator of the GAN to project the samples back onto the\ndata manifold. This cleaning procedure is independent of the classifier and\ntype of attack and thus can be deployed in existing systems.\n"], ["2018-05-25", "http://arxiv.org/abs/1805.10265", "Training verified learners with learned verifiers.", ["Krishnamurthy Dvijotham", " Sven Gowal", " Robert Stanforth", " Relja Arandjelovic", " Brendan O'Donoghue", " Jonathan Uesato", " Pushmeet Kohli"], "  This paper proposes a new algorithmic framework, predictor-verifier training,\nto train neural networks that are verifiable, i.e., networks that provably\nsatisfy some desired input-output properties. The key idea is to simultaneously\ntrain two networks: a predictor network that performs the task at hand,e.g.,\npredicting labels given inputs, and a verifier network that computes a bound on\nhow well the predictor satisfies the properties being verified. Both networks\ncan be trained simultaneously to optimize a weighted combination of the\nstandard data-fitting loss and a term that bounds the maximum violation of the\nproperty. Experiments show that not only is the predictor-verifier architecture\nable to train networks to achieve state of the art verified robustness to\nadversarial examples with much shorter training times (outperforming previous\nalgorithms on small datasets like MNIST and SVHN), but it can also be scaled to\nproduce the first known (to the best of our knowledge) verifiably robust\nnetworks for CIFAR-10.\n"], ["2018-05-25", "http://arxiv.org/abs/1805.10204", "Adversarial examples from computational constraints.", ["S\u00e9bastien Bubeck", " Eric Price", " Ilya Razenshteyn"], "  Why are classifiers in high dimension vulnerable to \"adversarial\"\nperturbations? We show that it is likely not due to information theoretic\nlimitations, but rather it could be due to computational constraints.\n  First we prove that, for a broad set of classification tasks, the mere\nexistence of a robust classifier implies that it can be found by a possibly\nexponential-time algorithm with relatively few training examples. Then we give\na particular classification task where learning a robust classifier is\ncomputationally intractable. More precisely we construct a binary\nclassification task in high dimensional space which is (i) information\ntheoretically easy to learn robustly for large perturbations, (ii) efficiently\nlearnable (non-robustly) by a simple linear separator, (iii) yet is not\nefficiently robustly learnable, even for small perturbations, by any algorithm\nin the statistical query (SQ) model. This example gives an exponential\nseparation between classical learning and robust learning in the statistical\nquery model. It suggests that adversarial examples may be an unavoidable\nbyproduct of computational limitations of learning algorithms.\n"], ["2018-05-24", "http://arxiv.org/abs/1805.10133", "Laplacian Networks: Bounding Indicator Function Smoothness for Neural Network Robustness.", ["Carlos Eduardo Rosar Kos Lassance", " Vincent Gripon", " Antonio Ortega"], "  For the past few years, Deep Neural Network (DNN) robustness has become a\nquestion of paramount importance. As a matter of fact, in sensitive settings\nmisclassification can lead to dramatic consequences. Such misclassifications\nare likely to occur when facing adversarial attacks, hardware failures or\nlimitations, and imperfect signal acquisition. To address this question,\nauthors have proposed different approaches aiming at increasing the robustness\nof DNNs, such as adding regularizers or training using noisy examples. In this\npaper we propose a new regularizer built upon the Laplacian of similarity\ngraphs obtained from the representation of training data at each layer of the\nDNN architecture. This regularizer penalizes large changes (across consecutive\nlayers in the architecture) in the distance between examples of different\nclasses, and as such enforces smooth variations of the class boundaries. Since\nit is agnostic to the type of deformations that are expected when predicting\nwith the DNN, the proposed regularizer can be combined with existing ad-hoc\nmethods. We provide theoretical justification for this regularizer and\ndemonstrate its effectiveness to improve robustness of DNNs on classical\nsupervised learning vision datasets.\n"], ["2018-05-23", "http://arxiv.org/abs/1805.09380", "Anonymizing k-Facial Attributes via Adversarial Perturbations.", ["Saheb Chhabra", " Richa Singh", " Mayank Vatsa", " Gaurav Gupta"], "  A face image not only provides details about the identity of a subject but\nalso reveals several attributes such as gender, race, sexual orientation, and\nage. Advancements in machine learning algorithms and popularity of sharing\nimages on the World Wide Web, including social media websites, have increased\nthe scope of data analytics and information profiling from photo collections.\nThis poses a serious privacy threat for individuals who do not want to be\nprofiled. This research presents a novel algorithm for anonymizing selective\nattributes which an individual does not want to share without affecting the\nvisual quality of images. Using the proposed algorithm, a user can select\nsingle or multiple attributes to be surpassed while preserving identity\ninformation and visual content. The proposed adversarial perturbation based\nalgorithm embeds imperceptible noise in an image such that attribute prediction\nalgorithm for the selected attribute yields incorrect classification result,\nthereby preserving the information according to user's choice. Experiments on\nthree popular databases i.e. MUCT, LFWcrop, and CelebA show that the proposed\nalgorithm not only anonymizes k-attributes, but also preserves image quality\nand identity information.\n"], ["2018-05-23", "http://arxiv.org/abs/1805.09370", "Towards Robust Training of Neural Networks by Regularizing Adversarial Gradients.", ["Fuxun Yu", " Zirui Xu", " Yanzhi Wang", " Chenchen Liu", " Xiang Chen"], "  In recent years, neural networks have demonstrated outstanding effectiveness\nin a large amount of applications.However, recent works have shown that neural\nnetworks are susceptible to adversarial examples, indicating possible flaws\nintrinsic to the network structures. To address this problem and improve the\nrobustness of neural networks, we investigate the fundamental mechanisms behind\nadversarial examples and propose a novel robust training method via regulating\nadversarial gradients. The regulation effectively squeezes the adversarial\ngradients of neural networks and significantly increases the difficulty of\nadversarial example generation.Without any adversarial example involved, the\nrobust training method could generate naturally robust networks, which are\nnear-immune to various types of adversarial examples. Experiments show the\nnaturally robust networks can achieve optimal accuracy against Fast Gradient\nSign Method (FGSM) and C\\&W attacks on MNIST, Cifar10, and Google Speech\nCommand dataset. Moreover, our proposed method also provides neural networks\nwith consistent robustness against transferable attacks.\n"], ["2018-05-23", "http://arxiv.org/abs/1805.09190", "Towards the first adversarially robust neural network model on MNIST.", ["Lukas Schott", " Jonas Rauber", " Matthias Bethge", " Wieland Brendel"], "  Despite much effort, deep neural networks remain highly susceptible to tiny\ninput perturbations and even for MNIST, one of the most common toy datasets in\ncomputer vision, no neural network model exists for which adversarial\nperturbations are large and make semantic sense to humans. We show that even\nthe widely recognized and by far most successful defense by Madry et al. (1)\noverfits on the L-infinity metric (it's highly susceptible to L2 and L0\nperturbations), (2) classifies unrecognizable images with high certainty, (3)\nperforms not much better than simple input binarization and (4) features\nadversarial perturbations that make little sense to humans. These results\nsuggest that MNIST is far from being solved in terms of adversarial robustness.\nWe present a novel robust classification model that performs analysis by\nsynthesis using learned class-conditional data distributions. We derive bounds\non the robustness and go to great length to empirically evaluate our model\nusing maximally effective adversarial attacks by (a) applying decision-based,\nscore-based, gradient-based and transfer-based attacks for several different Lp\nnorms, (b) by designing a new attack that exploits the structure of our\ndefended model and (c) by devising a novel decision-based attack that seeks to\nminimize the number of perturbed pixels (L0). The results suggest that our\napproach yields state-of-the-art robustness on MNIST against L0, L2 and\nL-infinity perturbations and we demonstrate that most adversarial examples are\nstrongly perturbed towards the perceptual boundary between the original and the\nadversarial class.\n"], ["2018-05-22", "http://arxiv.org/abs/1805.08736", "Adversarially Robust Training through Structured Gradient Regularization.", ["Kevin Roth", " Aurelien Lucchi", " Sebastian Nowozin", " Thomas Hofmann"], "  We propose a novel data-dependent structured gradient regularizer to increase\nthe robustness of neural networks vis-a-vis adversarial perturbations. Our\nregularizer can be derived as a controlled approximation from first principles,\nleveraging the fundamental link between training with noise and regularization.\nIt adds very little computational overhead during learning and is simple to\nimplement generically in standard deep learning frameworks. Our experiments\nprovide strong evidence that structured gradient regularization can act as an\neffective first line of defense against attacks based on low-level signal\ncorruption.\n"], ["2018-05-21", "http://arxiv.org/abs/1805.08006", "Bidirectional Learning for Robust Neural Networks.", ["Sidney Pontes-Filho", " Marcus Liwicki"], "  A multilayer perceptron can behave as a generative classifier by applying\nbidirectional learning (BL). It consists of training an undirected neural\nnetwork to map input to output and vice-versa; therefore it can produce a\nclassifier in one direction, and a generator in the opposite direction for the\nsame data. The learning process of BL tries to reproduce the neuroplasticity\nstated in Hebbian theory using only backward propagation of errors. In this\npaper, two novel learning techniques are introduced which use BL for improving\nrobustness to white noise static and adversarial examples. The first method is\nbidirectional propagation of errors, which the error propagation occurs in\nbackward and forward directions. Motivated by the fact that its generative\nmodel receives as input a constant vector per class, we introduce as a second\nmethod the hybrid adversarial networks (HAN). Its generative model receives a\nrandom vector as input and its training is based on generative adversarial\nnetworks (GAN). To assess the performance of BL, we perform experiments using\nseveral architectures with fully and convolutional layers, with and without\nbias. Experimental results show that both methods improve robustness to white\nnoise static and adversarial examples, and even increase accuracy, but have\ndifferent behavior depending on the architecture and task, being more\nbeneficial to use the one or the other. Nevertheless, HAN using a convolutional\narchitecture with batch normalization presents outstanding robustness, reaching\nstate-of-the-art accuracy on adversarial examples of hand-written digits.\n"], ["2018-05-21", "http://arxiv.org/abs/1805.08000", "Adversarial Noise Layer: Regularize Neural Network By Adding Noise.", ["Zhonghui You", " Jinmian Ye", " Kunming Li", " Zenglin Xu", " Ping Wang"], "  In this paper, we introduce a novel regularization method called Adversarial\nNoise Layer (ANL) and its efficient version called Class Adversarial Noise\nLayer (CANL), which are able to significantly improve CNN's generalization\nability by adding carefully crafted noise into the intermediate layer\nactivations. ANL and CANL can be easily implemented and integrated with most of\nthe mainstream CNN-based models. We compared the effects of the different types\nof noise and visually demonstrate that our proposed adversarial noise instruct\nCNN models to learn to extract cleaner feature maps, which further reduce the\nrisk of over-fitting. We also conclude that models trained with ANL or CANL are\nmore robust to the adversarial examples generated by FGSM than the traditional\nadversarial training approaches.\n"], ["2018-05-21", "http://arxiv.org/abs/1805.07984", "Adversarial Attacks on Neural Networks for Graph Data.", ["Daniel Z\u00fcgner", " Amir Akbarnejad", " Stephan G\u00fcnnemann"], "  Deep learning models for graphs have achieved strong performance for the task\nof node classification. Despite their proliferation, currently there is no\nstudy of their robustness to adversarial attacks. Yet, in domains where they\nare likely to be used, e.g. the web, adversaries are common. Can deep learning\nmodels for graphs be easily fooled? In this work, we introduce the first study\nof adversarial attacks on attributed graphs, specifically focusing on models\nexploiting ideas of graph convolutions. In addition to attacks at test time, we\ntackle the more challenging class of poisoning/causative attacks, which focus\non the training phase of a machine learning model. We generate adversarial\nperturbations targeting the node's features and the graph structure, thus,\ntaking the dependencies between instances in account. Moreover, we ensure that\nthe perturbations remain unnoticeable by preserving important data\ncharacteristics. To cope with the underlying discrete domain we propose an\nefficient algorithm Nettack exploiting incremental computations. Our\nexperimental study shows that accuracy of node classification significantly\ndrops even when performing only few perturbations. Even more, our attacks are\ntransferable: the learned attacks generalize to other state-of-the-art node\nclassification models and unsupervised approaches, and likewise are successful\neven when only limited knowledge about the graph is given.\n"], ["2018-05-21", "http://arxiv.org/abs/1805.07894", "Constructing Unrestricted Adversarial Examples with Generative Models.", ["Yang Song", " Rui Shu", " Nate Kushman", " Stefano Ermon"], "  Adversarial examples are typically constructed by perturbing an existing data\npoint within a small matrix norm, and current defense methods are focused on\nguarding against this type of attack. In this paper, we propose unrestricted\nadversarial examples, a new threat model where the attackers are not restricted\nto small norm-bounded perturbations. Different from perturbation-based attacks,\nwe propose to synthesize unrestricted adversarial examples entirely from\nscratch using conditional generative models. Specifically, we first train an\nAuxiliary Classifier Generative Adversarial Network (AC-GAN) to model the\nclass-conditional distribution over data samples. Then, conditioned on a\ndesired class, we search over the AC-GAN latent space to find images that are\nlikely under the generative model and are misclassified by a target classifier.\nWe demonstrate through human evaluation that unrestricted adversarial examples\ngenerated this way are legitimate and belong to the desired class. Our\nempirical results on the MNIST, SVHN, and CelebA datasets show that\nunrestricted adversarial examples can bypass strong adversarial training and\ncertified defense methods designed for traditional adversarial attacks.\n"], ["2018-05-20", "http://arxiv.org/abs/1805.07862", "Featurized Bidirectional GAN: Adversarial Defense via Adversarially Learned Semantic Inference.", ["Ruying Bao", " Sihang Liang", " Qingcan Wang"], "  Deep neural networks have been demonstrated to be vulnerable to adversarial\nattacks, where small perturbations intentionally added to the original inputs\ncan fool the classifier. In this paper, we propose a defense method, Featurized\nBidirectional Generative Adversarial Networks (FBGAN), to extract the semantic\nfeatures of the input and filter the non-semantic perturbation. FBGAN is\npre-trained on the clean dataset in an unsupervised manner, adversarially\nlearning a bidirectional mapping between the high-dimensional data space and\nthe low-dimensional semantic space; also mutual information is applied to\ndisentangle the semantically meaningful features. After the bidirectional\nmapping, the adversarial data can be reconstructed to denoised data, which\ncould be fed into any pre-trained classifier. We empirically show the quality\nof reconstruction images and the effectiveness of defense.\n"], ["2018-05-20", "http://arxiv.org/abs/1805.07816", "Towards Understanding Limitations of Pixel Discretization Against Adversarial Attacks.", ["Jiefeng Chen", " Xi Wu", " Vaibhav Rastogi", " Yingyu Liang", " Somesh Jha"], "  Wide adoption of artificial neural networks in various domains has led to an\nincreasing interest in defending adversarial attacks against them.\nPreprocessing defense methods such as pixel discretization are particularly\nattractive in practice due to their simplicity, low computational overhead, and\napplicability to various systems. It is observed that such methods work well on\nsimple datasets like MNIST, but break on more complicated ones like ImageNet\nunder recently proposed strong white-box attacks. To understand the conditions\nfor success and potentials for improvement, we study the pixel discretization\ndefense method, including more sophisticated variants that take into account\nthe properties of the dataset being discretized. Our results again show poor\nresistance against the strong attacks. We analyze our results in a theoretical\nframework and offer strong evidence that pixel discretization is unlikely to\nwork on all but the simplest of the datasets. Furthermore, our arguments\npresent insights why some other preprocessing defenses may be insecure.\n"], ["2018-05-20", "http://arxiv.org/abs/1805.07820", "Targeted Adversarial Examples for Black Box Audio Systems.", ["Rohan Taori", " Amog Kamsetty", " Brenton Chu", " Nikita Vemuri"], "  The application of deep recurrent networks to audio transcription has led to\nimpressive gains in automatic speech recognition (ASR) systems. Many have\ndemonstrated that small adversarial perturbations can fool deep neural networks\ninto incorrectly predicting a specified target with high confidence. Current\nwork on fooling ASR systems have focused on white-box attacks, in which the\nmodel architecture and parameters are known. In this paper, we adopt a\nblack-box approach to adversarial generation, combining the approaches of both\ngenetic algorithms and gradient estimation to solve the task. We achieve a\n89.25% targeted attack similarity after 3000 generations while maintaining\n94.6% audio file similarity.\n"], ["2018-05-17", "http://arxiv.org/abs/1805.06605", "Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models.", ["Pouya Samangouei", " Maya Kabkab", " Rama Chellappa"], "  In recent years, deep neural network approaches have been widely adopted for\nmachine learning tasks, including classification. However, they were shown to\nbe vulnerable to adversarial perturbations: carefully crafted small\nperturbations can cause misclassification of legitimate images. We propose\nDefense-GAN, a new framework leveraging the expressive capability of generative\nmodels to defend deep neural networks against such attacks. Defense-GAN is\ntrained to model the distribution of unperturbed images. At inference time, it\nfinds a close output to a given image which does not contain the adversarial\nchanges. This output is then fed to the classifier. Our proposed method can be\nused with any classification model and does not modify the classifier structure\nor training procedure. It can also be used as a defense against any attack as\nit does not assume knowledge of the process for generating the adversarial\nexamples. We empirically show that Defense-GAN is consistently effective\nagainst different attack methods and improves on existing defense strategies.\nOur code has been made publicly available at\nhttps://github.com/kabkabm/defensegan\n"], ["2018-05-16", "http://arxiv.org/abs/1805.06130", "Towards Robust Neural Machine Translation.", ["Yong Cheng", " Zhaopeng Tu", " Fandong Meng", " Junjie Zhai", " Yang Liu"], "  Small perturbations in the input can severely distort intermediate\nrepresentations and thus impact translation quality of neural machine\ntranslation (NMT) models. In this paper, we propose to improve the robustness\nof NMT models with adversarial stability training. The basic idea is to make\nboth the encoder and decoder in NMT models robust against input perturbations\nby enabling them to behave similarly for the original input and its perturbed\ncounterpart. Experimental results on Chinese-English, English-German and\nEnglish-French translation tasks show that our approaches can not only achieve\nsignificant improvements over strong NMT systems but also improve the\nrobustness of NMT models.\n"], ["2018-05-14", "http://arxiv.org/abs/1805.05010", "Detecting Adversarial Samples for Deep Neural Networks through Mutation Testing.", ["Jingyi Wang", " Jun Sun", " Peixin Zhang", " Xinyu Wang"], "  Recently, it has been shown that deep neural networks (DNN) are subject to\nattacks through adversarial samples. Adversarial samples are often crafted\nthrough adversarial perturbation, i.e., manipulating the original sample with\nminor modifications so that the DNN model labels the sample incorrectly. Given\nthat it is almost impossible to train perfect DNN, adversarial samples are\nshown to be easy to generate. As DNN are increasingly used in safety-critical\nsystems like autonomous cars, it is crucial to develop techniques for defending\nsuch attacks. Existing defense mechanisms which aim to make adversarial\nperturbation challenging have been shown to be ineffective. In this work, we\npropose an alternative approach. We first observe that adversarial samples are\nmuch more sensitive to perturbations than normal samples. That is, if we impose\nrandom perturbations on a normal and an adversarial sample respectively, there\nis a significant difference between the ratio of label change due to the\nperturbations. Observing this, we design a statistical adversary detection\nalgorithm called nMutant (inspired by mutation testing from software\nengineering community). Our experiments show that nMutant effectively detects\nmost of the adversarial samples generated by recently proposed attacking\nmethods. Furthermore, we provide an error bound with certain statistical\nsignificance along with the detection.\n"], ["2018-05-12", "http://arxiv.org/abs/1805.04810", "AttriGuard: A Practical Defense Against Attribute Inference Attacks via Adversarial Machine Learning.", ["Jinyuan Jia", " Neil Zhenqiang Gong"], "  Users in various web and mobile applications are vulnerable to attribute\ninference attacks, in which an attacker leverages a machine learning classifier\nto infer a target user's private attributes (e.g., location, sexual\norientation, political view) from its public data (e.g., rating scores, page\nlikes). Existing defenses leverage game theory or heuristics based on\ncorrelations between the public data and attributes. These defenses are not\npractical. Specifically, game-theoretic defenses require solving intractable\noptimization problems, while correlation-based defenses incur large utility\nloss of users' public data.\n  In this paper, we present AttriGuard, a practical defense against attribute\ninference attacks. AttriGuard is computationally tractable and has small\nutility loss. Our AttriGuard works in two phases. Suppose we aim to protect a\nuser's private attribute. In Phase I, for each value of the attribute, we find\na minimum noise such that if we add the noise to the user's public data, then\nthe attacker's classifier is very likely to infer the attribute value for the\nuser. We find the minimum noise via adapting existing evasion attacks in\nadversarial machine learning. In Phase II, we sample one attribute value\naccording to a certain probability distribution and add the corresponding noise\nfound in Phase I to the user's public data. We formulate finding the\nprobability distribution as solving a constrained convex optimization problem.\nWe extensively evaluate AttriGuard and compare it with existing methods using a\nreal-world dataset. Our results show that AttriGuard substantially outperforms\nexisting methods. Our work is the first one that shows evasion attacks can be\nused as defensive techniques for privacy protection.\n"], ["2018-05-12", "http://arxiv.org/abs/1805.04807", "Curriculum Adversarial Training.", ["Qi-Zhi Cai", " Min Du", " Chang Liu", " Dawn Song"], "  Recently, deep learning has been applied to many security-sensitive\napplications, such as facial authentication. The existence of adversarial\nexamples hinders such applications. The state-of-the-art result on defense\nshows that adversarial training can be applied to train a robust model on MNIST\nagainst adversarial examples; but it fails to achieve a high empirical\nworst-case accuracy on a more complex task, such as CIFAR-10 and SVHN. In our\nwork, we propose curriculum adversarial training (CAT) to resolve this issue.\nThe basic idea is to develop a curriculum of adversarial examples generated by\nattacks with a wide range of strengths. With two techniques to mitigate the\nforgetting and the generalization issues, we demonstrate that CAT can improve\nthe prior art's empirical worst-case accuracy by a large margin of 25% on\nCIFAR-10 and 35% on SVHN. At the same, the model's performance on\nnon-adversarial inputs is comparable to the state-of-the-art models.\n"], ["2018-05-11", "http://arxiv.org/abs/1805.04613", "Breaking Transferability of Adversarial Samples with Randomness.", ["Yan Zhou", " Murat Kantarcioglu", " Bowei Xi"], "  We investigate the role of transferability of adversarial attacks in the\nobserved vulnerabilities of Deep Neural Networks (DNNs). We demonstrate that\nintroducing randomness to the DNN models is sufficient to defeat adversarial\nattacks, given that the adversary does not have an unlimited attack budget.\nInstead of making one specific DNN model robust to perfect knowledge attacks\n(a.k.a, white box attacks), creating randomness within an army of DNNs\ncompletely eliminates the possibility of perfect knowledge acquisition,\nresulting in a significantly more robust DNN ensemble against the strongest\nform of attacks. We also show that when the adversary has an unlimited budget\nof data perturbation, all defensive techniques would eventually break down as\nthe budget increases. Therefore, it is important to understand the game saddle\npoint where the adversary would not further pursue this endeavor.\n  Furthermore, we explore the relationship between attack severity and decision\nboundary robustness in the version space. We empirically demonstrate that by\nsimply adding a small Gaussian random noise to the learned weights, a DNN model\ncan increase its resilience to adversarial attacks by as much as 74.2%. More\nimportantly, we show that by randomly activating/revealing a model from a pool\nof pre-trained DNNs at each query request, we can put a tremendous strain on\nthe adversary's attack strategies. We compare our randomization techniques to\nthe Ensemble Adversarial Training technique and show that our randomization\ntechniques are superior under different attack budget constraints.\n"], ["2018-05-09", "http://arxiv.org/abs/1805.03553", "On Visual Hallmarks of Robustness to Adversarial Malware.", ["Alex Huang", " Abdullah Al-Dujaili", " Erik Hemberg", " Una-May O'Reilly"], "  A central challenge of adversarial learning is to interpret the resulting\nhardened model. In this contribution, we ask how robust generalization can be\nvisually discerned and whether a concise view of the interactions between a\nhardened decision map and input samples is possible. We first provide a means\nof visually comparing a hardened model's loss behavior with respect to the\nadversarial variants generated during training versus loss behavior with\nrespect to adversarial variants generated from other sources. This allows us to\nconfirm that the association of observed flatness of a loss landscape with\ngeneralization that is seen with naturally trained models extends to\nadversarially hardened models and robust generalization. To complement these\nmeans of interpreting model parameter robustness we also use self-organizing\nmaps to provide a visual means of superimposing adversarial and natural\nvariants on a model's decision space, thus allowing the model's global\nrobustness to be comprehensively examined.\n"], ["2018-05-09", "http://arxiv.org/abs/1805.03438", "Robust Classification with Convolutional Prototype Learning.", ["Hong-Ming Yang", " Xu-Yao Zhang", " Fei Yin", " Cheng-Lin Liu"], "  Convolutional neural networks (CNNs) have been widely used for image\nclassification. Despite its high accuracies, CNN has been shown to be easily\nfooled by some adversarial examples, indicating that CNN is not robust enough\nfor pattern classification. In this paper, we argue that the lack of robustness\nfor CNN is caused by the softmax layer, which is a totally discriminative model\nand based on the assumption of closed world (i.e., with a fixed number of\ncategories). To improve the robustness, we propose a novel learning framework\ncalled convolutional prototype learning (CPL). The advantage of using\nprototypes is that it can well handle the open world recognition problem and\ntherefore improve the robustness. Under the framework of CPL, we design\nmultiple classification criteria to train the network. Moreover, a prototype\nloss (PL) is proposed as a regularization to improve the intra-class\ncompactness of the feature representation, which can be viewed as a generative\nmodel based on the Gaussian assumption of different classes. Experiments on\nseveral datasets demonstrate that CPL can achieve comparable or even better\nresults than traditional CNN, and from the robustness perspective, CPL shows\ngreat advantages for both the rejection and incremental category learning\ntasks.\n"], ["2018-05-08", "http://arxiv.org/abs/1805.02917", "Interpretable Adversarial Perturbation in Input Embedding Space for Text.", ["Motoki Sato", " Jun Suzuki", " Hiroyuki Shindo", " Yuji Matsumoto"], "  Following great success in the image processing field, the idea of\nadversarial training has been applied to tasks in the natural language\nprocessing (NLP) field. One promising approach directly applies adversarial\ntraining developed in the image processing field to the input word embedding\nspace instead of the discrete input space of texts. However, this approach\nabandons such interpretability as generating adversarial texts to significantly\nimprove the performance of NLP tasks. This paper restores interpretability to\nsuch methods by restricting the directions of perturbations toward the existing\nwords in the input embedding space. As a result, we can straightforwardly\nreconstruct each input with perturbations to an actual text by considering the\nperturbations to be the replacement of words in the sentence while maintaining\nor even improving the task performance.\n"], ["2018-05-05", "http://arxiv.org/abs/1805.02131", "A Counter-Forensic Method for CNN-Based Camera Model Identification.", ["David G\u00fcera", " Yu Wang", " Luca Bondi", " Paolo Bestagini", " Stefano Tubaro", " Edward J. Delp"], "  An increasing number of digital images are being shared and accessed through\nwebsites, media, and social applications. Many of these images have been\nmodified and are not authentic. Recent advances in the use of deep\nconvolutional neural networks (CNNs) have facilitated the task of analyzing the\nveracity and authenticity of largely distributed image datasets. We examine in\nthis paper the problem of identifying the camera model or type that was used to\ntake an image and that can be spoofed. Due to the linear nature of CNNs and the\nhigh-dimensionality of images, neural networks are vulnerable to attacks with\nadversarial examples. These examples are imperceptibly different from correctly\nclassified images but are misclassified with high confidence by CNNs. In this\npaper, we describe a counter-forensic method capable of subtly altering images\nto change their estimated camera model when they are analyzed by any CNN-based\ncamera model detector. Our method can use both the Fast Gradient Sign Method\n(FGSM) or the Jacobian-based Saliency Map Attack (JSMA) to craft these\nadversarial images and does not require direct access to the CNN. Our results\nshow that even advanced deep learning architectures trained to analyze images\nand obtain camera model information are still vulnerable to our proposed\nmethod.\n"], ["2018-05-03", "http://arxiv.org/abs/1805.01431", "Siamese networks for generating adversarial examples.", ["Mandar Kulkarni", " Aria Abubakar"], "  Machine learning models are vulnerable to adversarial examples. An adversary\nmodifies the input data such that humans still assign the same label, however,\nmachine learning models misclassify it. Previous approaches in the literature\ndemonstrated that adversarial examples can even be generated for the remotely\nhosted model. In this paper, we propose a Siamese network based approach to\ngenerate adversarial examples for a multiclass target CNN. We assume that the\nadversary do not possess any knowledge of the target data distribution, and we\nuse an unlabeled mismatched dataset to query the target, e.g., for the\nResNet-50 target, we use the Food-101 dataset as the query. Initially, the\ntarget model assigns labels to the query dataset, and a Siamese network is\ntrained on the image pairs derived from these multiclass labels. We learn the\n\\emph{adversarial perturbations} for the Siamese model and show that these\nperturbations are also adversarial w.r.t. the target model. In experimental\nresults, we demonstrate effectiveness of our approach on MNIST, CIFAR-10 and\nImageNet targets with TinyImageNet/Food-101 query datasets.\n"], ["2018-04-30", "http://arxiv.org/abs/1805.00089", "Concolic Testing for Deep Neural Networks.", ["Youcheng Sun", " Min Wu", " Wenjie Ruan", " Xiaowei Huang", " Marta Kwiatkowska", " Daniel Kroening"], "  Concolic testing combines program execution and symbolic analysis to explore\nthe execution paths of a software program. This paper presents the first\nconcolic testing approach for Deep Neural Networks (DNNs). More specifically,\nwe formalise coverage criteria for DNNs that have been studied in the\nliterature, and then develop a coherent method for performing concolic testing\nto increase test coverage. Our experimental results show the effectiveness of\nthe concolic testing approach in both achieving high coverage and finding\nadversarial examples.\n"], ["2018-04-30", "http://arxiv.org/abs/1804.11313", "How Robust are Deep Neural Networks?.", ["Biswa Sengupta", " Karl J. Friston"], "  Convolutional and Recurrent, deep neural networks have been successful in\nmachine learning systems for computer vision, reinforcement learning, and other\nallied fields. However, the robustness of such neural networks is seldom\napprised, especially after high classification accuracy has been attained. In\nthis paper, we evaluate the robustness of three recurrent neural networks to\ntiny perturbations, on three widely used datasets, to argue that high accuracy\ndoes not always mean a stable and a robust (to bounded perturbations,\nadversarial attacks, etc.) system. Especially, normalizing the spectrum of the\ndiscrete recurrent network to bound the spectrum (using power method, Rayleigh\nquotient, etc.) on a unit disk produces stable, albeit highly non-robust neural\nnetworks. Furthermore, using the $\\epsilon$-pseudo-spectrum, we show that\ntraining of recurrent networks, say using gradient-based methods, often result\nin non-normal matrices that may or may not be diagonalizable. Therefore, the\nopen problem lies in constructing methods that optimize not only for accuracy\nbut also for the stability and the robustness of the underlying neural network,\na criterion that is distinct from the other.\n"], ["2018-04-30", "http://arxiv.org/abs/1804.11285", "Adversarially Robust Generalization Requires More Data.", ["Ludwig Schmidt", " Shibani Santurkar", " Dimitris Tsipras", " Kunal Talwar", " Aleksander M\u0105dry"], "  Machine learning models are often susceptible to adversarial perturbations of\ntheir inputs. Even small perturbations can cause state-of-the-art classifiers\nwith high \"standard\" accuracy to produce an incorrect prediction with high\nconfidence. To better understand this phenomenon, we study adversarially robust\nlearning from the viewpoint of generalization. We show that already in a simple\nnatural data model, the sample complexity of robust learning can be\nsignificantly larger than that of \"standard\" learning. This gap is information\ntheoretic and holds irrespective of the training algorithm or the model family.\nWe complement our theoretical results with experiments on popular image\nclassification datasets and show that a similar gap exists here as well. We\npostulate that the difficulty of training robust classifiers stems, at least\npartially, from this inherently larger sample complexity.\n"], ["2018-04-29", "http://arxiv.org/abs/1804.11022", "Adversarial Regression for Detecting Attacks in Cyber-Physical Systems.", ["Amin Ghafouri", " Yevgeniy Vorobeychik", " Xenofon Koutsoukos"], "  Attacks in cyber-physical systems (CPS) which manipulate sensor readings can\ncause enormous physical damage if undetected. Detection of attacks on sensors\nis crucial to mitigate this issue. We study supervised regression as a means to\ndetect anomalous sensor readings, where each sensor's measurement is predicted\nas a function of other sensors. We show that several common learning approaches\nin this context are still vulnerable to \\emph{stealthy attacks}, which\ncarefully modify readings of compromised sensors to cause desired damage while\nremaining undetected. Next, we model the interaction between the CPS defender\nand attacker as a Stackelberg game in which the defender chooses detection\nthresholds, while the attacker deploys a stealthy attack in response. We\npresent a heuristic algorithm for finding an approximately optimal threshold\nfor the defender in this game, and show that it increases system resilience to\nattacks without significantly increasing the false alarm rate.\n"], ["2018-04-28", "http://arxiv.org/abs/1804.10829", "Formal Security Analysis of Neural Networks using Symbolic Intervals.", ["Shiqi Wang", " Kexin Pei", " Justin Whitehouse", " Junfeng Yang", " Suman Jana"], "  Due to the increasing deployment of Deep Neural Networks (DNNs) in real-world\nsecurity-critical domains including autonomous vehicles and collision avoidance\nsystems, formally checking security properties of DNNs, especially under\ndifferent attacker capabilities, is becoming crucial. Most existing security\ntesting techniques for DNNs try to find adversarial examples without providing\nany formal security guarantees about the non-existence of such adversarial\nexamples. Recently, several projects have used different types of\nSatisfiability Modulo Theory (SMT) solvers to formally check security\nproperties of DNNs. However, all of these approaches are limited by the high\noverhead caused by the solver.\n  In this paper, we present a new direction for formally checking security\nproperties of DNNs without using SMT solvers. Instead, we leverage interval\narithmetic to compute rigorous bounds on the DNN outputs. Our approach, unlike\nexisting solver-based approaches, is easily parallelizable. We further present\nsymbolic interval analysis along with several other optimizations to minimize\noverestimations of output bounds.\n  We design, implement, and evaluate our approach as part of ReluVal, a system\nfor formally checking security properties of Relu-based DNNs. Our extensive\nempirical results show that ReluVal outperforms Reluplex, a state-of-the-art\nsolver-based system, by 200 times on average. On a single 8-core machine\nwithout GPUs, within 4 hours, ReluVal is able to verify a security property\nthat Reluplex deemed inconclusive due to timeout after running for more than 5\ndays. Our experiments demonstrate that symbolic interval analysis is a\npromising new direction towards rigorously analyzing different security\nproperties of DNNs.\n"], ["2018-04-25", "http://arxiv.org/abs/1804.09699", "Towards Fast Computation of Certified Robustness for ReLU Networks.", ["Tsui-Wei Weng", " Huan Zhang", " Hongge Chen", " Zhao Song", " Cho-Jui Hsieh", " Duane Boning", " Inderjit S. Dhillon", " Luca Daniel"], "  Verifying the robustness property of a general Rectified Linear Unit (ReLU)\nnetwork is an NP-complete problem [Katz, Barrett, Dill, Julian and Kochenderfer\nCAV17]. Although finding the exact minimum adversarial distortion is hard,\ngiving a certified lower bound of the minimum distortion is possible. Current\navailable methods of computing such a bound are either time-consuming or\ndelivering low quality bounds that are too loose to be useful. In this paper,\nwe exploit the special structure of ReLU networks and provide two\ncomputationally efficient algorithms Fast-Lin and Fast-Lip that are able to\ncertify non-trivial lower bounds of minimum distortions, by bounding the ReLU\nunits with appropriate linear functions Fast-Lin, or by bounding the local\nLipschitz constant Fast-Lip. Experiments show that (1) our proposed methods\ndeliver bounds close to (the gap is 2-3X) exact minimum distortion found by\nReluplex in small MNIST networks while our algorithms are more than 10,000\ntimes faster; (2) our methods deliver similar quality of bounds (the gap is\nwithin 35% and usually around 10%; sometimes our bounds are even better) for\nlarger networks compared to the methods based on solving linear programming\nproblems but our algorithms are 33-14,000 times faster; (3) our method is\ncapable of solving large MNIST and CIFAR networks up to 7 layers with more than\n10,000 neurons within tens of seconds on a single CPU core.\n  In addition, we show that, in fact, there is no polynomial time algorithm\nthat can approximately find the minimum $\\ell_1$ adversarial distortion of a\nReLU network with a $0.99\\ln n$ approximation ratio unless\n$\\mathsf{NP}$=$\\mathsf{P}$, where $n$ is the number of neurons in the network.\n"], ["2018-04-23", "http://arxiv.org/abs/1804.08794", "Towards Dependable Deep Convolutional Neural Networks (CNNs) with Out-distribution Learning.", ["Mahdieh Abbasi", " Arezoo Rajabi", " Christian Gagn\u00e9", " Rakesh B. Bobba"], "  Detection and rejection of adversarial examples in security sensitive and\nsafety-critical systems using deep CNNs is essential. In this paper, we propose\nan approach to augment CNNs with out-distribution learning in order to reduce\nmisclassification rate by rejecting adversarial examples. We empirically show\nthat our augmented CNNs can either reject or classify correctly most\nadversarial examples generated using well-known methods ( >95% for MNIST and\n>75% for CIFAR-10 on average). Furthermore, we achieve this without requiring\nto train using any specific type of adversarial examples and without\nsacrificing the accuracy of models on clean samples significantly (< 4%).\n"], ["2018-04-23", "http://arxiv.org/abs/1804.08757", "Siamese Generative Adversarial Privatizer for Biometric Data.", ["Witold Oleszkiewicz", " Peter Kairouz", " Karol Piczak", " Ram Rajagopal", " Tomasz Trzcinski"], "  State-of-the-art machine learning algorithms can be fooled by carefully\ncrafted adversarial examples. As such, adversarial examples present a concrete\nproblem in AI safety. In this work we turn the tables and ask the following\nquestion: can we harness the power of adversarial examples to prevent malicious\nadversaries from learning identifying information from data while allowing\nnon-malicious entities to benefit from the utility of the same data? For\ninstance, can we use adversarial examples to anonymize biometric dataset of\nfaces while retaining usefulness of this data for other purposes, such as\nemotion recognition? To address this question, we propose a simple yet\neffective method, called Siamese Generative Adversarial Privatizer (SGAP), that\nexploits the properties of a Siamese neural network to find discriminative\nfeatures that convey identifying information. When coupled with a generative\nmodel, our approach is able to correctly locate and disguise identifying\ninformation, while minimally reducing the utility of the privatized dataset.\nExtensive evaluation on a biometric dataset of fingerprints and cartoon faces\nconfirms usefulness of our simple yet effective method.\n"], ["2018-04-23", "http://arxiv.org/abs/1804.08598", "Black-box Adversarial Attacks with Limited Queries and Information.", ["Andrew Ilyas", " Logan Engstrom", " Anish Athalye", " Jessy Lin"], "  Current neural network-based classifiers are susceptible to adversarial\nexamples even in the black-box setting, where the attacker only has query\naccess to the model. In practice, the threat model for real-world systems is\noften more restrictive than the typical black-box model where the adversary can\nobserve the full output of the network on arbitrarily many chosen inputs. We\ndefine three realistic threat models that more accurately characterize many\nreal-world classifiers: the query-limited setting, the partial-information\nsetting, and the label-only setting. We develop new attacks that fool\nclassifiers under these more restrictive threat models, where previous methods\nwould be impractical or ineffective. We demonstrate that our methods are\neffective against an ImageNet classifier under our proposed threat models. We\nalso demonstrate a targeted black-box attack against a commercial classifier,\novercoming the challenges of limited query access, partial information, and\nother practical issues to break the Google Cloud Vision API.\n"], ["2018-04-23", "http://arxiv.org/abs/1804.08529", "VectorDefense: Vectorization as a Defense to Adversarial Examples.", ["Vishaal Munusamy Kabilan", " Brandon Morris", " Anh Nguyen"], "  Training deep neural networks on images represented as grids of pixels has\nbrought to light an interesting phenomenon known as adversarial examples.\nInspired by how humans reconstruct abstract concepts, we attempt to codify the\ninput bitmap image into a set of compact, interpretable elements to avoid being\nfooled by the adversarial structures. We take the first step in this direction\nby experimenting with image vectorization as an input transformation step to\nmap the adversarial examples back into the natural manifold of MNIST\nhandwritten digits. We compare our method vs. state-of-the-art input\ntransformations and further discuss the trade-offs between a hand-designed and\na learned transformation defense.\n"], ["2018-04-23", "http://arxiv.org/abs/1804.08778", "Query-Efficient Black-Box Attack Against Sequence-Based Malware Classifiers.", ["Ishai Rosenberg", " Asaf Shabtai", " Yuval Elovici", " Lior Rokach"], "  In this paper, we present a generic, query-efficient black-box attack against\nAPI call-based machine learning malware classifiers. We generate adversarial\nexamples by modifying the malware's API call sequences and non-sequential\nfeatures (printable strings). The adversarial examples will be misclassified by\nthe target malware classifier without affecting the malware's functionality. In\ncontrast to previous studies, our attack minimizes the number of malware\nclassifier queries required. In addition, in our attack, the attacker must only\nknow the class predicted by the malware classifier; the attacker knowledge of\nthe malware classifier's confidence score is optional. We evaluate the attack\neffectiveness when attacks are performed against a variety of malware\nclassifier architectures, including recurrent neural network (RNN) variants,\ndeep neural networks, support vector machines, and gradient boosted decision\ntrees. Our attack success rate is about 98% when the classifier's confidence\nscore is known and 88% when just the classifier's predicted class is known. We\nimplement four state-of-the-art query-efficient attacks and show that our\nattack requires fewer queries and less knowledge about the attacked model's\narchitecture than other existing query-efficient attacks, making it practical\nfor attacking cloud-based malware classifiers at a minimal cost.\n"], ["2018-04-21", "http://arxiv.org/abs/1804.07998", "Generating Natural Language Adversarial Examples.", ["Moustafa Alzantot", " Yash Sharma", " Ahmed Elgohary", " Bo-Jhang Ho", " Mani Srivastava", " Kai-Wei Chang"], "  Deep neural networks (DNNs) are vulnerable to adversarial examples,\nperturbations to correctly classified examples which can cause the model to\nmisclassify. In the image domain, these perturbations are often virtually\nindistinguishable to human perception, causing humans and state-of-the-art\nmodels to disagree. However, in the natural language domain, small\nperturbations are clearly perceptible, and the replacement of a single word can\ndrastically alter the semantics of the document. Given these challenges, we use\na black-box population-based optimization algorithm to generate semantically\nand syntactically similar adversarial examples that fool well-trained sentiment\nanalysis and textual entailment models with success rates of 97% and 70%,\nrespectively. We additionally demonstrate that 92.3% of the successful\nsentiment analysis adversarial examples are classified to their original label\nby 20 human annotators, and that the examples are perceptibly quite similar.\nFinally, we discuss an attempt to use adversarial training as a defense, but\nfail to yield improvement, demonstrating the strength and diversity of our\nadversarial examples. We hope our findings encourage researchers to pursue\nimproving the robustness of DNNs in the natural language domain.\n"], ["2018-04-20", "http://arxiv.org/abs/1804.07870", "Gradient Masking Causes CLEVER to Overestimate Adversarial Perturbation Size.", ["Ian Goodfellow"], "  A key problem in research on adversarial examples is that vulnerability to\nadversarial examples is usually measured by running attack algorithms. Because\nthe attack algorithms are not optimal, the attack algorithms are prone to\noverestimating the size of perturbation needed to fool the target model. In\nother words, the attack-based methodology provides an upper-bound on the size\nof a perturbation that will fool the model, but security guarantees require a\nlower bound. CLEVER is a proposed scoring method to estimate a lower bound.\nUnfortunately, an estimate of a bound is not a bound. In this report, we show\nthat gradient masking, a common problem that causes attack methodologies to\nprovide only a very loose upper bound, causes CLEVER to overestimate the size\nof perturbation needed to fool the model. In other words, CLEVER does not\nresolve the key problem with the attack-based methodology, because it fails to\nprovide a lower bound.\n"], ["2018-04-20", "http://arxiv.org/abs/1804.07757", "Learning More Robust Features with Adversarial Training.", ["Shuangtao Li", " Yuanke Chen", " Yanlin Peng", " Lin Bai"], "  In recent years, it has been found that neural networks can be easily fooled\nby adversarial examples, which is a potential safety hazard in some\nsafety-critical applications. Many researchers have proposed various method to\nmake neural networks more robust to white-box adversarial attacks, but an\neffective method have not been found so far. In this short paper, we focus on\nthe robustness of the features learned by neural networks. We show that the\nfeatures learned by neural networks are not robust, and find that the\nrobustness of the learned features is closely related to the resistance against\nadversarial examples of neural networks. We also find that adversarial training\nagainst fast gradients sign method (FGSM) does not make the leaned features\nvery robust, even if it can make the trained networks very resistant to FGSM\nattack. Then we propose a method, which can be seen as an extension of\nadversarial training, to train neural networks to learn more robust features.\nWe perform experiments on MNIST and CIFAR-10 to evaluate our method, and the\nexperiment results show that this method greatly improves the robustness of the\nlearned features and the resistance to adversarial attacks.\n"], ["2018-04-20", "http://arxiv.org/abs/1804.07729", "ADef: an Iterative Algorithm to Construct Adversarial Deformations.", ["Rima Alaifari", " Giovanni S. Alberti", " Tandri Gauksson"], "  While deep neural networks have proven to be a powerful tool for many\nrecognition and classification tasks, their stability properties are still not\nwell understood. In the past, image classifiers have been shown to be\nvulnerable to so-called adversarial attacks, which are created by additively\nperturbing the correctly classified image. In this paper, we propose the ADef\nalgorithm to construct a different kind of adversarial attack created by\niteratively applying small deformations to the image, found through a gradient\ndescent step. We demonstrate our results on MNIST with convolutional neural\nnetworks and on ImageNet with Inception-v3 and ResNet-101.\n"], ["2018-04-19", "http://arxiv.org/abs/1804.07062", "Attacking Convolutional Neural Network using Differential Evolution.", ["Jiawei Su", " Danilo Vasconcellos Vargas", " Kouichi Sakurai"], "  The output of Convolutional Neural Networks (CNN) has been shown to be\ndiscontinuous which can make the CNN image classifier vulnerable to small\nwell-tuned artificial perturbations. That is, images modified by adding such\nperturbations(i.e. adversarial perturbations) that make little difference to\nhuman eyes, can completely alter the CNN classification results. In this paper,\nwe propose a practical attack using differential evolution(DE) for generating\neffective adversarial perturbations. We comprehensively evaluate the\neffectiveness of different types of DEs for conducting the attack on different\nnetwork structures. The proposed method is a black-box attack which only\nrequires the miracle feedback of the target CNN systems. The results show that\nunder strict constraints which simultaneously control the number of pixels\nchanged and overall perturbation strength, attacking can achieve 72.29%, 78.24%\nand 61.28% non-targeted attack success rates, with 88.68%, 99.85% and 73.07%\nconfidence on average, on three common types of CNNs. The attack only requires\nmodifying 5 pixels with 20.44, 14.76 and 22.98 pixel values distortion. Thus,\nthe result shows that the current DNNs are also vulnerable to such simpler\nblack-box attacks even under very limited attack conditions.\n"], ["2018-04-19", "http://arxiv.org/abs/1804.07045", "Semantic Adversarial Deep Learning.", ["Tommaso Dreossi", " Somesh Jha", " Sanjit A. Seshia"], "  Fueled by massive amounts of data, models produced by machine-learning (ML)\nalgorithms, especially deep neural networks, are being used in diverse domains\nwhere trustworthiness is a concern, including automotive systems, finance,\nhealth care, natural language processing, and malware detection. Of particular\nconcern is the use of ML algorithms in cyber-physical systems (CPS), such as\nself-driving cars and aviation, where an adversary can cause serious\nconsequences. However, existing approaches to generating adversarial examples\nand devising robust ML algorithms mostly ignore the semantics and context of\nthe overall system containing the ML component. For example, in an autonomous\nvehicle using deep learning for perception, not every adversarial example for\nthe neural network might lead to a harmful consequence. Moreover, one may want\nto prioritize the search for adversarial examples towards those that\nsignificantly modify the desired semantics of the overall system. Along the\nsame lines, existing algorithms for constructing robust ML algorithms ignore\nthe specification of the overall system. In this paper, we argue that the\nsemantics and specification of the overall system has a crucial role to play in\nthis line of research. We present preliminary research results that support\nthis claim.\n"], ["2018-04-18", "http://arxiv.org/abs/1804.06760", "Simulation-based Adversarial Test Generation for Autonomous Vehicles with Machine Learning Components.", ["Cumhur Erkan Tuncali", " Georgios Fainekos", " Hisahiro Ito", " James Kapinski"], "  Many organizations are developing autonomous driving systems, which are\nexpected to be deployed at a large scale in the near future. Despite this,\nthere is a lack of agreement on appropriate methods to test, debug, and certify\nthe performance of these systems. One of the main challenges is that many\nautonomous driving systems have machine learning components, such as deep\nneural networks, for which formal properties are difficult to characterize. We\npresent a testing framework that is compatible with test case generation and\nautomatic falsification methods, which are used to evaluate cyber-physical\nsystems. We demonstrate how the framework can be used to evaluate closed-loop\nproperties of an autonomous driving system model that includes the ML\ncomponents, all within a virtual environment. We demonstrate how to use test\ncase generation methods, such as covering arrays, as well as requirement\nfalsification methods to automatically identify problematic test scenarios. The\nresulting framework can be used to increase the reliability of autonomous\ndriving systems.\n"], ["2018-04-18", "http://arxiv.org/abs/1804.06898", "Neural Automated Essay Scoring and Coherence Modeling for Adversarially Crafted Input.", ["Youmna Farag", " Helen Yannakoudakis", " Ted Briscoe"], "  We demonstrate that current state-of-the-art approaches to Automated Essay\nScoring (AES) are not well-suited to capturing adversarially crafted input of\ngrammatical but incoherent sequences of sentences. We develop a neural model of\nlocal coherence that can effectively learn connectedness features between\nsentences, and propose a framework for integrating and jointly training the\nlocal coherence model with a state-of-the-art AES model. We evaluate our\napproach against a number of baselines and experimentally demonstrate its\neffectiveness on both the AES task and the task of flagging adversarial input,\nfurther contributing to the development of an approach that strengthens the\nvalidity of neural essay scoring models.\n"], ["2018-04-17", "http://arxiv.org/abs/1804.06473", "Robust Machine Comprehension Models via Adversarial Training.", ["Yicheng Wang", " Mohit Bansal"], "  It is shown that many published models for the Stanford Question Answering\nDataset (Rajpurkar et al., 2016) lack robustness, suffering an over 50%\ndecrease in F1 score during adversarial evaluation based on the AddSent (Jia\nand Liang, 2017) algorithm. It has also been shown that retraining models on\ndata generated by AddSent has limited effect on their robustness. We propose a\nnovel alternative adversary-generation algorithm, AddSentDiverse, that\nsignificantly increases the variance within the adversarial training data by\nproviding effective examples that punish the model for making certain\nsuperficial assumptions. Further, in order to improve robustness to AddSent's\nsemantic perturbations (e.g., antonyms), we jointly improve the model's\nsemantic-relationship learning capabilities in addition to our\nAddSentDiverse-based adversarial training data augmentation. With these\nadditions, we show that we can make a state-of-the-art model significantly more\nrobust, achieving a 36.5% increase in F1 score under many different types of\nadversarial evaluation while maintaining performance on the regular SQuAD task.\n"], ["2018-04-17", "http://arxiv.org/abs/1804.06059", "Adversarial Example Generation with Syntactically Controlled Paraphrase Networks.", ["Mohit Iyyer", " John Wieting", " Kevin Gimpel", " Luke Zettlemoyer"], "  We propose syntactically controlled paraphrase networks (SCPNs) and use them\nto generate adversarial examples. Given a sentence and a target syntactic form\n(e.g., a constituency parse), SCPNs are trained to produce a paraphrase of the\nsentence with the desired syntax. We show it is possible to create training\ndata for this task by first doing backtranslation at a very large scale, and\nthen using a parser to label the syntactic transformations that naturally occur\nduring this process. Such data allows us to train a neural encoder-decoder\nmodel with extra inputs to specify the target syntax. A combination of\nautomated and human evaluations show that SCPNs generate paraphrases that\nfollow their target specifications without decreasing paraphrase quality when\ncompared to baseline (uncontrolled) paraphrase systems. Furthermore, they are\nmore capable of generating syntactically adversarial examples that both (1)\n\"fool\" pretrained models and (2) improve the robustness of these models to\nsyntactic variation when used to augment their training data.\n"], ["2018-04-16", "http://arxiv.org/abs/1804.05805", "Global Robustness Evaluation of Deep Neural Networks with Provable Guarantees for the $L_0$ Norm.", ["Wenjie Ruan", " Min Wu", " Youcheng Sun", " Xiaowei Huang", " Daniel Kroening", " Marta Kwiatkowska"], "  Deployment of deep neural networks (DNNs) in safety- or security-critical\nsystems requires provable guarantees on their correct behaviour. A common\nrequirement is robustness to adversarial perturbations in a neighbourhood\naround an input. In this paper we focus on the $L_0$ norm and aim to compute,\nfor a trained DNN and an input, the maximal radius of a safe norm ball around\nthe input within which there are no adversarial examples. Then we define global\nrobustness as an expectation of the maximal safe radius over a test data set.\nWe first show that the problem is NP-hard, and then propose an approximate\napproach to iteratively compute lower and upper bounds on the network's\nrobustness. The approach is \\emph{anytime}, i.e., it returns intermediate\nbounds and robustness estimates that are gradually, but strictly, improved as\nthe computation proceeds; \\emph{tensor-based}, i.e., the computation is\nconducted over a set of inputs simultaneously, instead of one by one, to enable\nefficient GPU computation; and has \\emph{provable guarantees}, i.e., both the\nbounds and the robustness estimates can converge to their optimal values.\nFinally, we demonstrate the utility of the proposed approach in practice to\ncompute tight bounds by applying and adapting the anytime algorithm to a set of\nchallenging problems, including global robustness evaluation, competitive $L_0$\nattacks, test case generation for DNNs, and local robustness evaluation on\nlarge-scale ImageNet DNNs. We release the code of all case studies via GitHub.\n"], ["2018-04-16", "http://arxiv.org/abs/1804.05810", "ShapeShifter: Robust Physical Adversarial Attack on Faster R-CNN Object Detector.", ["Shang-Tse Chen", " Cory Cornelius", " Jason Martin", " Duen Horng Chau"], "  Given the ability to directly manipulate image pixels in the digital input\nspace, an adversary can easily generate imperceptible perturbations to fool a\nDeep Neural Network (DNN) image classifier, as demonstrated in prior work. In\nthis work, we propose ShapeShifter, an attack that tackles the more challenging\nproblem of crafting physical adversarial perturbations to fool image-based\nobject detectors like Faster R-CNN. Attacking an object detector is more\ndifficult than attacking an image classifier, as it needs to mislead the\nclassification results in multiple bounding boxes with different scales.\nExtending the digital attack to the physical world adds another layer of\ndifficulty, because it requires the perturbation to be robust enough to survive\nreal-world distortions due to different viewing distances and angles, lighting\nconditions, and camera limitations. We show that the Expectation over\nTransformation technique, which was originally proposed to enhance the\nrobustness of adversarial perturbations in image classification, can be\nsuccessfully adapted to the object detection setting. ShapeShifter can generate\nadversarially perturbed stop signs that are consistently mis-detected by Faster\nR-CNN as other objects, posing a potential threat to autonomous vehicles and\nother safety-critical computer vision systems.\n"], ["2018-04-14", "http://arxiv.org/abs/1805.00310", "On the Limitation of MagNet Defense against $L_1$-based Adversarial Examples.", ["Pei-Hsuan Lu", " Pin-Yu Chen", " Kang-Cheng Chen", " Chia-Mu Yu"], "  In recent years, defending adversarial perturbations to natural examples in\norder to build robust machine learning models trained by deep neural networks\n(DNNs) has become an emerging research field in the conjunction of deep\nlearning and security. In particular, MagNet consisting of an adversary\ndetector and a data reformer is by far one of the strongest defenses in the\nblack-box oblivious attack setting, where the attacker aims to craft\ntransferable adversarial examples from an undefended DNN model to bypass an\nunknown defense module deployed on the same DNN model. Under this setting,\nMagNet can successfully defend a variety of attacks in DNNs, including the\nhigh-confidence adversarial examples generated by the Carlini and Wagner's\nattack based on the $L_2$ distortion metric. However, in this paper, under the\nsame attack setting we show that adversarial examples crafted based on the\n$L_1$ distortion metric can easily bypass MagNet and mislead the target DNN\nimage classifiers on MNIST and CIFAR-10. We also provide explanations on why\nthe considered approach can yield adversarial examples with superior attack\nperformance and conduct extensive experiments on variants of MagNet to verify\nits lack of robustness to $L_1$ distortion based attacks. Notably, our results\nsubstantially weaken the assumption of effective threat models on MagNet that\nrequire knowing the deployed defense technique when attacking DNNs (i.e., the\ngray-box attack setting).\n"], ["2018-04-14", "http://arxiv.org/abs/1804.05296", "Adversarial Attacks Against Medical Deep Learning Systems.", ["Samuel G. Finlayson", " Hyung Won Chung", " Isaac S. Kohane", " Andrew L. Beam"], "  The discovery of adversarial examples has raised concerns about the practical\ndeployment of deep learning systems. In this paper, we demonstrate that\nadversarial examples are capable of manipulating deep learning systems across\nthree clinical domains. For each of our representative medical deep learning\nclassifiers, both white and black box attacks were highly successful. Our\nmodels are representative of the current state of the art in medical computer\nvision and, in some cases, directly reflect architectures already seeing\ndeployment in real world clinical settings. In addition to the technical\ncontribution of our paper, we synthesize a large body of knowledge about the\nhealthcare system to argue that medicine may be uniquely susceptible to\nadversarial attacks, both in terms of monetary incentives and technical\nvulnerability. To this end, we outline the healthcare economy and the\nincentives it creates for fraud and provide concrete examples of how and why\nsuch attacks could be realistically carried out. We urge practitioners to be\naware of current vulnerabilities when deploying deep learning systems in\nclinical settings, and encourage the machine learning community to further\ninvestigate the domain-specific characteristics of medical learning systems.\n"], ["2018-04-11", "http://arxiv.org/abs/1804.04177", "Detecting Malicious PowerShell Commands using Deep Neural Networks.", ["Danny Hendler", " Shay Kels", " Amir Rubin"], "  Microsoft's PowerShell is a command-line shell and scripting language that is\ninstalled by default on Windows machines. While PowerShell can be configured by\nadministrators for restricting access and reducing vulnerabilities, these\nrestrictions can be bypassed. Moreover, PowerShell commands can be easily\ngenerated dynamically, executed from memory, encoded and obfuscated, thus\nmaking the logging and forensic analysis of code executed by PowerShell\nchallenging.For all these reasons, PowerShell is increasingly used by\ncybercriminals as part of their attacks' tool chain, mainly for downloading\nmalicious contents and for lateral movement. Indeed, a recent comprehensive\ntechnical report by Symantec dedicated to PowerShell's abuse by cybercrimials\nreported on a sharp increase in the number of malicious PowerShell samples they\nreceived and in the number of penetration tools and frameworks that use\nPowerShell. This highlights the urgent need of developing effective methods for\ndetecting malicious PowerShell commands.In this work, we address this challenge\nby implementing several novel detectors of malicious PowerShell commands and\nevaluating their performance. We implemented both \"traditional\" natural\nlanguage processing (NLP) based detectors and detectors based on\ncharacter-level convolutional neural networks (CNNs). Detectors' performance\nwas evaluated using a large real-world dataset.Our evaluation results show\nthat, although our detectors individually yield high performance, an ensemble\ndetector that combines an NLP-based classifier with a CNN-based classifier\nprovides the best performance, since the latter classifier is able to detect\nmalicious commands that succeed in evading the former. Our analysis of these\nevasive commands reveals that some obfuscation patterns automatically detected\nby the CNN classifier are intrinsically difficult to detect using the NLP\ntechniques we applied.\n"], ["2018-04-10", "http://arxiv.org/abs/1804.03286", "On the Robustness of the CVPR 2018 White-Box Adversarial Example Defenses.", ["Anish Athalye", " Nicholas Carlini"], "  Neural networks are known to be vulnerable to adversarial examples. In this\nnote, we evaluate the two white-box defenses that appeared at CVPR 2018 and\nfind they are ineffective: when applying existing techniques, we can reduce the\naccuracy of the defended models to 0%.\n"], ["2018-04-09", "http://arxiv.org/abs/1804.03308", "Adversarial Training Versus Weight Decay.", ["Angus Galloway", " Thomas Tanay", " Graham W. Taylor"], "  Performance-critical machine learning models should be robust to input\nperturbations not seen during training. Adversarial training is a method for\nimproving a model's robustness to some perturbations by including them in the\ntraining process, but this tends to exacerbate other vulnerabilities of the\nmodel. The adversarial training framework has the effect of translating the\ndata with respect to the cost function, while weight decay has a scaling\neffect. Although weight decay could be considered a crude regularization\ntechnique, it appears superior to adversarial training as it remains stable\nover a broader range of regimes and reduces all generalization errors. Equipped\nwith these abstractions, we provide key baseline results and methodology for\ncharacterizing robustness. The two approaches can be combined to yield one\nsmall model that demonstrates good robustness to several white-box attacks\nassociated with different metrics.\n"], ["2018-04-09", "http://arxiv.org/abs/1804.03193", "An ADMM-Based Universal Framework for Adversarial Attacks on Deep Neural Networks.", ["Pu Zhao", " Sijia Liu", " Yanzhi Wang", " Xue Lin"], "  Deep neural networks (DNNs) are known vulnerable to adversarial attacks. That\nis, adversarial examples, obtained by adding delicately crafted distortions\nonto original legal inputs, can mislead a DNN to classify them as any target\nlabels. In a successful adversarial attack, the targeted mis-classification\nshould be achieved with the minimal distortion added. In the literature, the\nadded distortions are usually measured by L0, L1, L2, and L infinity norms,\nnamely, L0, L1, L2, and L infinity attacks, respectively. However, there lacks\na versatile framework for all types of adversarial attacks.\n  This work for the first time unifies the methods of generating adversarial\nexamples by leveraging ADMM (Alternating Direction Method of Multipliers), an\noperator splitting optimization approach, such that L0, L1, L2, and L infinity\nattacks can be effectively implemented by this general framework with little\nmodifications. Comparing with the state-of-the-art attacks in each category,\nour ADMM-based attacks are so far the strongest, achieving both the 100% attack\nsuccess rate and the minimal distortion.\n"], ["2018-04-08", "http://arxiv.org/abs/1804.02691", "Adaptive Spatial Steganography Based on Probability-Controlled Adversarial Examples.", ["Sai Ma", " Qingxiao Guan", " Xianfeng Zhao", " Yaqi Liu"], "  Explanation from Sai Ma: The experiments in this paper are conducted on Caffe\nframework. In Caffe, there is an API to directly set the gradient in Matlab. I\nwrongly use it to control the 'probability', in fact, I modify the gradient\ndirectly. The misusage of API leads to wrong experiment results, and wrong\ntheoretical analysis.\n  Apologize to readers who have read this paper. We have submitted a correct\nversion of this paper to Multimedia Tools and Applications and it is under\nrevision.\n  Thanks to Dr. Patrick Bas, who is the Associate Editor of TIFS and the\nanonymous reviewers of this paper.\n  Thanks to Tingting Song from Sun Yat-sen University. We discussed some\nproblems of this paper. Her advice helps me to improve the submitted paper to\nMultimedia Tools and Applications.\n"], ["2018-04-06", "http://arxiv.org/abs/1804.02485", "Fortified Networks: Improving the Robustness of Deep Networks by Modeling the Manifold of Hidden Representations.", ["Alex Lamb", " Jonathan Binas", " Anirudh Goyal", " Dmitriy Serdyuk", " Sandeep Subramanian", " Ioannis Mitliagkas", " Yoshua Bengio"], "  Deep networks have achieved impressive results across a variety of important\ntasks. However a known weakness is a failure to perform well when evaluated on\ndata which differ from the training distribution, even if these differences are\nvery small, as is the case with adversarial examples. We propose Fortified\nNetworks, a simple transformation of existing networks, which fortifies the\nhidden layers in a deep network by identifying when the hidden states are off\nof the data manifold, and maps these hidden states back to parts of the data\nmanifold where the network performs well. Our principal contribution is to show\nthat fortifying these hidden states improves the robustness of deep networks\nand our experiments (i) demonstrate improved robustness to standard adversarial\nattacks in both black-box and white-box threat models; (ii) suggest that our\nimprovements are not primarily due to the gradient masking problem and (iii)\nshow the advantage of doing this fortification in the hidden layers instead of\nthe input space.\n"], ["2018-04-04", "http://arxiv.org/abs/1804.01635", "Unifying Bilateral Filtering and Adversarial Training for Robust Neural Networks.", ["Neale Ratzlaff", " Li Fuxin"], "  Recent analysis of deep neural networks has revealed their vulnerability to\ncarefully structured adversarial examples. Many effective algorithms exist to\ncraft these adversarial examples, but performant defenses seem to be far away.\nIn this work, we explore the use of edge-aware bilateral filtering as a\nprojection back to the space of natural images. We show that bilateral\nfiltering is an effective defense in multiple attack settings, where the\nstrength of the adversary gradually increases. In the case of an adversary who\nhas no knowledge of the defense, bilateral filtering can remove more than 90%\nof adversarial examples from a variety of different attacks. To evaluate\nagainst an adversary with complete knowledge of our defense, we adapt the\nbilateral filter as a trainable layer in a neural network and show that adding\nthis layer makes ImageNet images significantly more robust to attacks. When\ntrained under a framework of adversarial training, we show that the resulting\nmodel is hard to fool with even the best attack methods.\n"], ["2018-03-30", "http://arxiv.org/abs/1804.00097", "Adversarial Attacks and Defences Competition.", ["Alexey Kurakin", " Ian Goodfellow", " Samy Bengio", " Yinpeng Dong", " Fangzhou Liao", " Ming Liang", " Tianyu Pang", " Jun Zhu", " Xiaolin Hu", " Cihang Xie", " Jianyu Wang", " Zhishuai Zhang", " Zhou Ren", " Alan Yuille", " Sangxia Huang", " Yao Zhao", " Yuzhe Zhao", " Zhonglin Han", " Junjiajia Long", " Yerkebulan Berdibekov", " Takuya Akiba", " Seiya Tokui", " Motoki Abe"], "  To accelerate research on adversarial examples and robustness of machine\nlearning classifiers, Google Brain organized a NIPS 2017 competition that\nencouraged researchers to develop new methods to generate adversarial examples\nas well as to develop new ways to defend against them. In this chapter, we\ndescribe the structure and organization of the competition and the solutions\ndeveloped by several of the top-placing teams.\n"], ["2018-03-29", "http://arxiv.org/abs/1803.11157", "Security Consideration For Deep Learning-Based Image Forensics.", ["Wei Zhao", " Pengpeng Yang", " Rongrong Ni", " Yao Zhao", " Haorui Wu"], "  Recently, image forensics community has paied attention to the research on\nthe design of effective algorithms based on deep learning technology and facts\nproved that combining the domain knowledge of image forensics and deep learning\nwould achieve more robust and better performance than the traditional schemes.\nInstead of improving it, in this paper, the safety of deep learning based\nmethods in the field of image forensics is taken into account. To the best of\nour knowledge, this is a first work focusing on this topic. Specifically, we\nexperimentally find that the method using deep learning would fail when adding\nthe slight noise into the images (adversarial images). Furthermore, two kinds\nof strategys are proposed to enforce security of deep learning-based method.\nFirstly, an extra penalty term to the loss function is added, which is referred\nto the 2-norm of the gradient of the loss with respect to the input images, and\nthen an novel training method are adopt to train the model by fusing the normal\nand adversarial images. Experimental results show that the proposed algorithm\ncan achieve good performance even in the case of adversarial images and provide\na safety consideration for deep learning-based image forensics\n"], ["2018-03-28", "http://arxiv.org/abs/1803.10840", "Defending against Adversarial Images using Basis Functions Transformations.", ["Uri Shaham", " James Garritano", " Yutaro Yamada", " Ethan Weinberger", " Alex Cloninger", " Xiuyuan Cheng", " Kelly Stanton", " Yuval Kluger"], "  We study the effectiveness of various approaches that defend against\nadversarial attacks on deep networks via manipulations based on basis function\nrepresentations of images. Specifically, we experiment with low-pass filtering,\nPCA, JPEG compression, low resolution wavelet approximation, and\nsoft-thresholding. We evaluate these defense techniques using three types of\npopular attacks in black, gray and white-box settings. Our results show JPEG\ncompression tends to outperform the other tested defenses in most of the\nsettings considered, in addition to soft-thresholding, which performs well in\nspecific cases, and yields a more mild decrease in accuracy on benign examples.\nIn addition, we also mathematically derive a novel white-box attack in which\nthe adversarial perturbation is composed only of terms corresponding a to\npre-determined subset of the basis functions, of which a \"low frequency attack\"\nis a special case.\n"], ["2018-03-28", "http://arxiv.org/abs/1803.10418", "The Effects of JPEG and JPEG2000 Compression on Attacks using Adversarial Examples.", ["Ayse Elvan Aydemir", " Alptekin Temizel", " Tugba Taskaya Temizel"], "  Adversarial examples are known to have a negative effect on the performance\nof classifiers which have otherwise good performance on undisturbed images.\nThese examples are generated by adding non-random noise to the testing samples\nin order to make classifier misclassify the given data. Adversarial attacks use\nthese intentionally generated examples and they pose a security risk to the\nmachine learning based systems. To be immune to such attacks, it is desirable\nto have a pre-processing mechanism which removes these effects causing\nmisclassification while keeping the content of the image. JPEG and JPEG2000 are\nwell-known image compression techniques which suppress the high-frequency\ncontent taking the human visual system into account. JPEG has been also shown\nto be an effective method for reducing adversarial noise. In this paper, we\npropose applying JPEG2000 compression as an alternative and systematically\ncompare the classification performance of adversarial images compressed using\nJPEG and JPEG2000 at different target PSNR values and maximum compression\nlevels. Our experiments show that JPEG2000 is more effective in reducing\nadversarial noise as it allows higher compression rates with less distortion\nand it does not introduce blocking artifacts.\n"], ["2018-03-26", "http://arxiv.org/abs/1803.09868", "Bypassing Feature Squeezing by Increasing Adversary Strength.", ["Yash Sharma", " Pin-Yu Chen"], "  Feature Squeezing is a recently proposed defense method which reduces the\nsearch space available to an adversary by coalescing samples that correspond to\nmany different feature vectors in the original space into a single sample. It\nhas been shown that feature squeezing defenses can be combined in a joint\ndetection framework to achieve high detection rates against state-of-the-art\nattacks. However, we demonstrate on the MNIST and CIFAR-10 datasets that by\nincreasing the adversary strength of said state-of-the-art attacks, one can\nbypass the detection framework with adversarial examples of minimal visual\ndistortion. These results suggest for proposed defenses to validate against\nstronger attack configurations.\n"], ["2018-03-26", "http://arxiv.org/abs/1803.09638", "On the Limitation of Local Intrinsic Dimensionality for Characterizing the Subspaces of Adversarial Examples.", ["Pei-Hsuan Lu", " Pin-Yu Chen", " Chia-Mu Yu"], "  Understanding and characterizing the subspaces of adversarial examples aid in\nstudying the robustness of deep neural networks (DNNs) to adversarial\nperturbations. Very recently, Ma et al. (ICLR 2018) proposed to use local\nintrinsic dimensionality (LID) in layer-wise hidden representations of DNNs to\nstudy adversarial subspaces. It was demonstrated that LID can be used to\ncharacterize the adversarial subspaces associated with different attack\nmethods, e.g., the Carlini and Wagner's (C&W) attack and the fast gradient sign\nattack.\n  In this paper, we use MNIST and CIFAR-10 to conduct two new sets of\nexperiments that are absent in existing LID analysis and report the limitation\nof LID in characterizing the corresponding adversarial subspaces, which are (i)\noblivious attacks and LID analysis using adversarial examples with different\nconfidence levels; and (ii) black-box transfer attacks. For (i), we find that\nthe performance of LID is very sensitive to the confidence parameter deployed\nby an attack, and the LID learned from ensembles of adversarial examples with\nvarying confidence levels surprisingly gives poor performance. For (ii), we\nfind that when adversarial examples are crafted from another DNN model, LID is\nineffective in characterizing their adversarial subspaces. These two findings\ntogether suggest the limited capability of LID in characterizing the subspaces\nof adversarial examples.\n"], ["2018-03-26", "http://arxiv.org/abs/1803.09468", "Clipping free attacks against artificial neural networks.", ["Boussad Addad", " Jerome Kodjabachian", " Christophe Meyer"], "  During the last years, a remarkable breakthrough has been made in AI domain\nthanks to artificial deep neural networks that achieved a great success in many\nmachine learning tasks in computer vision, natural language processing, speech\nrecognition, malware detection and so on. However, they are highly vulnerable\nto easily crafted adversarial examples. Many investigations have pointed out\nthis fact and different approaches have been proposed to generate attacks while\nadding a limited perturbation to the original data. The most robust known\nmethod so far is the so called C&W attack [1]. Nonetheless, a countermeasure\nknown as feature squeezing coupled with ensemble defense showed that most of\nthese attacks can be destroyed [6]. In this paper, we present a new method we\ncall Centered Initial Attack (CIA) whose advantage is twofold : first, it\ninsures by construction the maximum perturbation to be smaller than a threshold\nfixed beforehand, without the clipping process that degrades the quality of\nattacks. Second, it is robust against recently introduced defenses such as\nfeature squeezing, JPEG encoding and even against a voting ensemble of\ndefenses. While its application is not limited to images, we illustrate this\nusing five of the current best classifiers on ImageNet dataset among which two\nare adversarialy retrained on purpose to be robust against attacks. With a\nfixed maximum perturbation of only 1.5% on any pixel, around 80% of attacks\n(targeted) fool the voting ensemble defense and nearly 100% when the\nperturbation is only 6%. While this shows how it is difficult to defend against\nCIA attacks, the last section of the paper gives some guidelines to limit their\nimpact.\n"], ["2018-03-24", "http://arxiv.org/abs/1803.09163", "Security Theater: On the Vulnerability of Classifiers to Exploratory Attacks.", ["Tegjyot Singh Sethi", " Mehmed Kantardzic", " Joung Woo Ryu"], "  The increasing scale and sophistication of cyberattacks has led to the\nadoption of machine learning based classification techniques, at the core of\ncybersecurity systems. These techniques promise scale and accuracy, which\ntraditional rule or signature based methods cannot. However, classifiers\noperating in adversarial domains are vulnerable to evasion attacks by an\nadversary, who is capable of learning the behavior of the system by employing\nintelligently crafted probes. Classification accuracy in such domains provides\na false sense of security, as detection can easily be evaded by carefully\nperturbing the input samples. In this paper, a generic data driven framework is\npresented, to analyze the vulnerability of classification systems to black box\nprobing based attacks. The framework uses an exploration exploitation based\nstrategy, to understand an adversary's point of view of the attack defense\ncycle. The adversary assumes a black box model of the defender's classifier and\ncan launch indiscriminate attacks on it, without information of the defender's\nmodel type, training data or the domain of application. Experimental evaluation\non 10 real world datasets demonstrates that even models having high perceived\naccuracy (>90%), by a defender, can be effectively circumvented with a high\nevasion rate (>95%, on average). The detailed attack algorithms, adversarial\nmodel and empirical evaluation, serve.\n"], ["2018-03-24", "http://arxiv.org/abs/1803.09162", "A Dynamic-Adversarial Mining Approach to the Security of Machine Learning.", ["Tegjyot Singh Sethi", " Mehmed Kantardzic", " Lingyu Lyua", " Jiashun Chen"], "  Operating in a dynamic real world environment requires a forward thinking and\nadversarial aware design for classifiers, beyond fitting the model to the\ntraining data. In such scenarios, it is necessary to make classifiers - a)\nharder to evade, b) easier to detect changes in the data distribution over\ntime, and c) be able to retrain and recover from model degradation. While most\nworks in the security of machine learning has concentrated on the evasion\nresistance (a) problem, there is little work in the areas of reacting to\nattacks (b and c). Additionally, while streaming data research concentrates on\nthe ability to react to changes to the data distribution, they often take an\nadversarial agnostic view of the security problem. This makes them vulnerable\nto adversarial activity, which is aimed towards evading the concept drift\ndetection mechanism itself. In this paper, we analyze the security of machine\nlearning, from a dynamic and adversarial aware perspective. The existing\ntechniques of Restrictive one class classifier models, Complex learning models\nand Randomization based ensembles, are shown to be myopic as they approach\nsecurity as a static task. These methodologies are ill suited for a dynamic\nenvironment, as they leak excessive information to an adversary, who can\nsubsequently launch attacks which are indistinguishable from the benign data.\nBased on empirical vulnerability analysis against a sophisticated adversary, a\nnovel feature importance hiding approach for classifier design, is proposed.\nThe proposed design ensures that future attacks on classifiers can be detected\nand recovered from. The proposed work presents motivation, by serving as a\nblueprint, for future work in the area of Dynamic-Adversarial mining, which\ncombines lessons learned from Streaming data mining, Adversarial learning and\nCybersecurity.\n"], ["2018-03-24", "http://arxiv.org/abs/1803.09156", "An Overview of Vulnerabilities of Voice Controlled Systems.", ["Yuan Gong", " Christian Poellabauer"], "  Over the last few years, a rapidly increasing number of Internet-of-Things\n(IoT) systems that adopt voice as the primary user input have emerged. These\nsystems have been shown to be vulnerable to various types of voice spoofing\nattacks. However, how exactly these techniques differ or relate to each other\nhas not been extensively studied. In this paper, we provide a survey of recent\nattack and defense techniques for voice controlled systems and propose a\nclassification of these techniques. We also discuss the need for a universal\ndefense strategy that protects a system from various types of attacks.\n"], ["2018-03-23", "http://arxiv.org/abs/1804.00504", "Generalizability vs. Robustness: Adversarial Examples for Medical Imaging.", ["Magdalini Paschali", " Sailesh Conjeti", " Fernando Navarro", " Nassir Navab"], "  In this paper, for the first time, we propose an evaluation method for deep\nlearning models that assesses the performance of a model not only in an unseen\ntest scenario, but also in extreme cases of noise, outliers and ambiguous input\ndata. To this end, we utilize adversarial examples, images that fool machine\nlearning models, while looking imperceptibly different from original data, as a\nmeasure to evaluate the robustness of a variety of medical imaging models.\nThrough extensive experiments on skin lesion classification and whole brain\nsegmentation with state-of-the-art networks such as Inception and UNet, we show\nthat models that achieve comparable performance regarding generalizability may\nhave significant variations in their perception of the underlying data\nmanifold, leading to an extensive performance gap in their robustness.\n"], ["2018-03-23", "http://arxiv.org/abs/1803.09043", "CNN Based Adversarial Embedding with Minimum Alteration for Image Steganography.", ["Weixuan Tang", " Bin Li", " Shunquan Tan", " Mauro Barni", " Jiwu Huang"], "  Historically, steganographic schemes were designed in a way to preserve image\nstatistics or steganalytic features. Since most of the state-of-the-art\nsteganalytic methods employ a machine learning (ML) based classifier, it is\nreasonable to consider countering steganalysis by trying to fool the ML\nclassifiers. However, simply applying perturbations on stego images as\nadversarial examples may lead to the failure of data extraction and introduce\nunexpected artefacts detectable by other classifiers. In this paper, we present\na steganographic scheme with a novel operation called adversarial embedding,\nwhich achieves the goal of hiding a stego message while at the same time\nfooling a convolutional neural network (CNN) based steganalyzer. The proposed\nmethod works under the conventional framework of distortion minimization.\nAdversarial embedding is achieved by adjusting the costs of image element\nmodifications according to the gradients backpropagated from the CNN classifier\ntargeted by the attack. Therefore, modification direction has a higher\nprobability to be the same as the sign of the gradient. In this way, the so\ncalled adversarial stego images are generated. Experiments demonstrate that the\nproposed steganographic scheme is secure against the targeted adversary-unaware\nsteganalyzer. In addition, it deteriorates the performance of other\nadversary-aware steganalyzers opening the way to a new class of modern\nsteganographic schemes capable to overcome powerful CNN-based steganalysis.\n"], ["2018-03-23", "http://arxiv.org/abs/1803.08773", "Detecting Adversarial Perturbations with Saliency.", ["Chiliang Zhang", " Zhimou Yang", " Zuochang Ye"], "  In this paper we propose a novel method for detecting adversarial examples by\ntraining a binary classifier with both origin data and saliency data. In the\ncase of image classification model, saliency simply explain how the model make\ndecisions by identifying significant pixels for prediction. A model shows wrong\nclassification output always learns wrong features and shows wrong saliency as\nwell. Our approach shows good performance on detecting adversarial\nperturbations. We quantitatively evaluate generalization ability of the\ndetector, showing that detectors trained with strong adversaries perform well\non weak adversaries.\n"], ["2018-03-23", "http://arxiv.org/abs/1803.08680", "Improving DNN Robustness to Adversarial Attacks using Jacobian Regularization.", ["Daniel Jakubovitz", " Raja Giryes"], "  Deep neural networks have lately shown tremendous performance in various\napplications including vision and speech processing tasks. However, alongside\ntheir ability to perform these tasks with such high accuracy, it has been shown\nthat they are highly susceptible to adversarial attacks: a small change in the\ninput would cause the network to err with high confidence. This phenomenon\nexposes an inherent fault in these networks and their ability to generalize\nwell. For this reason, providing robustness to adversarial attacks is an\nimportant challenge in networks training, which has led to extensive research.\nIn this work, we suggest a theoretically inspired novel approach to improve the\nnetworks' robustness. Our method applies regularization using the Frobenius\nnorm of the Jacobian of the network, which is applied as post-processing, after\nregular training has finished. We demonstrate empirically that it leads to\nenhanced robustness results with a minimal change in the original network's\naccuracy.\n"], ["2018-03-22", "http://arxiv.org/abs/1803.08533", "Understanding Measures of Uncertainty for Adversarial Example Detection.", ["Lewis Smith", " Yarin Gal"], "  Measuring uncertainty is a promising technique for detecting adversarial\nexamples, crafted inputs on which the model predicts an incorrect class with\nhigh confidence. But many measures of uncertainty exist, including predictive\nen- tropy and mutual information, each capturing different types of\nuncertainty. We study these measures, and shed light on why mutual information\nseems to be effective at the task of adversarial example detection. We\nhighlight failure modes for MC dropout, a widely used approach for estimating\nuncertainty in deep models. This leads to an improved understanding of the\ndrawbacks of current methods, and a proposal to improve the quality of\nuncertainty estimates using probabilistic model ensembles. We give illustrative\nexperiments using MNIST to demonstrate the intuition underlying the different\nmeasures of uncertainty, as well as experiments on a real world Kaggle dogs vs\ncats classification dataset.\n"], ["2018-03-21", "http://arxiv.org/abs/1803.07994", "Adversarial Defense based on Structure-to-Signal Autoencoders.", ["Joachim Folz", " Sebastian Palacio", " Joern Hees", " Damian Borth", " Andreas Dengel"], "  Adversarial attack methods have demonstrated the fragility of deep neural\nnetworks. Their imperceptible perturbations are frequently able fool\nclassifiers into potentially dangerous misclassifications. We propose a novel\nway to interpret adversarial perturbations in terms of the effective input\nsignal that classifiers actually use. Based on this, we apply specially trained\nautoencoders, referred to as S2SNets, as defense mechanism. They follow a\ntwo-stage training scheme: first unsupervised, followed by a fine-tuning of the\ndecoder, using gradients from an existing classifier. S2SNets induce a shift in\nthe distribution of gradients propagated through them, stripping them from\nclass-dependent signal. We analyze their robustness against several white-box\nand gray-box scenarios on the large ImageNet dataset. Our approach reaches\ncomparable resilience in white-box attack scenarios as other state-of-the-art\ndefenses in gray-box scenarios. We further analyze the relationships of\nAlexNet, VGG 16, ResNet 50 and Inception v3 in adversarial space, and found\nthat VGG 16 is the easiest to fool, while perturbations from ResNet 50 are the\nmost transferable.\n"], ["2018-03-21", "http://arxiv.org/abs/1803.08134", "Task-specific Deep LDA pruning of neural networks.", ["Qing Tian", " Tal Arbel", " James J. Clark"], "  With deep learning's success, a limited number of popular deep nets have been\nwidely adopted for various vision tasks. However, this usually results in\nunnecessarily high complexities and possibly many features of low task utility.\nIn this paper, we address this problem by introducing a task-dependent deep\npruning framework based on Fisher's Linear Discriminant Analysis (LDA). The\napproach can be applied to convolutional, fully-connected, and module-based\ndeep network structures, in all cases leveraging the high decorrelation of\nneuron motifs found in the pre-decision layer and cross-layer deconv\ndependency. Moreover, we examine our approach's potential in network\narchitecture search for specific tasks and analyze the influence of our pruning\non model robustness to noises and adversarial attacks. Experimental results on\ndatasets of generic objects, as well as domain specific tasks (CIFAR100,\nAdience, and LFWA) illustrate our framework's superior performance over\nstate-of-the-art pruning approaches and fixed compact nets (e.g. SqueezeNet,\nMobileNet). The proposed method successfully maintains comparable accuracies\neven after discarding most parameters (98%-99% for VGG16, up to 82% for the\nalready compact InceptionNet) and with significant FLOP reductions (83% for\nVGG16, up to 64% for InceptionNet). Through pruning, we can also derive\nsmaller, but more accurate and more robust models suitable for the task.\n"], ["2018-03-20", "http://arxiv.org/abs/1803.07519", "DeepGauge: Multi-Granularity Testing Criteria for Deep Learning Systems.", ["Lei Ma", " Felix Juefei-Xu", " Fuyuan Zhang", " Jiyuan Sun", " Minhui Xue", " Bo Li", " Chunyang Chen", " Ting Su", " Li Li", " Yang Liu", " Jianjun Zhao", " Yadong Wang"], "  Deep learning (DL) defines a new data-driven programming paradigm that\nconstructs the internal system logic of a crafted neuron network through a set\nof training data. We have seen wide adoption of DL in many safety-critical\nscenarios. However, a plethora of studies have shown that the state-of-the-art\nDL systems suffer from various vulnerabilities which can lead to severe\nconsequences when applied to real-world applications. Currently, the testing\nadequacy of a DL system is usually measured by the accuracy of test data.\nConsidering the limitation of accessible high quality test data, good accuracy\nperformance on test data can hardly provide confidence to the testing adequacy\nand generality of DL systems. Unlike traditional software systems that have\nclear and controllable logic and functionality, the lack of interpretability in\na DL system makes system analysis and defect detection difficult, which could\npotentially hinder its real-world deployment. In this paper, we propose\nDeepGauge, a set of multi-granularity testing criteria for DL systems, which\naims at rendering a multi-faceted portrayal of the testbed. The in-depth\nevaluation of our proposed testing criteria is demonstrated on two well-known\ndatasets, five DL systems, and with four state-of-the-art adversarial attack\ntechniques against DL. The potential usefulness of DeepGauge sheds light on the\nconstruction of more generic and robust DL systems.\n"], ["2018-03-19", "http://arxiv.org/abs/1803.06975", "Technical Report: When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks.", ["Octavian Suciu", " Radu M\u0103rginean", " Yi\u011fitcan Kaya", " Hal III Daum\u00e9", " Tudor Dumitra\u015f"], "  Recent results suggest that attacks against supervised machine learning\nsystems are quite effective, while defenses are easily bypassed by new attacks.\nHowever, the specifications for machine learning systems currently lack precise\nadversary definitions, and the existing attacks make diverse, potentially\nunrealistic assumptions about the strength of the adversary who launches them.\nWe propose the FAIL attacker model, which describes the adversary's knowledge\nand control along four dimensions. Our model allows us to consider a wide range\nof weaker adversaries who have limited control and incomplete knowledge of the\nfeatures, learning algorithms and training instances utilized. To evaluate the\nutility of the FAIL model, we consider the problem of conducting targeted\npoisoning attacks in a realistic setting: the crafted poison samples must have\nclean labels, must be individually and collectively inconspicuous, and must\nexhibit a generalized form of transferability, defined by the FAIL model. By\ntaking these constraints into account, we design StingRay, a targeted poisoning\nattack that is practical against 4 machine learning applications, which use 3\ndifferent learning algorithms, and can bypass 2 existing defenses. Conversely,\nwe show that a prior evasion attack is less effective under generalized\ntransferability. Such attack evaluations, under the FAIL adversary model, may\nalso suggest promising directions for future defenses.\n"], ["2018-03-19", "http://arxiv.org/abs/1803.06978", "Improving Transferability of Adversarial Examples with Input Diversity.", ["Cihang Xie", " Zhishuai Zhang", " Yuyin Zhou", " Song Bai", " Jianyu Wang", " Zhou Ren", " Alan Yuille"], "  Though CNNs have achieved the state-of-the-art performance on various vision\ntasks, they are vulnerable to adversarial examples --- crafted by adding\nhuman-imperceptible perturbations to clean images. However, most of the\nexisting adversarial attacks only achieve relatively low success rates under\nthe challenging black-box setting, where the attackers have no knowledge of the\nmodel structure and parameters. To this end, we propose to improve the\ntransferability of adversarial examples by creating diverse input patterns.\nInstead of only using the original images to generate adversarial examples, our\nmethod applies random transformations to the input images at each iteration.\nExtensive experiments on ImageNet show that the proposed attack method can\ngenerate adversarial examples that transfer much better to different networks\nthan existing baselines. By evaluating our method against top defense solutions\nand official baselines from NIPS 2017 adversarial competition, the enhanced\nattack reaches an average success rate of 73.0%, which outperforms the top-1\nattack submission in the NIPS competition by a large margin of 6.6%. We hope\nthat our proposed attack strategy can serve as a strong benchmark baseline for\nevaluating the robustness of networks to adversaries and the effectiveness of\ndifferent defense methods in the future. Code is available at\nhttps://github.com/cihangxie/DI-2-FGSM.\n"], ["2018-03-17", "http://arxiv.org/abs/1803.06567", "A Dual Approach to Scalable Verification of Deep Networks.", ["Dj Krishnamurthy", " Dvijotham", " Robert Stanforth", " Sven Gowal", " Timothy Mann", " Pushmeet Kohli"], "  This paper addresses the problem of formally verifying desirable properties\nof neural networks, i.e., obtaining provable guarantees that neural networks\nsatisfy specifications relating their inputs and outputs (robustness to bounded\nnorm adversarial perturbations, for example). Most previous work on this topic\nwas limited in its applicability by the size of the network, network\narchitecture and the complexity of properties to be verified. In contrast, our\nframework applies to a general class of activation functions and specifications\non neural network inputs and outputs. We formulate verification as an\noptimization problem (seeking to find the largest violation of the\nspecification) and solve a Lagrangian relaxation of the optimization problem to\nobtain an upper bound on the worst case violation of the specification being\nverified. Our approach is anytime i.e. it can be stopped at any time and a\nvalid bound on the maximum violation can be obtained. We develop specialized\nverification algorithms with provable tightness guarantees under special\nassumptions and demonstrate the practical significance of our general\nverification approach on a variety of verification tasks.\n"], ["2018-03-16", "http://arxiv.org/abs/1803.06373", "Adversarial Logit Pairing.", ["Harini Kannan", " Alexey Kurakin", " Ian Goodfellow"], "  In this paper, we develop improved techniques for defending against\nadversarial examples at scale. First, we implement the state of the art version\nof adversarial training at unprecedented scale on ImageNet and investigate\nwhether it remains effective in this setting - an important open scientific\nquestion (Athalye et al., 2018). Next, we introduce enhanced defenses using a\ntechnique we call logit pairing, a method that encourages logits for pairs of\nexamples to be similar. When applied to clean examples and their adversarial\ncounterparts, logit pairing improves accuracy on adversarial examples over\nvanilla adversarial training; we also find that logit pairing on clean examples\nonly is competitive with adversarial training in terms of accuracy on two\ndatasets. Finally, we show that adversarial logit pairing achieves the state of\nthe art defense on ImageNet against PGD white box attacks, with an accuracy\nimprovement from 1.5% to 27.9%. Adversarial logit pairing also successfully\ndamages the current state of the art defense against black box attacks on\nImageNet (Tramer et al., 2018), dropping its accuracy from 66.6% to 47.1%. With\nthis new accuracy drop, adversarial logit pairing ties with Tramer et al.(2018)\nfor the state of the art on black box attacks on ImageNet.\n"], ["2018-03-16", "http://arxiv.org/abs/1804.00499", "Semantic Adversarial Examples.", ["Hossein Hosseini", " Radha Poovendran"], "  Deep neural networks are known to be vulnerable to adversarial examples,\ni.e., images that are maliciously perturbed to fool the model. Generating\nadversarial examples has been mostly limited to finding small perturbations\nthat maximize the model prediction error. Such images, however, contain\nartificial perturbations that make them somewhat distinguishable from natural\nimages. This property is used by several defense methods to counter adversarial\nexamples by applying denoising filters or training the model to be robust to\nsmall perturbations.\n  In this paper, we introduce a new class of adversarial examples, namely\n\"Semantic Adversarial Examples,\" as images that are arbitrarily perturbed to\nfool the model, but in such a way that the modified image semantically\nrepresents the same object as the original image. We formulate the problem of\ngenerating such images as a constrained optimization problem and develop an\nadversarial transformation based on the shape bias property of human cognitive\nsystem. In our method, we generate adversarial images by first converting the\nRGB image into the HSV (Hue, Saturation and Value) color space and then\nrandomly shifting the Hue and Saturation components, while keeping the Value\ncomponent the same. Our experimental results on CIFAR10 dataset show that the\naccuracy of VGG16 network on adversarial color-shifted images is 5.7%.\n"], ["2018-03-15", "http://arxiv.org/abs/1803.05598", "Large Margin Deep Networks for Classification.", ["Gamaleldin F. Elsayed", " Dilip Krishnan", " Hossein Mobahi", " Kevin Regan", " Samy Bengio"], "  We present a formulation of deep learning that aims at producing a large\nmargin classifier. The notion of margin, minimum distance to a decision\nboundary, has served as the foundation of several theoretically profound and\nempirically successful results for both classification and regression tasks.\nHowever, most large margin algorithms are applicable only to shallow models\nwith a preset feature representation; and conventional margin methods for\nneural networks only enforce margin at the output layer. Such methods are\ntherefore not well suited for deep networks.\n  In this work, we propose a novel loss function to impose a margin on any\nchosen set of layers of a deep network (including input and hidden layers). Our\nformulation allows choosing any norm on the metric measuring the margin. We\ndemonstrate that the decision boundary obtained by our loss has nice properties\ncompared to standard classification loss functions. Specifically, we show\nimproved empirical results on the MNIST, CIFAR-10 and ImageNet datasets on\nmultiple tasks: generalization from small training sets, corrupted labels, and\nrobustness against adversarial perturbations. The resulting loss is general and\ncomplementary to existing data augmentation (such as random/adversarial input\ntransform) and regularization techniques (such as weight decay, dropout, and\nbatch norm).\n"], ["2018-03-13", "http://arxiv.org/abs/1803.05787", "Feature Distillation: DNN-Oriented JPEG Compression Against Adversarial Examples.", ["Zihao Liu", " Qi Liu", " Tao Liu", " Nuo Xu", " Xue Lin", " Yanzhi Wang", " Wujie Wen"], "  Image compression-based approaches for defending against the\nadversarial-example attacks, which threaten the safety use of deep neural\nnetworks (DNN), have been investigated recently. However, prior works mainly\nrely on directly tuning parameters like compression rate, to blindly reduce\nimage features, thereby lacking guarantee on both defense efficiency (i.e.\naccuracy of polluted images) and classification accuracy of benign images,\nafter applying defense methods. To overcome these limitations, we propose a\nJPEG-based defensive compression framework, namely \"feature distillation\", to\neffectively rectify adversarial examples without impacting classification\naccuracy on benign data. Our framework significantly escalates the defense\nefficiency with marginal accuracy reduction using a two-step method: First, we\nmaximize malicious features filtering of adversarial input perturbations by\ndeveloping defensive quantization in frequency domain of JPEG compression or\ndecompression, guided by a semi-analytical method; Second, we suppress the\ndistortions of benign features to restore classification accuracy through a\nDNN-oriented quantization refine process. Our experimental results show that\nproposed \"feature distillation\" can significantly surpass the latest\ninput-transformation based mitigations such as Quilting and TV Minimization in\nthree aspects, including defense efficiency (improve classification accuracy\nfrom $\\sim20\\%$ to $\\sim90\\%$ on adversarial examples), accuracy of benign\nimages after defense ($\\le1\\%$ accuracy degradation), and processing time per\nimage ($\\sim259\\times$ Speedup). Moreover, our solution can also provide the\nbest defense efficiency ($\\sim60\\%$ accuracy) against the recent adaptive\nattack with least accuracy reduction ($\\sim1\\%$) on benign images when compared\nwith other input-transformation based defense methods.\n"], ["2018-03-13", "http://arxiv.org/abs/1803.05123", "Defending against Adversarial Attack towards Deep Neural Networks via Collaborative Multi-task Training.", ["Derek Wang", " Chaoran Li", " Sheng Wen", " Surya Nepal", " Yang Xiang"], "  Deep neural networks (DNNs) are known to be vulnerable to adversarial\nexamples which contain imperceptible perturbations. A series of defending\nmethods, either proactive defence or reactive defence, have been proposed in\nthe recent years. However, most of the methods can only handle specific\nattacks. For example, proactive defending methods are invalid against grey-box\nor white-box attack, while reactive defending methods are challenged by\nlow-distortion adversarial examples or transferring adversarial examples. This\nbecomes a critical problem since a defender usually do not have the type of the\nattack as a priori knowledge. Moreover, the two-pronged defence (e.g. MagNet),\nwhich takes the advantages of both proactive and reactive methods, has been\nreported as broken under transferring attacks. To address this problem, this\npaper proposed a novel defensive framework based on collaborative multi-task\ntraining, aiming at providing defence for different types of attacks. The\nproposed defence first encodes training labels into label pairs and counters\nblack-box attack leveraging adversarial training supervised by the encoded\nlabel pairs. The defence further constructs a detector to identify and reject\nhigh-confidence adversarial examples that bypass the black-box defence. In\naddition, the proposed collaborative architecture can prevent adversaries from\nfinding valid adversarial examples when the defending strategy is exposed. As\nfar as we know, our method is a new two-pronged defence that is resilient to\nthe transferring attack targeting MagNet.\n"], ["2018-03-13", "http://arxiv.org/abs/1803.04765", "Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning.", ["Nicolas Papernot", " Patrick McDaniel"], "  Deep neural networks (DNNs) enable innovative applications of machine\nlearning like image recognition, machine translation, or malware detection.\nHowever, deep learning is often criticized for its lack of robustness in\nadversarial settings (e.g., vulnerability to adversarial inputs) and general\ninability to rationalize its predictions. In this work, we exploit the\nstructure of deep learning to enable new learning-based inference and decision\nstrategies that achieve desirable properties such as robustness and\ninterpretability. We take a first step in this direction and introduce the Deep\nk-Nearest Neighbors (DkNN). This hybrid classifier combines the k-nearest\nneighbors algorithm with representations of the data learned by each layer of\nthe DNN: a test input is compared to its neighboring training points according\nto the distance that separates them in the representations. We show the labels\nof these neighboring points afford confidence estimates for inputs outside the\nmodel's training manifold, including on malicious inputs like adversarial\nexamples--and therein provides protections against inputs that are outside the\nmodels understanding. This is because the nearest neighbors can be used to\nestimate the nonconformity of, i.e., the lack of support for, a prediction in\nthe training data. The neighbors also constitute human-interpretable\nexplanations of predictions. We evaluate the DkNN algorithm on several\ndatasets, and show the confidence estimates accurately identify inputs outside\nthe model, and that the explanations provided by nearest neighbors are\nintuitive and useful in understanding model failures.\n"], ["2018-03-13", "http://arxiv.org/abs/1803.04683", "Invisible Mask: Practical Attacks on Face Recognition with Infrared.", ["Zhe Zhou", " Di Tang", " Xiaofeng Wang", " Weili Han", " Xiangyu Liu", " Kehuan Zhang"], "  Accurate face recognition techniques make a series of critical applications\npossible: policemen could employ it to retrieve criminals' faces from\nsurveillance video streams; cross boarder travelers could pass a face\nauthentication inspection line without the involvement of officers.\nNonetheless, when public security heavily relies on such intelligent systems,\nthe designers should deliberately consider the emerging attacks aiming at\nmisleading those systems employing face recognition.\n  We propose a kind of brand new attack against face recognition systems, which\nis realized by illuminating the subject using infrared according to the\nadversarial examples worked out by our algorithm, thus face recognition systems\ncan be bypassed or misled while simultaneously the infrared perturbations\ncannot be observed by raw eyes. Through launching this kind of attack, an\nattacker not only can dodge surveillance cameras. More importantly, he can\nimpersonate his target victim and pass the face authentication system, if only\nthe victim's photo is acquired by the attacker. Again, the attack is totally\nunobservable by nearby people, because not only the light is invisible, but\nalso the device we made to launch the attack is small enough. According to our\nstudy on a large dataset, attackers have a very high success rate with a over\n70\\% success rate for finding such an adversarial example that can be\nimplemented by infrared. To the best of our knowledge, our work is the first\none to shed light on the severity of threat resulted from infrared adversarial\nexamples against face recognition.\n"], ["2018-03-12", "http://arxiv.org/abs/1803.04173", "Adversarial Malware Binaries: Evading Deep Learning for Malware Detection in Executables.", ["Bojan Kolosnjaji", " Ambra Demontis", " Battista Biggio", " Davide Maiorca", " Giorgio Giacinto", " Claudia Eckert", " Fabio Roli"], "  Machine-learning methods have already been exploited as useful tools for\ndetecting malicious executable files. They leverage data retrieved from malware\nsamples, such as header fields, instruction sequences, or even raw bytes, to\nlearn models that discriminate between benign and malicious software. However,\nit has also been shown that machine learning and deep neural networks can be\nfooled by evasion attacks (also referred to as adversarial examples), i.e.,\nsmall changes to the input data that cause misclassification at test time. In\nthis work, we investigate the vulnerability of malware detection methods that\nuse deep networks to learn from raw bytes. We propose a gradient-based attack\nthat is capable of evading a recently-proposed deep network suited to this\npurpose by only changing few specific bytes at the end of each malware sample,\nwhile preserving its intrusive functionality. Promising results show that our\nadversarial malware binaries evade the targeted network with high probability,\neven though less than 1% of their bytes are modified.\n"], ["2018-03-10", "http://arxiv.org/abs/1803.03880", "Combating Adversarial Attacks Using Sparse Representations.", ["Soorya Gopalakrishnan", " Zhinus Marzi", " Upamanyu Madhow", " Ramtin Pedarsani"], "  It is by now well-known that small adversarial perturbations can induce\nclassification errors in deep neural networks (DNNs). In this paper, we make\nthe case that sparse representations of the input data are a crucial tool for\ncombating such attacks. For linear classifiers, we show that a sparsifying\nfront end is provably effective against $\\ell_{\\infty}$-bounded attacks,\nreducing output distortion due to the attack by a factor of roughly $K / N$\nwhere $N$ is the data dimension and $K$ is the sparsity level. We then extend\nthis concept to DNNs, showing that a \"locally linear\" model can be used to\ndevelop a theoretical foundation for crafting attacks and defenses.\nExperimental results for the MNIST dataset show the efficacy of the proposed\nsparsifying front end.\n"], ["2018-03-10", "http://arxiv.org/abs/1803.03870", "Detecting Adversarial Examples via Neural Fingerprinting.", ["Sumanth Dathathri", " Stephan Zheng", " Tianwei Yin", " Richard M. Murray", " Yisong Yue"], "  Deep neural networks are vulnerable to adversarial examples, which\ndramatically alter model output using small input changes. We propose Neural\nFingerprinting, a simple, yet effective method to detect adversarial examples\nby verifying whether model behavior is consistent with a set of secret\nfingerprints, inspired by the use of biometric and cryptographic signatures.\nThe benefits of our method are that 1) it is fast, 2) it is prohibitively\nexpensive for an attacker to reverse-engineer which fingerprints were used, and\n3) it does not assume knowledge of the adversary. In this work, we pose a\nformal framework to analyze fingerprints under various threat models, and\ncharacterize Neural Fingerprinting for linear models. For complex neural\nnetworks, we empirically demonstrate that Neural Fingerprinting significantly\nimproves on state-of-the-art detection mechanisms by detecting the strongest\nknown adversarial attacks with 98-100% AUC-ROC scores on the MNIST, CIFAR-10\nand MiniImagenet (20 classes) datasets. In particular, the detection accuracy\nof Neural Fingerprinting generalizes well to unseen test-data under various\nblack- and whitebox threat models, and is robust over a wide range of\nhyperparameters and choices of fingerprints.\n"], ["2018-03-09", "http://arxiv.org/abs/1803.03613", "Detecting Adversarial Examples - A Lesson from Multimedia Forensics.", ["Pascal Sch\u00f6ttle", " Alexander Schl\u00f6gl", " Cecilia Pasquini", " Rainer B\u00f6hme"], "  Adversarial classification is the task of performing robust classification in\nthe presence of a strategic attacker. Originating from information hiding and\nmultimedia forensics, adversarial classification recently received a lot of\nattention in a broader security context. In the domain of machine\nlearning-based image classification, adversarial classification can be\ninterpreted as detecting so-called adversarial examples, which are slightly\naltered versions of benign images. They are specifically crafted to be\nmisclassified with a very high probability by the classifier under attack.\nNeural networks, which dominate among modern image classifiers, have been shown\nto be especially vulnerable to these adversarial examples.\n  However, detecting subtle changes in digital images has always been the goal\nof multimedia forensics and steganalysis. In this paper, we highlight the\nparallels between these two fields and secure machine learning.\n  Furthermore, we adapt a linear filter, similar to early steganalysis methods,\nto detect adversarial examples that are generated with the projected gradient\ndescent (PGD) method, the state-of-the-art algorithm for this task. We test our\nmethod on the MNIST database and show for several parameter combinations of PGD\nthat our method can reliably detect adversarial examples.\n  Additionally, the combination of adversarial re-training and our detection\nmethod effectively reduces the attack surface of attacks against neural\nnetworks. Thus, we conclude that adversarial examples for image classification\npossibly do not withstand detection methods from steganalysis, and future work\nshould explore the effectiveness of known techniques from multimedia forensics\nin other adversarial settings.\n"], ["2018-03-09", "http://arxiv.org/abs/1803.03607", "On Generation of Adversarial Examples using Convex Programming.", ["Emilio Rafael Balda", " Arash Behboodi", " Rudolf Mathar"], "  It has been observed that deep learning architectures tend to make erroneous\ndecisions with high reliability for particularly designed adversarial\ninstances. In this work, we show that the perturbation analysis of these\narchitectures provides a framework for generating adversarial instances by\nconvex programming which, for classification tasks, is able to recover variants\nof existing non-adaptive adversarial methods. The proposed framework can be\nused for the design of adversarial noise under various desirable constraints\nand different types of networks. Moreover, this framework is capable of\nexplaining various existing adversarial methods and can be used to derive new\nalgorithms as well. We make use of these results to obtain novel algorithms.\nThe experiments show the competitive performance of the obtained solutions, in\nterms of fooling ratio, when benchmarked with well-known adversarial methods.\n"], ["2018-03-09", "http://arxiv.org/abs/1803.03544", "Explaining Black-box Android Malware Detection.", ["Marco Melis", " Davide Maiorca", " Battista Biggio", " Giorgio Giacinto", " Fabio Roli"], "  Machine-learning models have been recently used for detecting malicious\nAndroid applications, reporting impressive performances on benchmark datasets,\neven when trained only on features statically extracted from the application,\nsuch as system calls and permissions. However, recent findings have highlighted\nthe fragility of such in-vitro evaluations with benchmark datasets, showing\nthat very few changes to the content of Android malware may suffice to evade\ndetection. How can we thus trust that a malware detector performing well on\nbenchmark data will continue to do so when deployed in an operating\nenvironment? To mitigate this issue, the most popular Android malware detectors\nuse linear, explainable machine-learning models to easily identify the most\ninfluential features contributing to each decision. In this work, we generalize\nthis approach to any black-box machine- learning model, by leveraging a\ngradient-based approach to identify the most influential local features. This\nenables using nonlinear models to potentially increase accuracy without\nsacrificing interpretability of decisions. Our approach also highlights the\nglobal characteristics learned by the model to discriminate between benign and\nmalware applications. Finally, as shown by our empirical analysis on a popular\nAndroid malware detection task, it also helps identifying potential\nvulnerabilities of linear and nonlinear models against adversarial\nmanipulations.\n"], ["2018-03-08", "http://arxiv.org/abs/1803.02988", "Rethinking Feature Distribution for Loss Functions in Image Classification.", ["Weitao Wan", " Yuanyi Zhong", " Tianpeng Li", " Jiansheng Chen"], "  We propose a large-margin Gaussian Mixture (L-GM) loss for deep neural\nnetworks in classification tasks. Different from the softmax cross-entropy\nloss, our proposal is established on the assumption that the deep features of\nthe training set follow a Gaussian Mixture distribution. By involving a\nclassification margin and a likelihood regularization, the L-GM loss\nfacilitates both a high classification performance and an accurate modeling of\nthe training feature distribution. As such, the L-GM loss is superior to the\nsoftmax loss and its major variants in the sense that besides classification,\nit can be readily used to distinguish abnormal inputs, such as the adversarial\nexamples, based on their features' likelihood to the training feature\ndistribution. Extensive experiments on various recognition benchmarks like\nMNIST, CIFAR, ImageNet and LFW, as well as on adversarial examples demonstrate\nthe effectiveness of our proposal.\n"], ["2018-03-07", "http://arxiv.org/abs/1803.02536", "Sparse Adversarial Perturbations for Videos.", ["Xingxing Wei", " Jun Zhu", " Hang Su"], "  Although adversarial samples of deep neural networks (DNNs) have been\nintensively studied on static images, their extensions in videos are never\nexplored. Compared with images, attacking a video needs to consider not only\nspatial cues but also temporal cues. Moreover, to improve the imperceptibility\nas well as reduce the computation cost, perturbations should be added on as\nfewer frames as possible, i.e., adversarial perturbations are temporally\nsparse. This further motivates the propagation of perturbations, which denotes\nthat perturbations added on the current frame can transfer to the next frames\nvia their temporal interactions. Thus, no (or few) extra perturbations are\nneeded for these frames to misclassify them. To this end, we propose an\nl2,1-norm based optimization algorithm to compute the sparse adversarial\nperturbations for videos. We choose the action recognition as the targeted\ntask, and networks with a CNN+RNN architecture as threat models to verify our\nmethod. Thanks to the propagation, we can compute perturbations on a shortened\nversion video, and then adapt them to the long version video to fool DNNs.\nExperimental results on the UCF101 dataset demonstrate that even only one frame\nin a video is perturbed, the fooling rate can still reach 59.7%.\n"], ["2018-03-04", "http://arxiv.org/abs/1803.01442", "Stochastic Activation Pruning for Robust Adversarial Defense.", ["Guneet S. Dhillon", " Kamyar Azizzadenesheli", " Zachary C. Lipton", " Jeremy Bernstein", " Jean Kossaifi", " Aran Khanna", " Anima Anandkumar"], "  Neural networks are known to be vulnerable to adversarial examples. Carefully\nchosen perturbations to real images, while imperceptible to humans, induce\nmisclassification and threaten the reliability of deep learning systems in the\nwild. To guard against adversarial examples, we take inspiration from game\ntheory and cast the problem as a minimax zero-sum game between the adversary\nand the model. In general, for such games, the optimal strategy for both\nplayers requires a stochastic policy, also known as a mixed strategy. In this\nlight, we propose Stochastic Activation Pruning (SAP), a mixed strategy for\nadversarial defense. SAP prunes a random subset of activations (preferentially\npruning those with smaller magnitude) and scales up the survivors to\ncompensate. We can apply SAP to pretrained networks, including adversarially\ntrained models, without fine-tuning, providing robustness against adversarial\nexamples. Experiments demonstrate that SAP confers robustness against attacks,\nincreasing accuracy and preserving calibration.\n"], ["2018-03-03", "http://arxiv.org/abs/1803.01128", "Seq2Sick: Evaluating the Robustness of Sequence-to-Sequence Models with Adversarial Examples.", ["Minhao Cheng", " Jinfeng Yi", " Huan Zhang", " Pin-Yu Chen", " Cho-Jui Hsieh"], "  Crafting adversarial examples has become an important technique to evaluate\nthe robustness of deep neural networks (DNNs). However, most existing works\nfocus on attacking the image classification problem since its input space is\ncontinuous and output space is finite.\n  In this paper, we study the much more challenging problem of crafting\nadversarial examples for sequence-to-sequence (seq2seq) models, whose inputs\nare discrete text strings and outputs have an almost infinite number of\npossibilities. To address the challenges caused by the discrete input space, we\npropose a projected gradient method combined with group lasso and gradient\nregularization. To handle the almost infinite output space, we design some\nnovel loss functions to conduct non-overlapping attack and targeted keyword\nattack. We apply our algorithm to machine translation and text summarization\ntasks, and verify the effectiveness of the proposed algorithm: by changing less\nthan 3 words, we can make seq2seq model to produce desired outputs with high\nsuccess rates. On the other hand, we recognize that, compared with the\nwell-evaluated CNN-based classifiers, seq2seq models are intrinsically more\nrobust to adversarial attacks.\n"], ["2018-03-02", "http://arxiv.org/abs/1803.00940", "Protecting JPEG Images Against Adversarial Attacks.", ["Aaditya Prakash", " Nick Moran", " Solomon Garber", " Antonella DiLillo", " James Storer"], "  As deep neural networks (DNNs) have been integrated into critical systems,\nseveral methods to attack these systems have been developed. These adversarial\nattacks make imperceptible modifications to an image that fool DNN classifiers.\nWe present an adaptive JPEG encoder which defends against many of these\nattacks. Experimentally, we show that our method produces images with high\nvisual quality while greatly reducing the potency of state-of-the-art attacks.\nOur algorithm requires only a modest increase in encoding time, produces a\ncompressed image which can be decompressed by an off-the-shelf JPEG decoder,\nand classified by an unmodified classifier\n"], ["2018-02-26", "http://arxiv.org/abs/1802.09707", "Understanding and Enhancing the Transferability of Adversarial Examples.", ["Lei Wu", " Zhanxing Zhu", " Cheng Tai", " Weinan E"], "  State-of-the-art deep neural networks are known to be vulnerable to\nadversarial examples, formed by applying small but malicious perturbations to\nthe original inputs. Moreover, the perturbations can \\textit{transfer across\nmodels}: adversarial examples generated for a specific model will often mislead\nother unseen models. Consequently the adversary can leverage it to attack\ndeployed systems without any query, which severely hinder the application of\ndeep learning, especially in the areas where security is crucial. In this work,\nwe systematically study how two classes of factors that might influence the\ntransferability of adversarial examples. One is about model-specific factors,\nincluding network architecture, model capacity and test accuracy. The other is\nthe local smoothness of loss function for constructing adversarial examples.\nBased on these understanding, a simple but effective strategy is proposed to\nenhance transferability. We call it variance-reduced attack, since it utilizes\nthe variance-reduced gradient to generate adversarial example. The\neffectiveness is confirmed by a variety of experiments on both CIFAR-10 and\nImageNet datasets.\n"], ["2018-02-26", "http://arxiv.org/abs/1802.09653", "On the Suitability of $L_p$-norms for Creating and Preventing Adversarial Examples.", ["Mahmood Sharif", " Lujo Bauer", " Michael K. Reiter"], "  Much research effort has been devoted to better understanding adversarial\nexamples, which are specially crafted inputs to machine-learning models that\nare perceptually similar to benign inputs, but are classified differently\n(i.e., misclassified). Both algorithms that create adversarial examples and\nstrategies for defending against them typically use $L_p$-norms to measure the\nperceptual similarity between an adversarial input and its benign original.\nPrior work has already shown, however, that two images need not be close to\neach other as measured by an $L_p$-norm to be perceptually similar. In this\nwork, we show that nearness according to an $L_p$-norm is not just unnecessary\nfor perceptual similarity, but is also insufficient. Specifically, focusing on\ndatasets (CIFAR10 and MNIST), $L_p$-norms, and thresholds used in prior work,\nwe show through online user studies that \"adversarial examples\" that are closer\nto their benign counterparts than required by commonly used $L_p$-norm\nthresholds can nevertheless be perceptually different to humans from the\ncorresponding benign examples. Namely, the perceptual distance between two\nimages that are \"near\" each other according to an $L_p$-norm can be high enough\nthat participants frequently classify the two images as representing different\nobjects or digits. Combined with prior work, we thus demonstrate that nearness\nof inputs as measured by $L_p$-norms is neither necessary nor sufficient for\nperceptual similarity, which has implications for both creating and defending\nagainst adversarial examples. We propose and discuss alternative similarity\nmetrics to stimulate future research in the area.\n"], ["2018-02-26", "http://arxiv.org/abs/1802.09502", "Retrieval-Augmented Convolutional Neural Networks for Improved Robustness against Adversarial Examples.", ["Jake Zhao", " Kyunghyun Cho"], "  We propose a retrieval-augmented convolutional network and propose to train\nit with local mixup, a novel variant of the recently proposed mixup algorithm.\nThe proposed hybrid architecture combining a convolutional network and an\noff-the-shelf retrieval engine was designed to mitigate the adverse effect of\noff-manifold adversarial examples, while the proposed local mixup addresses\non-manifold ones by explicitly encouraging the classifier to locally behave\nlinearly on the data manifold. Our evaluation of the proposed approach against\nfive readily-available adversarial attacks on three datasets--CIFAR-10, SVHN\nand ImageNet--demonstrate the improved robustness compared to the vanilla\nconvolutional network.\n"], ["2018-02-26", "http://arxiv.org/abs/1802.09308", "Max-Mahalanobis Linear Discriminant Analysis Networks.", ["Tianyu Pang", " Chao Du", " Jun Zhu"], "  A deep neural network (DNN) consists of a nonlinear transformation from an\ninput to a feature representation, followed by a common softmax linear\nclassifier. Though many efforts have been devoted to designing a proper\narchitecture for nonlinear transformation, little investigation has been done\non the classifier part. In this paper, we show that a properly designed\nclassifier can improve robustness to adversarial attacks and lead to better\nprediction results. Specifically, we define a Max-Mahalanobis distribution\n(MMD) and theoretically show that if the input distributes as a MMD, the linear\ndiscriminant analysis (LDA) classifier will have the best robustness to\nadversarial examples. We further propose a novel Max-Mahalanobis linear\ndiscriminant analysis (MM-LDA) network, which explicitly maps a complicated\ndata distribution in the input space to a MMD in the latent feature space and\nthen applies LDA to make predictions. Our results demonstrate that the MM-LDA\nnetworks are significantly more robust to adversarial attacks, and have better\nperformance in class-biased classification.\n"], ["2018-02-23", "http://arxiv.org/abs/1803.00404", "Deep Defense: Training DNNs with Improved Adversarial Robustness.", ["Ziang Yan", " Yiwen Guo", " Changshui Zhang"], "  Despite the efficacy on a variety of computer vision tasks, deep neural\nnetworks (DNNs) are vulnerable to adversarial attacks, limiting their\napplications in security-critical systems. Recent works have shown the\npossibility of generating imperceptibly perturbed image inputs (a.k.a.,\nadversarial examples) to fool well-trained DNN classifiers into making\narbitrary predictions. To address this problem, we propose a training recipe\nnamed \"deep defense\". Our core idea is to integrate an adversarial\nperturbation-based regularizer into the classification objective, such that the\nobtained models learn to resist potential attacks, directly and precisely. The\nwhole optimization problem is solved just like training a recursive network.\nExperimental results demonstrate that our method outperforms training with\nadversarial/Parseval regularizations by large margins on various datasets\n(including MNIST, CIFAR-10 and ImageNet) and different DNN architectures. Code\nand models for reproducing our results are available at\nhttps://github.com/ZiangYan/deepdefense.pytorch\n"], ["2018-02-23", "http://arxiv.org/abs/1802.08760", "Sensitivity and Generalization in Neural Networks: an Empirical Study.", ["Roman Novak", " Yasaman Bahri", " Daniel A. Abolafia", " Jeffrey Pennington", " Jascha Sohl-Dickstein"], "  In practice it is often found that large over-parameterized neural networks\ngeneralize better than their smaller counterparts, an observation that appears\nto conflict with classical notions of function complexity, which typically\nfavor smaller models. In this work, we investigate this tension between\ncomplexity and generalization through an extensive empirical exploration of two\nnatural metrics of complexity related to sensitivity to input perturbations.\nOur experiments survey thousands of models with various fully-connected\narchitectures, optimizers, and other hyper-parameters, as well as four\ndifferent image classification datasets.\n  We find that trained neural networks are more robust to input perturbations\nin the vicinity of the training data manifold, as measured by the norm of the\ninput-output Jacobian of the network, and that it correlates well with\ngeneralization. We further establish that factors associated with poor\ngeneralization $-$ such as full-batch training or using random labels $-$\ncorrespond to lower robustness, while factors associated with good\ngeneralization $-$ such as data augmentation and ReLU non-linearities $-$ give\nrise to more robust functions. Finally, we demonstrate how the input-output\nJacobian norm can be predictive of generalization at the level of individual\ntest points.\n"], ["2018-02-23", "http://arxiv.org/abs/1802.08686", "Adversarial vulnerability for any classifier.", ["Alhussein Fawzi", " Hamza Fawzi", " Omar Fawzi"], "  Despite achieving impressive performance, state-of-the-art classifiers remain\nhighly vulnerable to small, imperceptible, adversarial perturbations. This\nvulnerability has proven empirically to be very intricate to address. In this\npaper, we study the phenomenon of adversarial perturbations under the\nassumption that the data is generated with a smooth generative model. We derive\nfundamental upper bounds on the robustness to perturbations of any\nclassification function, and prove the existence of adversarial perturbations\nthat transfer well across different classifiers with small risk. Our analysis\nof the robustness also provides insights onto key properties of generative\nmodels, such as their smoothness and dimensionality of latent space. We\nconclude with numerical experimental results showing that our bounds provide\ninformative baselines to the maximal achievable robustness on several datasets.\n"], ["2018-02-23", "http://arxiv.org/abs/1802.08678", "Verifying Controllers Against Adversarial Examples with Bayesian Optimization.", ["Shromona Ghosh", " Felix Berkenkamp", " Gireeja Ranade", " Shaz Qadeer", " Ashish Kapoor"], "  Recent successes in reinforcement learning have lead to the development of\ncomplex controllers for real-world robots. As these robots are deployed in\nsafety-critical applications and interact with humans, it becomes critical to\nensure safety in order to avoid causing harm. A first step in this direction is\nto test the controllers in simulation. To be able to do this, we need to\ncapture what we mean by safety and then efficiently search the space of all\nbehaviors to see if they are safe. In this paper, we present an active-testing\nframework based on Bayesian Optimization. We specify safety constraints using\nlogic and exploit structure in the problem in order to test the system for\nadversarial counter examples that violate the safety specifications. These\nspecifications are defined as complex boolean combinations of smooth functions\non the trajectories and, unlike reward functions in reinforcement learning, are\nexpressive and impose hard constraints on the system. In our framework, we\nexploit regularity assumptions on individual functions in form of a Gaussian\nProcess (GP) prior. We combine these into a coherent optimization framework\nusing problem structure. The resulting algorithm is able to provably verify\ncomplex safety specifications or alternatively find counter examples.\nExperimental results show that the proposed method is able to find adversarial\nexamples quickly.\n"], ["2018-02-22", "http://arxiv.org/abs/1803.00401", "Unravelling Robustness of Deep Learning based Face Recognition Against Adversarial Attacks.", ["Gaurav Goswami", " Nalini Ratha", " Akshay Agarwal", " Richa Singh", " Mayank Vatsa"], "  Deep neural network (DNN) architecture based models have high expressive\npower and learning capacity. However, they are essentially a black box method\nsince it is not easy to mathematically formulate the functions that are learned\nwithin its many layers of representation. Realizing this, many researchers have\nstarted to design methods to exploit the drawbacks of deep learning based\nalgorithms questioning their robustness and exposing their singularities. In\nthis paper, we attempt to unravel three aspects related to the robustness of\nDNNs for face recognition: (i) assessing the impact of deep architectures for\nface recognition in terms of vulnerabilities to attacks inspired by commonly\nobserved distortions in the real world that are well handled by shallow\nlearning methods along with learning based adversaries; (ii) detecting the\nsingularities by characterizing abnormal filter response behavior in the hidden\nlayers of deep networks; and (iii) making corrections to the processing\npipeline to alleviate the problem. Our experimental evaluation using multiple\nopen-source DNN-based face recognition networks, including OpenFace and\nVGG-Face, and two publicly available databases (MEDS and PaSC) demonstrates\nthat the performance of deep learning based face recognition algorithms can\nsuffer greatly in the presence of such distortions. The proposed method is also\ncompared with existing detection algorithms and the results show that it is\nable to detect the attacks with very high accuracy by suitably designing a\nclassifier using the response of the hidden layers in the network. Finally, we\npresent several effective countermeasures to mitigate the impact of adversarial\nattacks and improve the overall robustness of DNN-based face recognition.\n"], ["2018-02-22", "http://arxiv.org/abs/1802.08241", "Hessian-based Analysis of Large Batch Training and Robustness to Adversaries.", ["Zhewei Yao", " Amir Gholami", " Qi Lei", " Kurt Keutzer", " Michael W. Mahoney"], "  Large batch size training of Neural Networks has been shown to incur accuracy\nloss when trained with the current methods. The exact underlying reasons for\nthis are still not completely understood. Here, we study large batch size\ntraining through the lens of the Hessian operator and robust optimization. In\nparticular, we perform a Hessian based study to analyze exactly how the\nlandscape of the loss function changes when training with large batch size. We\ncompute the true Hessian spectrum, without approximation, by back-propagating\nthe second derivative. Extensive experiments on multiple networks show that\nsaddle-points are not the cause for generalization gap of large batch size\ntraining, and the results consistently show that large batch converges to\npoints with noticeably higher Hessian spectrum. Furthermore, we show that\nrobust training allows one to favor flat areas, as points with large Hessian\nspectrum show poor robustness to adversarial perturbation. We further study\nthis relationship, and provide empirical and theoretical proof that the inner\nloop for robust training is a saddle-free optimization problem \\textit{almost\neverywhere}. We present detailed experiments with five different network\narchitectures, including a residual network, tested on MNIST, CIFAR-10, and\nCIFAR-100 datasets. We have open sourced our method which can be accessed at\n[1].\n"], ["2018-02-22", "http://arxiv.org/abs/1802.08195", "Adversarial Examples that Fool both Computer Vision and Time-Limited Humans.", ["Gamaleldin F. Elsayed", " Shreya Shankar", " Brian Cheung", " Nicolas Papernot", " Alex Kurakin", " Ian Goodfellow", " Jascha Sohl-Dickstein"], "  Machine learning models are vulnerable to adversarial examples: small changes\nto images can cause computer vision models to make mistakes such as identifying\na school bus as an ostrich. However, it is still an open question whether\nhumans are prone to similar mistakes. Here, we address this question by\nleveraging recent techniques that transfer adversarial examples from computer\nvision models with known parameters and architecture to other models with\nunknown parameters and architecture, and by matching the initial processing of\nthe human visual system. We find that adversarial examples that strongly\ntransfer across computer vision models influence the classifications made by\ntime-limited human observers.\n"], ["2018-02-21", "http://arxiv.org/abs/1802.08567", "Adversarial Training for Probabilistic Spiking Neural Networks.", ["Alireza Bagheri", " Osvaldo Simeone", " Bipin Rajendran"], "  Classifiers trained using conventional empirical risk minimization or maximum\nlikelihood methods are known to suffer dramatic performance degradations when\ntested over examples adversarially selected based on knowledge of the\nclassifier's decision rule. Due to the prominence of Artificial Neural Networks\n(ANNs) as classifiers, their sensitivity to adversarial examples, as well as\nrobust training schemes, have been recently the subject of intense\ninvestigation. In this paper, for the first time, the sensitivity of spiking\nneural networks (SNNs), or third-generation neural networks, to adversarial\nexamples is studied. The study considers rate and time encoding, as well as\nrate and first-to-spike decoding. Furthermore, a robust training mechanism is\nproposed that is demonstrated to enhance the performance of SNNs under\nwhite-box attacks.\n"], ["2018-02-21", "http://arxiv.org/abs/1802.07896", "L2-Nonexpansive Neural Networks.", ["Haifeng Qian", " Mark N. Wegman"], "  This paper proposes a class of well-conditioned neural networks in which a\nunit amount of change in the inputs causes at most a unit amount of change in\nthe outputs or any of the internal layers. We develop the known methodology of\ncontrolling Lipschitz constants to realize its full potential in maximizing\nrobustness, with a new regularization scheme for linear layers, new ways to\nadapt nonlinearities and a new loss function. With MNIST and CIFAR-10\nclassifiers, we demonstrate a number of advantages. Without needing any\nadversarial training, the proposed classifiers exceed the state of the art in\nrobustness against white-box L2-bounded adversarial attacks. They generalize\nbetter than ordinary networks from noisy data with partially random labels.\nTheir outputs are quantitatively meaningful and indicate levels of confidence\nand generalization, among other desirable properties.\n"], ["2018-02-21", "http://arxiv.org/abs/1802.07770", "Generalizable Adversarial Examples Detection Based on Bi-model Decision Mismatch.", ["Jo\u00e3o Monteiro", " Isabela Albuquerque", " Zahid Akhtar", " Tiago H. Falk"], "  Modern applications of artificial neural networks have yielded remarkable\nperformance gains in a wide range of tasks. However, recent studies have\ndiscovered that such modelling strategy is vulnerable to Adversarial Examples,\ni.e. examples with subtle perturbations often too small and imperceptible to\nhumans, but that can easily fool neural networks. Defense techniques against\nadversarial examples have been proposed, but ensuring robust performance\nagainst varying or novel types of attacks remains an open problem. In this\nwork, we focus on the detection setting, in which case attackers become\nidentifiable while models remain vulnerable. Particularly, we employ the\ndecision layer of independently trained models as features for posterior\ndetection. The proposed framework does not require any prior knowledge of\nadversarial examples generation techniques, and can be directly employed along\nwith unmodified off-the-shelf models. Experiments on the standard MNIST and\nCIFAR10 datasets deliver empirical evidence that such detection approach\ngeneralizes well across not only different adversarial examples generation\nmethods but also quality degradation attacks. Non-linear binary classifiers\ntrained on top of our proposed features can achieve a high detection rate\n(>90%) in a set of white-box attacks and maintain such performance when tested\nagainst unseen attacks.\n"], ["2018-02-20", "http://arxiv.org/abs/1802.07295", "Attack Strength vs. Detectability Dilemma in Adversarial Machine Learning.", ["Christopher Frederickson", " Michael Moore", " Glenn Dawson", " Robi Polikar"], "  As the prevalence and everyday use of machine learning algorithms, along with\nour reliance on these algorithms grow dramatically, so do the efforts to attack\nand undermine these algorithms with malicious intent, resulting in a growing\ninterest in adversarial machine learning. A number of approaches have been\ndeveloped that can render a machine learning algorithm ineffective through\npoisoning or other types of attacks. Most attack algorithms typically use\nsophisticated optimization approaches, whose objective function is designed to\ncause maximum damage with respect to accuracy and performance of the algorithm\nwith respect to some task. In this effort, we show that while such an objective\nfunction is indeed brutally effective in causing maximum damage on an embedded\nfeature selection task, it often results in an attack mechanism that can be\neasily detected with an embarrassingly simple novelty or outlier detection\nalgorithm. We then propose an equally simple yet elegant solution by adding a\nregularization term to the attacker's objective function that penalizes\noutlying attack points.\n"], ["2018-02-20", "http://arxiv.org/abs/1802.07124", "Out-distribution training confers robustness to deep neural networks.", ["Mahdieh Abbasi", " Christian Gagn\u00e9"], "  The easiness at which adversarial instances can be generated in deep neural\nnetworks raises some fundamental questions on their functioning and concerns on\ntheir use in critical systems. In this paper, we draw a connection between\nover-generalization and adversaries: a possible cause of adversaries lies in\nmodels designed to make decisions all over the input space, leading to\ninappropriate high-confidence decisions in parts of the input space not\nrepresented in the training set. We empirically show an augmented neural\nnetwork, which is not trained on any types of adversaries, can increase the\nrobustness by detecting black-box one-step adversaries, i.e. assimilated to\nout-distribution samples, and making generation of white-box one-step\nadversaries harder.\n"], ["2018-02-19", "http://arxiv.org/abs/1802.06927", "On Lyapunov exponents and adversarial perturbation.", ["Vinay Uday Prabhu", " Nishant Desai", " John Whaley"], "  In this paper, we would like to disseminate a serendipitous discovery\ninvolving Lyapunov exponents of a 1-D time series and their use in serving as a\nfiltering defense tool against a specific kind of deep adversarial\nperturbation. To this end, we use the state-of-the-art CleverHans library to\ngenerate adversarial perturbations against a standard Convolutional Neural\nNetwork (CNN) architecture trained on the MNIST as well as the Fashion-MNIST\ndatasets. We empirically demonstrate how the Lyapunov exponents computed on the\nflattened 1-D vector representations of the images served as highly\ndiscriminative features that could be to pre-classify images as adversarial or\nlegitimate before feeding the image into the CNN for classification. We also\nexplore the issue of possible false-alarms when the input images are noisy in a\nnon-adversarial sense.\n"], ["2018-02-19", "http://arxiv.org/abs/1802.06816", "Shield: Fast, Practical Defense and Vaccination for Deep Learning using JPEG Compression.", ["Nilaksh Das", " Madhuri Shanbhogue", " Shang-Tse Chen", " Fred Hohman", " Siwei Li", " Li Chen", " Michael E. Kounavis", " Duen Horng Chau"], "  The rapidly growing body of research in adversarial machine learning has\ndemonstrated that deep neural networks (DNNs) are highly vulnerable to\nadversarially generated images. This underscores the urgent need for practical\ndefense that can be readily deployed to combat attacks in real-time. Observing\nthat many attack strategies aim to perturb image pixels in ways that are\nvisually imperceptible, we place JPEG compression at the core of our proposed\nShield defense framework, utilizing its capability to effectively \"compress\naway\" such pixel manipulation. To immunize a DNN model from artifacts\nintroduced by compression, Shield \"vaccinates\" a model by re-training it with\ncompressed images, where different compression levels are applied to generate\nmultiple vaccinated models that are ultimately used together in an ensemble\ndefense. On top of that, Shield adds an additional layer of protection by\nemploying randomization at test time that compresses different regions of an\nimage using random compression levels, making it harder for an adversary to\nestimate the transformation performed. This novel combination of vaccination,\nensembling, and randomization makes Shield a fortified multi-pronged\nprotection. We conducted extensive, large-scale experiments using the ImageNet\ndataset, and show that our approaches eliminate up to 94% of black-box attacks\nand 98% of gray-box attacks delivered by the recent, strongest attacks, such as\nCarlini-Wagner's L2 and DeepFool. Our approaches are fast and work without\nrequiring knowledge about the model.\n"], ["2018-02-19", "http://arxiv.org/abs/1802.06806", "Divide, Denoise, and Defend against Adversarial Attacks.", ["Seyed-Mohsen Moosavi-Dezfooli", " Ashish Shrivastava", " Oncel Tuzel"], "  Deep neural networks, although shown to be a successful class of machine\nlearning algorithms, are known to be extremely unstable to adversarial\nperturbations. Improving the robustness of neural networks against these\nattacks is important, especially for security-critical applications. To defend\nagainst such attacks, we propose dividing the input image into multiple\npatches, denoising each patch independently, and reconstructing the image,\nwithout losing significant image content. We call our method D3. This proposed\ndefense mechanism is non-differentiable which makes it non-trivial for an\nadversary to apply gradient-based attacks. Moreover, we do not fine-tune the\nnetwork with adversarial examples, making it more robust against unknown\nattacks. We present an analysis of the tradeoff between accuracy and robustness\nagainst adversarial attacks. We evaluate our method under black-box, grey-box,\nand white-box settings. On the ImageNet dataset, our method outperforms the\nstate-of-the-art by 19.7% under grey-box setting, and performs comparably under\nblack-box setting. For the white-box setting, the proposed method achieves\n34.4% accuracy compared to the 0% reported in the recent works.\n"], ["2018-02-19", "http://arxiv.org/abs/1802.06627", "Robustness of Rotation-Equivariant Networks to Adversarial Perturbations.", ["Beranger Dumont", " Simona Maggio", " Pablo Montalvo"], "  Deep neural networks have been shown to be vulnerable to adversarial\nexamples: very small perturbations of the input having a dramatic impact on the\npredictions. A wealth of adversarial attacks and distance metrics to quantify\nthe similarity between natural and adversarial images have been proposed,\nrecently enlarging the scope of adversarial examples with geometric\ntransformations beyond pixel-wise attacks. In this context, we investigate the\nrobustness to adversarial attacks of new Convolutional Neural Network\narchitectures providing equivariance to rotations. We found that\nrotation-equivariant networks are significantly less vulnerable to\ngeometric-based attacks than regular networks on the MNIST, CIFAR-10, and\nImageNet datasets.\n"], ["2018-02-19", "http://arxiv.org/abs/1802.06552", "Are Generative Classifiers More Robust to Adversarial Attacks?.", ["Yingzhen Li", " John Bradshaw", " Yash Sharma"], "  There is a rising interest in studying the robustness of deep neural network\nclassifiers against adversaries, with both advanced attack and defence\ntechniques being actively developed. However, most recent work focuses on\ndiscriminative classifiers, which only model the conditional distribution of\nthe labels given the inputs. In this paper, we propose and investigate the deep\nBayes classifier, which improves classical naive Bayes with conditional deep\ngenerative models. We further develop detection methods for adversarial\nexamples, which reject inputs with low likelihood under the generative model.\nExperimental results suggest that deep Bayes classifiers are more robust than\ndeep discriminative classifiers, and that the proposed detection methods are\neffective against many recently proposed attacks.\n"], ["2018-02-18", "http://arxiv.org/abs/1802.06430", "DARTS: Deceiving Autonomous Cars with Toxic Signs.", ["Chawin Sitawarin", " Arjun Nitin Bhagoji", " Arsalan Mosenia", " Mung Chiang", " Prateek Mittal"], "  Sign recognition is an integral part of autonomous cars. Any\nmisclassification of traffic signs can potentially lead to a multitude of\ndisastrous consequences, ranging from a life-threatening accident to even a\nlarge-scale interruption of transportation services relying on autonomous cars.\nIn this paper, we propose and examine security attacks against sign recognition\nsystems for Deceiving Autonomous caRs with Toxic Signs (we call the proposed\nattacks DARTS). In particular, we introduce two novel methods to create these\ntoxic signs. First, we propose Out-of-Distribution attacks, which expand the\nscope of adversarial examples by enabling the adversary to generate these\nstarting from an arbitrary point in the image space compared to prior attacks\nwhich are restricted to existing training/test data (In-Distribution). Second,\nwe present the Lenticular Printing attack, which relies on an optical\nphenomenon to deceive the traffic sign recognition system. We extensively\nevaluate the effectiveness of the proposed attacks in both virtual and\nreal-world settings and consider both white-box and black-box threat models.\nOur results demonstrate that the proposed attacks are successful under both\nsettings and threat models. We further show that Out-of-Distribution attacks\ncan outperform In-Distribution attacks on classifiers defended using the\nadversarial training defense, exposing a new attack vector for these defenses.\n"], ["2018-02-15", "http://arxiv.org/abs/1802.05763", "ASP:A Fast Adversarial Attack Example Generation Framework based on Adversarial Saliency Prediction.", ["Fuxun Yu", " Qide Dong", " Xiang Chen"], "  With the excellent accuracy and feasibility, the Neural Networks have been\nwidely applied into the novel intelligent applications and systems. However,\nwith the appearance of the Adversarial Attack, the NN based system performance\nbecomes extremely vulnerable:the image classification results can be\narbitrarily misled by the adversarial examples, which are crafted images with\nhuman unperceivable pixel-level perturbation. As this raised a significant\nsystem security issue, we implemented a series of investigations on the\nadversarial attack in this work: We first identify an image's pixel\nvulnerability to the adversarial attack based on the adversarial saliency\nanalysis. By comparing the analyzed saliency map and the adversarial\nperturbation distribution, we proposed a new evaluation scheme to\ncomprehensively assess the adversarial attack precision and efficiency. Then,\nwith a novel adversarial saliency prediction method, a fast adversarial example\ngeneration framework, namely \"ASP\", is proposed with significant attack\nefficiency improvement and dramatic computation cost reduction. Compared to the\nprevious methods, experiments show that ASP has at most 12 times speed-up for\nadversarial example generation, 2 times lower perturbation rate, and high\nattack success rate of 87% on both MNIST and Cifar10. ASP can be also well\nutilized to support the data-hungry NN adversarial training. By reducing the\nattack success rate as much as 90%, ASP can quickly and effectively enhance the\ndefense capability of NN based system to the adversarial attacks.\n"], ["2018-02-15", "http://arxiv.org/abs/1802.05666", "Adversarial Risk and the Dangers of Evaluating Against Weak Attacks.", ["Jonathan Uesato", " Brendan O'Donoghue", " Aaron van den Oord", " Pushmeet Kohli"], "  This paper investigates recently proposed approaches for defending against\nadversarial examples and evaluating adversarial robustness. We motivate\n'adversarial risk' as an objective for achieving models robust to worst-case\ninputs. We then frame commonly used attacks and evaluation metrics as defining\na tractable surrogate objective to the true adversarial risk. This suggests\nthat models may optimize this surrogate rather than the true adversarial risk.\nWe formalize this notion as 'obscurity to an adversary,' and develop tools and\nheuristics for identifying obscured models and designing transparent models. We\ndemonstrate that this is a significant problem in practice by repurposing\ngradient-free optimization techniques into adversarial attacks, which we use to\ndecrease the accuracy of several recently proposed defenses to near zero. Our\nhope is that our formulations and results will help researchers to develop more\npowerful defenses.\n"], ["2018-02-14", "http://arxiv.org/abs/1802.05385", "Fooling OCR Systems with Adversarial Text Images.", ["Congzheng Song", " Vitaly Shmatikov"], "  We demonstrate that state-of-the-art optical character recognition (OCR)\nbased on deep learning is vulnerable to adversarial images. Minor modifications\nto images of printed text, which do not change the meaning of the text to a\nhuman reader, cause the OCR system to \"recognize\" a different text where\ncertain words chosen by the adversary are replaced by their semantic opposites.\nThis completely changes the meaning of the output produced by the OCR system\nand by the NLP applications that use OCR for preprocessing their inputs.\n"], ["2018-02-14", "http://arxiv.org/abs/1802.05193", "Security Analysis and Enhancement of Model Compressed Deep Learning Systems under Adversarial Attacks.", ["Qi Liu", " Tao Liu", " Zihao Liu", " Yanzhi Wang", " Yier Jin", " Wujie Wen"], "  DNN is presenting human-level performance for many complex intelligent tasks\nin real-world applications. However, it also introduces ever-increasing\nsecurity concerns. For example, the emerging adversarial attacks indicate that\neven very small and often imperceptible adversarial input perturbations can\neasily mislead the cognitive function of deep learning systems (DLS). Existing\nDNN adversarial studies are narrowly performed on the ideal software-level DNN\nmodels with a focus on single uncertainty factor, i.e. input perturbations,\nhowever, the impact of DNN model reshaping on adversarial attacks, which is\nintroduced by various hardware-favorable techniques such as hash-based weight\ncompression during modern DNN hardware implementation, has never been\ndiscussed. In this work, we for the first time investigate the multi-factor\nadversarial attack problem in practical model optimized deep learning systems\nby jointly considering the DNN model-reshaping (e.g. HashNet based deep\ncompression) and the input perturbations. We first augment adversarial example\ngenerating method dedicated to the compressed DNN models by incorporating the\nsoftware-based approaches and mathematical modeled DNN reshaping. We then\nconduct a comprehensive robustness and vulnerability analysis of deep\ncompressed DNN models under derived adversarial attacks. A defense technique\nnamed \"gradient inhibition\" is further developed to ease the generating of\nadversarial examples thus to effectively mitigate adversarial attacks towards\nboth software and hardware-oriented DNNs. Simulation results show that\n\"gradient inhibition\" can decrease the average success rate of adversarial\nattacks from 87.99% to 4.77% (from 86.74% to 4.64%) on MNIST (CIFAR-10)\nbenchmark with marginal accuracy degradation across various DNNs.\n"], ["2018-02-13", "http://arxiv.org/abs/1802.09900", "Query-Free Attacks on Industry-Grade Face Recognition Systems under Resource Constraints.", ["Di Tang", " XiaoFeng Wang", " Kehuan Zhang"], "  To launch black-box attacks against a Deep Neural Network (DNN) based Face\nRecognition (FR) system, one needs to build \\textit{substitute} models to\nsimulate the target model, so the adversarial examples discovered from\nsubstitute models could also mislead the target model. Such\n\\textit{transferability} is achieved in recent studies through querying the\ntarget model to obtain data for training the substitute models. A real-world\ntarget, likes the FR system of law enforcement, however, is less accessible to\nthe adversary. To attack such a system, a substitute model with similar quality\nas the target model is needed to identify their common defects. This is hard\nsince the adversary often does not have the enough resources to train such a\npowerful model (hundreds of millions of images and rooms of GPUs are needed to\ntrain a commercial FR system).\n  We found in our research, however, that a resource-constrained adversary\ncould still effectively approximate the target model's capability to recognize\n\\textit{specific} individuals, by training \\textit{biased} substitute models on\nadditional images of those victims whose identities the attacker want to cover\nor impersonate. This is made possible by a new property we discovered, called\n\\textit{Nearly Local Linearity} (NLL), which models the observation that an\nideal DNN model produces the image representations (embeddings) whose distances\namong themselves truthfully describe the human perception of the differences\namong the input images. By simulating this property around the victim's images,\nwe significantly improve the transferability of black-box impersonation attacks\nby nearly 50\\%. Particularly, we successfully attacked a commercial system\ntrained over 20 million images, using 4 million images and 1/5 of the training\ntime but achieving 62\\% transferability in an impersonation attack and 89\\% in\na dodging attack.\n"], ["2018-02-13", "http://arxiv.org/abs/1802.04822", "Identify Susceptible Locations in Medical Records via Adversarial Attacks on Deep Predictive Models.", ["Mengying Sun", " Fengyi Tang", " Jinfeng Yi", " Fei Wang", " Jiayu Zhou"], "  The surging availability of electronic medical records (EHR) leads to\nincreased research interests in medical predictive modeling. Recently many deep\nlearning based predicted models are also developed for EHR data and\ndemonstrated impressive performance. However, a series of recent studies showed\nthat these deep models are not safe: they suffer from certain vulnerabilities.\nIn short, a well-trained deep network can be extremely sensitive to inputs with\nnegligible changes. These inputs are referred to as adversarial examples. In\nthe context of medical informatics, such attacks could alter the result of a\nhigh performance deep predictive model by slightly perturbing a patient's\nmedical records. Such instability not only reflects the weakness of deep\narchitectures, more importantly, it offers guide on detecting susceptible parts\non the inputs. In this paper, we propose an efficient and effective framework\nthat learns a time-preferential minimum attack targeting the LSTM model with\nEHR inputs, and we leverage this attack strategy to screen medical records of\npatients and identify susceptible events and measurements. The efficient\nscreening procedure can assist decision makers to pay extra attentions to the\nlocations that can cause severe consequence if not measured correctly. We\nconduct extensive empirical studies on a real-world urgent care cohort and\ndemonstrate the effectiveness of the proposed screening approach.\n"], ["2018-02-13", "http://arxiv.org/abs/1802.04528", "Deceiving End-to-End Deep Learning Malware Detectors using Adversarial Examples.", ["Felix Kreuk", " Assi Barak", " Shir Aviv-Reuven", " Moran Baruch", " Benny Pinkas", " Joseph Keshet"], "  In recent years, deep learning has shown performance breakthroughs in many\napplications, such as image detection, image segmentation, pose estimation, and\nspeech recognition. However, this comes with a major concern: deep networks\nhave been found to be vulnerable to adversarial examples. Adversarial examples\nare slightly modified inputs that are intentionally designed to cause a\nmisclassification by the model. In the domains of images and speech, the\nmodifications are so small that they are not seen or heard by humans, but\nnevertheless greatly affect the classification of the model.\n  Deep learning models have been successfully applied to malware detection. In\nthis domain, generating adversarial examples is not straightforward, as small\nmodifications to the bytes of the file could lead to significant changes in its\nfunctionality and validity. We introduce a novel loss function for generating\nadversarial examples specifically tailored for discrete input sets, such as\nexecutable bytes. We modify malicious binaries so that they would be detected\nas benign, while preserving their original functionality, by injecting a small\nsequence of bytes (payload) in the binary file. We applied this approach to an\nend-to-end convolutional deep learning malware detection model and show a high\nrate of detection evasion. Moreover, we show that our generated payload is\nrobust enough to be transferable within different locations of the same file\nand across different files, and that its entropy is low and similar to that of\nbenign data sections.\n"], ["2018-02-12", "http://arxiv.org/abs/1802.04034", "Lipschitz-Margin Training: Scalable Certification of Perturbation Invariance for Deep Neural Networks.", ["Yusuke Tsuzuku", " Issei Sato", " Masashi Sugiyama"], "  High sensitivity of neural networks against malicious perturbations on inputs\ncauses security concerns. To take a steady step towards robust classifiers, we\naim to create neural network models provably defended from perturbations. Prior\ncertification work requires strong assumptions on network structures and\nmassive computational costs, and thus the range of their applications was\nlimited. From the relationship between the Lipschitz constants and prediction\nmargins, we present a computationally efficient calculation technique to\nlower-bound the size of adversarial perturbations that can deceive networks,\nand that is widely applicable to various complicated networks. Moreover, we\npropose an efficient training procedure that robustifies networks and\nsignificantly improves the provably guarded areas around data points. In\nexperimental evaluations, our method showed its ability to provide a\nnon-trivial guarantee and enhance robustness for even large networks.\n"], ["2018-02-12", "http://arxiv.org/abs/1802.04457", "Predicting Adversarial Examples with High Confidence.", ["Angus Galloway", " Graham W. Taylor", " Medhat Moussa"], "  It has been suggested that adversarial examples cause deep learning models to\nmake incorrect predictions with high confidence. In this work, we take the\nopposite stance: an overly confident model is more likely to be vulnerable to\nadversarial examples. This work is one of the most proactive approaches taken\nto date, as we link robustness with non-calibrated model confidence on noisy\nimages, providing a data-augmentation-free path forward. The adversarial\nexamples phenomenon is most easily explained by the trend of increasing\nnon-regularized model capacity, while the diversity and number of samples in\ncommon datasets has remained flat. Test accuracy has incorrectly been\nassociated with true generalization performance, ignoring that training and\ntest splits are often extremely similar in terms of the overall representation\nspace. The transferability property of adversarial examples was previously used\nas evidence against overfitting arguments, a perceived random effect, but\noverfitting is not always random.\n"], ["2018-02-09", "http://arxiv.org/abs/1802.03471", "Certified Robustness to Adversarial Examples with Differential Privacy.", ["Mathias Lecuyer", " Vaggelis Atlidakis", " Roxana Geambasu", " Daniel Hsu", " Suman Jana"], "  Adversarial examples that fool machine learning models, particularly deep\nneural networks, have been a topic of intense research interest, with attacks\nand defenses being developed in a tight back-and-forth. Most past defenses are\nbest effort and have been shown to be vulnerable to sophisticated attacks.\nRecently a set of certified defenses have been introduced, which provide\nguarantees of robustness to norm-bounded attacks, but they either do not scale\nto large datasets or are limited in the types of models they can support. This\npaper presents the first certified defense that both scales to large networks\nand datasets (such as Google's Inception network for ImageNet) and applies\nbroadly to arbitrary model types. Our defense, called PixelDP, is based on a\nnovel connection between robustness against adversarial examples and\ndifferential privacy, a cryptographically-inspired formalism, that provides a\nrigorous, generic, and flexible foundation for defense.\n"], ["2018-02-08", "http://arxiv.org/abs/1802.03041", "Detection of Adversarial Training Examples in Poisoning Attacks through Anomaly Detection.", ["Andrea Paudice", " Luis Mu\u00f1oz-Gonz\u00e1lez", " Andras Gyorgy", " Emil C. Lupu"], "  Machine learning has become an important component for many systems and\napplications including computer vision, spam filtering, malware and network\nintrusion detection, among others. Despite the capabilities of machine learning\nalgorithms to extract valuable information from data and produce accurate\npredictions, it has been shown that these algorithms are vulnerable to attacks.\nData poisoning is one of the most relevant security threats against machine\nlearning systems, where attackers can subvert the learning process by injecting\nmalicious samples in the training data. Recent work in adversarial machine\nlearning has shown that the so-called optimal attack strategies can\nsuccessfully poison linear classifiers, degrading the performance of the system\ndramatically after compromising a small fraction of the training dataset. In\nthis paper we propose a defence mechanism to mitigate the effect of these\noptimal poisoning attacks based on outlier detection. We show empirically that\nthe adversarial examples generated by these attack strategies are quite\ndifferent from genuine points, as no detectability constrains are considered to\ncraft the attack. Hence, they can be detected with an appropriate pre-filtering\nof the training dataset.\n"], ["2018-02-05", "http://arxiv.org/abs/1802.01549", "Blind Pre-Processing: A Robust Defense Method Against Adversarial Examples.", ["Adnan Siraj Rakin", " Zhezhi He", " Boqing Gong", " Deliang Fan"], "  Deep learning algorithms and networks are vulnerable to perturbed inputs\nwhich is known as the adversarial attack. Many defense methodologies have been\ninvestigated to defend against such adversarial attack. In this work, we\npropose a novel methodology to defend the existing powerful attack model. We\nfor the first time introduce a new attacking scheme for the attacker and set a\npractical constraint for white box attack. Under this proposed attacking\nscheme, we present the best defense ever reported against some of the recent\nstrong attacks. It consists of a set of nonlinear function to process the input\ndata which will make it more robust over the adversarial attack. However, we\nmake this processing layer completely hidden from the attacker. Blind\npre-processing improves the white box attack accuracy of MNIST from 94.3\\% to\n98.7\\%. Even with increasing defense when others defenses completely fail,\nblind pre-processing remains one of the strongest ever reported. Another\nstrength of our defense is that it eliminates the need for adversarial training\nas it can significantly increase the MNIST accuracy without adversarial\ntraining as well. Additionally, blind pre-processing can also increase the\ninference accuracy in the face of a powerful attack on CIFAR-10 and SVHN data\nset as well without much sacrificing clean data accuracy.\n"], ["2018-02-05", "http://arxiv.org/abs/1802.01421", "First-order Adversarial Vulnerability of Neural Networks and Input Dimension.", ["Carl-Johann Simon-Gabriel", " Yann Ollivier", " L\u00e9on Bottou", " Bernhard Sch\u00f6lkopf", " David Lopez-Paz"], "  Over the past few years, neural networks were proven vulnerable to\nadversarial images: targeted but imperceptible image perturbations lead to\ndrastically different predictions. We show that adversarial vulnerability\nincreases with the gradients of the training objective when viewed as a\nfunction of the inputs. Surprisingly, vulnerability does not depend on network\ntopology: for many standard network architectures, we prove that at\ninitialization, the $\\ell_1$-norm of these gradients grows as the square root\nof the input dimension, leaving the networks increasingly vulnerable with\ngrowing image size. We empirically show that this dimension dependence persists\nafter either usual or robust training, but gets attenuated with higher\nregularization.\n"], ["2018-02-02", "http://arxiv.org/abs/1802.00573", "Secure Detection of Image Manipulation by means of Random Feature Selection.", ["Zhipeng Chen", " Benedetta Tondi", " Xiaolong Li", " Rongrong Ni", " Yao Zhao", " Mauro Barni"], "  We address the problem of data-driven image manipulation detection in the\npresence of an attacker with limited knowledge about the detector.\nSpecifically, we assume that the attacker knows the architecture of the\ndetector, the training data and the class of features V the detector can rely\non. In order to get an advantage in his race of arms with the attacker, the\nanalyst designs the detector by relying on a subset of features chosen at\nrandom in V. Given its ignorance about the exact feature set, the adversary\nattacks a version of the detector based on the entire feature set. In this way,\nthe effectiveness of the attack diminishes since there is no guarantee that\nattacking a detector working in the full feature space will result in a\nsuccessful attack against the reduced-feature detector. We theoretically prove\nthat, thanks to random feature selection, the security of the detector\nincreases significantly at the expense of a negligible loss of performance in\nthe absence of attacks. We also provide an experimental validation of the\nproposed procedure by focusing on the detection of two specific kinds of image\nmanipulations, namely adaptive histogram equalization and median filtering. The\nexperiments confirm the gain in security at the expense of a negligible loss of\nperformance in the absence of attacks.\n"], ["2018-02-02", "http://arxiv.org/abs/1802.01448", "Hardening Deep Neural Networks via Adversarial Model Cascades.", ["Deepak Vijaykeerthy", " Anshuman Suri", " Sameep Mehta", " Ponnurangam Kumaraguru"], "  Deep neural networks (DNNs) are vulnerable to malicious inputs crafted by an\nadversary to produce erroneous outputs. Works on securing neural networks\nagainst adversarial examples achieve high empirical robustness on simple\ndatasets such as MNIST. However, these techniques are inadequate when\nempirically tested on complex data sets such as CIFAR-10 and SVHN. Further,\nexisting techniques are designed to target specific attacks and fail to\ngeneralize across attacks. We propose the Adversarial Model Cascades (AMC) as a\nway to tackle the above inadequacies. Our approach trains a cascade of models\nsequentially where each model is optimized to be robust towards a mixture of\nmultiple attacks. Ultimately, it yields a single model which is secure against\na wide range of attacks; namely FGSM, Elastic, Virtual Adversarial\nPerturbations and Madry. On an average, AMC increases the model's empirical\nrobustness against various attacks simultaneously, by a significant margin (of\n6.225% for MNIST, 5.075% for SVHN and 2.65% for CIFAR10). At the same time, the\nmodel's performance on non-adversarial inputs is comparable to the\nstate-of-the-art models.\n"], ["2018-02-01", "http://arxiv.org/abs/1802.00420", "Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples.", ["Anish Athalye", " Nicholas Carlini", " David Wagner"], "  We identify obfuscated gradients, a kind of gradient masking, as a phenomenon\nthat leads to a false sense of security in defenses against adversarial\nexamples. While defenses that cause obfuscated gradients appear to defeat\niterative optimization-based attacks, we find defenses relying on this effect\ncan be circumvented. We describe characteristic behaviors of defenses\nexhibiting the effect, and for each of the three types of obfuscated gradients\nwe discover, we develop attack techniques to overcome it. In a case study,\nexamining non-certified white-box-secure defenses at ICLR 2018, we find\nobfuscated gradients are a common occurrence, with 7 of 9 defenses relying on\nobfuscated gradients. Our new attacks successfully circumvent 6 completely, and\n1 partially, in the original threat model each paper considers.\n"], ["2018-01-31", "http://arxiv.org/abs/1801.10578", "Evaluating the Robustness of Neural Networks: An Extreme Value Theory Approach.", ["Tsui-Wei Weng", " Huan Zhang", " Pin-Yu Chen", " Jinfeng Yi", " Dong Su", " Yupeng Gao", " Cho-Jui Hsieh", " Luca Daniel"], "  The robustness of neural networks to adversarial examples has received great\nattention due to security implications. Despite various attack approaches to\ncrafting visually imperceptible adversarial examples, little has been developed\ntowards a comprehensive measure of robustness. In this paper, we provide a\ntheoretical justification for converting robustness analysis into a local\nLipschitz constant estimation problem, and propose to use the Extreme Value\nTheory for efficient evaluation. Our analysis yields a novel robustness metric\ncalled CLEVER, which is short for Cross Lipschitz Extreme Value for nEtwork\nRobustness. The proposed CLEVER score is attack-agnostic and computationally\nfeasible for large neural networks. Experimental results on various networks,\nincluding ResNet, Inception-v3 and MobileNet, show that (i) CLEVER is aligned\nwith the robustness indication measured by the $\\ell_2$ and $\\ell_\\infty$ norms\nof adversarial examples from powerful attacks, and (ii) defended networks using\ndefensive distillation or bounded ReLU indeed achieve better CLEVER scores. To\nthe best of our knowledge, CLEVER is the first attack-independent robustness\nmetric that can be applied to any neural network classifier.\n"], ["2018-01-29", "http://arxiv.org/abs/1801.09827", "Robustness of classification ability of spiking neural networks.", ["Jie Yang", " Pingping Zhang", " Yan Liu"], "  It is well-known that the robustness of artificial neural networks (ANNs) is\nimportant for their wide ranges of applications. In this paper, we focus on the\nrobustness of the classification ability of a spiking neural network which\nreceives perturbed inputs. Actually, the perturbation is allowed to be\narbitrary styles. However, Gaussian perturbation and other regular ones have\nbeen rarely investigated. For classification problems, the closer to the\ndesired point, the more perturbed points there are in the input space. In\naddition, the perturbation may be periodic. Based on these facts, we only\nconsider sinusoidal and Gaussian perturbations in this paper. With the\nSpikeProp algorithm, we perform extensive experiments on the classical XOR\nproblem and other three benchmark datasets. The numerical results show that\nthere is not significant reduction in the classification ability of the network\nif the input signals are subject to sinusoidal and Gaussian perturbations.\n"], ["2018-01-28", "http://arxiv.org/abs/1801.09344", "Certified Defenses against Adversarial Examples.", ["Aditi Raghunathan", " Jacob Steinhardt", " Percy Liang"], "  While neural networks have achieved high accuracy on standard image\nclassification benchmarks, their accuracy drops to nearly zero in the presence\nof small adversarial perturbations to test inputs. Defenses based on\nregularization and adversarial training have been proposed, but often followed\nby new, stronger attacks that defeat these defenses. Can we somehow end this\narms race? In this work, we study this problem for neural networks with one\nhidden layer. We first propose a method based on a semidefinite relaxation that\noutputs a certificate that for a given network and test input, no attack can\nforce the error to exceed a certain value. Second, as this certificate is\ndifferentiable, we jointly optimize it with the network parameters, providing\nan adaptive regularizer that encourages robustness against all attacks. On\nMNIST, our approach produces a network and a certificate that no attack that\nperturbs each pixel by at most \\epsilon = 0.1 can cause more than 35% test\nerror.\n"], ["2018-01-27", "http://arxiv.org/abs/1801.09097", "Towards an Understanding of Neural Networks in Natural-Image Spaces.", ["Yifei Fan", " Anthony Yezzi"], "  Two major uncertainties, dataset bias and adversarial examples, prevail in\nstate-of-the-art AI algorithms with deep neural networks. In this paper, we\npresent an intuitive explanation for these issues as well as an interpretation\nof the performance of deep networks in a natural-image space. The explanation\nconsists of two parts: the philosophy of neural networks and a hypothetical\nmodel of natural-image spaces. Following the explanation, we 1) demonstrate\nthat the values of training samples differ, 2) provide incremental boost to the\naccuracy of a CIFAR-10 classifier by introducing an additional \"random-noise\"\ncategory during training, 3) alleviate over-fitting thereby enhancing the\nrobustness against adversarial examples by detecting and excluding illusive\ntraining samples that are consistently misclassified. Our overall contribution\nis therefore twofold. First, while most existing algorithms treat data equally\nand have a strong appetite for more data, we demonstrate in contrast that an\nindividual datum can sometimes have disproportionate and counterproductive\ninfluence and that it is not always better to train neural networks with more\ndata. Next, we consider more thoughtful strategies by taking into account the\ngeometric and topological properties of natural-image spaces to which deep\nnetworks are applied.\n"], ["2018-01-26", "http://arxiv.org/abs/1801.08926", "Deflecting Adversarial Attacks with Pixel Deflection.", ["Aaditya Prakash", " Nick Moran", " Solomon Garber", " Antonella DiLillo", " James Storer"], "  CNNs are poised to become integral parts of many critical systems. Despite\ntheir robustness to natural variations, image pixel values can be manipulated,\nvia small, carefully crafted, imperceptible perturbations, to cause a model to\nmisclassify images. We present an algorithm to process an image so that\nclassification accuracy is significantly preserved in the presence of such\nadversarial manipulations. Image classifiers tend to be robust to natural\nnoise, and adversarial attacks tend to be agnostic to object location. These\nobservations motivate our strategy, which leverages model robustness to defend\nagainst adversarial perturbations by forcing the image to match natural image\nstatistics. Our algorithm locally corrupts the image by redistributing pixel\nvalues via a process we term pixel deflection. A subsequent wavelet-based\ndenoising operation softens this corruption, as well as some of the adversarial\nchanges. We demonstrate experimentally that the combination of these techniques\nenables the effective recovery of the true class, against a variety of robust\nattacks. Our results compare favorably with current state-of-the-art defenses,\nwithout requiring retraining or modifying the CNN.\n"], ["2018-01-26", "http://arxiv.org/abs/1801.08917", "Learning to Evade Static PE Machine Learning Malware Models via Reinforcement Learning.", ["Hyrum S. Anderson", " Anant Kharkar", " Bobby Filar", " David Evans", " Phil Roth"], "  Machine learning is a popular approach to signatureless malware detection\nbecause it can generalize to never-before-seen malware families and polymorphic\nstrains. This has resulted in its practical use for either primary detection\nengines or for supplementary heuristic detection by anti-malware vendors.\nRecent work in adversarial machine learning has shown that deep learning models\nare susceptible to gradient-based attacks, whereas non-differentiable models\nthat report a score can be attacked by genetic algorithms that aim to\nsystematically reduce the score. We propose a more general framework based on\nreinforcement learning (RL) for attacking static portable executable (PE)\nanti-malware engines. The general framework does not require a differentiable\nmodel nor does it require the engine to produce a score. Instead, an RL agent\nis equipped with a set of functionality-preserving operations that it may\nperform on the PE file. Through a series of games played against the\nanti-malware engine, it learns which sequences of operations are likely to\nresult in evading the detector for any given malware sample. This enables\ncompletely black-box attacks against static PE anti-malware, and produces\nfunctional evasive malware samples as a direct result. We show in experiments\nthat our method can attack a gradient-boosted machine learning model with\nevasion rates that are substantial and appear to be strongly dependent on the\ndataset. We demonstrate that attacks against this model appear to also evade\ncomponents of publicly hosted antivirus engines. Adversarial training results\nare also presented: by retraining the model on evasive ransomware samples, a\nsubsequent attack is 33% less effective. However, there are overfitting dangers\nwhen adversarial training, which we note. We release code to allow researchers\nto reproduce and improve this approach.\n"], ["2018-01-24", "http://arxiv.org/abs/1801.08535", "CommanderSong: A Systematic Approach for Practical Adversarial Voice Recognition.", ["Xuejing Yuan", " Yuxuan Chen", " Yue Zhao", " Yunhui Long", " Xiaokang Liu", " Kai Chen", " Shengzhi Zhang", " Heqing Huang", " Xiaofeng Wang", " Carl A. Gunter"], "  The popularity of ASR (automatic speech recognition) systems, like Google\nVoice, Cortana, brings in security concerns, as demonstrated by recent attacks.\nThe impacts of such threats, however, are less clear, since they are either\nless stealthy (producing noise-like voice commands) or requiring the physical\npresence of an attack device (using ultrasound). In this paper, we demonstrate\nthat not only are more practical and surreptitious attacks feasible but they\ncan even be automatically constructed. Specifically, we find that the voice\ncommands can be stealthily embedded into songs, which, when played, can\neffectively control the target system through ASR without being noticed. For\nthis purpose, we developed novel techniques that address a key technical\nchallenge: integrating the commands into a song in a way that can be\neffectively recognized by ASR through the air, in the presence of background\nnoise, while not being detected by a human listener. Our research shows that\nthis can be done automatically against real world ASR applications. We also\ndemonstrate that such CommanderSongs can be spread through Internet (e.g.,\nYouTube) and radio, potentially affecting millions of ASR users. We further\npresent a new mitigation technique that controls this threat.\n"], ["2018-01-24", "http://arxiv.org/abs/1801.08092", "Generalizable Data-free Objective for Crafting Universal Adversarial Perturbations.", ["Konda Reddy Mopuri", " Aditya Ganeshan", " R. Venkatesh Babu"], "  Machine learning models are susceptible to adversarial perturbations: small\nchanges to input that can cause large changes in output. It is also\ndemonstrated that there exist input-agnostic perturbations, called universal\nadversarial perturbations, which can change the inference of target model on\nmost of the data samples. However, existing methods to craft universal\nperturbations are (i) task specific, (ii) require samples from the training\ndata distribution, and (iii) perform complex optimizations. Additionally,\nbecause of the data dependence, fooling ability of the crafted perturbations is\nproportional to the available training data. In this paper, we present a novel,\ngeneralizable and data-free approaches for crafting universal adversarial\nperturbations. Independent of the underlying task, our objective achieves\nfooling via corrupting the extracted features at multiple layers. Therefore,\nthe proposed objective is generalizable to craft image-agnostic perturbations\nacross multiple vision tasks such as object recognition, semantic segmentation,\nand depth estimation. In the practical setting of black-box attack scenario\n(when the attacker does not have access to the target model and it's training\ndata), we show that our objective outperforms the data dependent objectives to\nfool the learned models. Further, via exploiting simple priors related to the\ndata distribution, our objective remarkably boosts the fooling ability of the\ncrafted perturbations. Significant fooling rates achieved by our objective\nemphasize that the current deep learning models are now at an increased risk,\nsince our objective generalizes across multiple tasks without the requirement\nof training data for crafting the perturbations. To encourage reproducible\nresearch, we have released the codes for our proposed algorithm.\n"], ["2018-01-22", "http://arxiv.org/abs/1801.07175", "Adversarial Texts with Gradient Methods.", ["Zhitao Gong", " Wenlu Wang", " Bo Li", " Dawn Song", " Wei-Shinn Ku"], "  Adversarial samples for images have been extensively studied in the\nliterature. Among many of the attacking methods, gradient-based methods are\nboth effective and easy to compute. In this work, we propose a framework to\nadapt the gradient attacking methods on images to text domain. The main\ndifficulties for generating adversarial texts with gradient methods are i) the\ninput space is discrete, which makes it difficult to accumulate small noise\ndirectly in the inputs, and ii) the measurement of the quality of the\nadversarial texts is difficult. We tackle the first problem by searching for\nadversarials in the embedding space and then reconstruct the adversarial texts\nvia nearest neighbor search. For the latter problem, we employ the Word Mover's\nDistance (WMD) to quantify the quality of adversarial texts. Through extensive\nexperiments on three datasets, IMDB movie reviews, Reuters-2 and Reuters-5\nnewswires, we show that our framework can leverage gradient attacking methods\nto generate very high-quality adversarial texts that are only a few words\ndifferent from the original texts. There are many cases where we can change one\nword to alter the label of the whole piece of text. We successfully incorporate\nFGM and DeepFool into our framework. In addition, we empirically show that WMD\nis closely related to the quality of adversarial texts.\n"], ["2018-01-15", "http://arxiv.org/abs/1801.05420", "A Comparative Study of Rule Extraction for Recurrent Neural Networks.", ["Qinglong Wang", " Kaixuan Zhang", " Alexander G. II Ororbia", " Xinyu Xing", " Xue Liu", " C. Lee Giles"], "  Understanding recurrent networks through rule extraction has a long history.\nThis has taken on new interests due to the need for interpreting or verifying\nneural networks. One basic form for representing stateful rules is\ndeterministic finite automata (DFA). Previous research shows that extracting\nDFAs from trained second-order recurrent networks is not only possible but also\nrelatively stable. Recently, several new types of recurrent networks with more\ncomplicated architectures have been introduced. These handle challenging\nlearning tasks usually involving sequential data. However, it remains an open\nproblem whether DFAs can be adequately extracted from these models.\nSpecifically, it is not clear how DFA extraction will be affected when applied\nto different recurrent networks trained on data sets with different levels of\ncomplexity. Here, we investigate DFA extraction on several widely adopted\nrecurrent networks that are trained to learn a set of seven regular Tomita\ngrammars. We first formally analyze the complexity of Tomita grammars and\ncategorize these grammars according to that complexity. Then we empirically\nevaluate different recurrent networks for their performance of DFA extraction\non all Tomita grammars. Our experiments show that for most recurrent networks,\ntheir extraction performance decreases as the complexity of the underlying\ngrammar increases. On grammars of lower complexity, most recurrent networks\nobtain desirable extraction performance. As for grammars with the highest level\nof complexity, while several complicated models fail with only certain\nrecurrent networks having satisfactory extraction performance.\n"], ["2018-01-15", "http://arxiv.org/abs/1801.04695", "Sparsity-based Defense against Adversarial Attacks on Linear Classifiers.", ["Zhinus Marzi", " Soorya Gopalakrishnan", " Upamanyu Madhow", " Ramtin Pedarsani"], "  Deep neural networks represent the state of the art in machine learning in a\ngrowing number of fields, including vision, speech and natural language\nprocessing. However, recent work raises important questions about the\nrobustness of such architectures, by showing that it is possible to induce\nclassification errors through tiny, almost imperceptible, perturbations.\nVulnerability to such \"adversarial attacks\", or \"adversarial examples\", has\nbeen conjectured to be due to the excessive linearity of deep networks. In this\npaper, we study this phenomenon in the setting of a linear classifier, and show\nthat it is possible to exploit sparsity in natural data to combat\n$\\ell_{\\infty}$-bounded adversarial perturbations. Specifically, we demonstrate\nthe efficacy of a sparsifying front end via an ensemble averaged analysis, and\nexperimental results for the MNIST handwritten digit database. To the best of\nour knowledge, this is the first work to show that sparsity provides a\ntheoretically rigorous framework for defense against adversarial attacks.\n"], ["2018-01-15", "http://arxiv.org/abs/1801.04693", "Towards Imperceptible and Robust Adversarial Example Attacks against Neural Networks.", ["Bo Luo", " Yannan Liu", " Lingxiao Wei", " Qiang Xu"], "  Machine learning systems based on deep neural networks, being able to produce\nstate-of-the-art results on various perception tasks, have gained mainstream\nadoption in many applications. However, they are shown to be vulnerable to\nadversarial example attack, which generates malicious output by adding slight\nperturbations to the input. Previous adversarial example crafting methods,\nhowever, use simple metrics to evaluate the distances between the original\nexamples and the adversarial ones, which could be easily detected by human\neyes. In addition, these attacks are often not robust due to the inevitable\nnoises and deviation in the physical world. In this work, we present a new\nadversarial example attack crafting method, which takes the human perceptual\nsystem into consideration and maximizes the noise tolerance of the crafted\nadversarial example. Experimental results demonstrate the efficacy of the\nproposed technique.\n"], ["2018-01-12", "http://arxiv.org/abs/1801.04354", "Black-box Generation of Adversarial Text Sequences to Evade Deep Learning Classifiers.", ["Ji Gao", " Jack Lanchantin", " Mary Lou Soffa", " Yanjun Qi"], "  Although various techniques have been proposed to generate adversarial\nsamples for white-box attacks on text, little attention has been paid to\nblack-box attacks, which are more realistic scenarios. In this paper, we\npresent a novel algorithm, DeepWordBug, to effectively generate small text\nperturbations in a black-box setting that forces a deep-learning classifier to\nmisclassify a text input. We employ novel scoring strategies to identify the\ncritical tokens that, if modified, cause the classifier to make an incorrect\nprediction. Simple character-level transformations are applied to the\nhighest-ranked tokens in order to minimize the edit distance of the\nperturbation, yet change the original classification. We evaluated DeepWordBug\non eight real-world text datasets, including text classification, sentiment\nanalysis, and spam detection. We compare the result of DeepWordBug with two\nbaselines: Random (Black-box) and Gradient (White-box). Our experimental\nresults indicate that DeepWordBug reduces the prediction accuracy of current\nstate-of-the-art deep-learning models, including a decrease of 68\\% on average\nfor a Word-LSTM model and 48\\% on average for a Char-CNN model.\n"], ["2018-01-11", "http://arxiv.org/abs/1801.04055", "A3T: Adversarially Augmented Adversarial Training.", ["Akram Erraqabi", " Aristide Baratin", " Yoshua Bengio", " Simon Lacoste-Julien"], "  Recent research showed that deep neural networks are highly sensitive to\nso-called adversarial perturbations, which are tiny perturbations of the input\ndata purposely designed to fool a machine learning classifier. Most\nclassification models, including deep learning models, are highly vulnerable to\nadversarial attacks. In this work, we investigate a procedure to improve\nadversarial robustness of deep neural networks through enforcing representation\ninvariance. The idea is to train the classifier jointly with a discriminator\nattached to one of its hidden layer and trained to filter the adversarial\nnoise. We perform preliminary experiments to test the viability of the approach\nand to compare it to other standard adversarial training methods.\n"], ["2018-01-10", "http://arxiv.org/abs/1801.03339", "Fooling End-to-end Speaker Verification by Adversarial Examples.", ["Felix Kreuk", " Yossi Adi", " Moustapha Cisse", " Joseph Keshet"], "  Automatic speaker verification systems are increasingly used as the primary\nmeans to authenticate costumers. Recently, it has been proposed to train\nspeaker verification systems using end-to-end deep neural models. In this\npaper, we show that such systems are vulnerable to adversarial example attack.\nAdversarial examples are generated by adding a peculiar noise to original\nspeaker examples, in such a way that they are almost indistinguishable from the\noriginal examples by a human listener. Yet, the generated waveforms, which\nsound as speaker A can be used to fool such a system by claiming as if the\nwaveforms were uttered by speaker B. We present white-box attacks on an\nend-to-end deep network that was either trained on YOHO or NTIMIT. We also\npresent two black-box attacks: where the adversarial examples were generated\nwith a system that was trained on YOHO, but the attack is on a system that was\ntrained on NTIMIT; and when the adversarial examples were generated with a\nsystem that was trained on Mel-spectrum feature set, but the attack is on a\nsystem that was trained on MFCC. Results suggest that the accuracy of the\nattacked system was decreased and the false-positive rate was dramatically\nincreased.\n"], ["2018-01-09", "http://arxiv.org/abs/1801.02950", "Adversarial Deep Learning for Robust Detection of Binary Encoded Malware.", ["Abdullah Al-Dujaili", " Alex Huang", " Erik Hemberg", " Una-May O'Reilly"], "  Malware is constantly adapting in order to avoid detection. Model based\nmalware detectors, such as SVM and neural networks, are vulnerable to so-called\nadversarial examples which are modest changes to detectable malware that allows\nthe resulting malware to evade detection. Continuous-valued methods that are\nrobust to adversarial examples of images have been developed using saddle-point\noptimization formulations. We are inspired by them to develop similar methods\nfor the discrete, e.g. binary, domain which characterizes the features of\nmalware. A specific extra challenge of malware is that the adversarial examples\nmust be generated in a way that preserves their malicious functionality. We\nintroduce methods capable of generating functionally preserved adversarial\nmalware examples in the binary domain. Using the saddle-point formulation, we\nincorporate the adversarial examples into the training of models that are\nrobust to them. We evaluate the effectiveness of the methods and others in the\nliterature on a set of Portable Execution~(PE) files. Comparison prompts our\nintroduction of an online measure computed during training to assess general\nexpectation of robustness.\n"], ["2018-01-09", "http://arxiv.org/abs/1801.02850", "Less is More: Culling the Training Set to Improve Robustness of Deep Neural Networks.", ["Yongshuai Liu", " Jiyu Chen", " Hao Chen"], "  Deep neural networks are vulnerable to adversarial examples. Prior defenses\nattempted to make deep networks more robust by either changing the network\narchitecture or augmenting the training set with adversarial examples, but both\nhave inherent limitations. Motivated by recent research that shows outliers in\nthe training set have a high negative influence on the trained model, we\nstudied the relationship between model robustness and the quality of the\ntraining set. We first show that outliers give the model better generalization\nability but weaker robustness. Next, we propose an adversarial example\ndetection framework, in which we design two methods for removing outliers from\ntraining set to obtain the sanitized model and then detect adversarial example\nby calculating the difference of outputs between the original and the sanitized\nmodel. We evaluated the framework on both MNIST and SVHN. Based on the\ndifference measured by Kullback-Leibler divergence, we could detect adversarial\nexamples with accuracy between 94.67% to 99.89%.\n"], ["2018-01-08", "http://arxiv.org/abs/1801.02780", "Rogue Signs: Deceiving Traffic Sign Recognition with Malicious Ads and Logos.", ["Chawin Sitawarin", " Arjun Nitin Bhagoji", " Arsalan Mosenia", " Prateek Mittal", " Mung Chiang"], "  We propose a new real-world attack against the computer vision based systems\nof autonomous vehicles (AVs). Our novel Sign Embedding attack exploits the\nconcept of adversarial examples to modify innocuous signs and advertisements in\nthe environment such that they are classified as the adversary's desired\ntraffic sign with high confidence. Our attack greatly expands the scope of the\nthreat posed to AVs since adversaries are no longer restricted to just\nmodifying existing traffic signs as in previous work. Our attack pipeline\ngenerates adversarial samples which are robust to the environmental conditions\nand noisy image transformations present in the physical world. We ensure this\nby including a variety of possible image transformations in the optimization\nproblem used to generate adversarial samples. We verify the robustness of the\nadversarial samples by printing them out and carrying out drive-by tests\nsimulating the conditions under which image capture would occur in a real-world\nscenario. We experimented with physical attack samples for different distances,\nlighting conditions and camera angles. In addition, extensive evaluations were\ncarried out in the virtual setting for a variety of image transformations. The\nadversarial samples generated using our method have adversarial success rates\nin excess of 95% in the physical as well as virtual settings.\n"], ["2018-01-08", "http://arxiv.org/abs/1801.02774", "Adversarial Spheres.", ["Justin Gilmer", " Luke Metz", " Fartash Faghri", " Samuel S. Schoenholz", " Maithra Raghu", " Martin Wattenberg", " Ian Goodfellow"], "  State of the art computer vision models have been shown to be vulnerable to\nsmall adversarial perturbations of the input. In other words, most images in\nthe data distribution are both correctly classified by the model and are very\nclose to a visually similar misclassified image. Despite substantial research\ninterest, the cause of the phenomenon is still poorly understood and remains\nunsolved. We hypothesize that this counter intuitive behavior is a naturally\noccurring result of the high dimensional geometry of the data manifold. As a\nfirst step towards exploring this hypothesis, we study a simple synthetic\ndataset of classifying between two concentric high dimensional spheres. For\nthis dataset we show a fundamental tradeoff between the amount of test error\nand the average distance to nearest error. In particular, we prove that any\nmodel which misclassifies a small constant fraction of a sphere will be\nvulnerable to adversarial perturbations of size $O(1/\\sqrt{d})$. Surprisingly,\nwhen we train several different architectures on this dataset, all of their\nerror sets naturally approach this theoretical bound. As a result of the\ntheory, the vulnerability of neural networks to small adversarial perturbations\nis a logical consequence of the amount of test error observed. We hope that our\ntheoretical analysis of this very simple case will point the way forward to\nexplore how the geometry of complex real-world data sets leads to adversarial\nexamples.\n"], ["2018-01-08", "http://arxiv.org/abs/1801.02613", "Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality.", ["Xingjun Ma", " Bo Li", " Yisen Wang", " Sarah M. Erfani", " Sudanthi Wijewickrema", " Grant Schoenebeck", " Dawn Song", " Michael E. Houle", " James Bailey"], "  Deep Neural Networks (DNNs) have recently been shown to be vulnerable against\nadversarial examples, which are carefully crafted instances that can mislead\nDNNs to make errors during prediction. To better understand such attacks, a\ncharacterization is needed of the properties of regions (the so-called\n'adversarial subspaces') in which adversarial examples lie. We tackle this\nchallenge by characterizing the dimensional properties of adversarial regions,\nvia the use of Local Intrinsic Dimensionality (LID). LID assesses the\nspace-filling capability of the region surrounding a reference example, based\non the distance distribution of the example to its neighbors. We first provide\nexplanations about how adversarial perturbation can affect the LID\ncharacteristic of adversarial regions, and then show empirically that LID\ncharacteristics can facilitate the distinction of adversarial examples\ngenerated using state-of-the-art attacks. As a proof-of-concept, we show that a\npotential application of LID is to distinguish adversarial examples, and the\npreliminary results show that it can outperform several state-of-the-art\ndetection measures by large margins for five attack strategies considered in\nthis paper across three benchmark datasets. Our analysis of the LID\ncharacteristic for adversarial regions not only motivates new directions of\neffective adversarial defense, but also opens up more challenges for developing\nnew attacks to better understand the vulnerabilities of DNNs.\n"], ["2018-01-08", "http://arxiv.org/abs/1801.02612", "Spatially Transformed Adversarial Examples.", ["Chaowei Xiao", " Jun-Yan Zhu", " Bo Li", " Warren He", " Mingyan Liu", " Dawn Song"], "  Recent studies show that widely used deep neural networks (DNNs) are\nvulnerable to carefully crafted adversarial examples. Many advanced algorithms\nhave been proposed to generate adversarial examples by leveraging the\n$\\mathcal{L}_p$ distance for penalizing perturbations. Researchers have\nexplored different defense methods to defend against such adversarial attacks.\nWhile the effectiveness of $\\mathcal{L}_p$ distance as a metric of perceptual\nquality remains an active research area, in this paper we will instead focus on\na different type of perturbation, namely spatial transformation, as opposed to\nmanipulating the pixel values directly as in prior works. Perturbations\ngenerated through spatial transformation could result in large $\\mathcal{L}_p$\ndistance measures, but our extensive experiments show that such spatially\ntransformed adversarial examples are perceptually realistic and more difficult\nto defend against with existing defense systems. This potentially provides a\nnew direction in adversarial example generation and the design of corresponding\ndefenses. We visualize the spatial transformation based perturbation for\ndifferent examples and show that our technique can produce realistic\nadversarial examples with smooth image deformation. Finally, we visualize the\nattention of deep networks with different types of adversarial examples to\nbetter understand how these examples are interpreted.\n"], ["2018-01-08", "http://arxiv.org/abs/1801.02610", "Generating Adversarial Examples with Adversarial Networks.", ["Chaowei Xiao", " Bo Li", " Jun-Yan Zhu", " Warren He", " Mingyan Liu", " Dawn Song"], "  Deep neural networks (DNNs) have been found to be vulnerable to adversarial\nexamples resulting from adding small-magnitude perturbations to inputs. Such\nadversarial examples can mislead DNNs to produce adversary-selected results.\nDifferent attack strategies have been proposed to generate adversarial\nexamples, but how to produce them with high perceptual quality and more\nefficiently requires more research efforts. In this paper, we propose AdvGAN to\ngenerate adversarial examples with generative adversarial networks (GANs),\nwhich can learn and approximate the distribution of original instances. For\nAdvGAN, once the generator is trained, it can generate adversarial\nperturbations efficiently for any instance, so as to potentially accelerate\nadversarial training as defenses. We apply AdvGAN in both semi-whitebox and\nblack-box attack settings. In semi-whitebox attacks, there is no need to access\nthe original target model after the generator is trained, in contrast to\ntraditional white-box attacks. In black-box attacks, we dynamically train a\ndistilled model for the black-box model and optimize the generator accordingly.\nAdversarial examples generated by AdvGAN on different target models have high\nattack success rate under state-of-the-art defenses compared to other attacks.\nOur attack has placed the first with 92.76% accuracy on a public MNIST\nblack-box attack challenge.\n"], ["2018-01-08", "http://arxiv.org/abs/1801.02608", "LaVAN: Localized and Visible Adversarial Noise.", ["Danny Karmon", " Daniel Zoran", " Yoav Goldberg"], "  Most works on adversarial examples for deep-learning based image classifiers\nuse noise that, while small, covers the entire image. We explore the case where\nthe noise is allowed to be visible but confined to a small, localized patch of\nthe image, without covering any of the main object(s) in the image. We show\nthat it is possible to generate localized adversarial noises that cover only 2%\nof the pixels in the image, none of them over the main object, and that are\ntransferable across images and locations, and successfully fool a\nstate-of-the-art Inception v3 model with very high success rates.\n"], ["2018-01-08", "http://arxiv.org/abs/1801.02384", "Attacking Speaker Recognition With Deep Generative Models.", ["Wilson Cai", " Anish Doshi", " Rafael Valle"], "  In this paper we investigate the ability of generative adversarial networks\n(GANs) to synthesize spoofing attacks on modern speaker recognition systems. We\nfirst show that samples generated with SampleRNN and WaveNet are unable to fool\na CNN-based speaker recognition system. We propose a modification of the\nWasserstein GAN objective function to make use of data that is real but not\nfrom the class being learned. Our semi-supervised learning method is able to\nperform both targeted and untargeted attacks, raising questions related to\nsecurity in speaker authentication systems.\n"], ["2018-01-08", "http://arxiv.org/abs/1801.02318", "HeNet: A Deep Learning Approach on Intel$^\\circledR$ Processor Trace for Effective Exploit Detection.", ["Li Chen", " Salmin Sultana", " Ravi Sahita"], "  This paper presents HeNet, a hierarchical ensemble neural network, applied to\nclassify hardware-generated control flow traces for malware detection. Deep\nlearning-based malware detection has so far focused on analyzing executable\nfiles and runtime API calls. Static code analysis approaches face challenges\ndue to obfuscated code and adversarial perturbations. Behavioral data collected\nduring execution is more difficult to obfuscate but recent research has shown\nsuccessful attacks against API call based malware classifiers. We investigate\ncontrol flow based characterization of a program execution to build robust deep\nlearning malware classifiers. HeNet consists of a low-level behavior model and\na top-level ensemble model. The low-level model is a per-application behavior\nmodel, trained via transfer learning on a time-series of images generated from\ncontrol flow trace of an execution. We use Intel$^\\circledR$ Processor Trace\nenabled processor for low overhead execution tracing and design a lightweight\nimage conversion and segmentation of the control flow trace. The top-level\nensemble model aggregates the behavior classification of all the trace segments\nand detects an attack. The use of hardware trace adds portability to our system\nand the use of deep learning eliminates the manual effort of feature\nengineering. We evaluate HeNet against real-world exploitations of PDF readers.\nHeNet achieves 100\\% accuracy and 0\\% false positive on test set, and higher\nclassification accuracy compared to classical machine learning algorithms.\n"], ["2018-01-07", "http://arxiv.org/abs/1801.02257", "Denoising Dictionary Learning Against Adversarial Perturbations.", ["John Mitro", " Derek Bridge", " Steven Prestwich"], "  We propose denoising dictionary learning (DDL), a simple yet effective\ntechnique as a protection measure against adversarial perturbations. We\nexamined denoising dictionary learning on MNIST and CIFAR10 perturbed under two\ndifferent perturbation techniques, fast gradient sign (FGSM) and jacobian\nsaliency maps (JSMA). We evaluated it against five different deep neural\nnetworks (DNN) representing the building blocks of most recent architectures\nindicating a successive progression of model complexity of each other. We show\nthat each model tends to capture different representations based on their\narchitecture. For each model we recorded its accuracy both on the perturbed\ntest data previously misclassified with high confidence and on the denoised one\nafter the reconstruction using dictionary learning. The reconstruction quality\nof each data point is assessed by means of PSNR (Peak Signal to Noise Ratio)\nand Structure Similarity Index (SSI). We show that after applying (DDL) the\nreconstruction of the original data point from a noisy\n"], ["2018-01-05", "http://arxiv.org/abs/1801.01953", "Adversarial Perturbation Intensity Achieving Chosen Intra-Technique Transferability Level for Logistic Regression.", ["Martin Gubri"], "  Machine Learning models have been shown to be vulnerable to adversarial\nexamples, ie. the manipulation of data by a attacker to defeat a defender's\nclassifier at test time. We present a novel probabilistic definition of\nadversarial examples in perfect or limited knowledge setting using prior\nprobability distributions on the defender's classifier. Using the asymptotic\nproperties of the logistic regression, we derive a closed-form expression of\nthe intensity of any adversarial perturbation, in order to achieve a given\nexpected misclassification rate. This technique is relevant in a threat model\nof known model specifications and unknown training data. To our knowledge, this\nis the first method that allows an attacker to directly choose the probability\nof attack success. We evaluate our approach on two real-world datasets.\n"], ["2018-01-05", "http://arxiv.org/abs/1801.01944", "Audio Adversarial Examples: Targeted Attacks on Speech-to-Text.", ["Nicholas Carlini", " David Wagner"], "  We construct targeted audio adversarial examples on automatic speech\nrecognition. Given any audio waveform, we can produce another that is over\n99.9% similar, but transcribes as any phrase we choose (recognizing up to 50\ncharacters per second of audio). We apply our white-box iterative\noptimization-based attack to Mozilla's implementation DeepSpeech end-to-end,\nand show it has a 100% success rate. The feasibility of this attack introduce a\nnew domain to study adversarial examples.\n"], ["2018-01-05", "http://arxiv.org/abs/1801.01828", "Shielding Google's language toxicity model against adversarial attacks.", ["Nestor Rodriguez", " Sergio Rojas-Galeano"], "  Lack of moderation in online communities enables participants to incur in\npersonal aggression, harassment or cyberbullying, issues that have been\naccentuated by extremist radicalisation in the contemporary post-truth politics\nscenario. This kind of hostility is usually expressed by means of toxic\nlanguage, profanity or abusive statements. Recently Google has developed a\nmachine-learning-based toxicity model in an attempt to assess the hostility of\na comment; unfortunately, it has been suggested that said model can be deceived\nby adversarial attacks that manipulate the text sequence of the comment. In\nthis paper we firstly characterise such adversarial attacks as using\nobfuscation and polarity transformations. The former deceives by corrupting\ntoxic trigger content with typographic edits, whereas the latter deceives by\ngrammatical negation of the toxic content. Then, we propose a two--stage\napproach to counter--attack these anomalies, bulding upon a recently proposed\ntext deobfuscation method and the toxicity scoring model. Lastly, we conducted\nan experiment with approximately 24000 distorted comments, showing how in this\nway it is feasible to restore toxicity of the adversarial variants, while\nincurring roughly on a twofold increase in processing time. Even though novel\nadversary challenges would keep coming up derived from the versatile nature of\nwritten language, we anticipate that techniques combining machine learning and\ntext pattern recognition methods, each one targeting different layers of\nlinguistic features, would be needed to achieve robust detection of toxic\nlanguage, thus fostering aggression--free digital interaction.\n"], ["2018-01-03", "http://arxiv.org/abs/1801.02480", "Facial Attributes: Accuracy and Adversarial Robustness.", ["Andras Rozsa", " Manuel G\u00fcnther", " Ethan M. Rudd", " Terrance E. Boult"], "  Facial attributes, emerging soft biometrics, must be automatically and\nreliably extracted from images in order to be usable in stand-alone systems.\nWhile recent methods extract facial attributes using deep neural networks\n(DNNs) trained on labeled facial attribute data, the robustness of deep\nattribute representations has not been evaluated. In this paper, we examine the\nrepresentational stability of several approaches that recently advanced the\nstate of the art on the CelebA benchmark by generating adversarial examples\nformed by adding small, non-random perturbations to inputs yielding altered\nclassifications. We show that our fast flipping attribute (FFA) technique\ngenerates more adversarial examples than traditional algorithms, and that the\nadversarial robustness of DNNs varies highly between facial attributes. We also\ntest the correlation of facial attributes and find that only for related\nattributes do the formed adversarial perturbations change the classification of\nothers. Finally, we introduce the concept of natural adversarial samples, i.e.,\nmisclassified images where predictions can be corrected via small\nperturbations. We demonstrate that natural adversarial samples commonly occur\nand show that many of these images remain misclassified even with additional\ntraining epochs, even though their correct classification may require only a\nsmall adjustment to network parameters.\n"], ["2018-01-03", "http://arxiv.org/abs/1801.00905", "Neural Networks in Adversarial Setting and Ill-Conditioned Weight Space.", ["Mayank Singh", " Abhishek Sinha", " Balaji Krishnamurthy"], "  Recently, Neural networks have seen a huge surge in its adoption due to their\nability to provide high accuracy on various tasks. On the other hand, the\nexistence of adversarial examples have raised suspicions regarding the\ngeneralization capabilities of neural networks. In this work, we focus on the\nweight matrix learnt by the neural networks and hypothesize that ill\nconditioned weight matrix is one of the contributing factors in neural\nnetwork's susceptibility towards adversarial examples. For ensuring that the\nlearnt weight matrix's condition number remains sufficiently low, we suggest\nusing orthogonal regularizer. We show that this indeed helps in increasing the\nadversarial accuracy on MNIST and F-MNIST datasets.\n"], ["2018-01-02", "http://arxiv.org/abs/1801.00634", "High Dimensional Spaces, Deep Learning and Adversarial Examples.", ["Simant Dube"], "  In this paper, we analyze deep learning from a mathematical point of view and\nderive several novel results. The results are based on intriguing mathematical\nproperties of high dimensional spaces. We first look at perturbation based\nadversarial examples and show how they can be understood using topological and\ngeometrical arguments in high dimensions. We point out mistake in an argument\npresented in prior published literature, and we present a more rigorous,\ngeneral and correct mathematical result to explain adversarial examples in\nterms of topology of image manifolds. Second, we look at optimization\nlandscapes of deep neural networks and examine the number of saddle points\nrelative to that of local minima. Third, we show how multiresolution nature of\nimages explains perturbation based adversarial examples in form of a stronger\nresult. Our results state that expectation of $L_2$-norm of adversarial\nperturbations is $O\\left(\\frac{1}{\\sqrt{n}}\\right)$ and therefore shrinks to 0\nas image resolution $n$ becomes arbitrarily large. Finally, by incorporating\nthe parts-whole manifold learning hypothesis for natural images, we investigate\nthe working of deep neural networks and root causes of adversarial examples and\ndiscuss how future improvements can be made and how adversarial examples can be\neliminated.\n"], ["2018-01-02", "http://arxiv.org/abs/1801.00554", "Did you hear that? Adversarial Examples Against Automatic Speech Recognition.", ["Moustafa Alzantot", " Bharathan Balaji", " Mani Srivastava"], "  Speech is a common and effective way of communication between humans, and\nmodern consumer devices such as smartphones and home hubs are equipped with\ndeep learning based accurate automatic speech recognition to enable natural\ninteraction between humans and machines. Recently, researchers have\ndemonstrated powerful attacks against machine learning models that can fool\nthem to produceincorrect results. However, nearly all previous research in\nadversarial attacks has focused on image recognition and object detection\nmodels. In this short paper, we present a first of its kind demonstration of\nadversarial attacks against speech classification model. Our algorithm performs\ntargeted attacks with 87% success by adding small background noise without\nhaving to know the underlying model parameter and architecture. Our attack only\nchanges the least significant bits of a subset of audio clip samples, and the\nnoise does not change 89% the human listener's perception of the audio clip as\nevaluated in our human study.\n"], ["2018-01-02", "http://arxiv.org/abs/1801.00553", "Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey.", ["Naveed Akhtar", " Ajmal Mian"], "  Deep learning is at the heart of the current rise of machine learning and\nartificial intelligence. In the field of Computer Vision, it has become the\nworkhorse for applications ranging from self-driving cars to surveillance and\nsecurity. Whereas deep neural networks have demonstrated phenomenal success\n(often beyond human capabilities) in solving complex problems, recent studies\nshow that they are vulnerable to adversarial attacks in the form of subtle\nperturbations to inputs that lead a model to predict incorrect outputs. For\nimages, such perturbations are often too small to be perceptible, yet they\ncompletely fool the deep learning models. Adversarial attacks pose a serious\nthreat to the success of deep learning in practice. This fact has lead to a\nlarge influx of contributions in this direction. This article presents the\nfirst comprehensive survey on adversarial attacks on deep learning in Computer\nVision. We review the works that design adversarial attacks, analyze the\nexistence of such attacks and propose defenses against them. To emphasize that\nadversarial attacks are possible in practical conditions, we separately review\nthe contributions that evaluate adversarial attacks in the real-world\nscenarios. Finally, we draw on the literature to provide a broader outlook of\nthe research direction.\n"], ["2017-12-31", "http://arxiv.org/abs/1801.00349", "A General Framework for Adversarial Examples with Objectives.", ["Mahmood Sharif", " Sruti Bhagavatula", " Lujo Bauer", " Michael K. Reiter"], "  Images perturbed subtly to be misclassified by neural networks, called\nadversarial examples, have emerged as a technically deep challenge and an\nimportant concern for several application domains. Most research on adversarial\nexamples takes as its only constraint that the perturbed images are similar to\nthe originals. However, real-world application of these ideas often requires\nthe examples to satisfy additional objectives, which are typically enforced\nthrough custom modifications of the perturbation process. In this paper, we\npropose adversarial generative nets (AGNs), a general methodology to train a\ngenerator neural network to emit adversarial examples satisfying desired\nobjectives. We demonstrate the ability of AGNs to accommodate a wide range of\nobjectives, including imprecise ones difficult to model, in two application\ndomains. In particular, we demonstrate physical adversarial examples---eyeglass\nframes designed to fool face recognition---with better robustness,\ninconspicuousness, and scalability than previous approaches, as well as a new\nattack to fool a handwritten-digit classifier.\n"], ["2017-12-28", "http://arxiv.org/abs/1712.09936", "Gradient Regularization Improves Accuracy of Discriminative Models.", ["D\u00e1niel Varga", " Adri\u00e1n Csisz\u00e1rik", " Zsolt Zombori"], "  Regularizing the gradient norm of the output of a neural network with respect\nto its inputs is a powerful technique, rediscovered several times. This paper\npresents evidence that gradient regularization can consistently improve\nclassification accuracy on vision tasks, using modern deep neural networks,\nespecially when the amount of training data is small. We introduce our\nregularizers as members of a broader class of Jacobian-based regularizers. We\ndemonstrate empirically on real and synthetic data that the learning process\nleads to gradients controlled beyond the training points, and results in\nsolutions that generalize well.\n"], ["2017-12-27", "http://arxiv.org/abs/1712.09665", "Adversarial Patch.", ["Tom B. Brown", " Dandelion Man\u00e9", " Aurko Roy", " Mart\u00edn Abadi", " Justin Gilmer"], "  We present a method to create universal, robust, targeted adversarial image\npatches in the real world. The patches are universal because they can be used\nto attack any scene, robust because they work under a wide variety of\ntransformations, and targeted because they can cause a classifier to output any\ntarget class. These adversarial patches can be printed, added to any scene,\nphotographed, and presented to image classifiers; even when the patches are\nsmall, they cause the classifiers to ignore the other items in the scene and\nreport a chosen target class.\n  To reproduce the results from the paper, our code is available at\nhttps://github.com/tensorflow/cleverhans/tree/master/examples/adversarial_patch\n"], ["2017-12-26", "http://arxiv.org/abs/1712.09491", "Exploring the Space of Black-box Attacks on Deep Neural Networks.", ["Arjun Nitin Bhagoji", " Warren He", " Bo Li", " Dawn Song"], "  Existing black-box attacks on deep neural networks (DNNs) so far have largely\nfocused on transferability, where an adversarial instance generated for a\nlocally trained model can \"transfer\" to attack other learning models. In this\npaper, we propose novel Gradient Estimation black-box attacks for adversaries\nwith query access to the target model's class probabilities, which do not rely\non transferability. We also propose strategies to decouple the number of\nqueries required to generate each adversarial sample from the dimensionality of\nthe input. An iterative variant of our attack achieves close to 100%\nadversarial success rates for both targeted and untargeted attacks on DNNs. We\ncarry out extensive experiments for a thorough comparative evaluation of\nblack-box attacks and show that the proposed Gradient Estimation attacks\noutperform all transferability based black-box attacks we tested on both MNIST\nand CIFAR-10 datasets, achieving adversarial success rates similar to well\nknown, state-of-the-art white-box attacks. We also apply the Gradient\nEstimation attacks successfully against a real-world Content Moderation\nclassifier hosted by Clarifai. Furthermore, we evaluate black-box attacks\nagainst state-of-the-art defenses. We show that the Gradient Estimation attacks\nare very effective even against these defenses.\n"], ["2017-12-26", "http://arxiv.org/abs/1712.09327", "Building Robust Deep Neural Networks for Road Sign Detection.", ["Arkar Min Aung", " Yousef Fadila", " Radian Gondokaryono", " Luis Gonzalez"], "  Deep Neural Networks are built to generalize outside of training set in mind\nby using techniques such as regularization, early stopping and dropout. But\nconsiderations to make them more resilient to adversarial examples are rarely\ntaken. As deep neural networks become more prevalent in mission-critical and\nreal-time systems, miscreants start to attack them by intentionally making deep\nneural networks to misclassify an object of one type to be seen as another\ntype. This can be catastrophic in some scenarios where the classification of a\ndeep neural network can lead to a fatal decision by a machine. In this work, we\nused GTSRB dataset to craft adversarial samples by Fast Gradient Sign Method\nand Jacobian Saliency Method, used those crafted adversarial samples to attack\nanother Deep Convolutional Neural Network and built the attacked network to be\nmore resilient against adversarial attacks by making it more robust by\nDefensive Distillation and Adversarial Training\n"], ["2017-12-26", "http://arxiv.org/abs/1712.09196", "The Robust Manifold Defense: Adversarial Training using Generative Models.", ["Ajil Jalal", " Andrew Ilyas", " Constantinos Daskalakis", " Alexandros G. Dimakis"], "  We propose a new type of attack for finding adversarial examples for image\nclassifiers. Our method exploits spanners, i.e. deep neural networks whose\ninput space is low-dimensional and whose output range approximates the set of\nimages of interest. Spanners may be generators of GANs or decoders of VAEs. The\nkey idea in our attack is to search over latent code pairs to find ones that\ngenerate nearby images with different classifier outputs. We argue that our\nattack is stronger than searching over perturbations of real images. Moreover,\nwe show that our stronger attack can be used to reduce the accuracy of\nDefense-GAN to 3\\%, resolving an open problem from the well-known paper by\nAthalye et al. We combine our attack with normal adversarial training to obtain\nthe most robust known MNIST classifier, significantly improving the state of\nthe art against PGD attacks. Our formulation involves solving a min-max\nproblem, where the min player sets the parameters of the classifier and the max\nplayer is running our attack, and is thus searching for adversarial examples in\nthe {\\em low-dimensional} input space of the spanner.\n  All code and models are available at\n\\url{https://github.com/ajiljalal/manifold-defense.git}\n"], ["2017-12-24", "http://arxiv.org/abs/1712.08996", "Android Malware Detection using Deep Learning on API Method Sequences.", ["ElMouatez Billah Karbab", " Mourad Debbabi", " Abdelouahid Derhab", " Djedjiga Mouheb"], "  Android OS experiences a blazing popularity since the last few years. This\npredominant platform has established itself not only in the mobile world but\nalso in the Internet of Things (IoT) devices. This popularity, however, comes\nat the expense of security, as it has become a tempting target of malicious\napps. Hence, there is an increasing need for sophisticated, automatic, and\nportable malware detection solutions. In this paper, we propose MalDozer, an\nautomatic Android malware detection and family attribution framework that\nrelies on sequences classification using deep learning techniques. Starting\nfrom the raw sequence of the app's API method calls, MalDozer automatically\nextracts and learns the malicious and the benign patterns from the actual\nsamples to detect Android malware. MalDozer can serve as a ubiquitous malware\ndetection system that is not only deployed on servers, but also on mobile and\neven IoT devices. We evaluate MalDozer on multiple Android malware datasets\nranging from 1K to 33K malware apps, and 38K benign apps. The results show that\nMalDozer can correctly detect malware and attribute them to their actual\nfamilies with an F1-Score of 96%-99% and a false positive rate of 0.06%-2%,\nunder all tested datasets and settings.\n"], ["2017-12-23", "http://arxiv.org/abs/1712.09344", "Whatever Does Not Kill Deep Reinforcement Learning, Makes It Stronger.", ["Vahid Behzadan", " Arslan Munir"], "  Recent developments have established the vulnerability of deep Reinforcement\nLearning (RL) to policy manipulation attacks via adversarial perturbations. In\nthis paper, we investigate the robustness and resilience of deep RL to\ntraining-time and test-time attacks. Through experimental results, we\ndemonstrate that under noncontiguous training-time attacks, Deep Q-Network\n(DQN) agents can recover and adapt to the adversarial conditions by reactively\nadjusting the policy. Our results also show that policies learned under\nadversarial perturbations are more robust to test-time attacks. Furthermore, we\ncompare the performance of $\\epsilon$-greedy and parameter-space noise\nexploration methods in terms of robustness and resilience against adversarial\nperturbations.\n"], ["2017-12-22", "http://arxiv.org/abs/1712.08713", "Query-limited Black-box Attacks to Classifiers.", ["Fnu Suya", " Yuan Tian", " David Evans", " Paolo Papotti"], "  We study black-box attacks on machine learning classifiers where each query\nto the model incurs some cost or risk of detection to the adversary. We focus\nexplicitly on minimizing the number of queries as a major objective.\nSpecifically, we consider the problem of attacking machine learning classifiers\nsubject to a budget of feature modification cost while minimizing the number of\nqueries, where each query returns only a class and confidence score. We\ndescribe an approach that uses Bayesian optimization to minimize the number of\nqueries, and find that the number of queries can be reduced to approximately\none tenth of the number needed through a random strategy for scenarios where\nthe feature modification cost budget is low.\n"], ["2017-12-21", "http://arxiv.org/abs/1712.08263", "Using LIP to Gloss Over Faces in Single-Stage Face Detection Networks.", ["Siqi Yang", " Arnold Wiliem", " Shaokang Chen", " Brian C. Lovell"], "  This work shows that it is possible to fool/attack recent state-of-the-art\nface detectors which are based on the single-stage networks. Successfully\nattacking face detectors could be a serious malware vulnerability when\ndeploying a smart surveillance system utilizing face detectors. We show that\nexisting adversarial perturbation methods are not effective to perform such an\nattack, especially when there are multiple faces in the input image. This is\nbecause the adversarial perturbation specifically generated for one face may\ndisrupt the adversarial perturbation for another face. In this paper, we call\nthis problem the Instance Perturbation Interference (IPI) problem. This IPI\nproblem is addressed by studying the relationship between the deep neural\nnetwork receptive field and the adversarial perturbation. As such, we propose\nthe Localized Instance Perturbation (LIP) that uses adversarial perturbation\nconstrained to the Effective Receptive Field (ERF) of a target to perform the\nattack. Experiment results show the LIP method massively outperforms existing\nadversarial perturbation generation methods -- often by a factor of 2 to 10.\n"], ["2017-12-21", "http://arxiv.org/abs/1712.08250", "ReabsNet: Detecting and Revising Adversarial Examples.", ["Jiefeng Chen", " Zihang Meng", " Changtian Sun", " Wei Tang", " Yinglun Zhu"], "  Though deep neural network has hit a huge success in recent studies and\napplica- tions, it still remains vulnerable to adversarial perturbations which\nare imperceptible to humans. To address this problem, we propose a novel\nnetwork called ReabsNet to achieve high classification accuracy in the face of\nvarious attacks. The approach is to augment an existing classification network\nwith a guardian network to detect if a sample is natural or has been\nadversarially perturbed. Critically, instead of simply rejecting adversarial\nexamples, we revise them to get their true labels. We exploit the observation\nthat a sample containing adversarial perturbations has a possibility of\nreturning to its true class after revision. We demonstrate that our ReabsNet\noutperforms the state-of-the-art defense method under various adversarial\nattacks.\n"], ["2017-12-21", "http://arxiv.org/abs/1712.08062", "Note on Attacking Object Detectors with Adversarial Stickers.", ["Kevin Eykholt", " Ivan Evtimov", " Earlence Fernandes", " Bo Li", " Dawn Song", " Tadayoshi Kohno", " Amir Rahmati", " Atul Prakash", " Florian Tramer"], "  Deep learning has proven to be a powerful tool for computer vision and has\nseen widespread adoption for numerous tasks. However, deep learning algorithms\nare known to be vulnerable to adversarial examples. These adversarial inputs\nare created such that, when provided to a deep learning algorithm, they are\nvery likely to be mislabeled. This can be problematic when deep learning is\nused to assist in safety critical decisions. Recent research has shown that\nclassifiers can be attacked by physical adversarial examples under various\nphysical conditions. Given the fact that state-of-the-art objection detection\nalgorithms are harder to be fooled by the same set of adversarial examples,\nhere we show that these detectors can also be attacked by physical adversarial\nexamples. In this note, we briefly show both static and dynamic test results.\nWe design an algorithm that produces physical adversarial inputs, which can\nfool the YOLO object detector and can also attack Faster-RCNN with relatively\nhigh success rate based on transferability. Furthermore, our algorithm can\ncompress the size of the adversarial inputs to stickers that, when attached to\nthe targeted object, result in the detector either mislabeling or not detecting\nthe object a high percentage of the time. This note provides a small set of\nresults. Our upcoming paper will contain a thorough evaluation on other object\ndetectors, and will present the algorithm.\n"], ["2017-12-21", "http://arxiv.org/abs/1712.07805", "Wolf in Sheep's Clothing - The Downscaling Attack Against Deep Learning Applications.", ["Qixue Xiao", " Kang Li", " Deyue Zhang", " Yier Jin"], "  This paper considers security risks buried in the data processing pipeline in\ncommon deep learning applications. Deep learning models usually assume a fixed\nscale for their training and input data. To allow deep learning applications to\nhandle a wide range of input data, popular frameworks, such as Caffe,\nTensorFlow, and Torch, all provide data scaling functions to resize input to\nthe dimensions used by deep learning models. Image scaling algorithms are\nintended to preserve the visual features of an image after scaling. However,\ncommon image scaling algorithms are not designed to handle human crafted\nimages. Attackers can make the scaling outputs look dramatically different from\nthe corresponding input images.\n  This paper presents a downscaling attack that targets the data scaling\nprocess in deep learning applications. By carefully crafting input data that\nmismatches with the dimension used by deep learning models, attackers can\ncreate deceiving effects. A deep learning application effectively consumes data\nthat are not the same as those presented to users. The visual inconsistency\nenables practical evasion and data poisoning attacks to deep learning\napplications. This paper presents proof-of-concept attack samples to popular\ndeep-learning-based image classification applications. To address the\ndownscaling attacks, the paper also suggests multiple potential mitigation\nstrategies.\n"], ["2017-12-19", "http://arxiv.org/abs/1712.07113", "Query-Efficient Black-box Adversarial Examples (superceded).", ["Andrew Ilyas", " Logan Engstrom", " Anish Athalye", " Jessy Lin"], "  Note that this paper is superceded by \"Black-Box Adversarial Attacks with\nLimited Queries and Information.\"\n  Current neural network-based image classifiers are susceptible to adversarial\nexamples, even in the black-box setting, where the attacker is limited to query\naccess without access to gradients. Previous methods --- substitute networks\nand coordinate-based finite-difference methods --- are either unreliable or\nquery-inefficient, making these methods impractical for certain problems.\n  We introduce a new method for reliably generating adversarial examples under\nmore restricted, practical black-box threat models. First, we apply natural\nevolution strategies to perform black-box attacks using two to three orders of\nmagnitude fewer queries than previous methods. Second, we introduce a new\nalgorithm to perform targeted adversarial attacks in the partial-information\nsetting, where the attacker only has access to a limited number of target\nclasses. Using these techniques, we successfully perform the first targeted\nadversarial attack against a commercially deployed machine learning system, the\nGoogle Cloud Vision API, in the partial information setting.\n"], ["2017-12-19", "http://arxiv.org/abs/1712.07107", "Adversarial Examples: Attacks and Defenses for Deep Learning.", ["Xiaoyong Yuan", " Pan He", " Qile Zhu", " Xiaolin Li"], "  With rapid progress and significant successes in a wide spectrum of\napplications, deep learning is being applied in many safety-critical\nenvironments. However, deep neural networks have been recently found vulnerable\nto well-designed input samples, called adversarial examples. Adversarial\nexamples are imperceptible to human but can easily fool deep neural networks in\nthe testing/deploying stage. The vulnerability to adversarial examples becomes\none of the major risks for applying deep neural networks in safety-critical\nenvironments. Therefore, attacks and defenses on adversarial examples draw\ngreat attention. In this paper, we review recent findings on adversarial\nexamples for deep neural networks, summarize the methods for generating\nadversarial examples, and propose a taxonomy of these methods. Under the\ntaxonomy, applications for adversarial examples are investigated. We further\nelaborate on countermeasures for adversarial examples and explore the\nchallenges and the potential solutions.\n"], ["2017-12-18", "http://arxiv.org/abs/1712.06751", "HotFlip: White-Box Adversarial Examples for Text Classification.", ["Javid Ebrahimi", " Anyi Rao", " Daniel Lowd", " Dejing Dou"], "  We propose an efficient method to generate white-box adversarial examples to\ntrick a character-level neural classifier. We find that only a few\nmanipulations are needed to greatly decrease the accuracy. Our method relies on\nan atomic flip operation, which swaps one token for another, based on the\ngradients of the one-hot input vectors. Due to efficiency of our method, we can\nperform adversarial training which makes the model more robust to attacks at\ntest time. With the use of a few semantics-preserving constraints, we\ndemonstrate that HotFlip can be adapted to attack a word-level classifier as\nwell.\n"], ["2017-12-18", "http://arxiv.org/abs/1712.06646", "When Not to Classify: Anomaly Detection of Attacks (ADA) on DNN Classifiers at Test Time.", ["David J. Miller", " Yulia Wang", " George Kesidis"], "  A significant threat to the recent, wide deployment of machine learning-based\nsystems, including deep neural networks (DNNs), is adversarial learning\nattacks. We analyze possible test-time evasion-attack mechanisms and show that,\nin some important cases, when the image has been attacked, correctly\nclassifying it has no utility: i) when the image to be attacked is (even\narbitrarily) selected from the attacker's cache; ii) when the sole recipient of\nthe classifier's decision is the attacker. Moreover, in some application\ndomains and scenarios it is highly actionable to detect the attack irrespective\nof correctly classifying in the face of it (with classification still performed\nif no attack is detected). We hypothesize that, even if human-imperceptible,\nadversarial perturbations are machine-detectable. We propose a purely\nunsupervised anomaly detector (AD) that, unlike previous works: i) models the\njoint density of a deep layer using highly suitable null hypothesis density\nmodels (matched in particular to the non- negative support for RELU layers);\nii) exploits multiple DNN layers; iii) leverages a \"source\" and \"destination\"\nclass concept, source class uncertainty, the class confusion matrix, and DNN\nweight information in constructing a novel decision statistic grounded in the\nKullback-Leibler divergence. Tested on MNIST and CIFAR-10 image databases under\nthree prominent attack strategies, our approach outperforms previous detection\nmethods, achieving strong ROC AUC detection accuracy on two attacks and better\naccuracy than recently reported for a variety of methods on the strongest (CW)\nattack. We also evaluate a fully white box attack on our system. Finally, we\nevaluate other important performance measures, such as classification accuracy,\nversus detection rate and attack strength.\n"], ["2017-12-17", "http://arxiv.org/abs/1712.06174", "Deep Neural Networks as 0-1 Mixed Integer Linear Programs: A Feasibility Study.", ["Matteo Fischetti", " Jason Jo"], "  Deep Neural Networks (DNNs) are very popular these days, and are the subject\nof a very intense investigation. A DNN is made by layers of internal units (or\nneurons), each of which computes an affine combination of the output of the\nunits in the previous layer, applies a nonlinear operator, and outputs the\ncorresponding value (also known as activation). A commonly-used nonlinear\noperator is the so-called rectified linear unit (ReLU), whose output is just\nthe maximum between its input value and zero. In this (and other similar cases\nlike max pooling, where the max operation involves more than one input value),\none can model the DNN as a 0-1 Mixed Integer Linear Program (0-1 MILP) where\nthe continuous variables correspond to the output values of each unit, and a\nbinary variable is associated with each ReLU to model its yes/no nature. In\nthis paper we discuss the peculiarity of this kind of 0-1 MILP models, and\ndescribe an effective bound-tightening technique intended to ease its solution.\nWe also present possible applications of the 0-1 MILP model arising in feature\nvisualization and in the construction of adversarial examples. Preliminary\ncomputational results are reported, aimed at investigating (on small DNNs) the\ncomputational performance of a state-of-the-art MILP solver when applied to a\nknown test case, namely, hand-written digit recognition.\n"], ["2017-12-17", "http://arxiv.org/abs/1712.06131", "Super-sparse Learning in Similarity Spaces.", ["Ambra Demontis", " Marco Melis", " Battista Biggio", " Giorgio Fumera", " Fabio Roli"], "  In several applications, input samples are more naturally represented in\nterms of similarities between each other, rather than in terms of feature\nvectors. In these settings, machine-learning algorithms can become very\ncomputationally demanding, as they may require matching the test samples\nagainst a very large set of reference prototypes. To mitigate this issue,\ndifferent approaches have been developed to reduce the number of required\nreference prototypes. Current reduction approaches select a small subset of\nrepresentative prototypes in the space induced by the similarity measure, and\nthen separately train the classification function on the reduced subset.\nHowever, decoupling these two steps may not allow reducing the number of\nprototypes effectively without compromising accuracy. We overcome this\nlimitation by jointly learning the classification function along with an\noptimal set of virtual prototypes, whose number can be either fixed a priori or\noptimized according to application-specific criteria. Creating a super-sparse\nset of virtual prototypes provides much sparser solutions, drastically reducing\ncomplexity at test time, at the expense of a slightly increased complexity\nduring training. A much smaller set of prototypes also results in\neasier-to-interpret decisions. We empirically show that our approach can reduce\nup to ten times the complexity of Support Vector Machines, LASSO and ridge\nregression at test time, without almost affecting their classification\naccuracy.\n"], ["2017-12-16", "http://arxiv.org/abs/1712.05919", "Attack and Defense of Dynamic Analysis-Based, Adversarial Neural Malware Classification Models.", ["Jack W. Stokes", " De Wang", " Mady Marinescu", " Marc Marino", " Brian Bussone"], "  Recently researchers have proposed using deep learning-based systems for\nmalware detection. Unfortunately, all deep learning classification systems are\nvulnerable to adversarial attacks. Previous work has studied adversarial\nattacks against static analysis-based malware classifiers which only classify\nthe content of the unknown file without execution. However, since the majority\nof malware is either packed or encrypted, malware classification based on\nstatic analysis often fails to detect these types of files. To overcome this\nlimitation, anti-malware companies typically perform dynamic analysis by\nemulating each file in the anti-malware engine or performing in-depth scanning\nin a virtual machine. These strategies allow the analysis of the malware after\nunpacking or decryption. In this work, we study different strategies of\ncrafting adversarial samples for dynamic analysis. These strategies operate on\nsparse, binary inputs in contrast to continuous inputs such as pixels in\nimages. We then study the effects of two, previously proposed defensive\nmechanisms against crafted adversarial samples including the distillation and\nensemble defenses. We also propose and evaluate the weight decay defense.\nExperiments show that with these three defensive strategies, the number of\nsuccessfully crafted adversarial samples is reduced compared to a standard\nbaseline system without any defenses. In particular, the ensemble defense is\nthe most resilient to adversarial attacks. Importantly, none of the defenses\nsignificantly reduce the classification accuracy for detecting malware.\nFinally, we demonstrate that while adding additional hidden layers to neural\nmodels does not significantly improve the malware classification accuracy, it\ndoes significantly increase the classifier's robustness to adversarial attacks.\n"], ["2017-12-14", "http://arxiv.org/abs/1712.05419", "DANCin SEQ2SEQ: Fooling Text Classifiers with Adversarial Text Example Generation.", ["Catherine Wong"], "  Machine learning models are powerful but fallible. Generating adversarial\nexamples - inputs deliberately crafted to cause model misclassification or\nother errors - can yield important insight into model assumptions and\nvulnerabilities. Despite significant recent work on adversarial example\ngeneration targeting image classifiers, relatively little work exists exploring\nadversarial example generation for text classifiers; additionally, many\nexisting adversarial example generation algorithms require full access to\ntarget model parameters, rendering them impractical for many real-world\nattacks. In this work, we introduce DANCin SEQ2SEQ, a GAN-inspired algorithm\nfor adversarial text example generation targeting largely black-box text\nclassifiers. We recast adversarial text example generation as a reinforcement\nlearning problem, and demonstrate that our algorithm offers preliminary but\npromising steps towards generating semantically meaningful adversarial text\nexamples in a real-world attack scenario.\n"], ["2017-12-12", "http://arxiv.org/abs/1712.04248", "Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models.", ["Wieland Brendel", " Jonas Rauber", " Matthias Bethge"], "  Many machine learning algorithms are vulnerable to almost imperceptible\nperturbations of their inputs. So far it was unclear how much risk adversarial\nperturbations carry for the safety of real-world machine learning applications\nbecause most methods used to generate such perturbations rely either on\ndetailed model information (gradient-based attacks) or on confidence scores\nsuch as class probabilities (score-based attacks), neither of which are\navailable in most real-world scenarios. In many such cases one currently needs\nto retreat to transfer-based attacks which rely on cumbersome substitute\nmodels, need access to the training data and can be defended against. Here we\nemphasise the importance of attacks which solely rely on the final model\ndecision. Such decision-based attacks are (1) applicable to real-world\nblack-box models such as autonomous cars, (2) need less knowledge and are\neasier to apply than transfer-based attacks and (3) are more robust to simple\ndefences than gradient- or score-based attacks. Previous attacks in this\ncategory were limited to simple models or simple datasets. Here we introduce\nthe Boundary Attack, a decision-based attack that starts from a large\nadversarial perturbation and then seeks to reduce the perturbation while\nstaying adversarial. The attack is conceptually simple, requires close to no\nhyperparameter tuning, does not rely on substitute models and is competitive\nwith the best gradient-based attacks in standard computer vision tasks like\nImageNet. We apply the attack on two black-box algorithms from Clarifai.com.\nThe Boundary Attack in particular and the class of decision-based attacks in\ngeneral open new avenues to study the robustness of machine learning models and\nraise new questions regarding the safety of deployed machine learning systems.\nAn implementation of the attack is available as part of Foolbox at\nhttps://github.com/bethgelab/foolbox .\n"], ["2017-12-11", "http://arxiv.org/abs/1712.04006", "Training Ensembles to Detect Adversarial Examples.", ["Alexander Bagnall", " Razvan Bunescu", " Gordon Stewart"], "  We propose a new ensemble method for detecting and classifying adversarial\nexamples generated by state-of-the-art attacks, including DeepFool and C&W. Our\nmethod works by training the members of an ensemble to have low classification\nerror on random benign examples while simultaneously minimizing agreement on\nexamples outside the training distribution. We evaluate on both MNIST and\nCIFAR-10, against oblivious and both white- and black-box adversaries.\n"], ["2017-12-10", "http://arxiv.org/abs/1712.03632", "Robust Deep Reinforcement Learning with Adversarial Attacks.", ["Anay Pattanaik", " Zhenyi Tang", " Shuijing Liu", " Gautham Bommannan", " Girish Chowdhary"], "  This paper proposes adversarial attacks for Reinforcement Learning (RL) and\nthen improves the robustness of Deep Reinforcement Learning algorithms (DRL) to\nparameter uncertainties with the help of these attacks. We show that even a\nnaively engineered attack successfully degrades the performance of DRL\nalgorithm. We further improve the attack using gradient information of an\nengineered loss function which leads to further degradation in performance.\nThese attacks are then leveraged during training to improve the robustness of\nRL within robust control framework. We show that this adversarial training of\nDRL algorithms like Deep Double Q learning and Deep Deterministic Policy\nGradients leads to significant increase in robustness to parameter variations\nfor RL benchmarks such as Cart-pole, Mountain Car, Hopper and Half Cheetah\nenvironment.\n"], ["2017-12-09", "http://arxiv.org/abs/1712.03390", "NAG: Network for Adversary Generation.", ["Konda Reddy Mopuri", " Utkarsh Ojha", " Utsav Garg", " R. Venkatesh Babu"], "  Adversarial perturbations can pose a serious threat for deploying machine\nlearning systems. Recent works have shown existence of image-agnostic\nperturbations that can fool classifiers over most natural images. Existing\nmethods present optimization approaches that solve for a fooling objective with\nan imperceptibility constraint to craft the perturbations. However, for a given\nclassifier, they generate one perturbation at a time, which is a single\ninstance from the manifold of adversarial perturbations. Also, in order to\nbuild robust models, it is essential to explore the manifold of adversarial\nperturbations. In this paper, we propose for the first time, a generative\napproach to model the distribution of adversarial perturbations. The\narchitecture of the proposed model is inspired from that of GANs and is trained\nusing fooling and diversity objectives. Our trained generator network attempts\nto capture the distribution of adversarial perturbations for a given classifier\nand readily generates a wide variety of such perturbations. Our experimental\nevaluation demonstrates that perturbations crafted by our model (i) achieve\nstate-of-the-art fooling rates, (ii) exhibit wide variety and (iii) deliver\nexcellent cross model generalizability. Our work can be deemed as an important\nstep in the process of inferring about the complex manifolds of adversarial\nperturbations.\n"], ["2017-12-08", "http://arxiv.org/abs/1712.03141", "Wild Patterns: Ten Years After the Rise of Adversarial Machine Learning.", ["Battista Biggio", " Fabio Roli"], "  Learning-based pattern classifiers, including deep networks, have shown\nimpressive performance in several application domains, ranging from computer\nvision to cybersecurity. However, it has also been shown that adversarial input\nperturbations carefully crafted either at training or at test time can easily\nsubvert their predictions. The vulnerability of machine learning to such wild\npatterns (also referred to as adversarial examples), along with the design of\nsuitable countermeasures, have been investigated in the research field of\nadversarial machine learning. In this work, we provide a thorough overview of\nthe evolution of this research area over the last ten years and beyond,\nstarting from pioneering, earlier work on the security of non-deep learning\nalgorithms up to more recent work aimed to understand the security properties\nof deep learning algorithms, in the context of computer vision and\ncybersecurity tasks. We report interesting connections between these\napparently-different lines of work, highlighting common misconceptions related\nto the security evaluation of machine-learning algorithms. We review the main\nthreat models and attacks defined to this end, and discuss the main limitations\nof current work, along with the corresponding future challenges towards the\ndesign of more secure learning algorithms.\n"], ["2017-12-08", "http://arxiv.org/abs/1712.02976", "Defense against Adversarial Attacks Using High-Level Representation Guided Denoiser.", ["Fangzhou Liao", " Ming Liang", " Yinpeng Dong", " Tianyu Pang", " Xiaolin Hu", " Jun Zhu"], "  Neural networks are vulnerable to adversarial examples, which poses a threat\nto their application in security sensitive systems. We propose high-level\nrepresentation guided denoiser (HGD) as a defense for image classification.\nStandard denoiser suffers from the error amplification effect, in which small\nresidual adversarial noise is progressively amplified and leads to wrong\nclassifications. HGD overcomes this problem by using a loss function defined as\nthe difference between the target model's outputs activated by the clean image\nand denoised image. Compared with ensemble adversarial training which is the\nstate-of-the-art defending method on large images, HGD has three advantages.\nFirst, with HGD as a defense, the target model is more robust to either\nwhite-box or black-box adversarial attacks. Second, HGD can be trained on a\nsmall subset of the images and generalizes well to other images and unseen\nclasses. Third, HGD can be transferred to defend models other than the one\nguiding it. In NIPS competition on defense against adversarial attacks, our HGD\nsolution won the first place and outperformed other models by a large margin.\n"], ["2017-12-07", "http://arxiv.org/abs/1712.02494", "Adversarial Examples that Fool Detectors.", ["Jiajun Lu", " Hussein Sibai", " Evan Fabry"], "  An adversarial example is an example that has been adjusted to produce a\nwrong label when presented to a system at test time. To date, adversarial\nexample constructions have been demonstrated for classifiers, but not for\ndetectors. If adversarial examples that could fool a detector exist, they could\nbe used to (for example) maliciously create security hazards on roads populated\nwith smart vehicles. In this paper, we demonstrate a construction that\nsuccessfully fools two standard detectors, Faster RCNN and YOLO. The existence\nof such examples is surprising, as attacking a classifier is very different\nfrom attacking a detector, and that the structure of detectors - which must\nsearch for their own bounding box, and which cannot estimate that box very\naccurately - makes it quite likely that adversarial patterns are strongly\ndisrupted. We show that our construction produces adversarial examples that\ngeneralize well across sequences digitally, even though large perturbations are\nneeded. We also show that our construction yields physical objects that are\nadversarial.\n"], ["2017-12-07", "http://arxiv.org/abs/1712.02779", "Exploring the Landscape of Spatial Robustness.", ["Logan Engstrom", " Brandon Tran", " Dimitris Tsipras", " Ludwig Schmidt", " Aleksander Madry"], "  The study of adversarial robustness has so far largely focused on\nperturbations bound in p-norms. However, state-of-the-art models turn out to be\nalso vulnerable to other, more natural classes of perturbations such as\ntranslations and rotations. In this work, we thoroughly investigate the\nvulnerability of neural network--based classifiers to rotations and\ntranslations. While data augmentation offers relatively small robustness, we\nuse ideas from robust optimization and test-time input aggregation to\nsignificantly improve robustness. Finally we find that, in contrast to the\np-norm case, first-order methods cannot reliably find worst-case perturbations.\nThis highlights spatial robustness as a fundamentally different setting\nrequiring additional study. Code available at\nhttps://github.com/MadryLab/adversarial_spatial and\nhttps://github.com/MadryLab/spatial-pytorch.\n"], ["2017-12-06", "http://arxiv.org/abs/1712.02328", "Generative Adversarial Perturbations.", ["Omid Poursaeed", " Isay Katsman", " Bicheng Gao", " Serge Belongie"], "  In this paper, we propose novel generative models for creating adversarial\nexamples, slightly perturbed images resembling natural images but maliciously\ncrafted to fool pre-trained models. We present trainable deep neural networks\nfor transforming images to adversarial perturbations. Our proposed models can\nproduce image-agnostic and image-dependent perturbations for both targeted and\nnon-targeted attacks. We also demonstrate that similar architectures can\nachieve impressive results in fooling classification and semantic segmentation\nmodels, obviating the need for hand-crafting attack methods for each task.\nUsing extensive experiments on challenging high-resolution datasets such as\nImageNet and Cityscapes, we show that our perturbations achieve high fooling\nrates with small perturbation norms. Moreover, our attacks are considerably\nfaster than current iterative methods at inference time.\n"], ["2017-12-06", "http://arxiv.org/abs/1712.02051", "Attacking Visual Language Grounding with Adversarial Examples: A Case Study on Neural Image Captioning.", ["Hongge Chen", " Huan Zhang", " Pin-Yu Chen", " Jinfeng Yi", " Cho-Jui Hsieh"], "  Visual language grounding is widely studied in modern neural image captioning\nsystems, which typically adopts an encoder-decoder framework consisting of two\nprincipal components: a convolutional neural network (CNN) for image feature\nextraction and a recurrent neural network (RNN) for language caption\ngeneration. To study the robustness of language grounding to adversarial\nperturbations in machine vision and perception, we propose Show-and-Fool, a\nnovel algorithm for crafting adversarial examples in neural image captioning.\nThe proposed algorithm provides two evaluation approaches, which check whether\nneural image captioning systems can be mislead to output some randomly chosen\ncaptions or keywords. Our extensive experiments show that our algorithm can\nsuccessfully craft visually-similar adversarial examples with randomly targeted\ncaptions or keywords, and the adversarial examples can be made highly\ntransferable to other image captioning systems. Consequently, our approach\nleads to new robustness implications of neural image captioning and novel\ninsights in visual language grounding.\n"], ["2017-12-05", "http://arxiv.org/abs/1712.01785", "Towards Practical Verification of Machine Learning: The Case of Computer Vision Systems.", ["Kexin Pei", " Yinzhi Cao", " Junfeng Yang", " Suman Jana"], "  Due to the increasing usage of machine learning (ML) techniques in security-\nand safety-critical domains, such as autonomous systems and medical diagnosis,\nensuring correct behavior of ML systems, especially for different corner cases,\nis of growing importance. In this paper, we propose a generic framework for\nevaluating security and robustness of ML systems using different real-world\nsafety properties. We further design, implement and evaluate VeriVis, a\nscalable methodology that can verify a diverse set of safety properties for\nstate-of-the-art computer vision systems with only blackbox access. VeriVis\nleverage different input space reduction techniques for efficient verification\nof different safety properties. VeriVis is able to find thousands of safety\nviolations in fifteen state-of-the-art computer vision systems including ten\nDeep Neural Networks (DNNs) such as Inception-v3 and Nvidia's Dave self-driving\nsystem with thousands of neurons as well as five commercial third-party vision\nAPIs including Google vision and Clarifai for twelve different safety\nproperties. Furthermore, VeriVis can successfully verify local safety\nproperties, on average, for around 31.7% of the test images. VeriVis finds up\nto 64.8x more violations than existing gradient-based methods that, unlike\nVeriVis, cannot ensure non-existence of any violations. Finally, we show that\nretraining using the safety violations detected by VeriVis can reduce the\naverage number of violations up to 60.2%.\n"], ["2017-12-02", "http://arxiv.org/abs/1712.00699", "Improving Network Robustness against Adversarial Attacks with Compact Convolution.", ["Rajeev Ranjan", " Swami Sankaranarayanan", " Carlos D. Castillo", " Rama Chellappa"], "  Though Convolutional Neural Networks (CNNs) have surpassed human-level\nperformance on tasks such as object classification and face verification, they\ncan easily be fooled by adversarial attacks. These attacks add a small\nperturbation to the input image that causes the network to misclassify the\nsample. In this paper, we focus on neutralizing adversarial attacks by compact\nfeature learning. In particular, we show that learning features in a closed and\nbounded space improves the robustness of the network. We explore the effect of\nL2-Softmax Loss, that enforces compactness in the learned features, thus\nresulting in enhanced robustness to adversarial perturbations. Additionally, we\npropose compact convolution, a novel method of convolution that when\nincorporated in conventional CNNs improves their robustness. Compact\nconvolution ensures feature compactness at every layer such that they are\nbounded and close to each other. Extensive experiments show that Compact\nConvolutional Networks (CCNs) neutralize multiple types of attacks, and perform\nbetter than existing methods in defending adversarial attacks, without\nincurring any additional training overhead compared to CNNs.\n"], ["2017-12-02", "http://arxiv.org/abs/1712.00673", "Towards Robust Neural Networks via Random Self-ensemble.", ["Xuanqing Liu", " Minhao Cheng", " Huan Zhang", " Cho-Jui Hsieh"], "  Recent studies have revealed the vulnerability of deep neural networks: A\nsmall adversarial perturbation that is imperceptible to human can easily make a\nwell-trained deep neural network misclassify. This makes it unsafe to apply\nneural networks in security-critical applications. In this paper, we propose a\nnew defense algorithm called Random Self-Ensemble (RSE) by combining two\nimportant concepts: {\\bf randomness} and {\\bf ensemble}. To protect a targeted\nmodel, RSE adds random noise layers to the neural network to prevent the strong\ngradient-based attacks, and ensembles the prediction over random noises to\nstabilize the performance. We show that our algorithm is equivalent to ensemble\nan infinite number of noisy models $f_\\epsilon$ without any additional memory\noverhead, and the proposed training procedure based on noisy stochastic\ngradient descent can ensure the ensemble model has a good predictive\ncapability. Our algorithm significantly outperforms previous defense techniques\non real data sets. For instance, on CIFAR-10 with VGG network (which has 92\\%\naccuracy without any attack), under the strong C\\&W attack within a certain\ndistortion tolerance, the accuracy of unprotected model drops to less than\n10\\%, the best previous defense technique has $48\\%$ accuracy, while our method\nstill has $86\\%$ prediction accuracy under the same level of attack. Finally,\nour method is simple and easy to integrate into any neural network.\n"], ["2017-12-02", "http://arxiv.org/abs/1712.00558", "Where Classification Fails, Interpretation Rises.", ["Chanh Nguyen", " Georgi Georgiev", " Yujie Ji", " Ting Wang"], "  An intriguing property of deep neural networks is their inherent\nvulnerability to adversarial inputs, which significantly hinders their\napplication in security-critical domains. Most existing detection methods\nattempt to use carefully engineered patterns to distinguish adversarial inputs\nfrom their genuine counterparts, which however can often be circumvented by\nadaptive adversaries. In this work, we take a completely different route by\nleveraging the definition of adversarial inputs: while deceiving for deep\nneural networks, they are barely discernible for human visions. Building upon\nrecent advances in interpretable models, we construct a new detection framework\nthat contrasts an input's interpretation against its classification. We\nvalidate the efficacy of this framework through extensive experiments using\nbenchmark datasets and attacks. We believe that this work opens a new direction\nfor designing adversarial input detection methods.\n"], ["2017-11-30", "http://arxiv.org/abs/1711.11561", "Measuring the tendency of CNNs to Learn Surface Statistical Regularities.", ["Jason Jo", " Yoshua Bengio"], "  Deep CNNs are known to exhibit the following peculiarity: on the one hand\nthey generalize extremely well to a test set, while on the other hand they are\nextremely sensitive to so-called adversarial perturbations. The extreme\nsensitivity of high performance CNNs to adversarial examples casts serious\ndoubt that these networks are learning high level abstractions in the dataset.\nWe are concerned with the following question: How can a deep CNN that does not\nlearn any high level semantics of the dataset manage to generalize so well? The\ngoal of this article is to measure the tendency of CNNs to learn surface\nstatistical regularities of the dataset. To this end, we use Fourier filtering\nto construct datasets which share the exact same high level abstractions but\nexhibit qualitatively different surface statistical regularities. For the SVHN\nand CIFAR-10 datasets, we present two Fourier filtered variants: a low\nfrequency variant and a randomly filtered variant. Each of the Fourier\nfiltering schemes is tuned to preserve the recognizability of the objects. Our\nmain finding is that CNNs exhibit a tendency to latch onto the Fourier image\nstatistics of the training dataset, sometimes exhibiting up to a 28%\ngeneralization gap across the various test sets. Moreover, we observe that\nsignificantly increasing the depth of a network has a very marginal impact on\nclosing the aforementioned generalization gap. Thus we provide quantitative\nevidence supporting the hypothesis that deep CNNs tend to learn surface\nstatistical regularities in the dataset rather than higher-level abstract\nconcepts.\n"], ["2017-11-27", "http://arxiv.org/abs/1711.10056", "Adversary Detection in Neural Networks via Persistent Homology.", ["Thomas Gebhart", " Paul Schrater"], "  We outline a detection method for adversarial inputs to deep neural networks.\nBy viewing neural network computations as graphs upon which information flows\nfrom input space to out- put distribution, we compare the differences in graphs\ninduced by different inputs. Specifically, by applying persistent homology to\nthese induced graphs, we observe that the structure of the most persistent\nsubgraphs which generate the first homology group differ between adversarial\nand unperturbed inputs. Based on this observation, we build a detection\nalgorithm that depends only on the topological information extracted during\ntraining. We test our algorithm on MNIST and achieve 98% detection adversary\naccuracy with F1-score 0.98.\n"], ["2017-11-27", "http://arxiv.org/abs/1711.09856", "On the Robustness of Semantic Segmentation Models to Adversarial Attacks.", ["Anurag Arnab", " Ondrej Miksik", " Philip H. S. Torr"], "  Deep Neural Networks (DNNs) have demonstrated exceptional performance on most\nrecognition tasks such as image classification and segmentation. However, they\nhave also been shown to be vulnerable to adversarial examples. This phenomenon\nhas recently attracted a lot of attention but it has not been extensively\nstudied on multiple, large-scale datasets and structured prediction tasks such\nas semantic segmentation which often require more specialised networks with\nadditional components such as CRFs, dilated convolutions, skip-connections and\nmultiscale processing. In this paper, we present what to our knowledge is the\nfirst rigorous evaluation of adversarial attacks on modern semantic\nsegmentation models, using two large-scale datasets. We analyse the effect of\ndifferent network architectures, model capacity and multiscale processing, and\nshow that many observations made on the task of classification do not always\ntransfer to this more complex task. Furthermore, we show how mean-field\ninference in deep structured models, multiscale processing (and more generally,\ninput transformations) naturally implement recently proposed adversarial\ndefenses. Our observations will aid future efforts in understanding and\ndefending against adversarial examples. Moreover, in the shorter term, we show\nhow to effectively benchmark robustness and show which segmentation models\nshould currently be preferred in safety-critical applications due to their\ninherent robustness.\n"], ["2017-11-27", "http://arxiv.org/abs/1711.09681", "Butterfly Effect: Bidirectional Control of Classification Performance by Small Additive Perturbation.", ["YoungJoon Yoo", " Seonguk Park", " Junyoung Choi", " Sangdoo Yun", " Nojun Kwak"], "  This paper proposes a new algorithm for controlling classification results by\ngenerating a small additive perturbation without changing the classifier\nnetwork. Our work is inspired by existing works generating adversarial\nperturbation that worsens classification performance. In contrast to the\nexisting methods, our work aims to generate perturbations that can enhance\noverall classification performance. To solve this performance enhancement\nproblem, we newly propose a perturbation generation network (PGN) influenced by\nthe adversarial learning strategy. In our problem, the information in a large\nexternal dataset is summarized by a small additive perturbation, which helps to\nimprove the performance of the classifier trained with the target dataset. In\naddition to this performance enhancement problem, we show that the proposed PGN\ncan be adopted to solve the classical adversarial problem without utilizing the\ninformation on the target classifier. The mentioned characteristics of our\nmethod are verified through extensive experiments on publicly available visual\ndatasets.\n"], ["2017-11-26", "http://arxiv.org/abs/1711.09404", "Improving the Adversarial Robustness and Interpretability of Deep Neural Networks by Regularizing their Input Gradients.", ["Andrew Slavin Ross", " Finale Doshi-Velez"], "  Deep neural networks have proven remarkably effective at solving many\nclassification problems, but have been criticized recently for two major\nweaknesses: the reasons behind their predictions are uninterpretable, and the\npredictions themselves can often be fooled by small adversarial perturbations.\nThese problems pose major obstacles for the adoption of neural networks in\ndomains that require security or transparency. In this work, we evaluate the\neffectiveness of defenses that differentiably penalize the degree to which\nsmall changes in inputs can alter model predictions. Across multiple attacks,\narchitectures, defenses, and datasets, we find that neural networks trained\nwith this input gradient regularization exhibit robustness to transferred\nadversarial examples generated to fool all of the other models. We also find\nthat adversarial examples generated to fool gradient-regularized models fool\nall other models equally well, and actually lead to more \"legitimate,\"\ninterpretable misclassifications as rated by people (which we confirm in a\nhuman subject experiment). Finally, we demonstrate that regularizing input\ngradients makes them more naturally interpretable as rationales for model\npredictions. We conclude by discussing this relationship between\ninterpretability and robustness in deep neural networks.\n"], ["2017-11-24", "http://arxiv.org/abs/1711.09115", "Geometric robustness of deep networks: analysis and improvement.", ["Can Kanbak", " Seyed-Mohsen Moosavi-Dezfooli", " Pascal Frossard"], "  Deep convolutional neural networks have been shown to be vulnerable to\narbitrary geometric transformations. However, there is no systematic method to\nmeasure the invariance properties of deep networks to such transformations. We\npropose ManiFool as a simple yet scalable algorithm to measure the invariance\nof deep networks. In particular, our algorithm measures the robustness of deep\nnetworks to geometric transformations in a worst-case regime as they can be\nproblematic for sensitive applications. Our extensive experimental results show\nthat ManiFool can be used to measure the invariance of fairly complex networks\non high dimensional datasets and these values can be used for analyzing the\nreasons for it. Furthermore, we build on Manifool to propose a new adversarial\ntraining scheme and we show its effectiveness on improving the invariance\nproperties of deep neural networks.\n"], ["2017-11-22", "http://arxiv.org/abs/1711.08534", "Safer Classification by Synthesis.", ["William Wang", " Angelina Wang", " Aviv Tamar", " Xi Chen", " Pieter Abbeel"], "  The discriminative approach to classification using deep neural networks has\nbecome the de-facto standard in various fields. Complementing recent\nreservations about safety against adversarial examples, we show that\nconventional discriminative methods can easily be fooled to provide incorrect\nlabels with very high confidence to out of distribution examples. We posit that\na generative approach is the natural remedy for this problem, and propose a\nmethod for classification using generative models. At training time, we learn a\ngenerative model for each class, while at test time, given an example to\nclassify, we query each generator for its most similar generation, and select\nthe class corresponding to the most similar one. Our approach is general and\ncan be used with expressive models such as GANs and VAEs. At test time, our\nmethod accurately \"knows when it does not know,\" and provides resilience to out\nof distribution examples while maintaining competitive performance for standard\nexamples.\n"], ["2017-11-22", "http://arxiv.org/abs/1711.08478", "MagNet and \"Efficient Defenses Against Adversarial Attacks\" are Not Robust to Adversarial Examples.", ["Nicholas Carlini", " David Wagner"], "  MagNet and \"Efficient Defenses...\" were recently proposed as a defense to\nadversarial examples. We find that we can construct adversarial examples that\ndefeat these defenses with only a slight increase in distortion.\n"], ["2017-11-22", "http://arxiv.org/abs/1711.08244", "Adversarial Phenomenon in the Eyes of Bayesian Deep Learning.", ["Ambrish Rawat", " Martin Wistuba", " Maria-Irina Nicolae"], "  Deep Learning models are vulnerable to adversarial examples, i.e.\\ images\nobtained via deliberate imperceptible perturbations, such that the model\nmisclassifies them with high confidence. However, class confidence by itself is\nan incomplete picture of uncertainty. We therefore use principled Bayesian\nmethods to capture model uncertainty in prediction for observing adversarial\nmisclassification. We provide an extensive study with different Bayesian neural\nnetworks attacked in both white-box and black-box setups. The behaviour of the\nnetworks for noise, attacks and clean test data is compared. We observe that\nBayesian neural networks are uncertain in their predictions for adversarial\nperturbations, a behaviour similar to the one observed for random Gaussian\nperturbations. Thus, we conclude that Bayesian neural networks can be\nconsidered for detecting adversarial examples.\n"], ["2017-11-21", "http://arxiv.org/abs/1711.08001", "Reinforcing Adversarial Robustness using Model Confidence Induced by Adversarial Training.", ["Xi Wu", " Uyeong Jang", " Jiefeng Chen", " Lingjiao Chen", " Somesh Jha"], "  In this paper we study leveraging confidence information induced by\nadversarial training to reinforce adversarial robustness of a given\nadversarially trained model. A natural measure of confidence is $\\|F({\\bf\nx})\\|_\\infty$ (i.e. how confident $F$ is about its prediction?). We start by\nanalyzing an adversarial training formulation proposed by Madry et al.. We\ndemonstrate that, under a variety of instantiations, an only somewhat good\nsolution to their objective induces confidence to be a discriminator, which can\ndistinguish between right and wrong model predictions in a neighborhood of a\npoint sampled from the underlying distribution. Based on this, we propose\nHighly Confident Near Neighbor (${\\tt HCNN}$), a framework that combines\nconfidence information and nearest neighbor search, to reinforce adversarial\nrobustness of a base model. We give algorithms in this framework and perform a\ndetailed empirical study. We report encouraging experimental results that\nsupport our analysis, and also discuss problems we observed with existing\nadversarial training.\n"], ["2017-11-20", "http://arxiv.org/abs/1711.07356", "Evaluating Robustness of Neural Networks with Mixed Integer Programming.", ["Vincent Tjeng", " Kai Xiao", " Russ Tedrake"], "  Neural networks have demonstrated considerable success on a wide variety of\nreal-world problems. However, networks trained only to optimize for training\naccuracy can often be fooled by adversarial examples - slightly perturbed\ninputs that are misclassified with high confidence. Verification of networks\nenables us to gauge their vulnerability to such adversarial examples. We\nformulate verification of piecewise-linear neural networks as a mixed integer\nprogram. On a representative task of finding minimum adversarial distortions,\nour verifier is two to three orders of magnitude quicker than the\nstate-of-the-art. We achieve this computational speedup via tight formulations\nfor non-linearities, as well as a novel presolve algorithm that makes full use\nof all information available. The computational speedup allows us to verify\nproperties on convolutional networks with an order of magnitude more ReLUs than\nnetworks previously verified by any complete verifier. In particular, we\ndetermine for the first time the exact adversarial accuracy of an MNIST\nclassifier to perturbations with bounded $l_\\infty$ norm $\\epsilon=0.1$: for\nthis classifier, we find an adversarial example for 4.38% of samples, and a\ncertificate of robustness (to perturbations with bounded norm) for the\nremainder. Across all robust training procedures and network architectures\nconsidered, we are able to certify more samples than the state-of-the-art and\nfind more adversarial examples than a strong first-order attack.\n"], ["2017-11-20", "http://arxiv.org/abs/1711.07183", "Adversarial Attacks Beyond the Image Space.", ["Xiaohui Zeng", " Chenxi Liu", " Yu-Siang Wang", " Weichao Qiu", " Lingxi Xie", " Yu-Wing Tai", " Chi Keung Tang", " Alan L. Yuille"], "  Generating adversarial examples is an intriguing problem and an important way\nof understanding the working mechanism of deep neural networks. Most existing\napproaches generated perturbations in the image space, i.e., each pixel can be\nmodified independently. However, in this paper we pay special attention to the\nsubset of adversarial examples that correspond to meaningful changes in 3D\nphysical properties (like rotation and translation, illumination condition,\netc.). These adversaries arguably pose a more serious concern, as they\ndemonstrate the possibility of causing neural network failure by easy\nperturbations of real-world 3D objects and scenes.\n  In the contexts of object classification and visual question answering, we\naugment state-of-the-art deep neural networks that receive 2D input images with\na rendering module (either differentiable or not) in front, so that a 3D scene\n(in the physical space) is rendered into a 2D image (in the image space), and\nthen mapped to a prediction (in the output space). The adversarial\nperturbations can now go beyond the image space, and have clear meanings in the\n3D physical world. Though image-space adversaries can be interpreted as\nper-pixel albedo change, we verify that they cannot be well explained along\nthese physically meaningful dimensions, which often have a non-local effect.\nBut it is still possible to successfully attack beyond the image space on the\nphysical space, though this is more difficult than image-space attacks,\nreflected in lower success rates and heavier perturbations required.\n"], ["2017-11-17", "http://arxiv.org/abs/1711.06598", "How Wrong Am I? - Studying Adversarial Examples and their Impact on Uncertainty in Gaussian Process Machine Learning Models.", ["Kathrin Grosse", " David Pfaff", " Michael Thomas Smith", " Michael Backes"], "  Machine learning models are vulnerable to Adversarial Examples: minor\nperturbations to input samples intended to deliberately cause\nmisclassification. Current defenses against adversarial examples, especially\nfor Deep Neural Networks (DNN), are primarily derived from empirical\ndevelopments, and their security guarantees are often only justified\nretroactively. Many defenses therefore rely on hidden assumptions that are\nsubsequently subverted by increasingly elaborate attacks. This is not\nsurprising: deep learning notoriously lacks a comprehensive mathematical\nframework to provide meaningful guarantees.\n  In this paper, we leverage Gaussian Processes to investigate adversarial\nexamples in the framework of Bayesian inference. Across different models and\ndatasets, we find deviating levels of uncertainty reflect the perturbation\nintroduced to benign samples by state-of-the-art attacks, including novel\nwhite-box attacks on Gaussian Processes. Our experiments demonstrate that even\nunoptimized uncertainty thresholds already reject adversarial examples in many\nscenarios.\n  Comment: Thresholds can be broken in a modified attack, which was done in\narXiv:1812.02606 (The limitations of model uncertainty in adversarial\nsettings).\n"], ["2017-11-16", "http://arxiv.org/abs/1711.05934", "Enhanced Attacks on Defensively Distilled Deep Neural Networks.", ["Yujia Liu", " Weiming Zhang", " Shaohua Li", " Nenghai Yu"], "  Deep neural networks (DNNs) have achieved tremendous success in many tasks of\nmachine learning, such as the image classification. Unfortunately, researchers\nhave shown that DNNs are easily attacked by adversarial examples, slightly\nperturbed images which can mislead DNNs to give incorrect classification\nresults. Such attack has seriously hampered the deployment of DNN systems in\nareas where security or safety requirements are strict, such as autonomous\ncars, face recognition, malware detection. Defensive distillation is a\nmechanism aimed at training a robust DNN which significantly reduces the\neffectiveness of adversarial examples generation. However, the state-of-the-art\nattack can be successful on distilled networks with 100% probability. But it is\na white-box attack which needs to know the inner information of DNN. Whereas,\nthe black-box scenario is more general. In this paper, we first propose the\nepsilon-neighborhood attack, which can fool the defensively distilled networks\nwith 100% success rate in the white-box setting, and it is fast to generate\nadversarial examples with good visual quality. On the basis of this attack, we\nfurther propose the region-based attack against defensively distilled DNNs in\nthe black-box setting. And we also perform the bypass attack to indirectly\nbreak the distillation defense as a complementary method. The experimental\nresults show that our black-box attacks have a considerable success rate on\ndefensively distilled networks.\n"], ["2017-11-16", "http://arxiv.org/abs/1711.05929", "Defense against Universal Adversarial Perturbations.", ["Naveed Akhtar", " Jian Liu", " Ajmal Mian"], "  Recent advances in Deep Learning show the existence of image-agnostic\nquasi-imperceptible perturbations that when applied to `any' image can fool a\nstate-of-the-art network classifier to change its prediction about the image\nlabel. These `Universal Adversarial Perturbations' pose a serious threat to the\nsuccess of Deep Learning in practice. We present the first dedicated framework\nto effectively defend the networks against such perturbations. Our approach\nlearns a Perturbation Rectifying Network (PRN) as `pre-input' layers to a\ntargeted model, such that the targeted model needs no modification. The PRN is\nlearned from real and synthetic image-agnostic perturbations, where an\nefficient method to compute the latter is also proposed. A perturbation\ndetector is separately trained on the Discrete Cosine Transform of the\ninput-output difference of the PRN. A query image is first passed through the\nPRN and verified by the detector. If a perturbation is detected, the output of\nthe PRN is used for label prediction instead of the actual image. A rigorous\nevaluation shows that our framework can defend the network classifiers against\nunseen adversarial perturbations in the real-world scenarios with up to 97.5%\nsuccess rate. The PRN also generalizes well in the sense that training for one\ntargeted network defends another network with a comparable success rate.\n"], ["2017-11-15", "http://arxiv.org/abs/1711.05475", "The best defense is a good offense: Countering black box attacks by predicting slightly wrong labels.", ["Yannic Kilcher", " Thomas Hofmann"], "  Black-Box attacks on machine learning models occur when an attacker, despite\nhaving no access to the inner workings of a model, can successfully craft an\nattack by means of model theft. The attacker will train an own substitute model\nthat mimics the model to be attacked. The substitute can then be used to design\nattacks against the original model, for example by means of adversarial\nsamples. We put ourselves in the shoes of the defender and present a method\nthat can successfully avoid model theft by mounting a counter-attack.\nSpecifically, to any incoming query, we slightly perturb our output label\ndistribution in a way that makes substitute training infeasible. We demonstrate\nthat the perturbation does not affect the ordinary use of our model, but\nresults in an effective defense against attacks based on model theft.\n"], ["2017-11-12", "http://arxiv.org/abs/1711.04368", "Machine vs Machine: Minimax-Optimal Defense Against Adversarial Examples.", ["Jihun Hamm", " Akshay Mehra"], "  Recently, researchers have discovered that the state-of-the-art object\nclassifiers can be fooled easily by small perturbations in the input\nunnoticeable to human eyes. It is also known that an attacker can generate\nstrong adversarial examples if she knows the classifier parameters. Conversely,\na defender can robustify the classifier by retraining if she has access to the\nadversarial examples. We explain and formulate this adversarial example problem\nas a two-player continuous zero-sum game, and demonstrate the fallacy of\nevaluating a defense or an attack as a static problem. To find the best\nworst-case defense against whitebox attacks, we propose a continuous minimax\noptimization algorithm. We demonstrate the minimax defense with two types of\nattack classes -- gradient-based and neural network-based attacks. Experiments\nwith the MNIST and the CIFAR-10 datasets demonstrate that the defense found by\nnumerical minimax optimization is indeed more robust than non-minimax defenses.\nWe discuss directions for improving the result toward achieving robustness\nagainst multiple types of attack classes.\n"], ["2017-11-09", "http://arxiv.org/abs/1711.03280", "Crafting Adversarial Examples For Speech Paralinguistics Applications.", ["Yuan Gong", " Christian Poellabauer"], "  Computational paralinguistic analysis is increasingly being used in a wide\nrange of cyber applications, including security-sensitive applications such as\nspeaker verification, deceptive speech detection, and medical diagnostics.\nWhile state-of-the-art machine learning techniques, such as deep neural\nnetworks, can provide robust and accurate speech analysis, they are susceptible\nto adversarial attacks. In this work, we propose an end-to-end scheme to\ngenerate adversarial examples for computational paralinguistic applications by\nperturbing directly the raw waveform of an audio recording rather than specific\nacoustic features. Our experiments show that the proposed adversarial\nperturbation can lead to a significant performance drop of state-of-the-art\ndeep neural networks, while only minimally impairing the audio quality.\n"], ["2017-11-08", "http://arxiv.org/abs/1711.02846", "Intriguing Properties of Adversarial Examples.", ["Ekin D. Cubuk", " Barret Zoph", " Samuel S. Schoenholz", " Quoc V. Le"], "  It is becoming increasingly clear that many machine learning classifiers are\nvulnerable to adversarial examples. In attempting to explain the origin of\nadversarial examples, previous studies have typically focused on the fact that\nneural networks operate on high dimensional data, they overfit, or they are too\nlinear. Here we argue that the origin of adversarial examples is primarily due\nto an inherent uncertainty that neural networks have about their predictions.\nWe show that the functional form of this uncertainty is independent of\narchitecture, dataset, and training protocol; and depends only on the\nstatistics of the logit differences of the network, which do not change\nsignificantly during training. This leads to adversarial error having a\nuniversal scaling, as a power-law, with respect to the size of the adversarial\nperturbation. We show that this universality holds for a broad range of\ndatasets (MNIST, CIFAR10, ImageNet, and random data), models (including\nstate-of-the-art deep networks, linear models, adversarially trained networks,\nand networks trained on randomly shuffled labels), and attacks (FGSM, step\nl.l., PGD). Motivated by these results, we study the effects of reducing\nprediction entropy on adversarial robustness. Finally, we study the effect of\nnetwork architectures on adversarial sensitivity. To do this, we use neural\narchitecture search with reinforcement learning to find adversarially robust\narchitectures on CIFAR10. Our resulting architecture is more robust to white\n\\emph{and} black box attacks compared to previous attempts.\n"], ["2017-11-06", "http://arxiv.org/abs/1711.01991", "Mitigating Adversarial Effects Through Randomization.", ["Cihang Xie", " Jianyu Wang", " Zhishuai Zhang", " Zhou Ren", " Alan Yuille"], "  Convolutional neural networks have demonstrated high accuracy on various\ntasks in recent years. However, they are extremely vulnerable to adversarial\nexamples. For example, imperceptible perturbations added to clean images can\ncause convolutional neural networks to fail. In this paper, we propose to\nutilize randomization at inference time to mitigate adversarial effects.\nSpecifically, we use two randomization operations: random resizing, which\nresizes the input images to a random size, and random padding, which pads zeros\naround the input images in a random manner. Extensive experiments demonstrate\nthat the proposed randomization method is very effective at defending against\nboth single-step and iterative attacks. Our method provides the following\nadvantages: 1) no additional training or fine-tuning, 2) very few additional\ncomputations, 3) compatible with other adversarial defense methods. By\ncombining the proposed randomization method with an adversarially trained\nmodel, it achieves a normalized score of 0.924 (ranked No.2 among 107 defense\nteams) in the NIPS 2017 adversarial examples defense challenge, which is far\nbetter than using adversarial training alone with a normalized score of 0.773\n(ranked No.56). The code is public available at\nhttps://github.com/cihangxie/NIPS2017_adv_challenge_defense.\n"], ["2017-11-06", "http://arxiv.org/abs/1711.01791", "HyperNetworks with statistical filtering for defending adversarial examples.", ["Zhun Sun", " Mete Ozay", " Takayuki Okatani"], "  Deep learning algorithms have been known to be vulnerable to adversarial\nperturbations in various tasks such as image classification. This problem was\naddressed by employing several defense methods for detection and rejection of\nparticular types of attacks. However, training and manipulating networks\naccording to particular defense schemes increases computational complexity of\nthe learning algorithms. In this work, we propose a simple yet effective method\nto improve robustness of convolutional neural networks (CNNs) to adversarial\nattacks by using data dependent adaptive convolution kernels. To this end, we\npropose a new type of HyperNetwork in order to employ statistical properties of\ninput data and features for computation of statistical adaptive maps. Then, we\nfilter convolution weights of CNNs with the learned statistical maps to compute\ndynamic kernels. Thereby, weights and kernels are collectively optimized for\nlearning of image classification models robust to adversarial attacks without\nemployment of additional target detection and rejection algorithms. We\nempirically demonstrate that the proposed method enables CNNs to spontaneously\ndefend against different types of attacks, e.g. attacks generated by Gaussian\nnoise, fast gradient sign methods (Goodfellow et al., 2014) and a black-box\nattack(Narodytska & Kasiviswanathan, 2016).\n"], ["2017-11-06", "http://arxiv.org/abs/1711.01768", "Towards Reverse-Engineering Black-Box Neural Networks.", ["Seong Joon Oh", " Max Augustin", " Bernt Schiele", " Mario Fritz"], "  Many deployed learned models are black boxes: given input, returns output.\nInternal information about the model, such as the architecture, optimisation\nprocedure, or training data, is not disclosed explicitly as it might contain\nproprietary information or make the system more vulnerable. This work shows\nthat such attributes of neural networks can be exposed from a sequence of\nqueries. This has multiple implications. On the one hand, our work exposes the\nvulnerability of black-box neural networks to different types of attacks -- we\nshow that the revealed internal information helps generate more effective\nadversarial examples against the black box model. On the other hand, this\ntechnique can be used for better protection of private content from automatic\nrecognition models using adversarial examples. Our paper suggests that it is\nactually hard to draw a line between white box and black box models.\n"], ["2017-11-02", "http://arxiv.org/abs/1711.00867", "The (Un)reliability of saliency methods.", ["Pieter-Jan Kindermans", " Sara Hooker", " Julius Adebayo", " Maximilian Alber", " Kristof T. Sch\u00fctt", " Sven D\u00e4hne", " Dumitru Erhan", " Been Kim"], "  Saliency methods aim to explain the predictions of deep neural networks.\nThese methods lack reliability when the explanation is sensitive to factors\nthat do not contribute to the model prediction. We use a simple and common\npre-processing step ---adding a constant shift to the input data--- to show\nthat a transformation with no effect on the model can cause numerous methods to\nincorrectly attribute. In order to guarantee reliability, we posit that methods\nshould fulfill input invariance, the requirement that a saliency method mirror\nthe sensitivity of the model with respect to transformations of the input. We\nshow, through several examples, that saliency methods that do not satisfy input\ninvariance result in misleading attribution.\n"], ["2017-11-02", "http://arxiv.org/abs/1711.00851", "Provable defenses against adversarial examples via the convex outer adversarial polytope.", ["Eric Wong", " J. Zico Kolter"], "  We propose a method to learn deep ReLU-based classifiers that are provably\nrobust against norm-bounded adversarial perturbations on the training data. For\npreviously unseen examples, the approach is guaranteed to detect all\nadversarial examples, though it may flag some non-adversarial examples as well.\nThe basic idea is to consider a convex outer approximation of the set of\nactivations reachable through a norm-bounded perturbation, and we develop a\nrobust optimization procedure that minimizes the worst case loss over this\nouter region (via a linear program). Crucially, we show that the dual problem\nto this linear program can be represented itself as a deep network similar to\nthe backpropagation network, leading to very efficient optimization approaches\nthat produce guaranteed bounds on the robust loss. The end result is that by\nexecuting a few more forward and backward passes through a slightly modified\nversion of the original network (though possibly with much larger batch sizes),\nwe can learn a classifier that is provably robust to any norm-bounded\nadversarial attack. We illustrate the approach on a number of tasks to train\nclassifiers with robust adversarial guarantees (e.g. for MNIST, we produce a\nconvolutional classifier that provably has less than 5.8% test error for any\nadversarial attack with bounded $\\ell_\\infty$ norm less than $\\epsilon = 0.1$),\nand code for all experiments in the paper is available at\nhttps://github.com/locuslab/convex_adversarial.\n"], ["2017-11-01", "http://arxiv.org/abs/1711.00449", "Attacking Binarized Neural Networks.", ["Angus Galloway", " Graham W. Taylor", " Medhat Moussa"], "  Neural networks with low-precision weights and activations offer compelling\nefficiency advantages over their full-precision equivalents. The two most\nfrequently discussed benefits of quantization are reduced memory consumption,\nand a faster forward pass when implemented with efficient bitwise operations.\nWe propose a third benefit of very low-precision neural networks: improved\nrobustness against some adversarial attacks, and in the worst case, performance\nthat is on par with full-precision models. We focus on the very low-precision\ncase where weights and activations are both quantized to $\\pm$1, and note that\nstochastically quantizing weights in just one layer can sharply reduce the\nimpact of iterative attacks. We observe that non-scaled binary neural networks\nexhibit a similar effect to the original defensive distillation procedure that\nled to gradient masking, and a false notion of security. We address this by\nconducting both black-box and white-box experiments with binary models that do\nnot artificially mask gradients.\n"], ["2017-10-31", "http://arxiv.org/abs/1711.00117", "Countering Adversarial Images using Input Transformations.", ["Chuan Guo", " Mayank Rana", " Moustapha Cisse", " der Maaten Laurens van"], "  This paper investigates strategies that defend against adversarial-example\nattacks on image-classification systems by transforming the inputs before\nfeeding them to the system. Specifically, we study applying image\ntransformations such as bit-depth reduction, JPEG compression, total variance\nminimization, and image quilting before feeding the image to a convolutional\nnetwork classifier. Our experiments on ImageNet show that total variance\nminimization and image quilting are very effective defenses in practice, in\nparticular, when the network is trained on transformed images. The strength of\nthose defenses lies in their non-differentiable nature and their inherent\nrandomness, which makes it difficult for an adversary to circumvent the\ndefenses. Our best defense eliminates 60% of strong gray-box and 90% of strong\nblack-box attacks by a variety of major attack methods\n"], ["2017-10-31", "http://arxiv.org/abs/1710.11469", "Conditional Variance Penalties and Domain Shift Robustness.", ["Christina Heinze-Deml", " Nicolai Meinshausen"], "  When training a deep neural network for image classification, one can broadly\ndistinguish between two types of latent features of images that will drive the\nclassification. We can divide latent features into (i) \"core\" or \"conditionally\ninvariant\" features $X^\\text{core}$ whose distribution $X^\\text{core}\\vert Y$,\nconditional on the class $Y$, does not change substantially across domains and\n(ii) \"style\" features $X^{\\text{style}}$ whose distribution $X^{\\text{style}}\n\\vert Y$ can change substantially across domains. Examples for style features\ninclude position, rotation, image quality or brightness but also more complex\nones like hair color, image quality or posture for images of persons. Our goal\nis to minimize a loss that is robust under changes in the distribution of these\nstyle features. In contrast to previous work, we assume that the domain itself\nis not observed and hence a latent variable.\n  We do assume that we can sometimes observe a typically discrete identifier or\n\"$\\mathrm{ID}$ variable\". In some applications we know, for example, that two\nimages show the same person, and $\\mathrm{ID}$ then refers to the identity of\nthe person. The proposed method requires only a small fraction of images to\nhave $\\mathrm{ID}$ information. We group observations if they share the same\nclass and identifier $(Y,\\mathrm{ID})=(y,\\mathrm{id})$ and penalize the\nconditional variance of the prediction or the loss if we condition on\n$(Y,\\mathrm{ID})$. Using a causal framework, this conditional variance\nregularization (CoRe) is shown to protect asymptotically against shifts in the\ndistribution of the style variables. Empirically, we show that the CoRe penalty\nimproves predictive accuracy substantially in settings where domain changes\noccur in terms of image quality, brightness and color while we also look at\nmore complex changes such as changes in movement and posture.\n"], ["2017-10-31", "http://arxiv.org/abs/1710.11342", "Generating Natural Adversarial Examples.", ["Zhengli Zhao", " Dheeru Dua", " Sameer Singh"], "  Due to their complex nature, it is hard to characterize the ways in which\nmachine learning models can misbehave or be exploited when deployed. Recent\nwork on adversarial examples, i.e. inputs with minor perturbations that result\nin substantially different model predictions, is helpful in evaluating the\nrobustness of these models by exposing the adversarial scenarios where they\nfail. However, these malicious perturbations are often unnatural, not\nsemantically meaningful, and not applicable to complicated domains such as\nlanguage. In this paper, we propose a framework to generate natural and legible\nadversarial examples that lie on the data manifold, by searching in semantic\nspace of dense and continuous data representation, utilizing the recent\nadvances in generative adversarial networks. We present generated adversaries\nto demonstrate the potential of the proposed approach for black-box classifiers\nfor a wide range of applications such as image classification, textual\nentailment, and machine translation. We include experiments to show that the\ngenerated adversaries are natural, legible to humans, and useful in evaluating\nand analyzing black-box classifiers.\n"], ["2017-10-30", "http://arxiv.org/abs/1710.10766", "PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples.", ["Yang Song", " Taesup Kim", " Sebastian Nowozin", " Stefano Ermon", " Nate Kushman"], "  Adversarial perturbations of normal images are usually imperceptible to\nhumans, but they can seriously confuse state-of-the-art machine learning\nmodels. What makes them so special in the eyes of image classifiers? In this\npaper, we show empirically that adversarial examples mainly lie in the low\nprobability regions of the training distribution, regardless of attack types\nand targeted models. Using statistical hypothesis testing, we find that modern\nneural density models are surprisingly good at detecting imperceptible image\nperturbations. Based on this discovery, we devised PixelDefend, a new approach\nthat purifies a maliciously perturbed image by moving it back towards the\ndistribution seen in the training data. The purified image is then run through\nan unmodified classifier, making our method agnostic to both the classifier and\nthe attacking method. As a result, PixelDefend can be used to protect already\ndeployed models and be combined with other model-specific defenses. Experiments\nshow that our method greatly improves resilience across a wide variety of\nstate-of-the-art attacking methods, increasing accuracy on the strongest attack\nfrom 63% to 84% for Fashion MNIST and from 32% to 70% for CIFAR-10.\n"], ["2017-10-29", "http://arxiv.org/abs/1710.10733", "Attacking the Madry Defense Model with $L_1$-based Adversarial Examples.", ["Yash Sharma", " Pin-Yu Chen"], "  The Madry Lab recently hosted a competition designed to test the robustness\nof their adversarially trained MNIST model. Attacks were constrained to perturb\neach pixel of the input image by a scaled maximal $L_\\infty$ distortion\n$\\epsilon$ = 0.3. This discourages the use of attacks which are not optimized\non the $L_\\infty$ distortion metric. Our experimental results demonstrate that\nby relaxing the $L_\\infty$ constraint of the competition, the elastic-net\nattack to deep neural networks (EAD) can generate transferable adversarial\nexamples which, despite their high average $L_\\infty$ distortion, have minimal\nvisual distortion. These results call into question the use of $L_\\infty$ as a\nsole measure for visual distortion, and further demonstrate the power of EAD at\ngenerating robust adversarial examples.\n"], ["2017-10-29", "http://arxiv.org/abs/1710.10571", "Certifying Some Distributional Robustness with Principled Adversarial Training.", ["Aman Sinha", " Hongseok Namkoong", " John Duchi"], "  Neural networks are vulnerable to adversarial examples and researchers have\nproposed many heuristic attack and defense mechanisms. We address this problem\nthrough the principled lens of distributionally robust optimization, which\nguarantees performance under adversarial input perturbations. By considering a\nLagrangian penalty formulation of perturbing the underlying data distribution\nin a Wasserstein ball, we provide a training procedure that augments model\nparameter updates with worst-case perturbations of training data. For smooth\nlosses, our procedure provably achieves moderate levels of robustness with\nlittle computational or statistical cost relative to empirical risk\nminimization. Furthermore, our statistical guarantees allow us to efficiently\ncertify robustness for the population loss. For imperceptible perturbations,\nour method matches or outperforms heuristic approaches.\n"], ["2017-10-28", "http://arxiv.org/abs/1710.10547", "Interpretation of Neural Networks is Fragile.", ["Amirata Ghorbani", " Abubakar Abid", " James Zou"], "  In order for machine learning to be deployed and trusted in many\napplications, it is crucial to be able to reliably explain why the machine\nlearning algorithm makes certain predictions. For example, if an algorithm\nclassifies a given pathology image to be a malignant tumor, then the doctor may\nneed to know which parts of the image led the algorithm to this classification.\nHow to interpret black-box predictors is thus an important and active area of\nresearch. A fundamental question is: how much can we trust the interpretation\nitself? In this paper, we show that interpretation of deep learning predictions\nis extremely fragile in the following sense: two perceptively indistinguishable\ninputs with the same predicted label can be assigned very different\ninterpretations. We systematically characterize the fragility of several\nwidely-used feature-importance interpretation methods (saliency maps, relevance\npropagation, and DeepLIFT) on ImageNet and CIFAR-10. Our experiments show that\neven small random perturbation can change the feature importance and new\nsystematic perturbations can lead to dramatically different interpretations\nwithout changing the label. We extend these results to show that\ninterpretations based on exemplars (e.g. influence functions) are similarly\nfragile. Our analysis of the geometry of the Hessian matrix gives insight on\nwhy fragility could be a fundamental challenge to the current interpretation\napproaches.\n"], ["2017-10-27", "http://arxiv.org/abs/1710.10225", "Adversarial Detection of Flash Malware: Limitations and Open Issues.", ["Davide Maiorca", " Battista Biggio", " Maria Elena Chiappe", " Giorgio Giacinto"], "  During the past two years, Flash malware has become one of the most insidious\nthreats to detect, with almost 600 critical vulnerabilities targeting Adobe\nFlash Player disclosed in the wild. Research has shown that machine learning\ncan be successfully used to tackle this increasing variability and\nsophistication of Flash malware, by simply leveraging static analysis to\nextract information from the structure of the file or from its bytecode.\nHowever, the robustness of such systems against well-crafted evasion attempts -\nalso known as adversarial examples - has never been investigated. In this\npaper, we first discuss how to craft adversarial Flash malware examples, and\nshow that it suffices to only slightly manipulate them to evade detection. We\nthen empirically demonstrate that popular defense techniques proposed to\nmitigate such threat, including re-training on adversarial examples, may not\nalways be effective. We argue that this occurs when the feature vectors\nextracted from adversarial examples become indistinguishable from those of\nbenign data, meaning that the given feature representation is intrinsically\nvulnerable. In this respect, we are the first to formally define and\nquantitatively characterize this vulnerability, highlighting when an attack can\nbe countered by solely improving the security of the learning algorithm, or\nwhen it requires also considering additional features. We conclude the paper by\nsuggesting alternative research directions to improve the security of\nlearning-based Flash malware detectors.\n"], ["2017-10-25", "http://arxiv.org/abs/1710.09412", "mixup: Beyond Empirical Risk Minimization.", ["Hongyi Zhang", " Moustapha Cisse", " Yann N. Dauphin", " David Lopez-Paz"], "  Large deep neural networks are powerful, but exhibit undesirable behaviors\nsuch as memorization and sensitivity to adversarial examples. In this work, we\npropose mixup, a simple learning principle to alleviate these issues. In\nessence, mixup trains a neural network on convex combinations of pairs of\nexamples and their labels. By doing so, mixup regularizes the neural network to\nfavor simple linear behavior in-between training examples. Our experiments on\nthe ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show\nthat mixup improves the generalization of state-of-the-art neural network\narchitectures. We also find that mixup reduces the memorization of corrupt\nlabels, increases the robustness to adversarial examples, and stabilizes the\ntraining of generative adversarial networks.\n"], ["2017-10-24", "http://arxiv.org/abs/1710.08864", "One pixel attack for fooling deep neural networks.", ["Jiawei Su", " Danilo Vasconcellos Vargas", " Sakurai Kouichi"], "  Recent research has revealed that the output of Deep Neural Networks (DNN)\ncan be easily altered by adding relatively small perturbations to the input\nvector. In this paper, we analyze an attack in an extremely limited scenario\nwhere only one pixel can be modified. For that we propose a novel method for\ngenerating one-pixel adversarial perturbations based on differential evolution\n(DE). It requires less adversarial information (a black-box attack) and can\nfool more types of networks due to the inherent features of DE. The results\nshow that 67.97% of the natural images in Kaggle CIFAR-10 test dataset and\n16.04% of the ImageNet (ILSVRC 2012) test images can be perturbed to at least\none target class by modifying just one pixel with 74.03% and 22.91% confidence\non average. We also show the same vulnerability on the original CIFAR-10\ndataset. Thus, the proposed attack explores a different take on adversarial\nmachine learning in an extreme limited scenario, showing that current DNNs are\nalso vulnerable to such low dimension attacks. Besides, we also illustrate an\nimportant application of DE (or broadly speaking, evolutionary computation) in\nthe domain of adversarial machine learning: creating tools that can effectively\ngenerate low-cost adversarial attacks against neural networks for evaluating\nrobustness.\n"], ["2017-10-21", "http://arxiv.org/abs/1710.07859", "Feature-Guided Black-Box Safety Testing of Deep Neural Networks.", ["Matthew Wicker", " Xiaowei Huang", " Marta Kwiatkowska"], "  Despite the improved accuracy of deep neural networks, the discovery of\nadversarial examples has raised serious safety concerns. Most existing\napproaches for crafting adversarial examples necessitate some knowledge\n(architecture, parameters, etc.) of the network at hand. In this paper, we\nfocus on image classifiers and propose a feature-guided black-box approach to\ntest the safety of deep neural networks that requires no such knowledge. Our\nalgorithm employs object detection techniques such as SIFT (Scale Invariant\nFeature Transform) to extract features from an image. These features are\nconverted into a mutable saliency distribution, where high probability is\nassigned to pixels that affect the composition of the image with respect to the\nhuman visual system. We formulate the crafting of adversarial examples as a\ntwo-player turn-based stochastic game, where the first player's objective is to\nminimise the distance to an adversarial example by manipulating the features,\nand the second player can be cooperative, adversarial, or random. We show that,\ntheoretically, the two-player game can con- verge to the optimal strategy, and\nthat the optimal strategy represents a globally minimal adversarial image. For\nLipschitz networks, we also identify conditions that provide safety guarantees\nthat no adversarial examples exist. Using Monte Carlo tree search we gradually\nexplore the game state space to search for adversarial examples. Our\nexperiments show that, despite the black-box setting, manipulations guided by a\nperception-based saliency distribution are competitive with state-of-the-art\nmethods that rely on white-box saliency matrices or sophisticated optimization\nprocedures. Finally, we show how our method can be used to evaluate robustness\nof neural networks in safety-critical applications such as traffic sign\nrecognition in self-driving cars.\n"], ["2017-10-17", "http://arxiv.org/abs/1710.06081", "Boosting Adversarial Attacks with Momentum.", ["Yinpeng Dong", " Fangzhou Liao", " Tianyu Pang", " Hang Su", " Jun Zhu", " Xiaolin Hu", " Jianguo Li"], "  Deep neural networks are vulnerable to adversarial examples, which poses\nsecurity concerns on these algorithms due to the potentially severe\nconsequences. Adversarial attacks serve as an important surrogate to evaluate\nthe robustness of deep learning models before they are deployed. However, most\nof existing adversarial attacks can only fool a black-box model with a low\nsuccess rate. To address this issue, we propose a broad class of momentum-based\niterative algorithms to boost adversarial attacks. By integrating the momentum\nterm into the iterative process for attacks, our methods can stabilize update\ndirections and escape from poor local maxima during the iterations, resulting\nin more transferable adversarial examples. To further improve the success rates\nfor black-box attacks, we apply momentum iterative algorithms to an ensemble of\nmodels, and show that the adversarially trained models with a strong defense\nability are also vulnerable to our black-box attacks. We hope that the proposed\nmethods will serve as a benchmark for evaluating the robustness of various deep\nmodels and defense methods. With this method, we won the first places in NIPS\n2017 Non-targeted Adversarial Attack and Targeted Adversarial Attack\ncompetitions.\n"], ["2017-10-12", "http://arxiv.org/abs/1710.04677", "Game-Theoretic Design of Secure and Resilient Distributed Support Vector Machines with Adversaries.", ["Rui Zhang", " Quanyan Zhu"], "  With a large number of sensors and control units in networked systems,\ndistributed support vector machines (DSVMs) play a fundamental role in scalable\nand efficient multi-sensor classification and prediction tasks. However, DSVMs\nare vulnerable to adversaries who can modify and generate data to deceive the\nsystem to misclassification and misprediction. This work aims to design defense\nstrategies for DSVM learner against a potential adversary. We establish a\ngame-theoretic framework to capture the conflicting interests between the DSVM\nlearner and the attacker. The Nash equilibrium of the game allows predicting\nthe outcome of learning algorithms in adversarial environments, and enhancing\nthe resilience of the machine learning through dynamic distributed learning\nalgorithms. We show that the DSVM learner is less vulnerable when he uses a\nbalanced network with fewer nodes and higher degree. We also show that adding\nmore training samples is an efficient defense strategy against an attacker. We\npresent secure and resilient DSVM algorithms with verification method and\nrejection method, and show their resiliency against adversary with numerical\nexperiments.\n"], ["2017-10-09", "http://arxiv.org/abs/1710.03337", "Standard detectors aren't (currently) fooled by physical adversarial stop signs.", ["Jiajun Lu", " Hussein Sibai", " Evan Fabry", " David Forsyth"], "  An adversarial example is an example that has been adjusted to produce the\nwrong label when presented to a system at test time. If adversarial examples\nexisted that could fool a detector, they could be used to (for example) wreak\nhavoc on roads populated with smart vehicles. Recently, we described our\ndifficulties creating physical adversarial stop signs that fool a detector.\nMore recently, Evtimov et al. produced a physical adversarial stop sign that\nfools a proxy model of a detector. In this paper, we show that these physical\nadversarial stop signs do not fool two standard detectors (YOLO and Faster\nRCNN) in standard configuration. Evtimov et al.'s construction relies on a crop\nof the image to the stop sign; this crop is then resized and presented to a\nclassifier. We argue that the cropping and resizing procedure largely\neliminates the effects of rescaling and of view angle. Whether an adversarial\nattack is robust under rescaling and change of view direction remains moot. We\nargue that attacking a classifier is very different from attacking a detector,\nand that the structure of detectors - which must search for their own bounding\nbox, and which cannot estimate that box very accurately - likely makes it\ndifficult to make adversarial patterns. Finally, an adversarial pattern on a\nphysical object that could fool a detector would have to be adversarial in the\nface of a wide family of parametric distortions (scale; view angle; box shift\ninside the detector; illumination; and so on). Such a pattern would be of great\ntheoretical and practical interest. There is currently no evidence that such\npatterns exist.\n"], ["2017-10-09", "http://arxiv.org/abs/1710.03107", "Verification of Binarized Neural Networks via Inter-Neuron Factoring.", ["Chih-Hong Cheng", " Georg N\u00fchrenberg", " Chung-Hao Huang", " Harald Ruess"], "  We study the problem of formal verification of Binarized Neural Networks\n(BNN), which have recently been proposed as a energy-efficient alternative to\ntraditional learning networks. The verification of BNNs, using the reduction to\nhardware verification, can be even more scalable by factoring computations\namong neurons within the same layer. By proving the NP-hardness of finding\noptimal factoring as well as the hardness of PTAS approximability, we design\npolynomial-time search heuristics to generate factoring solutions. The overall\nframework allows applying verification techniques to moderately-sized BNNs for\nembedded devices with thousands of neurons and inputs.\n"], ["2017-10-02", "http://arxiv.org/abs/1710.00814", "Detecting Adversarial Attacks on Neural Network Policies with Visual Foresight.", ["Yen-Chen Lin", " Ming-Yu Liu", " Min Sun", " Jia-Bin Huang"], "  Deep reinforcement learning has shown promising results in learning control\npolicies for complex sequential decision-making tasks. However, these neural\nnetwork-based policies are known to be vulnerable to adversarial examples. This\nvulnerability poses a potentially serious threat to safety-critical systems\nsuch as autonomous vehicles. In this paper, we propose a defense mechanism to\ndefend reinforcement learning agents from adversarial attacks by leveraging an\naction-conditioned frame prediction module. Our core idea is that the\nadversarial examples targeting at a neural network-based policy are not\neffective for the frame prediction model. By comparing the action distribution\nproduced by a policy from processing the current observed frame to the action\ndistribution produced by the same policy from processing the predicted frame\nfrom the action-conditioned frame prediction module, we can detect the presence\nof adversarial examples. Beyond detecting the presence of adversarial examples,\nour method allows the agent to continue performing the task using the predicted\nframe when the agent is under attack. We evaluate the performance of our\nalgorithm using five games in Atari 2600. Our results demonstrate that the\nproposed defense mechanism achieves favorable performance against baseline\nalgorithms in detecting adversarial examples and in earning rewards when the\nagents are under attack.\n"], ["2017-10-02", "http://arxiv.org/abs/1710.00486", "DeepSafe: A Data-driven Approach for Checking Adversarial Robustness in Neural Networks.", ["Divya Gopinath", " Guy Katz", " Corina S. Pasareanu", " Clark Barrett"], "  Deep neural networks have become widely used, obtaining remarkable results in\ndomains such as computer vision, speech recognition, natural language\nprocessing, audio recognition, social network filtering, machine translation,\nand bio-informatics, where they have produced results comparable to human\nexperts. However, these networks can be easily fooled by adversarial\nperturbations: minimal changes to correctly-classified inputs, that cause the\nnetwork to mis-classify them. This phenomenon represents a concern for both\nsafety and security, but it is currently unclear how to measure a network's\nrobustness against such perturbations. Existing techniques are limited to\nchecking robustness around a few individual input points, providing only very\nlimited guarantees. We propose a novel approach for automatically identifying\nsafe regions of the input space, within which the network is robust against\nadversarial perturbations. The approach is data-guided, relying on clustering\nto identify well-defined geometric regions as candidate safe regions. We then\nutilize verification techniques to confirm that these regions are safe or to\nprovide counter-examples showing that they are not safe. We also introduce the\nnotion of targeted robustness which, for a given target label and region,\nensures that a NN does not map any input in the region to the target label. We\nevaluated our technique on the MNIST dataset and on a neural network\nimplementation of a controller for the next-generation Airborne Collision\nAvoidance System for unmanned aircraft (ACAS Xu). For these networks, our\napproach identified multiple regions which were completely safe as well as some\nwhich were only safe for specific labels. It also discovered several\nadversarial perturbations of interest.\n"], ["2017-09-28", "http://arxiv.org/abs/1709.10207", "Provably Minimally-Distorted Adversarial Examples.", ["Nicholas Carlini", " Guy Katz", " Clark Barrett", " David L. Dill"], "  The ability to deploy neural networks in real-world, safety-critical systems\nis severely limited by the presence of adversarial examples: slightly perturbed\ninputs that are misclassified by the network. In recent years, several\ntechniques have been proposed for increasing robustness to adversarial examples\n--- and yet most of these have been quickly shown to be vulnerable to future\nattacks. For example, over half of the defenses proposed by papers accepted at\nICLR 2018 have already been broken. We propose to address this difficulty\nthrough formal verification techniques. We show how to construct provably\nminimally distorted adversarial examples: given an arbitrary neural network and\ninput sample, we can construct adversarial examples which we prove are of\nminimal distortion. Using this approach, we demonstrate that one of the recent\nICLR defense proposals, adversarial retraining, provably succeeds at increasing\nthe distortion required to construct adversarial examples by a factor of 4.2.\n"], ["2017-09-28", "http://arxiv.org/abs/1709.09917", "DR.SGX: Hardening SGX Enclaves against Cache Attacks with Data Location Randomization.", ["Ferdinand Technische Universit\u00e4t Darmstadt, Germany Brasser", " Srdjan ETH Zurich, Switzerland Capkun", " Alexandra University of W\u00fcrzburg Dmitrienko", " Tommaso Technische Universit\u00e4t Darmstadt, Germany Frassetto", " Kari ETH Zurich, Switzerland Kostiainen", " Ahmad-Reza Technische Universit\u00e4t Darmstadt, Germany Sadeghi"], "  Recent research has demonstrated that Intel's SGX is vulnerable to\nsoftware-based side-channel attacks. In a common attack, the adversary monitors\nCPU caches to infer secret-dependent data accesses patterns. Known defenses\nhave major limitations, as they require either error-prone developer\nassistance, incur extremely high runtime overhead, or prevent only specific\nattacks. In this paper, we propose data location randomization as a novel\ndefense against side-channel attacks that target data access patterns. Our goal\nis to break the link between the memory observations by the adversary and the\nactual data accesses by the victim. We design and implement a compiler-based\ntool called DR.SGX that instruments the enclave code, permuting data locations\nat fine granularity. To prevent correlation of repeated memory accesses we\nperiodically re-randomize all enclave data. Our solution requires no developer\nassistance and strikes the balance between side-channel protection and\nperformance based on an adjustable security parameter.\n"], ["2017-09-26", "http://arxiv.org/abs/1709.09130", "Output Range Analysis for Deep Neural Networks.", ["Souradeep Dutta", " Susmit Jha", " Sriram Sanakaranarayanan", " Ashish Tiwari"], "  Deep neural networks (NN) are extensively used for machine learning tasks\nsuch as image classification, perception and control of autonomous systems.\nIncreasingly, these deep NNs are also been deployed in high-assurance\napplications. Thus, there is a pressing need for developing techniques to\nverify neural networks to check whether certain user-expected properties are\nsatisfied. In this paper, we study a specific verification problem of computing\na guaranteed range for the output of a deep neural network given a set of\ninputs represented as a convex polyhedron. Range estimation is a key primitive\nfor verifying deep NNs. We present an efficient range estimation algorithm that\nuses a combination of local search and linear programming problems to\nefficiently find the maximum and minimum values taken by the outputs of the NN\nover the given input set. In contrast to recently proposed \"monolithic\"\noptimization approaches, we use local gradient descent to repeatedly find and\neliminate local minima of the function. The final global optimum is certified\nusing a mixed integer programming instance. We implement our approach and\ncompare it with Reluplex, a recently proposed solver for deep neural networks.\nWe demonstrate the effectiveness of the proposed approach for verification of\nNNs used in automated control as well as those used in classification.\n"], ["2017-09-25", "http://arxiv.org/abs/1709.08693", "Fooling Vision and Language Models Despite Localization and Attention Mechanism.", ["Xiaojun Xu", " Xinyun Chen", " Chang Liu", " Anna Rohrbach", " Trevor Darrell", " Dawn Song"], "  Adversarial attacks are known to succeed on classifiers, but it has been an\nopen question whether more complex vision systems are vulnerable. In this\npaper, we study adversarial examples for vision and language models, which\nincorporate natural language understanding and complex structures such as\nattention, localization, and modular architectures. In particular, we\ninvestigate attacks on a dense captioning model and on two visual question\nanswering (VQA) models. Our evaluation shows that we can generate adversarial\nexamples with a high success rate (i.e., > 90%) for these models. Our work\nsheds new light on understanding adversarial attacks on vision systems which\nhave a language component and shows that attention, bounding box localization,\nand compositional internal structures are vulnerable to adversarial attacks.\nThese observations will inform future work towards building effective defenses.\n"], ["2017-09-19", "http://arxiv.org/abs/1709.06662", "Verifying Properties of Binarized Deep Neural Networks.", ["Nina Narodytska", " Shiva Prasad Kasiviswanathan", " Leonid Ryzhyk", " Mooly Sagiv", " Toby Walsh"], "  Understanding properties of deep neural networks is an important challenge in\ndeep learning. In this paper, we take a step in this direction by proposing a\nrigorous way of verifying properties of a popular class of neural networks,\nBinarized Neural Networks, using the well-developed means of Boolean\nsatisfiability. Our main contribution is a construction that creates a\nrepresentation of a binarized neural network as a Boolean formula. Our encoding\nis the first exact Boolean representation of a deep neural network. Using this\nencoding, we leverage the power of modern SAT solvers along with a proposed\ncounterexample-guided search procedure to verify various properties of these\nnetworks. A particular focus will be on the critical property of robustness to\nadversarial perturbations. For this property, our experimental results\ndemonstrate that our approach scales to medium-size deep neural networks used\nin image classification tasks. To the best of our knowledge, this is the first\nwork on verifying properties of deep neural networks using an exact Boolean\nencoding of the network.\n"], ["2017-09-16", "http://arxiv.org/abs/1709.05583", "Mitigating Evasion Attacks to Deep Neural Networks via Region-based Classification.", ["Xiaoyu Cao", " Neil Zhenqiang Gong"], "  Deep neural networks (DNNs) have transformed several artificial intelligence\nresearch areas including computer vision, speech recognition, and natural\nlanguage processing. However, recent studies demonstrated that DNNs are\nvulnerable to adversarial manipulations at testing time. Specifically, suppose\nwe have a testing example, whose label can be correctly predicted by a DNN\nclassifier. An attacker can add a small carefully crafted noise to the testing\nexample such that the DNN classifier predicts an incorrect label, where the\ncrafted testing example is called adversarial example. Such attacks are called\nevasion attacks. Evasion attacks are one of the biggest challenges for\ndeploying DNNs in safety and security critical applications such as\nself-driving cars. In this work, we develop new methods to defend against\nevasion attacks. Our key observation is that adversarial examples are close to\nthe classification boundary. Therefore, we propose region-based classification\nto be robust to adversarial examples. For a benign/adversarial testing example,\nwe ensemble information in a hypercube centered at the example to predict its\nlabel. In contrast, traditional classifiers are point-based classification,\ni.e., given a testing example, the classifier predicts its label based on the\ntesting example alone. Our evaluation results on MNIST and CIFAR-10 datasets\ndemonstrate that our region-based classification can significantly mitigate\nevasion attacks without sacrificing classification accuracy on benign examples.\nSpecifically, our region-based classification achieves the same classification\naccuracy on testing benign examples as point-based classification, but our\nregion-based classification is significantly more robust than point-based\nclassification to various evasion attacks.\n"], ["2017-09-13", "http://arxiv.org/abs/1709.04447", "A Learning and Masking Approach to Secure Learning.", ["Linh Nguyen", " Sky Wang", " Arunesh Sinha"], "  Deep Neural Networks (DNNs) have been shown to be vulnerable against\nadversarial examples, which are data points cleverly constructed to fool the\nclassifier. Such attacks can be devastating in practice, especially as DNNs are\nbeing applied to ever increasing critical tasks like image recognition in\nautonomous driving. In this paper, we introduce a new perspective on the\nproblem. We do so by first defining robustness of a classifier to adversarial\nexploitation. Next, we show that the problem of adversarial example generation\ncan be posed as learning problem. We also categorize attacks in literature into\nhigh and low perturbation attacks; well-known attacks like fast-gradient sign\nmethod (FGSM) and our attack produce higher perturbation adversarial examples\nwhile the more potent but computationally inefficient Carlini-Wagner (CW)\nattack is low perturbation. Next, we show that the dual approach of the attack\nlearning problem can be used as a defensive technique that is effective against\nhigh perturbation attacks. Finally, we show that a classifier masking method\nachieved by adding noise to the a neural network's logit output protects\nagainst low distortion attacks such as the CW attack. We also show that both\nour learning and masking defense can work simultaneously to protect against\nmultiple attacks. We demonstrate the efficacy of our techniques by\nexperimenting with the MNIST and CIFAR-10 datasets.\n"], ["2017-09-13", "http://arxiv.org/abs/1709.04137", "Models and Framework for Adversarial Attacks on Complex Adaptive Systems.", ["Vahid Behzadan", " Arslan Munir"], "  We introduce the paradigm of adversarial attacks that target the dynamics of\nComplex Adaptive Systems (CAS). To facilitate the analysis of such attacks, we\npresent multiple approaches to the modeling of CAS as dynamical, data-driven,\nand game-theoretic systems, and develop quantitative definitions of attack,\nvulnerability, and resilience in the context of CAS security. Furthermore, we\npropose a comprehensive set of schemes for classification of attacks and attack\nsurfaces in CAS, complemented with examples of practical attacks. Building on\nthis foundation, we propose a framework based on reinforcement learning for\nsimulation and analysis of attacks on CAS, and demonstrate its performance\nthrough three real-world case studies of targeting power grids, destabilization\nof terrorist organizations, and manipulation of machine learning agents. We\nalso discuss potential mitigation techniques, and remark on future research\ndirections in analysis and design of secure complex adaptive systems.\n"], ["2017-09-12", "http://arxiv.org/abs/1709.04114", "EAD: Elastic-Net Attacks to Deep Neural Networks via Adversarial Examples.", ["Pin-Yu Chen", " Yash Sharma", " Huan Zhang", " Jinfeng Yi", " Cho-Jui Hsieh"], "  Recent studies have highlighted the vulnerability of deep neural networks\n(DNNs) to adversarial examples - a visually indistinguishable adversarial image\ncan easily be crafted to cause a well-trained model to misclassify. Existing\nmethods for crafting adversarial examples are based on $L_2$ and $L_\\infty$\ndistortion metrics. However, despite the fact that $L_1$ distortion accounts\nfor the total variation and encourages sparsity in the perturbation, little has\nbeen developed for crafting $L_1$-based adversarial examples. In this paper, we\nformulate the process of attacking DNNs via adversarial examples as an\nelastic-net regularized optimization problem. Our elastic-net attacks to DNNs\n(EAD) feature $L_1$-oriented adversarial examples and include the\nstate-of-the-art $L_2$ attack as a special case. Experimental results on MNIST,\nCIFAR10 and ImageNet show that EAD can yield a distinct set of adversarial\nexamples with small $L_1$ distortion and attains similar attack performance to\nthe state-of-the-art methods in different attack scenarios. More importantly,\nEAD leads to improved attack transferability and complements adversarial\ntraining for DNNs, suggesting novel insights on leveraging $L_1$ distortion in\nadversarial machine learning and security implications of DNNs.\n"], ["2017-09-11", "http://arxiv.org/abs/1709.03582", "Art of singular vectors and universal adversarial perturbations.", ["Valentin Khrulkov", " Ivan Oseledets"], "  Vulnerability of Deep Neural Networks (DNNs) to adversarial attacks has been\nattracting a lot of attention in recent studies. It has been shown that for\nmany state of the art DNNs performing image classification there exist\nuniversal adversarial perturbations --- image-agnostic perturbations mere\naddition of which to natural images with high probability leads to their\nmisclassification. In this work we propose a new algorithm for constructing\nsuch universal perturbations. Our approach is based on computing the so-called\n$(p, q)$-singular vectors of the Jacobian matrices of hidden layers of a\nnetwork. Resulting perturbations present interesting visual patterns, and by\nusing only 64 images we were able to construct universal perturbations with\nmore than 60 \\% fooling rate on the dataset consisting of 50000 images. We also\ninvestigate a correlation between the maximal singular value of the Jacobian\nmatrix and the fooling rate of the corresponding singular vector, and show that\nthe constructed perturbations generalize across networks.\n"], ["2017-09-11", "http://arxiv.org/abs/1709.03423", "Ensemble Methods as a Defense to Adversarial Perturbations Against Deep Neural Networks.", ["Thilo Strauss", " Markus Hanselmann", " Andrej Junginger", " Holger Ulmer"], "  Deep learning has become the state of the art approach in many machine\nlearning problems such as classification. It has recently been shown that deep\nlearning is highly vulnerable to adversarial perturbations. Taking the camera\nsystems of self-driving cars as an example, small adversarial perturbations can\ncause the system to make errors in important tasks, such as classifying traffic\nsigns or detecting pedestrians. Hence, in order to use deep learning without\nsafety concerns a proper defense strategy is required. We propose to use\nensemble methods as a defense strategy against adversarial perturbations. We\nfind that an attack leading one model to misclassify does not imply the same\nfor other networks performing the same task. This makes ensemble methods an\nattractive defense strategy against adversarial attacks. We empirically show\nfor the MNIST and the CIFAR-10 data sets that ensemble methods not only improve\nthe accuracy of neural networks on test data but also increase their robustness\nagainst adversarial perturbations.\n"], ["2017-09-08", "http://arxiv.org/abs/1709.02802", "Towards Proving the Adversarial Robustness of Deep Neural Networks.", ["Guy Stanford University Katz", " Clark Stanford University Barrett", " David L. Stanford University Dill", " Kyle Stanford University Julian", " Mykel J. Stanford University Kochenderfer"], "  Autonomous vehicles are highly complex systems, required to function reliably\nin a wide variety of situations. Manually crafting software controllers for\nthese vehicles is difficult, but there has been some success in using deep\nneural networks generated using machine-learning. However, deep neural networks\nare opaque to human engineers, rendering their correctness very difficult to\nprove manually; and existing automated techniques, which were not designed to\noperate on neural networks, fail to scale to large systems. This paper focuses\non proving the adversarial robustness of deep neural networks, i.e. proving\nthat small perturbations to a correctly-classified input to the network cannot\ncause it to be misclassified. We describe some of our recent and ongoing work\non verifying the adversarial robustness of networks, and discuss some of the\nopen questions we have encountered and how they might be addressed.\n"], ["2017-09-08", "http://arxiv.org/abs/1709.02538", "DeepFense: Online Accelerated Defense Against Adversarial Deep Learning.", ["Bita Darvish Rouhani", " Mohammad Samragh", " Mojan Javaheripi", " Tara Javidi", " Farinaz Koushanfar"], "  Recent advances in adversarial Deep Learning (DL) have opened up a largely\nunexplored surface for malicious attacks jeopardizing the integrity of\nautonomous DL systems. With the wide-spread usage of DL in critical and\ntime-sensitive applications, including unmanned vehicles, drones, and video\nsurveillance systems, online detection of malicious inputs is of utmost\nimportance. We propose DeepFense, the first end-to-end automated framework that\nsimultaneously enables efficient and safe execution of DL models. DeepFense\nformalizes the goal of thwarting adversarial attacks as an optimization problem\nthat minimizes the rarely observed regions in the latent feature space spanned\nby a DL network. To solve the aforementioned minimization problem, a set of\ncomplementary but disjoint modular redundancies are trained to validate the\nlegitimacy of the input samples in parallel with the victim DL model. DeepFense\nleverages hardware/software/algorithm co-design and customized acceleration to\nachieve just-in-time performance in resource-constrained settings. The proposed\ncountermeasure is unsupervised, meaning that no adversarial sample is leveraged\nto train modular redundancies. We further provide an accompanying API to reduce\nthe non-recurring engineering cost and ensure automated adaptation to various\nplatforms. Extensive evaluations on FPGAs and GPUs demonstrate up to two orders\nof magnitude performance improvement while enabling online adversarial sample\ndetection.\n"], ["2017-09-02", "http://arxiv.org/abs/1709.00609", "Security Evaluation of Pattern Classifiers under Attack.", ["Battista Biggio", " Giorgio Fumera", " Fabio Roli"], "  Pattern classification systems are commonly used in adversarial applications,\nlike biometric authentication, network intrusion detection, and spam filtering,\nin which data can be purposely manipulated by humans to undermine their\noperation. As this adversarial scenario is not taken into account by classical\ndesign methods, pattern classification systems may exhibit vulnerabilities,\nwhose exploitation may severely affect their performance, and consequently\nlimit their practical utility. Extending pattern classification theory and\ndesign methods to adversarial settings is thus a novel and very relevant\nresearch direction, which has not yet been pursued in a systematic way. In this\npaper, we address one of the main open issues: evaluating at design phase the\nsecurity of pattern classifiers, namely, the performance degradation under\npotential attacks they may incur during operation. We propose a framework for\nempirical evaluation of classifier security that formalizes and generalizes the\nmain ideas proposed in the literature, and give examples of its use in three\nreal applications. Reported results show that security evaluation can provide a\nmore complete understanding of the classifier's behavior in adversarial\nenvironments, and lead to better design choices.\n"], ["2017-08-31", "http://arxiv.org/abs/1709.00045", "On Security and Sparsity of Linear Classifiers for Adversarial Settings.", ["Ambra Demontis", " Paolo Russu", " Battista Biggio", " Giorgio Fumera", " Fabio Roli"], "  Machine-learning techniques are widely used in security-related applications,\nlike spam and malware detection. However, in such settings, they have been\nshown to be vulnerable to adversarial attacks, including the deliberate\nmanipulation of data at test time to evade detection. In this work, we focus on\nthe vulnerability of linear classifiers to evasion attacks. This can be\nconsidered a relevant problem, as linear classifiers have been increasingly\nused in embedded systems and mobile devices for their low processing time and\nmemory requirements. We exploit recent findings in robust optimization to\ninvestigate the link between regularization and security of linear classifiers,\ndepending on the type of attack. We also analyze the relationship between the\nsparsity of feature weights, which is desirable for reducing processing cost,\nand the security of linear classifiers. We further propose a novel octagonal\nregularizer that allows us to achieve a proper trade-off between them. Finally,\nwe empirically show how this regularizer can improve classifier security and\nsparsity in real-world application examples including spam and malware\ndetection.\n"], ["2017-08-31", "http://arxiv.org/abs/1708.09790", "Be Selfish and Avoid Dilemmas: Fork After Withholding (FAW) Attacks on Bitcoin.", ["Yujin Kwon", " Dohyun Kim", " Yunmok Son", " Eugene Vasserman", " Yongdae Kim"], "  In the Bitcoin system, participants are rewarded for solving cryptographic\npuzzles. In order to receive more consistent rewards over time, some\nparticipants organize mining pools and split the rewards from the pool in\nproportion to each participant's contribution. However, several attacks\nthreaten the ability to participate in pools. The block withholding (BWH)\nattack makes the pool reward system unfair by letting malicious participants\nreceive unearned wages while only pretending to contribute work. When two pools\nlaunch BWH attacks against each other, they encounter the miner's dilemma: in a\nNash equilibrium, the revenue of both pools is diminished. In another attack\ncalled selfish mining, an attacker can unfairly earn extra rewards by\ndeliberately generating forks. In this paper, we propose a novel attack called\na fork after withholding (FAW) attack. FAW is not just another attack. The\nreward for an FAW attacker is always equal to or greater than that for a BWH\nattacker, and it is usable up to four times more often per pool than in BWH\nattack. When considering multiple pools - the current state of the Bitcoin\nnetwork - the extra reward for an FAW attack is about 56% more than that for a\nBWH attack. Furthermore, when two pools execute FAW attacks on each other, the\nminer's dilemma may not hold: under certain circumstances, the larger pool can\nconsistently win. More importantly, an FAW attack, while using intentional\nforks, does not suffer from practicality issues, unlike selfish mining. We also\ndiscuss partial countermeasures against the FAW attack, but finding a cheap and\nefficient countermeasure remains an open problem. As a result, we expect to see\nFAW attacks among mining pools.\n"], ["2017-08-29", "http://arxiv.org/abs/1708.09056", "Practical Attacks Against Graph-based Clustering.", ["Yizheng Chen", " Yacin Nadji", " Athanasios Kountouras", " Fabian Monrose", " Roberto Perdisci", " Manos Antonakakis", " Nikolaos Vasiloglou"], "  Graph modeling allows numerous security problems to be tackled in a general\nway, however, little work has been done to understand their ability to\nwithstand adversarial attacks. We design and evaluate two novel graph attacks\nagainst a state-of-the-art network-level, graph-based detection system. Our\nwork highlights areas in adversarial machine learning that have not yet been\naddressed, specifically: graph-based clustering techniques, and a global\nfeature space where realistic attackers without perfect knowledge must be\naccounted for (by the defenders) in order to be practical. Even though less\ninformed attackers can evade graph clustering with low cost, we show that some\npractical defenses are possible.\n"], ["2017-08-28", "http://arxiv.org/abs/1708.08559", "DeepTest: Automated Testing of Deep-Neural-Network-driven Autonomous Cars.", ["Yuchi Tian", " Kexin Pei", " Suman Jana", " Baishakhi Ray"], "  Recent advances in Deep Neural Networks (DNNs) have led to the development of\nDNN-driven autonomous cars that, using sensors like camera, LiDAR, etc., can\ndrive without any human intervention. Most major manufacturers including Tesla,\nGM, Ford, BMW, and Waymo/Google are working on building and testing different\ntypes of autonomous vehicles. The lawmakers of several US states including\nCalifornia, Texas, and New York have passed new legislation to fast-track the\nprocess of testing and deployment of autonomous vehicles on their roads.\n  However, despite their spectacular progress, DNNs, just like traditional\nsoftware, often demonstrate incorrect or unexpected corner case behaviors that\ncan lead to potentially fatal collisions. Several such real-world accidents\ninvolving autonomous cars have already happened including one which resulted in\na fatality. Most existing testing techniques for DNN-driven vehicles are\nheavily dependent on the manual collection of test data under different driving\nconditions which become prohibitively expensive as the number of test\nconditions increases.\n  In this paper, we design, implement and evaluate DeepTest, a systematic\ntesting tool for automatically detecting erroneous behaviors of DNN-driven\nvehicles that can potentially lead to fatal crashes. First, our tool is\ndesigned to automatically generated test cases leveraging real-world changes in\ndriving conditions like rain, fog, lighting conditions, etc. DeepTest\nsystematically explores different parts of the DNN logic by generating test\ninputs that maximize the numbers of activated neurons. DeepTest found thousands\nof erroneous behaviors under different realistic driving conditions (e.g.,\nblurring, rain, fog, etc.) many of which lead to potentially fatal crashes in\nthree top performing DNNs in the Udacity self-driving car challenge.\n"], ["2017-08-28", "http://arxiv.org/abs/1708.08327", "Improving Robustness of ML Classifiers against Realizable Evasion Attacks Using Conserved Features.", ["Liang Tong", " Bo Li", " Chen Hajaj", " Chaowei Xiao", " Ning Zhang", " Yevgeniy Vorobeychik"], "  Machine learning (ML) techniques are increasingly common in security\napplications, such as malware and intrusion detection. However, ML models are\noften susceptible to evasion attacks, in which an adversary makes changes to\nthe input (such as malware) in order to avoid being detected. A conventional\napproach to evaluate ML robustness to such attacks, as well as to design robust\nML, is by considering simplified feature-space models of attacks, where the\nattacker changes ML features directly to effect evasion, while minimizing or\nconstraining the magnitude of this change. We investigate the effectiveness of\nthis approach to designing robust ML in the face of attacks that can be\nrealized in actual malware (realizable attacks). We demonstrate that in the\ncontext of structure-based PDF malware detection, such techniques appear to\nhave limited effectiveness, but they are effective with content-based\ndetectors. In either case, we show that augmenting the feature space models\nwith conserved features (those that cannot be unilaterally modified without\ncompromising malicious functionality) significantly improves performance.\nFinally, we show that feature space models enable generalized robustness when\nfaced with a variety of realizable attacks, as compared to classifiers which\nare tuned to be robust to a specific realizable attack.\n"], ["2017-08-23", "http://arxiv.org/abs/1708.06939", "Is Deep Learning Safe for Robot Vision? Adversarial Examples against the iCub Humanoid.", ["Marco Melis", " Ambra Demontis", " Battista Biggio", " Gavin Brown", " Giorgio Fumera", " Fabio Roli"], "  Deep neural networks have been widely adopted in recent years, exhibiting\nimpressive performances in several application domains. It has however been\nshown that they can be fooled by adversarial examples, i.e., images altered by\na barely-perceivable adversarial noise, carefully crafted to mislead\nclassification. In this work, we aim to evaluate the extent to which\nrobot-vision systems embodying deep-learning algorithms are vulnerable to\nadversarial examples, and propose a computationally efficient countermeasure to\nmitigate this threat, based on rejecting classification of anomalous inputs. We\nthen provide a clearer understanding of the safety properties of deep networks\nthrough an intuitive empirical analysis, showing that the mapping learned by\nsuch networks essentially violates the smoothness assumption of learning\nalgorithms. We finally discuss the main limitations of this work, including the\ncreation of real-world adversarial examples, and sketch promising research\ndirections.\n"], ["2017-08-22", "http://arxiv.org/abs/1708.06670", "CNN Fixations: An unraveling approach to visualize the discriminative image regions.", ["Konda Reddy Mopuri", " Utsav Garg", " R. Venkatesh Babu"], "  Deep convolutional neural networks (CNN) have revolutionized various fields\nof vision research and have seen unprecedented adoption for multiple tasks such\nas classification, detection, captioning, etc. However, they offer little\ntransparency into their inner workings and are often treated as black boxes\nthat deliver excellent performance. In this work, we aim at alleviating this\nopaqueness of CNNs by providing visual explanations for the network's\npredictions. Our approach can analyze variety of CNN based models trained for\nvision applications such as object recognition and caption generation. Unlike\nexisting methods, we achieve this via unraveling the forward pass operation.\nProposed method exploits feature dependencies across the layer hierarchy and\nuncovers the discriminative image locations that guide the network's\npredictions. We name these locations CNN-Fixations, loosely analogous to human\neye fixations.\n  Our approach is a generic method that requires no architectural changes,\nadditional training or gradient computation and computes the important image\nlocations (CNN Fixations). We demonstrate through a variety of applications\nthat our approach is able to localize the discriminative image locations across\ndifferent network architectures, diverse vision tasks and data modalities.\n"], ["2017-08-21", "http://arxiv.org/abs/1708.06131", "Evasion Attacks against Machine Learning at Test Time.", ["Battista Biggio", " Igino Corona", " Davide Maiorca", " Blaine Nelson", " Nedim Srndic", " Pavel Laskov", " Giorgio Giacinto", " Fabio Roli"], "  In security-sensitive applications, the success of machine learning depends\non a thorough vetting of their resistance to adversarial data. In one\npertinent, well-motivated attack scenario, an adversary may attempt to evade a\ndeployed system at test time by carefully manipulating attack samples. In this\nwork, we present a simple but effective gradient-based approach that can be\nexploited to systematically assess the security of several, widely-used\nclassification algorithms against evasion attacks. Following a recently\nproposed framework for security evaluation, we simulate attack scenarios that\nexhibit different risk levels for the classifier by increasing the attacker's\nknowledge of the system and her ability to manipulate attack samples. This\ngives the classifier designer a better picture of the classifier performance\nunder evasion attacks, and allows him to perform a more informed model\nselection (or parameter setting). We evaluate our approach on the relevant\nsecurity task of malware detection in PDF files, and show that such systems can\nbe easily evaded. We also sketch some countermeasures suggested by our\nanalysis.\n"], ["2017-08-17", "http://arxiv.org/abs/1708.05493", "Towards Interpretable Deep Neural Networks by Leveraging Adversarial Examples.", ["Yinpeng Dong", " Hang Su", " Jun Zhu", " Fan Bao"], "  Deep neural networks (DNNs) have demonstrated impressive performance on a\nwide array of tasks, but they are usually considered opaque since internal\nstructure and learned parameters are not interpretable. In this paper, we\nre-examine the internal representations of DNNs using adversarial images, which\nare generated by an ensemble-optimization algorithm. We find that: (1) the\nneurons in DNNs do not truly detect semantic objects/parts, but respond to\nobjects/parts only as recurrent discriminative patches; (2) deep visual\nrepresentations are not robust distributed codes of visual concepts because the\nrepresentations of adversarial images are largely not consistent with those of\nreal images, although they have similar visual appearance, both of which are\ndifferent from previous findings. To further improve the interpretability of\nDNNs, we propose an adversarial training scheme with a consistent loss such\nthat the neurons are endowed with human-interpretable concepts. The induced\ninterpretable representations enable us to trace eventual outcomes back to\ninfluential neurons. Therefore, human users can know how the models make\npredictions, as well as when and why they make errors.\n"], ["2017-08-17", "http://arxiv.org/abs/1708.05207", "Learning Universal Adversarial Perturbations with Generative Models.", ["Jamie Hayes", " George Danezis"], "  Neural networks are known to be vulnerable to adversarial examples, inputs\nthat have been intentionally perturbed to remain visually similar to the source\ninput, but cause a misclassification. It was recently shown that given a\ndataset and classifier, there exists so called universal adversarial\nperturbations, a single perturbation that causes a misclassification when\napplied to any input. In this work, we introduce universal adversarial\nnetworks, a generative network that is capable of fooling a target classifier\nwhen it's generated output is added to a clean sample from a dataset. We show\nthat this technique improves on known universal adversarial attacks.\n"], ["2017-08-14", "http://arxiv.org/abs/1708.04301", "Attacking Automatic Video Analysis Algorithms: A Case Study of Google Cloud Video Intelligence API.", ["Hossein Hosseini", " Baicen Xiao", " Andrew Clark", " Radha Poovendran"], "  Due to the growth of video data on Internet, automatic video analysis has\ngained a lot of attention from academia as well as companies such as Facebook,\nTwitter and Google. In this paper, we examine the robustness of video analysis\nalgorithms in adversarial settings. Specifically, we propose targeted attacks\non two fundamental classes of video analysis algorithms, namely video\nclassification and shot detection. We show that an adversary can subtly\nmanipulate a video in such a way that a human observer would perceive the\ncontent of the original video, but the video analysis algorithm will return the\nadversary's desired outputs.\n  We then apply the attacks on the recently released Google Cloud Video\nIntelligence API. The API takes a video file and returns the video labels\n(objects within the video), shot changes (scene changes within the video) and\nshot labels (description of video events over time). Through experiments, we\nshow that the API generates video and shot labels by processing only the first\nframe of every second of the video. Hence, an adversary can deceive the API to\noutput only her desired video and shot labels by periodically inserting an\nimage into the video at the rate of one frame per second. We also show that the\npattern of shot changes returned by the API can be mostly recovered by an\nalgorithm that compares the histograms of consecutive frames. Based on our\nequivalent model, we develop a method for slightly modifying the video frames,\nin order to deceive the API into generating our desired pattern of shot\nchanges. We perform extensive experiments with different videos and show that\nour attacks are consistently successful across videos with different\ncharacteristics. At the end, we propose introducing randomness to video\nanalysis algorithms as a countermeasure to our attacks.\n"], ["2017-08-13", "http://arxiv.org/abs/1708.03999", "ZOO: Zeroth Order Optimization based Black-box Attacks to Deep Neural Networks without Training Substitute Models.", ["Pin-Yu Chen", " Huan Zhang", " Yash Sharma", " Jinfeng Yi", " Cho-Jui Hsieh"], "  Deep neural networks (DNNs) are one of the most prominent technologies of our\ntime, as they achieve state-of-the-art performance in many machine learning\ntasks, including but not limited to image classification, text mining, and\nspeech processing. However, recent research on DNNs has indicated\never-increasing concern on the robustness to adversarial examples, especially\nfor security-critical tasks such as traffic sign identification for autonomous\ndriving. Studies have unveiled the vulnerability of a well-trained DNN by\ndemonstrating the ability of generating barely noticeable (to both human and\nmachines) adversarial images that lead to misclassification. Furthermore,\nresearchers have shown that these adversarial images are highly transferable by\nsimply training and attacking a substitute model built upon the target model,\nknown as a black-box attack to DNNs.\n  Similar to the setting of training substitute models, in this paper we\npropose an effective black-box attack that also only has access to the input\n(images) and the output (confidence scores) of a targeted DNN. However,\ndifferent from leveraging attack transferability from substitute models, we\npropose zeroth order optimization (ZOO) based attacks to directly estimate the\ngradients of the targeted DNN for generating adversarial examples. We use\nzeroth order stochastic coordinate descent along with dimension reduction,\nhierarchical attack and importance sampling techniques to efficiently attack\nblack-box models. By exploiting zeroth order optimization, improved attacks to\nthe targeted DNN can be accomplished, sparing the need for training substitute\nmodels and avoiding the loss in attack transferability. Experimental results on\nMNIST, CIFAR10 and ImageNet show that the proposed ZOO attack is as effective\nas the state-of-the-art white-box attack and significantly outperforms existing\nblack-box attacks via substitute models.\n"], ["2017-08-08", "http://arxiv.org/abs/1708.02582", "Cascade Adversarial Machine Learning Regularized with a Unified Embedding.", ["Taesik Na", " Jong Hwan Ko", " Saibal Mukhopadhyay"], "  Injecting adversarial examples during training, known as adversarial\ntraining, can improve robustness against one-step attacks, but not for unknown\niterative attacks. To address this challenge, we first show iteratively\ngenerated adversarial images easily transfer between networks trained with the\nsame strategy. Inspired by this observation, we propose cascade adversarial\ntraining, which transfers the knowledge of the end results of adversarial\ntraining. We train a network from scratch by injecting iteratively generated\nadversarial images crafted from already defended networks in addition to\none-step adversarial images from the network being trained. We also propose to\nutilize embedding space for both classification and low-level (pixel-level)\nsimilarity learning to ignore unknown pixel level perturbation. During\ntraining, we inject adversarial images without replacing their corresponding\nclean images and penalize the distance between the two embeddings (clean and\nadversarial). Experimental results show that cascade adversarial training\ntogether with our proposed low-level similarity learning efficiently enhances\nthe robustness against iterative attacks, but at the expense of decreased\nrobustness against one-step attacks. We show that combining those two\ntechniques can also improve robustness under the worst case black box attack\nscenario.\n"], ["2017-08-04", "http://arxiv.org/abs/1708.01697", "Adversarial Robustness: Softmax versus Openmax.", ["Andras Rozsa", " Manuel G\u00fcnther", " Terrance E. Boult"], "  Deep neural networks (DNNs) provide state-of-the-art results on various tasks\nand are widely used in real world applications. However, it was discovered that\nmachine learning models, including the best performing DNNs, suffer from a\nfundamental problem: they can unexpectedly and confidently misclassify examples\nformed by slightly perturbing otherwise correctly recognized inputs. Various\napproaches have been developed for efficiently generating these so-called\nadversarial examples, but those mostly rely on ascending the gradient of loss.\nIn this paper, we introduce the novel logits optimized targeting system (LOTS)\nto directly manipulate deep features captured at the penultimate layer. Using\nLOTS, we analyze and compare the adversarial robustness of DNNs using the\ntraditional Softmax layer with Openmax, which was designed to provide open set\nrecognition by defining classes derived from deep representations, and is\nclaimed to be more robust to adversarial perturbations. We demonstrate that\nOpenmax provides less vulnerable systems than Softmax to traditional attacks,\nhowever, we show that it can be equally susceptible to more sophisticated\nadversarial generation techniques that directly work on deep representations.\n"], ["2017-08-01", "http://arxiv.org/abs/1708.00807", "Adversarial-Playground: A Visualization Suite Showing How Adversarial Examples Fool Deep Learning.", ["Andrew P. Norton", " Yanjun Qi"], "  Recent studies have shown that attackers can force deep learning models to\nmisclassify so-called \"adversarial examples\": maliciously generated images\nformed by making imperceptible modifications to pixel values. With growing\ninterest in deep learning for security applications, it is important for\nsecurity experts and users of machine learning to recognize how learning\nsystems may be attacked. Due to the complex nature of deep learning, it is\nchallenging to understand how deep models can be fooled by adversarial\nexamples. Thus, we present a web-based visualization tool,\nAdversarial-Playground, to demonstrate the efficacy of common adversarial\nmethods against a convolutional neural network (CNN) system.\nAdversarial-Playground is educational, modular and interactive. (1) It enables\nnon-experts to compare examples visually and to understand why an adversarial\nexample can fool a CNN-based image classifier. (2) It can help security experts\nexplore more vulnerability of deep learning as a software module. (3) Building\nan interactive visualization is challenging in this domain due to the large\nfeature space of image classification (generating adversarial examples is slow\nin general and visualizing images are costly). Through multiple novel design\nchoices, our tool can provide fast and accurate responses to user requests.\nEmpirically, we find that our client-server division strategy reduced the\nresponse time by an average of 1.5 seconds per sample. Our other innovation, a\nfaster variant of JSMA evasion algorithm, empirically performed twice as fast\nas JSMA and yet maintains a comparable evasion rate.\n  Project source code and data from our experiments available at:\nhttps://github.com/QData/AdversarialDNN-Playground\n"], ["2017-07-27", "http://arxiv.org/abs/1707.08945", "Robust Physical-World Attacks on Deep Learning Models.", ["Kevin Eykholt", " Ivan Evtimov", " Earlence Fernandes", " Bo Li", " Amir Rahmati", " Chaowei Xiao", " Atul Prakash", " Tadayoshi Kohno", " Dawn Song"], "  Recent studies show that the state-of-the-art deep neural networks (DNNs) are\nvulnerable to adversarial examples, resulting from small-magnitude\nperturbations added to the input. Given that that emerging physical systems are\nusing DNNs in safety-critical situations, adversarial examples could mislead\nthese systems and cause dangerous situations.Therefore, understanding\nadversarial examples in the physical world is an important step towards\ndeveloping resilient learning algorithms. We propose a general attack\nalgorithm,Robust Physical Perturbations (RP2), to generate robust visual\nadversarial perturbations under different physical conditions. Using the\nreal-world case of road sign classification, we show that adversarial examples\ngenerated using RP2 achieve high targeted misclassification rates against\nstandard-architecture road sign classifiers in the physical world under various\nenvironmental conditions, including viewpoints. Due to the current lack of a\nstandardized testing method, we propose a two-stage evaluation methodology for\nrobust physical adversarial examples consisting of lab and field tests. Using\nthis methodology, we evaluate the efficacy of physical adversarial\nmanipulations on real objects. Witha perturbation in the form of only black and\nwhite stickers,we attack a real stop sign, causing targeted misclassification\nin 100% of the images obtained in lab settings, and in 84.8%of the captured\nvideo frames obtained on a moving vehicle(field test) for the target\nclassifier.\n"], ["2017-07-24", "http://arxiv.org/abs/1707.07397", "Synthesizing Robust Adversarial Examples.", ["Anish Athalye", " Logan Engstrom", " Andrew Ilyas", " Kevin Kwok"], "  Standard methods for generating adversarial examples for neural networks do\nnot consistently fool neural network classifiers in the physical world due to a\ncombination of viewpoint shifts, camera noise, and other natural\ntransformations, limiting their relevance to real-world systems. We demonstrate\nthe existence of robust 3D adversarial objects, and we present the first\nalgorithm for synthesizing examples that are adversarial over a chosen\ndistribution of transformations. We synthesize two-dimensional adversarial\nimages that are robust to noise, distortion, and affine transformation. We\napply our algorithm to complex three-dimensional objects, using 3D-printing to\nmanufacture the first physical adversarial objects. Our results demonstrate the\nexistence of 3D adversarial objects in the physical world.\n"], ["2017-07-23", "http://arxiv.org/abs/1707.07328", "Adversarial Examples for Evaluating Reading Comprehension Systems.", ["Robin Jia", " Percy Liang"], "  Standard accuracy metrics indicate that reading comprehension systems are\nmaking rapid progress, but the extent to which these systems truly understand\nlanguage remains unclear. To reward systems with real language understanding\nabilities, we propose an adversarial evaluation scheme for the Stanford\nQuestion Answering Dataset (SQuAD). Our method tests whether systems can answer\nquestions about paragraphs that contain adversarially inserted sentences, which\nare automatically generated to distract computer systems without changing the\ncorrect answer or misleading humans. In this adversarial setting, the accuracy\nof sixteen published models drops from an average of $75\\%$ F1 score to $36\\%$;\nwhen the adversary is allowed to add ungrammatical sequences of words, average\naccuracy on four models decreases further to $7\\%$. We hope our insights will\nmotivate the development of new models that understand language more precisely.\n"], ["2017-07-21", "http://arxiv.org/abs/1707.07013", "Confidence estimation in Deep Neural networks via density modelling.", ["Akshayvarun Subramanya", " Suraj Srinivas", " R. Venkatesh Babu"], "  State-of-the-art Deep Neural Networks can be easily fooled into providing\nincorrect high-confidence predictions for images with small amounts of\nadversarial noise. Does this expose a flaw with deep neural networks, or do we\nsimply need a better way to estimate confidence? In this paper we consider the\nproblem of accurately estimating predictive confidence. We formulate this\nproblem as that of density modelling, and show how traditional methods such as\nsoftmax produce poor estimates. To address this issue, we propose a novel\nconfidence measure based on density modelling approaches. We test these\nmeasures on images distorted by blur, JPEG compression, random noise and\nadversarial noise. Experiments show that our confidence measure consistently\nshows reduced confidence scores in the presence of such distortions - a\nproperty which softmax often lacks.\n"], ["2017-07-20", "http://arxiv.org/abs/1707.06728", "Efficient Defenses Against Adversarial Attacks.", ["Valentina Zantedeschi", " Maria-Irina Nicolae", " Ambrish Rawat"], "  Following the recent adoption of deep neural networks (DNN) accross a wide\nrange of applications, adversarial attacks against these models have proven to\nbe an indisputable threat. Adversarial samples are crafted with a deliberate\nintention of undermining a system. In the case of DNNs, the lack of better\nunderstanding of their working has prevented the development of efficient\ndefenses. In this paper, we propose a new defense method based on practical\nobservations which is easy to integrate into models and performs better than\nstate-of-the-art defenses. Our proposed solution is meant to reinforce the\nstructure of a DNN, making its prediction more stable and less likely to be\nfooled by adversarial samples. We conduct an extensive experimental study\nproving the efficiency of our method against multiple attacks, comparing it to\nnumerous defenses, both in white-box and black-box setups. Additionally, the\nimplementation of our method brings almost no overhead to the training\nprocedure, while maintaining the prediction performance of the original model\non clean samples.\n"], ["2017-07-19", "http://arxiv.org/abs/1707.05970", "Generic Black-Box End-to-End Attack Against State of the Art API Call Based Malware Classifiers.", ["Ishai Rosenberg", " Asaf Shabtai", " Lior Rokach", " Yuval Elovici"], "  In this paper, we present a black-box attack against API call based machine\nlearning malware classifiers, focusing on generating adversarial sequences\ncombining API calls and static features (e.g., printable strings) that will be\nmisclassified by the classifier without affecting the malware functionality. We\nshow that this attack is effective against many classifiers due to the\ntransferability principle between RNN variants, feed forward DNNs, and\ntraditional machine learning classifiers such as SVM. We also implement GADGET,\na software framework to convert any malware binary to a binary undetected by\nmalware classifiers, using the proposed attack, without access to the malware\nsource code.\n"], ["2017-07-18", "http://arxiv.org/abs/1707.05572", "Fast Feature Fool: A data independent approach to universal adversarial perturbations.", ["Konda Reddy Mopuri", " Utsav Garg", " R. Venkatesh Babu"], "  State-of-the-art object recognition Convolutional Neural Networks (CNNs) are\nshown to be fooled by image agnostic perturbations, called universal\nadversarial perturbations. It is also observed that these perturbations\ngeneralize across multiple networks trained on the same target data. However,\nthese algorithms require training data on which the CNNs were trained and\ncompute adversarial perturbations via complex optimization. The fooling\nperformance of these approaches is directly proportional to the amount of\navailable training data. This makes them unsuitable for practical attacks since\nits unreasonable for an attacker to have access to the training data. In this\npaper, for the first time, we propose a novel data independent approach to\ngenerate image agnostic perturbations for a range of CNNs trained for object\nrecognition. We further show that these perturbations are transferable across\nmultiple network architectures trained either on same or different data. In the\nabsence of data, our method generates universal adversarial perturbations\nefficiently via fooling the features learned at multiple layers thereby causing\nCNNs to misclassify. Experiments demonstrate impressive fooling rates and\nsurprising transferability for the proposed universal perturbations generated\nwithout any training data.\n"], ["2017-07-18", "http://arxiv.org/abs/1707.05474", "APE-GAN: Adversarial Perturbation Elimination with GAN.", ["Shiwei Shen", " Guoqing Jin", " Ke Gao", " Yongdong Zhang"], "  Although neural networks could achieve state-of-the-art performance while\nrecongnizing images, they often suffer a tremendous defeat from adversarial\nexamples--inputs generated by utilizing imperceptible but intentional\nperturbation to clean samples from the datasets. How to defense against\nadversarial examples is an important problem which is well worth researching.\nSo far, very few methods have provided a significant defense to adversarial\nexamples. In this paper, a novel idea is proposed and an effective framework\nbased Generative Adversarial Nets named APE-GAN is implemented to defense\nagainst the adversarial examples. The experimental results on three benchmark\ndatasets including MNIST, CIFAR10 and ImageNet indicate that APE-GAN is\neffective to resist adversarial examples generated from five attacks.\n"], ["2017-07-17", "http://arxiv.org/abs/1707.05373", "Houdini: Fooling Deep Structured Prediction Models.", ["Moustapha Cisse", " Yossi Adi", " Natalia Neverova", " Joseph Keshet"], "  Generating adversarial examples is a critical step for evaluating and\nimproving the robustness of learning machines. So far, most existing methods\nonly work for classification and are not designed to alter the true performance\nmeasure of the problem at hand. We introduce a novel flexible approach named\nHoudini for generating adversarial examples specifically tailored for the final\nperformance measure of the task considered, be it combinatorial and\nnon-decomposable. We successfully apply Houdini to a range of applications such\nas speech recognition, pose estimation and semantic segmentation. In all cases,\nthe attacks based on Houdini achieve higher success rate than those based on\nthe traditional surrogates used to train the models while using a less\nperceptible adversarial perturbation.\n"], ["2017-07-13", "http://arxiv.org/abs/1707.04131", "Foolbox: A Python toolbox to benchmark the robustness of machine learning models.", ["Jonas Rauber", " Wieland Brendel", " Matthias Bethge"], "  Even todays most advanced machine learning models are easily fooled by almost\nimperceptible perturbations of their inputs. Foolbox is a new Python package to\ngenerate such adversarial perturbations and to quantify and compare the\nrobustness of machine learning models. It is build around the idea that the\nmost comparable robustness measure is the minimum perturbation needed to craft\nan adversarial example. To this end, Foolbox provides reference implementations\nof most published adversarial attack methods alongside some new ones, all of\nwhich perform internal hyperparameter tuning to find the minimum adversarial\nperturbation. Additionally, Foolbox interfaces with most popular deep learning\nframeworks such as PyTorch, Keras, TensorFlow, Theano and MXNet and allows\ndifferent adversarial criteria such as targeted misclassification and top-k\nmisclassification as well as different distance measures. The code is licensed\nunder the MIT license and is openly available at\nhttps://github.com/bethgelab/foolbox . The most up-to-date documentation can be\nfound at http://foolbox.readthedocs.io .\n"], ["2017-07-11", "http://arxiv.org/abs/1707.03501", "NO Need to Worry about Adversarial Examples in Object Detection in Autonomous Vehicles.", ["Jiajun Lu", " Hussein Sibai", " Evan Fabry", " David Forsyth"], "  It has been shown that most machine learning algorithms are susceptible to\nadversarial perturbations. Slightly perturbing an image in a carefully chosen\ndirection in the image space may cause a trained neural network model to\nmisclassify it. Recently, it was shown that physical adversarial examples\nexist: printing perturbed images then taking pictures of them would still\nresult in misclassification. This raises security and safety concerns.\n  However, these experiments ignore a crucial property of physical objects: the\ncamera can view objects from different distances and at different angles. In\nthis paper, we show experiments that suggest that current constructions of\nphysical adversarial examples do not disrupt object detection from a moving\nplatform. Instead, a trained neural network classifies most of the pictures\ntaken from different distances and angles of a perturbed image correctly. We\nbelieve this is because the adversarial property of the perturbation is\nsensitive to the scale at which the perturbed picture is viewed, so (for\nexample) an autonomous car will misclassify a stop sign only from a small range\nof distances.\n  Our work raises an important question: can one construct examples that are\nadversarial for many or most viewing conditions? If so, the construction should\noffer very significant insights into the internal representation of patterns by\ndeep networks. If not, there is a good prospect that adversarial examples can\nbe reduced to a curiosity with little practical impact.\n"], ["2017-07-11", "http://arxiv.org/abs/1707.03184", "A Survey on Resilient Machine Learning.", ["Atul Kumar", " Sameep Mehta"], "  Machine learning based system are increasingly being used for sensitive tasks\nsuch as security surveillance, guiding autonomous vehicle, taking investment\ndecisions, detecting and blocking network intrusion and malware etc. However,\nrecent research has shown that machine learning models are venerable to attacks\nby adversaries at all phases of machine learning (eg, training data collection,\ntraining, operation). All model classes of machine learning systems can be\nmisled by providing carefully crafted inputs making them wrongly classify\ninputs. Maliciously created input samples can affect the learning process of a\nML system by either slowing down the learning process, or affecting the\nperformance of the learned mode, or causing the system make error(s) only in\nattacker's planned scenario. Because of these developments, understanding\nsecurity of machine learning algorithms and systems is emerging as an important\nresearch area among computer security and machine learning researchers and\npractitioners. We present a survey of this emerging area in machine learning.\n"], ["2017-07-10", "http://arxiv.org/abs/1707.02812", "Towards Crafting Text Adversarial Samples.", ["Suranjana Samanta", " Sameep Mehta"], "  Adversarial samples are strategically modified samples, which are crafted\nwith the purpose of fooling a classifier at hand. An attacker introduces\nspecially crafted adversarial samples to a deployed classifier, which are being\nmis-classified by the classifier. However, the samples are perceived to be\ndrawn from entirely different classes and thus it becomes hard to detect the\nadversarial samples. Most of the prior works have been focused on synthesizing\nadversarial samples in the image domain. In this paper, we propose a new method\nof crafting adversarial text samples by modification of the original samples.\nModifications of the original text samples are done by deleting or replacing\nthe important or salient words in the text or by introducing new words in the\ntext sample. Our algorithm works best for the datasets which have\nsub-categories within each of the classes of examples. While crafting\nadversarial samples, one of the key constraint is to generate meaningful\nsentences which can at pass off as legitimate from language (English)\nviewpoint. Experimental results on IMDB movie review dataset for sentiment\nanalysis and Twitter dataset for gender detection show the efficiency of our\nproposed method.\n"], ["2017-07-04", "http://arxiv.org/abs/1707.01159", "UPSET and ANGRI : Breaking High Performance Image Classifiers.", ["Sayantan Sarkar", " Ankan Bansal", " Upal Mahbub", " Rama Chellappa"], "  In this paper, targeted fooling of high performance image classifiers is\nachieved by developing two novel attack methods. The first method generates\nuniversal perturbations for target classes and the second generates image\nspecific perturbations. Extensive experiments are conducted on MNIST and\nCIFAR10 datasets to provide insights about the proposed algorithms and show\ntheir effectiveness.\n"], ["2017-06-21", "http://arxiv.org/abs/1706.06969", "Comparing deep neural networks against humans: object recognition when the signal gets weaker.", ["Robert Geirhos", " David H. J. Janssen", " Heiko H. Sch\u00fctt", " Jonas Rauber", " Matthias Bethge", " Felix A. Wichmann"], "  Human visual object recognition is typically rapid and seemingly effortless,\nas well as largely independent of viewpoint and object orientation. Until very\nrecently, animate visual systems were the only ones capable of this remarkable\ncomputational feat. This has changed with the rise of a class of computer\nvision algorithms called deep neural networks (DNNs) that achieve human-level\nclassification performance on object recognition tasks. Furthermore, a growing\nnumber of studies report similarities in the way DNNs and the human visual\nsystem process objects, suggesting that current DNNs may be good models of\nhuman visual object recognition. Yet there clearly exist important\narchitectural and processing differences between state-of-the-art DNNs and the\nprimate visual system. The potential behavioural consequences of these\ndifferences are not well understood. We aim to address this issue by comparing\nhuman and DNN generalisation abilities towards image degradations. We find the\nhuman visual system to be more robust to image manipulations like contrast\nreduction, additive noise or novel eidolon-distortions. In addition, we find\nprogressively diverging classification error-patterns between humans and DNNs\nwhen the signal gets weaker, indicating that there may still be marked\ndifferences in the way humans and current DNNs perform visual object\nrecognition. We envision that our findings as well as our carefully measured\nand freely available behavioural datasets provide a new useful benchmark for\nthe computer vision community to improve the robustness of DNNs and a\nmotivation for neuroscientists to search for mechanisms in the brain that could\nfacilitate this robustness.\n"], ["2017-06-19", "http://arxiv.org/abs/1706.06083", "Towards Deep Learning Models Resistant to Adversarial Attacks.", ["Aleksander Madry", " Aleksandar Makelov", " Ludwig Schmidt", " Dimitris Tsipras", " Adrian Vladu"], "  Recent work has demonstrated that deep neural networks are vulnerable to\nadversarial examples---inputs that are almost indistinguishable from natural\ndata and yet classified incorrectly by the network. In fact, some of the latest\nfindings suggest that the existence of adversarial attacks may be an inherent\nweakness of deep learning models. To address this problem, we study the\nadversarial robustness of neural networks through the lens of robust\noptimization. This approach provides us with a broad and unifying view on much\nof the prior work on this topic. Its principled nature also enables us to\nidentify methods for both training and attacking neural networks that are\nreliable and, in a certain sense, universal. In particular, they specify a\nconcrete security guarantee that would protect against any adversary. These\nmethods let us train networks with significantly improved resistance to a wide\nrange of adversarial attacks. They also suggest the notion of security against\na first-order adversary as a natural and broad security guarantee. We believe\nthat robustness against such well-defined classes of adversaries is an\nimportant stepping stone towards fully resistant deep learning models. Code and\npre-trained models are available at https://github.com/MadryLab/mnist_challenge\nand https://github.com/MadryLab/cifar10_challenge.\n"], ["2017-06-14", "http://arxiv.org/abs/1706.04701", "Adversarial Example Defenses: Ensembles of Weak Defenses are not Strong.", ["Warren He", " James Wei", " Xinyun Chen", " Nicholas Carlini", " Dawn Song"], "  Ongoing research has proposed several methods to defend neural networks\nagainst adversarial examples, many of which researchers have shown to be\nineffective. We ask whether a strong defense can be created by combining\nmultiple (possibly weak) defenses. To answer this question, we study three\ndefenses that follow this approach. Two of these are recently proposed defenses\nthat intentionally combine components designed to work well together. A third\ndefense combines three independent defenses. For all the components of these\ndefenses and the combined defenses themselves, we show that an adaptive\nadversary can create adversarial examples successfully with low distortion.\nThus, our work implies that ensemble of weak defenses is not sufficient to\nprovide strong defense against adversarial examples.\n"], ["2017-06-13", "http://arxiv.org/abs/1706.03922", "Analyzing the Robustness of Nearest Neighbors to Adversarial Examples.", ["Yizhen Wang", " Somesh Jha", " Kamalika Chaudhuri"], "  Motivated by safety-critical applications, test-time attacks on classifiers\nvia adversarial examples has recently received a great deal of attention.\nHowever, there is a general lack of understanding on why adversarial examples\narise; whether they originate due to inherent properties of data or due to lack\nof training samples remains ill-understood. In this work, we introduce a\ntheoretical framework analogous to bias-variance theory for understanding these\neffects.\n  We use our framework to analyze the robustness of a canonical non-parametric\nclassifier - the k-nearest neighbors. Our analysis shows that its robustness\nproperties depend critically on the value of k - the classifier may be\ninherently non-robust for small k, but its robustness approaches that of the\nBayes Optimal classifier for fast-growing k. We propose a novel modified\n1-nearest neighbor classifier, and guarantee its robustness in the large sample\nlimit. Our experiments suggest that this classifier may have good robustness\nproperties even for reasonable data set sizes.\n"], ["2017-06-06", "http://arxiv.org/abs/1706.01763", "Adversarial-Playground: A Visualization Suite for Adversarial Sample Generation.", ["Andrew Norton", " Yanjun Qi"], "  With growing interest in adversarial machine learning, it is important for\nmachine learning practitioners and users to understand how their models may be\nattacked. We propose a web-based visualization tool, Adversarial-Playground, to\ndemonstrate the efficacy of common adversarial methods against a deep neural\nnetwork (DNN) model, built on top of the TensorFlow library.\nAdversarial-Playground provides users an efficient and effective experience in\nexploring techniques generating adversarial examples, which are inputs crafted\nby an adversary to fool a machine learning system. To enable\nAdversarial-Playground to generate quick and accurate responses for users, we\nuse two primary tactics: (1) We propose a faster variant of the\nstate-of-the-art Jacobian saliency map approach that maintains a comparable\nevasion rate. (2) Our visualization does not transmit the generated adversarial\nimages to the client, but rather only the matrix describing the sample and the\nvector representing classification likelihoods.\n  The source code along with the data from all of our experiments are available\nat \\url{https://github.com/QData/AdversarialDNN-Playground}.\n"], ["2017-06-02", "http://arxiv.org/abs/1706.00633", "Towards Robust Detection of Adversarial Examples.", ["Tianyu Pang", " Chao Du", " Yinpeng Dong", " Jun Zhu"], "  Although the recent progress is substantial, deep learning methods can be\nvulnerable to the maliciously generated adversarial examples. In this paper, we\npresent a novel training procedure and a thresholding test strategy, towards\nrobust detection of adversarial examples. In training, we propose to minimize\nthe reverse cross-entropy (RCE), which encourages a deep network to learn\nlatent representations that better distinguish adversarial examples from normal\nones. In testing, we propose to use a thresholding strategy as the detector to\nfilter out adversarial examples for reliable predictions. Our method is simple\nto implement using standard algorithms, with little extra training cost\ncompared to the common cross-entropy minimization. We apply our method to\ndefend various attacking methods on the widely used MNIST and CIFAR-10\ndatasets, and achieve significant improvements on robust predictions under all\nthe threat models in the adversarial setting.\n"], ["2017-05-30", "http://arxiv.org/abs/1705.10686", "Feature Squeezing Mitigates and Detects Carlini/Wagner Adversarial Examples.", ["Weilin Xu", " David Evans", " Yanjun Qi"], "  Feature squeezing is a recently-introduced framework for mitigating and\ndetecting adversarial examples. In previous work, we showed that it is\neffective against several earlier methods for generating adversarial examples.\nIn this short note, we report on recent results showing that simple feature\nsqueezing techniques also make deep learning models significantly more robust\nagainst the Carlini/Wagner attacks, which are the best known adversarial\nmethods discovered to date.\n"], ["2017-05-27", "http://arxiv.org/abs/1705.09764", "MAT: A Multi-strength Adversarial Training Method to Mitigate Adversarial Attacks.", ["Chang Song", " Hsin-Pai Cheng", " Huanrui Yang", " Sicheng Li", " Chunpeng Wu", " Qing Wu", " Hai Li", " Yiran Chen"], "  Some recent works revealed that deep neural networks (DNNs) are vulnerable to\nso-called adversarial attacks where input examples are intentionally perturbed\nto fool DNNs. In this work, we revisit the DNN training process that includes\nadversarial examples into the training dataset so as to improve DNN's\nresilience to adversarial attacks, namely, adversarial training. Our\nexperiments show that different adversarial strengths, i.e., perturbation\nlevels of adversarial examples, have different working zones to resist the\nattack. Based on the observation, we propose a multi-strength adversarial\ntraining method (MAT) that combines the adversarial training examples with\ndifferent adversarial strengths to defend adversarial attacks. Two training\nstructures - mixed MAT and parallel MAT - are developed to facilitate the\ntradeoffs between training time and memory occupation. Our results show that\nMAT can substantially minimize the accuracy degradation of deep learning\nsystems to adversarial attacks on MNIST, CIFAR-10, CIFAR-100, and SVHN.\n"], ["2017-05-26", "http://arxiv.org/abs/1705.09554", "Analysis of universal adversarial perturbations.", ["Seyed-Mohsen Moosavi-Dezfooli", " Alhussein Fawzi", " Omar Fawzi", " Pascal Frossard", " Stefano Soatto"], "  Deep networks have recently been shown to be vulnerable to universal\nperturbations: there exist very small image-agnostic perturbations that cause\nmost natural images to be misclassified by such classifiers. In this paper, we\npropose the first quantitative analysis of the robustness of classifiers to\nuniversal perturbations, and draw a formal link between the robustness to\nuniversal perturbations, and the geometry of the decision boundary.\nSpecifically, we establish theoretical bounds on the robustness of classifiers\nunder two decision boundary models (flat and curved models). We show in\nparticular that the robustness of deep networks to universal perturbations is\ndriven by a key property of their curvature: there exists shared directions\nalong which the decision boundary of deep networks is systematically positively\ncurved. Under such conditions, we prove the existence of small universal\nperturbations. Our analysis further provides a novel geometric method for\ncomputing universal perturbations, in addition to explaining their properties.\n"], ["2017-05-26", "http://arxiv.org/abs/1705.09552", "Classification regions of deep neural networks.", ["Alhussein Fawzi", " Seyed-Mohsen Moosavi-Dezfooli", " Pascal Frossard", " Stefano Soatto"], "  The goal of this paper is to analyze the geometric properties of deep neural\nnetwork classifiers in the input space. We specifically study the topology of\nclassification regions created by deep networks, as well as their associated\ndecision boundary. Through a systematic empirical investigation, we show that\nstate-of-the-art deep nets learn connected classification regions, and that the\ndecision boundary in the vicinity of datapoints is flat along most directions.\nWe further draw an essential connection between two seemingly unrelated\nproperties of deep networks: their sensitivity to additive perturbations in the\ninputs, and the curvature of their decision boundary. The directions where the\ndecision boundary is curved in fact remarkably characterize the directions to\nwhich the classifier is the most vulnerable. We finally leverage a fundamental\nasymmetry in the curvature of the decision boundary of deep nets, and propose a\nmethod to discriminate between original images, and images perturbed with small\nadversarial examples. We show the effectiveness of this purely geometric\napproach for detecting small adversarial perturbations in images, and for\nrecovering the labels of perturbed images.\n"], ["2017-05-25", "http://arxiv.org/abs/1705.09064", "MagNet: a Two-Pronged Defense against Adversarial Examples.", ["Dongyu Meng", " Hao Chen"], "  Deep learning has shown promising results on hard perceptual problems in\nrecent years. However, deep learning systems are found to be vulnerable to\nsmall adversarial perturbations that are nearly imperceptible to human. Such\nspecially crafted perturbations cause deep learning systems to output incorrect\ndecisions, with potentially disastrous consequences. These vulnerabilities\nhinder the deployment of deep learning systems where safety or security is\nimportant. Attempts to secure deep learning systems either target specific\nattacks or have been shown to be ineffective.\n  In this paper, we propose MagNet, a framework for defending neural network\nclassifiers against adversarial examples. MagNet does not modify the protected\nclassifier or know the process for generating adversarial examples. MagNet\nincludes one or more separate detector networks and a reformer network.\nDifferent from previous work, MagNet learns to differentiate between normal and\nadversarial examples by approximating the manifold of normal examples. Since it\ndoes not rely on any process for generating adversarial examples, it has\nsubstantial generalization power. Moreover, MagNet reconstructs adversarial\nexamples by moving them towards the manifold, which is effective for helping\nclassify adversarial examples with small perturbation correctly. We discuss the\nintrinsic difficulty in defending against whitebox attack and propose a\nmechanism to defend against graybox attack. Inspired by the use of randomness\nin cryptography, we propose to use diversity to strengthen MagNet. We show\nempirically that MagNet is effective against most advanced state-of-the-art\nattacks in blackbox and graybox scenarios while keeping false positive rate on\nnormal examples very low.\n"], ["2017-05-23", "http://arxiv.org/abs/1705.08475", "Formal Guarantees on the Robustness of a Classifier against Adversarial Manipulation.", ["Matthias Hein", " Maksym Andriushchenko"], "  Recent work has shown that state-of-the-art classifiers are quite brittle, in\nthe sense that a small adversarial change of an originally with high confidence\ncorrectly classified input leads to a wrong classification again with high\nconfidence. This raises concerns that such classifiers are vulnerable to\nattacks and calls into question their usage in safety-critical systems. We show\nin this paper for the first time formal guarantees on the robustness of a\nclassifier by giving instance-specific lower bounds on the norm of the input\nmanipulation required to change the classifier decision. Based on this analysis\nwe propose the Cross-Lipschitz regularization functional. We show that using\nthis form of regularization in kernel methods resp. neural networks improves\nthe robustness of the classifier without any loss in prediction performance.\n"], ["2017-05-23", "http://arxiv.org/abs/1705.08378", "Detecting Adversarial Image Examples in Deep Networks with Adaptive Noise Reduction.", ["Bin Liang", " Hongcheng Li", " Miaoqiang Su", " Xirong Li", " Wenchang Shi", " Xiaofeng Wang"], "  Recently, many studies have demonstrated deep neural network (DNN)\nclassifiers can be fooled by the adversarial example, which is crafted via\nintroducing some perturbations into an original sample. Accordingly, some\npowerful defense techniques were proposed. However, existing defense techniques\noften require modifying the target model or depend on the prior knowledge of\nattacks. In this paper, we propose a straightforward method for detecting\nadversarial image examples, which can be directly deployed into unmodified\noff-the-shelf DNN models. We consider the perturbation to images as a kind of\nnoise and introduce two classic image processing techniques, scalar\nquantization and smoothing spatial filter, to reduce its effect. The image\nentropy is employed as a metric to implement an adaptive noise reduction for\ndifferent kinds of images. Consequently, the adversarial example can be\neffectively detected by comparing the classification results of a given sample\nand its denoised version, without referring to any prior knowledge of attacks.\nMore than 20,000 adversarial examples against some state-of-the-art DNN models\nare used to evaluate the proposed method, which are crafted with different\nattack techniques. The experiments show that our detection method can achieve a\nhigh overall F1 score of 96.39% and certainly raises the bar for defense-aware\nattacks.\n"], ["2017-05-23", "http://arxiv.org/abs/1705.08131", "Black-Box Attacks against RNN based Malware Detection Algorithms.", ["Weiwei Hu", " Ying Tan"], "  Recent researches have shown that machine learning based malware detection\nalgorithms are very vulnerable under the attacks of adversarial examples. These\nworks mainly focused on the detection algorithms which use features with fixed\ndimension, while some researchers have begun to use recurrent neural networks\n(RNN) to detect malware based on sequential API features. This paper proposes a\nnovel algorithm to generate sequential adversarial examples, which are used to\nattack a RNN based malware detection system. It is usually hard for malicious\nattackers to know the exact structures and weights of the victim RNN. A\nsubstitute RNN is trained to approximate the victim RNN. Then we propose a\ngenerative RNN to output sequential adversarial examples from the original\nsequential malware inputs. Experimental results showed that RNN based malware\ndetection algorithms fail to detect most of the generated malicious adversarial\nexamples, which means the proposed model is able to effectively bypass the\ndetection algorithms.\n"], ["2017-05-22", "http://arxiv.org/abs/1705.07819", "Regularizing deep networks using efficient layerwise adversarial training.", ["Swami Sankaranarayanan", " Arpit Jain", " Rama Chellappa", " Ser Nam Lim"], "  Adversarial training has been shown to regularize deep neural networks in\naddition to increasing their robustness to adversarial examples. However, its\nimpact on very deep state of the art networks has not been fully investigated.\nIn this paper, we present an efficient approach to perform adversarial training\nby perturbing intermediate layer activations and study the use of such\nperturbations as a regularizer during training. We use these perturbations to\ntrain very deep models such as ResNets and show improvement in performance both\non adversarial and original test data. Our experiments highlight the benefits\nof perturbing intermediate layer activations compared to perturbing only the\ninputs. The results on CIFAR-10 and CIFAR-100 datasets show the merits of the\nproposed adversarial training approach. Additional results on WideResNets show\nthat our approach provides significant improvement in classification accuracy\nfor a given base model, outperforming dropout and other base models of larger\nsize.\n"], ["2017-05-21", "http://arxiv.org/abs/1705.07535", "Evading Classifiers by Morphing in the Dark.", ["Hung Dang", " Yue Huang", " Ee-Chien Chang"], "  Learning-based systems have been shown to be vulnerable to evasion through\nadversarial data manipulation. These attacks have been studied under\nassumptions that the adversary has certain knowledge of either the target model\ninternals, its training dataset or at least classification scores it assigns to\ninput samples. In this paper, we investigate a much more constrained and\nrealistic attack scenario wherein the target classifier is minimally exposed to\nthe adversary, revealing on its final classification decision (e.g., reject or\naccept an input sample). Moreover, the adversary can only manipulate malicious\nsamples using a blackbox morpher. That is, the adversary has to evade the\ntarget classifier by morphing malicious samples \"in the dark\". We present a\nscoring mechanism that can assign a real-value score which reflects evasion\nprogress to each sample based on the limited information available. Leveraging\non such scoring mechanism, we propose an evasion method -- EvadeHC -- and\nevaluate it against two PDF malware detectors, namely PDFRate and Hidost. The\nexperimental evaluation demonstrates that the proposed evasion attacks are\neffective, attaining $100\\%$ evasion rate on the evaluation dataset.\nInterestingly, EvadeHC outperforms the known classifier evasion technique that\noperates based on classification scores output by the classifiers. Although our\nevaluations are conducted on PDF malware classifier, the proposed approaches\nare domain-agnostic and is of wider application to other learning-based\nsystems.\n"], ["2017-05-20", "http://arxiv.org/abs/1705.07263", "Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods.", ["Nicholas Carlini", " David Wagner"], "  Neural networks are known to be vulnerable to adversarial examples: inputs\nthat are close to natural inputs but classified incorrectly. In order to better\nunderstand the space of adversarial examples, we survey ten recent proposals\nthat are designed for detection and compare their efficacy. We show that all\ncan be defeated by constructing new loss functions. We conclude that\nadversarial examples are significantly harder to detect than previously\nappreciated, and the properties believed to be intrinsic to adversarial\nexamples are in fact not. Finally, we propose several simple guidelines for\nevaluating future proposed defenses.\n"], ["2017-05-19", "http://arxiv.org/abs/1705.07204", "Ensemble Adversarial Training: Attacks and Defenses.", ["Florian Tram\u00e8r", " Alexey Kurakin", " Nicolas Papernot", " Ian Goodfellow", " Dan Boneh", " Patrick McDaniel"], "  Adversarial examples are perturbed inputs designed to fool machine learning\nmodels. Adversarial training injects such examples into training data to\nincrease robustness. To scale this technique to large datasets, perturbations\nare crafted using fast single-step methods that maximize a linear approximation\nof the model's loss. We show that this form of adversarial training converges\nto a degenerate global minimum, wherein small curvature artifacts near the data\npoints obfuscate a linear approximation of the loss. The model thus learns to\ngenerate weak perturbations, rather than defend against strong ones. As a\nresult, we find that adversarial training remains vulnerable to black-box\nattacks, where we transfer perturbations computed on undefended models, as well\nas to a powerful novel single-step attack that escapes the non-smooth vicinity\nof the input data via a small random step. We further introduce Ensemble\nAdversarial Training, a technique that augments training data with\nperturbations transferred from other models. On ImageNet, Ensemble Adversarial\nTraining yields models with strong robustness to black-box attacks. In\nparticular, our most robust model won the first round of the NIPS 2017\ncompetition on Defenses against Adversarial Attacks.\n"], ["2017-05-19", "http://arxiv.org/abs/1705.07213", "MTDeep: Boosting the Security of Deep Neural Nets Against Adversarial Attacks with Moving Target Defense.", ["Sailik Sengupta", " Tathagata Chakraborti", " Subbarao Kambhampati"], "  Present attack methods can make state-of-the-art classification systems based\non deep neural networks misclassify every adversarially modified test example.\nThe design of general defense strategies against a wide range of such attacks\nstill remains a challenging problem. In this paper, we draw inspiration from\nthe fields of cybersecurity and multi-agent systems and propose to leverage the\nconcept of Moving Target Defense (MTD) in designing a meta-defense for\n'boosting' the robustness of an ensemble of deep neural networks (DNNs) for\nvisual classification tasks against such adversarial attacks. To classify an\ninput image, a trained network is picked randomly from this set of networks by\nformulating the interaction between a Defender (who hosts the classification\nnetworks) and their (Legitimate and Malicious) users as a Bayesian Stackelberg\nGame (BSG). We empirically show that this approach, MTDeep, reduces\nmisclassification on perturbed images in various datasets such as MNIST,\nFashionMNIST, and ImageNet while maintaining high classification accuracy on\nlegitimate test images. We then demonstrate that our framework, being the first\nmeta-defense technique, can be used in conjunction with any existing defense\nmechanism to provide more resilience against adversarial attacks that can be\nafforded by these defense mechanisms. Lastly, to quantify the increase in\nrobustness of an ensemble-based classification system when we use MTDeep, we\nanalyze the properties of a set of DNNs and introduce the concept of\ndifferential immunity that formalizes the notion of attack transferability.\n"], ["2017-05-18", "http://arxiv.org/abs/1705.06640", "DeepXplore: Automated Whitebox Testing of Deep Learning Systems.", ["Kexin Pei", " Yinzhi Cao", " Junfeng Yang", " Suman Jana"], "  Deep learning (DL) systems are increasingly deployed in safety- and\nsecurity-critical domains including self-driving cars and malware detection,\nwhere the correctness and predictability of a system's behavior for corner case\ninputs are of great importance. Existing DL testing depends heavily on manually\nlabeled data and therefore often fails to expose erroneous behaviors for rare\ninputs.\n  We design, implement, and evaluate DeepXplore, the first whitebox framework\nfor systematically testing real-world DL systems. First, we introduce neuron\ncoverage for systematically measuring the parts of a DL system exercised by\ntest inputs. Next, we leverage multiple DL systems with similar functionality\nas cross-referencing oracles to avoid manual checking. Finally, we demonstrate\nhow finding inputs for DL systems that both trigger many differential behaviors\nand achieve high neuron coverage can be represented as a joint optimization\nproblem and solved efficiently using gradient-based search techniques.\n  DeepXplore efficiently finds thousands of incorrect corner case behaviors\n(e.g., self-driving cars crashing into guard rails and malware masquerading as\nbenign software) in state-of-the-art DL models with thousands of neurons\ntrained on five popular datasets including ImageNet and Udacity self-driving\nchallenge data. For all tested DL models, on average, DeepXplore generated one\ntest input demonstrating incorrect behavior within one second while running\nonly on a commodity laptop. We further show that the test inputs generated by\nDeepXplore can also be used to retrain the corresponding DL model to improve\nthe model's accuracy by up to 3%.\n"], ["2017-05-18", "http://arxiv.org/abs/1705.06452", "Delving into adversarial attacks on deep policies.", ["Jernej Kos", " Dawn Song"], "  Adversarial examples have been shown to exist for a variety of deep learning\narchitectures. Deep reinforcement learning has shown promising results on\ntraining agent policies directly on raw inputs such as image pixels. In this\npaper we present a novel study into adversarial attacks on deep reinforcement\nlearning polices. We compare the effectiveness of the attacks using adversarial\nexamples vs. random noise. We present a novel method for reducing the number of\ntimes adversarial examples need to be injected for a successful attack, based\non the value function. We further explore how re-training on random noise and\nFGSM perturbations affects the resilience against adversarial examples.\n"], ["2017-05-15", "http://arxiv.org/abs/1705.05264", "Extending Defensive Distillation.", ["Nicolas Papernot", " Patrick McDaniel"], "  Machine learning is vulnerable to adversarial examples: inputs carefully\nmodified to force misclassification. Designing defenses against such inputs\nremains largely an open problem. In this work, we revisit defensive\ndistillation---which is one of the mechanisms proposed to mitigate adversarial\nexamples---to address its limitations. We view our results not only as an\neffective way of addressing some of the recently discovered attacks but also as\nreinforcing the importance of improved training techniques.\n"], ["2017-05-09", "http://arxiv.org/abs/1705.03387", "Generative Adversarial Trainer: Defense to Adversarial Perturbations with GAN.", ["Hyeungill Lee", " Sungyeob Han", " Jungwoo Lee"], "  We propose a novel technique to make neural network robust to adversarial\nexamples using a generative adversarial network. We alternately train both\nclassifier and generator networks. The generator network generates an\nadversarial perturbation that can easily fool the classifier network by using a\ngradient of each image. Simultaneously, the classifier network is trained to\nclassify correctly both original and adversarial images generated by the\ngenerator. These procedures help the classifier network to become more robust\nto adversarial perturbations. Furthermore, our adversarial training framework\nefficiently reduces overfitting and outperforms other regularization methods\nsuch as Dropout. We applied our method to supervised learning for CIFAR\ndatasets, and experimantal results show that our method significantly lowers\nthe generalization error of the network. To the best of our knowledge, this is\nthe first method which uses GAN to improve supervised learning.\n"], ["2017-05-08", "http://arxiv.org/abs/1705.02900", "Keeping the Bad Guys Out: Protecting and Vaccinating Deep Learning with JPEG Compression.", ["Nilaksh Das", " Madhuri Shanbhogue", " Shang-Tse Chen", " Fred Hohman", " Li Chen", " Michael E. Kounavis", " Duen Horng Chau"], "  Deep neural networks (DNNs) have achieved great success in solving a variety\nof machine learning (ML) problems, especially in the domain of image\nrecognition. However, recent research showed that DNNs can be highly vulnerable\nto adversarially generated instances, which look seemingly normal to human\nobservers, but completely confuse DNNs. These adversarial samples are crafted\nby adding small perturbations to normal, benign images. Such perturbations,\nwhile imperceptible to the human eye, are picked up by DNNs and cause them to\nmisclassify the manipulated instances with high confidence. In this work, we\nexplore and demonstrate how systematic JPEG compression can work as an\neffective pre-processing step in the classification pipeline to counter\nadversarial attacks and dramatically reduce their effects (e.g., Fast Gradient\nSign Method, DeepFool). An important component of JPEG compression is its\nability to remove high frequency signal components, inside square blocks of an\nimage. Such an operation is equivalent to selective blurring of the image,\nhelping remove additive perturbations. Further, we propose an ensemble-based\ntechnique that can be constructed quickly from a given well-performing DNN, and\nempirically show how such an ensemble that leverages JPEG compression can\nprotect a model from multiple types of adversarial attacks, without requiring\nknowledge about the model.\n"], ["2017-05-05", "http://arxiv.org/abs/1705.02224", "Detecting Adversarial Samples Using Density Ratio Estimates.", ["Lovedeep Gondara"], "  Machine learning models, especially based on deep architectures are used in\neveryday applications ranging from self driving cars to medical diagnostics. It\nhas been shown that such models are dangerously susceptible to adversarial\nsamples, indistinguishable from real samples to human eye, adversarial samples\nlead to incorrect classifications with high confidence. Impact of adversarial\nsamples is far-reaching and their efficient detection remains an open problem.\nWe propose to use direct density ratio estimation as an efficient model\nagnostic measure to detect adversarial samples. Our proposed method works\nequally well with single and multi-channel samples, and with different\nadversarial sample generation methods. We also propose a method to use density\nratio estimates for generating adversarial samples with an added constraint of\npreserving density ratio.\n"], ["2017-04-28", "http://arxiv.org/abs/1704.08996", "Yes, Machine Learning Can Be More Secure! A Case Study on Android Malware Detection.", ["Ambra Demontis", " Marco Melis", " Battista Biggio", " Davide Maiorca", " Daniel Arp", " Konrad Rieck", " Igino Corona", " Giorgio Giacinto", " Fabio Roli"], "  To cope with the increasing variability and sophistication of modern attacks,\nmachine learning has been widely adopted as a statistically-sound tool for\nmalware detection. However, its security against well-crafted attacks has not\nonly been recently questioned, but it has been shown that machine learning\nexhibits inherent vulnerabilities that can be exploited to evade detection at\ntest time. In other words, machine learning itself can be the weakest link in a\nsecurity system. In this paper, we rely upon a previously-proposed attack\nframework to categorize potential attack scenarios against learning-based\nmalware detection tools, by modeling attackers with different skills and\ncapabilities. We then define and implement a set of corresponding evasion\nattacks to thoroughly assess the security of Drebin, an Android malware\ndetector. The main contribution of this work is the proposal of a simple and\nscalable secure-learning paradigm that mitigates the impact of evasion attacks,\nwhile only slightly worsening the detection rate in the absence of attack. We\nfinally argue that our secure-learning approach can also be readily applied to\nother malware detection tasks.\n"], ["2017-04-28", "http://arxiv.org/abs/1704.08847", "Parseval Networks: Improving Robustness to Adversarial Examples.", ["Moustapha Cisse", " Piotr Bojanowski", " Edouard Grave", " Yann Dauphin", " Nicolas Usunier"], "  We introduce Parseval networks, a form of deep neural networks in which the\nLipschitz constant of linear, convolutional and aggregation layers is\nconstrained to be smaller than 1. Parseval networks are empirically and\ntheoretically motivated by an analysis of the robustness of the predictions\nmade by deep neural networks when their input is subject to an adversarial\nperturbation. The most important feature of Parseval networks is to maintain\nweight matrices of linear and convolutional layers to be (approximately)\nParseval tight frames, which are extensions of orthogonal matrices to\nnon-square matrices. We describe how these constraints can be maintained\nefficiently during SGD. We show that Parseval networks match the\nstate-of-the-art in terms of accuracy on CIFAR-10/100 and Street View House\nNumbers (SVHN) while being more robust than their vanilla counterpart against\nadversarial examples. Incidentally, Parseval networks also tend to train faster\nand make a better usage of the full capacity of the networks.\n"], ["2017-04-26", "http://arxiv.org/abs/1704.08006", "Deep Text Classification Can be Fooled.", ["Bin Liang", " Hongcheng Li", " Miaoqiang Su", " Pan Bian", " Xirong Li", " Wenchang Shi"], "  In this paper, we present an effective method to craft text adversarial\nsamples, revealing one important yet underestimated fact that DNN-based text\nclassifiers are also prone to adversarial sample attack. Specifically,\nconfronted with different adversarial scenarios, the text items that are\nimportant for classification are identified by computing the cost gradients of\nthe input (white-box attack) or generating a series of occluded test samples\n(black-box attack). Based on these items, we design three perturbation\nstrategies, namely insertion, modification, and removal, to generate\nadversarial samples. The experiment results show that the adversarial samples\ngenerated by our method can successfully fool both state-of-the-art\ncharacter-level and word-level DNN-based text classifiers. The adversarial\nsamples can be perturbed to any desirable classes without compromising their\nutilities. At the same time, the introduced perturbation is difficult to be\nperceived.\n"], ["2017-04-19", "http://arxiv.org/abs/1704.05712", "Universal Adversarial Perturbations Against Semantic Image Segmentation.", ["Jan Hendrik Metzen", " Mummadi Chaithanya Kumar", " Thomas Brox", " Volker Fischer"], "  While deep learning is remarkably successful on perceptual tasks, it was also\nshown to be vulnerable to adversarial perturbations of the input. These\nperturbations denote noise added to the input that was generated specifically\nto fool the system while being quasi-imperceptible for humans. More severely,\nthere even exist universal perturbations that are input-agnostic but fool the\nnetwork on the majority of inputs. While recent work has focused on image\nclassification, this work proposes attacks against semantic image segmentation:\nwe present an approach for generating (universal) adversarial perturbations\nthat make the network yield a desired target segmentation as output. We show\nempirically that there exist barely perceptible universal noise patterns which\nresult in nearly the same predicted segmentation for arbitrary inputs.\nFurthermore, we also show the existence of universal noise which removes a\ntarget class (e.g., all pedestrians) from the segmentation while leaving the\nsegmentation mostly unchanged otherwise.\n"], ["2017-04-17", "http://arxiv.org/abs/1704.04960", "Adversarial and Clean Data Are Not Twins.", ["Zhitao Gong", " Wenlu Wang", " Wei-Shinn Ku"], "  Adversarial attack has cast a shadow on the massive success of deep neural\nnetworks. Despite being almost visually identical to the clean data, the\nadversarial images can fool deep neural networks into wrong predictions with\nvery high confidence. In this paper, however, we show that we can build a\nsimple binary classifier separating the adversarial apart from the clean data\nwith accuracy over 99%. We also empirically show that the binary classifier is\nrobust to a second-round adversarial attack. In other words, it is difficult to\ndisguise adversarial samples to bypass the binary classifier. Further more, we\nempirically investigate the generalization limitation which lingers on all\ncurrent defensive methods, including the binary classifier approach. And we\nhypothesize that this is the result of intrinsic property of adversarial\ncrafting algorithms.\n"], ["2017-04-16", "http://arxiv.org/abs/1704.05051", "Google's Cloud Vision API Is Not Robust To Noise.", ["Hossein Hosseini", " Baicen Xiao", " Radha Poovendran"], "  Google has recently introduced the Cloud Vision API for image analysis.\nAccording to the demonstration website, the API \"quickly classifies images into\nthousands of categories, detects individual objects and faces within images,\nand finds and reads printed words contained within images.\" It can be also used\nto \"detect different types of inappropriate content from adult to violent\ncontent.\"\n  In this paper, we evaluate the robustness of Google Cloud Vision API to input\nperturbation. In particular, we show that by adding sufficient noise to the\nimage, the API generates completely different outputs for the noisy image,\nwhile a human observer would perceive its original content. We show that the\nattack is consistently successful, by performing extensive experiments on\ndifferent image types, including natural images, images containing faces and\nimages with texts. For instance, using images from ImageNet dataset, we found\nthat adding an average of 14.25% impulse noise is enough to deceive the API.\nOur findings indicate the vulnerability of the API in adversarial environments.\nFor example, an adversary can bypass an image filtering system by adding noise\nto inappropriate images. We then show that when a noise filter is applied on\ninput images, the API generates mostly the same outputs for restored images as\nfor original images. This observation suggests that cloud vision API can\nreadily benefit from noise filtering, without the need for updating image\nanalysis algorithms.\n"], ["2017-04-11", "http://arxiv.org/abs/1704.03453", "The Space of Transferable Adversarial Examples.", ["Florian Tram\u00e8r", " Nicolas Papernot", " Ian Goodfellow", " Dan Boneh", " Patrick McDaniel"], "  Adversarial examples are maliciously perturbed inputs designed to mislead\nmachine learning (ML) models at test-time. They often transfer: the same\nadversarial example fools more than one model.\n  In this work, we propose novel methods for estimating the previously unknown\ndimensionality of the space of adversarial inputs. We find that adversarial\nexamples span a contiguous subspace of large (~25) dimensionality. Adversarial\nsubspaces with higher dimensionality are more likely to intersect. We find that\nfor two different models, a significant fraction of their subspaces is shared,\nthus enabling transferability.\n  In the first quantitative analysis of the similarity of different models'\ndecision boundaries, we show that these boundaries are actually close in\narbitrary directions, whether adversarial or benign. We conclude by formally\nstudying the limits of transferability. We derive (1) sufficient conditions on\nthe data distribution that imply transferability for simple model classes and\n(2) examples of scenarios in which transfer does not occur. These findings\nindicate that it may be possible to design defenses against transfer-based\nattacks, even for models that are vulnerable to direct attacks.\n"], ["2017-04-09", "http://arxiv.org/abs/1704.02654", "Enhancing Robustness of Machine Learning Systems via Data Transformations.", ["Arjun Nitin Bhagoji", " Daniel Cullina", " Chawin Sitawarin", " Prateek Mittal"], "  We propose the use of data transformations as a defense against evasion\nattacks on ML classifiers. We present and investigate strategies for\nincorporating a variety of data transformations including dimensionality\nreduction via Principal Component Analysis and data `anti-whitening' to enhance\nthe resilience of machine learning, targeting both the classification and the\ntraining phase. We empirically evaluate and demonstrate the feasibility of\nlinear transformations of data as a defense mechanism against evasion attacks\nusing multiple real-world datasets. Our key findings are that the defense is\n(i) effective against the best known evasion attacks from the literature,\nresulting in a two-fold increase in the resources required by a white-box\nadversary with knowledge of the defense for a successful attack, (ii)\napplicable across a range of ML classifiers, including Support Vector Machines\nand Deep Neural Networks, and (iii) generalizable to multiple application\ndomains, including image classification and human activity classification.\n"], ["2017-04-06", "http://arxiv.org/abs/1704.01704", "Adequacy of the Gradient-Descent Method for Classifier Evasion Attacks.", ["Yi Han", " Benjamin I. P. Rubinstein"], "  Despite the wide use of machine learning in adversarial settings including\ncomputer security, recent studies have demonstrated vulnerabilities to evasion\nattacks---carefully crafted adversarial samples that closely resemble\nlegitimate instances, but cause misclassification. In this paper, we examine\nthe adequacy of the leading approach to generating adversarial samples---the\ngradient descent approach. In particular (1) we perform extensive experiments\non three datasets, MNIST, USPS and Spambase, in order to analyse the\neffectiveness of the gradient-descent method against non-linear support vector\nmachines, and conclude that carefully reduced kernel smoothness can\nsignificantly increase robustness to the attack; (2) we demonstrate that\nseparated inter-class support vectors lead to more secure models, and propose a\nquantity similar to margin that can efficiently predict potential\nsusceptibility to gradient-descent attacks, before the attack is launched; and\n(3) we design a new adversarial sample construction algorithm based on\noptimising the multiplicative ratio of class decision functions.\n"], ["2017-04-05", "http://arxiv.org/abs/1704.01547", "Comment on \"Biologically inspired protection of deep networks from adversarial attacks\".", ["Wieland Brendel", " Matthias Bethge"], "  A recent paper suggests that Deep Neural Networks can be protected from\ngradient-based adversarial perturbations by driving the network activations\ninto a highly saturated regime. Here we analyse such saturated networks and\nshow that the attacks fail due to numerical limitations in the gradient\ncomputations. A simple stabilisation of the gradient estimates enables\nsuccessful and efficient attacks. Thus, it has yet to be shown that the\nrobustness observed in highly saturated networks is not simply due to numerical\nlimitations.\n"], ["2017-04-04", "http://arxiv.org/abs/1704.01155", "Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks.", ["Weilin Xu", " David Evans", " Yanjun Qi"], "  Although deep neural networks (DNNs) have achieved great success in many\ntasks, they can often be fooled by \\emph{adversarial examples} that are\ngenerated by adding small but purposeful distortions to natural examples.\nPrevious studies to defend against adversarial examples mostly focused on\nrefining the DNN models, but have either shown limited success or required\nexpensive computation. We propose a new strategy, \\emph{feature squeezing},\nthat can be used to harden DNN models by detecting adversarial examples.\nFeature squeezing reduces the search space available to an adversary by\ncoalescing samples that correspond to many different feature vectors in the\noriginal space into a single sample. By comparing a DNN model's prediction on\nthe original input with that on squeezed inputs, feature squeezing detects\nadversarial examples with high accuracy and few false positives. This paper\nexplores two feature squeezing methods: reducing the color bit depth of each\npixel and spatial smoothing. These simple strategies are inexpensive and\ncomplementary to other defenses, and can be combined in a joint detection\nframework to achieve high detection rates against state-of-the-art attacks.\n"], ["2017-03-31", "http://arxiv.org/abs/1704.00103", "SafetyNet: Detecting and Rejecting Adversarial Examples Robustly.", ["Jiajun Lu", " Theerasit Issaranon", " David Forsyth"], "  We describe a method to produce a network where current methods such as\nDeepFool have great difficulty producing adversarial samples. Our construction\nsuggests some insights into how deep networks work. We provide a reasonable\nanalyses that our construction is difficult to defeat, and show experimentally\nthat our method is hard to defeat with both Type I and Type II attacks using\nseveral standard networks and datasets. This SafetyNet architecture is used to\nan important and novel application SceneProof, which can reliably detect\nwhether an image is a picture of a real scene or not. SceneProof applies to\nimages captured with depth maps (RGBD images) and checks if a pair of image and\ndepth map is consistent. It relies on the relative difficulty of producing\nnaturalistic depth maps for images in post processing. We demonstrate that our\nSafetyNet is robust to adversarial examples built from currently known\nattacking approaches.\n"], ["2017-03-27", "http://arxiv.org/abs/1703.09387", "Adversarial Transformation Networks: Learning to Generate Adversarial Examples.", ["Shumeet Baluja", " Ian Fischer"], "  Multiple different approaches of generating adversarial examples have been\nproposed to attack deep neural networks. These approaches involve either\ndirectly computing gradients with respect to the image pixels, or directly\nsolving an optimization on the image pixels. In this work, we present a\nfundamentally new method for generating adversarial examples that is fast to\nexecute and provides exceptional diversity of output. We efficiently train\nfeed-forward neural networks in a self-supervised manner to generate\nadversarial examples against a target network or set of networks. We call such\na network an Adversarial Transformation Network (ATN). ATNs are trained to\ngenerate adversarial examples that minimally modify the classifier's outputs\ngiven the original input, while constraining the new classification to match an\nadversarial target class. We present methods to train ATNs and analyze their\neffectiveness targeting a variety of MNIST classifiers as well as the latest\nstate-of-the-art ImageNet classifier Inception ResNet v2.\n"], ["2017-03-27", "http://arxiv.org/abs/1703.09202", "Biologically inspired protection of deep networks from adversarial attacks.", ["Aran Nayebi", " Surya Ganguli"], "  Inspired by biophysical principles underlying nonlinear dendritic computation\nin neural circuits, we develop a scheme to train deep neural networks to make\nthem robust to adversarial attacks. Our scheme generates highly nonlinear,\nsaturated neural networks that achieve state of the art performance on gradient\nbased adversarial examples on MNIST, despite never being exposed to\nadversarially chosen examples during training. Moreover, these networks exhibit\nunprecedented robustness to targeted, iterative schemes for generating\nadversarial examples, including second-order methods. We further identify\nprinciples governing how these networks achieve their robustness, drawing on\nmethods from information geometry. We find these networks progressively create\nhighly flat and compressed internal representations that are sensitive to very\nfew input dimensions, while still solving the task. Moreover, they employ\nhighly kurtotic weight distributions, also found in the brain, and we\ndemonstrate how such kurtosis can protect even linear classifiers from\nadversarial attack.\n"], ["2017-03-26", "http://arxiv.org/abs/1703.09793", "Deceiving Google's Cloud Video Intelligence API Built for Summarizing Videos.", ["Hossein Hosseini", " Baicen Xiao", " Radha Poovendran"], "  Despite the rapid progress of the techniques for image classification, video\nannotation has remained a challenging task. Automated video annotation would be\na breakthrough technology, enabling users to search within the videos.\nRecently, Google introduced the Cloud Video Intelligence API for video\nanalysis. As per the website, the system can be used to \"separate signal from\nnoise, by retrieving relevant information at the video, shot or per frame\"\nlevel. A demonstration website has been also launched, which allows anyone to\nselect a video for annotation. The API then detects the video labels (objects\nwithin the video) as well as shot labels (description of the video events over\ntime). In this paper, we examine the usability of the Google's Cloud Video\nIntelligence API in adversarial environments. In particular, we investigate\nwhether an adversary can subtly manipulate a video in such a way that the API\nwill return only the adversary-desired labels. For this, we select an image,\nwhich is different from the video content, and insert it, periodically and at a\nvery low rate, into the video. We found that if we insert one image every two\nseconds, the API is deceived into annotating the video as if it only contained\nthe inserted image. Note that the modification to the video is hardly\nnoticeable as, for instance, for a typical frame rate of 25, we insert only one\nimage per 50 video frames. We also found that, by inserting one image per\nsecond, all the shot labels returned by the API are related to the inserted\nimage. We perform the experiments on the sample videos provided by the API\ndemonstration website and show that our attack is successful with different\nvideos and images.\n"], ["2017-03-24", "http://arxiv.org/abs/1703.08603", "Adversarial Examples for Semantic Segmentation and Object Detection.", ["Cihang Xie", " Jianyu Wang", " Zhishuai Zhang", " Yuyin Zhou", " Lingxi Xie", " Alan Yuille"], "  It has been well demonstrated that adversarial examples, i.e., natural images\nwith visually imperceptible perturbations added, generally exist for deep\nnetworks to fail on image classification. In this paper, we extend adversarial\nexamples to semantic segmentation and object detection which are much more\ndifficult. Our observation is that both segmentation and detection are based on\nclassifying multiple targets on an image (e.g., the basic target is a pixel or\na receptive field in segmentation, and an object proposal in detection), which\ninspires us to optimize a loss function over a set of pixels/proposals for\ngenerating adversarial perturbations. Based on this idea, we propose a novel\nalgorithm named Dense Adversary Generation (DAG), which generates a large\nfamily of adversarial examples, and applies to a wide range of state-of-the-art\ndeep networks for segmentation and detection. We also find that the adversarial\nperturbations can be transferred across networks with different training data,\nbased on different architectures, and even for different recognition tasks. In\nparticular, the transferability across networks with the same architecture is\nmore significant than in other cases. Besides, summing up heterogeneous\nperturbations often leads to better transfer performance, which provides an\neffective method of black-box adversarial attack.\n"], ["2017-03-23", "http://arxiv.org/abs/1703.07928", "Self corrective Perturbations for Semantic Segmentation and Classification.", ["Swami Sankaranarayanan", " Arpit Jain", " Ser Nam Lim"], "  Convolutional Neural Networks have been a subject of great importance over\nthe past decade and great strides have been made in their utility for producing\nstate of the art performance in many computer vision problems. However, the\nbehavior of deep networks is yet to be fully understood and is still an active\narea of research. In this work, we present an intriguing behavior: pre-trained\nCNNs can be made to improve their predictions by structurally perturbing the\ninput. We observe that these perturbations - referred as Guided Perturbations -\nenable a trained network to improve its prediction performance without any\nlearning or change in network weights. We perform various ablative experiments\nto understand how these perturbations affect the local context and feature\nrepresentations. Furthermore, we demonstrate that this idea can improve\nperformance of several existing approaches on semantic segmentation and scene\nlabeling tasks on the PASCAL VOC dataset and supervised classification tasks on\nMNIST and CIFAR10 datasets.\n"], ["2017-03-22", "http://arxiv.org/abs/1703.07909", "Data Driven Exploratory Attacks on Black Box Classifiers in Adversarial Domains.", ["Tegjyot Singh Sethi", " Mehmed Kantardzic"], "  While modern day web applications aim to create impact at the civilization\nlevel, they have become vulnerable to adversarial activity, where the next\ncyber-attack can take any shape and can originate from anywhere. The increasing\nscale and sophistication of attacks, has prompted the need for a data driven\nsolution, with machine learning forming the core of many cybersecurity systems.\nMachine learning was not designed with security in mind, and the essential\nassumption of stationarity, requiring that the training and testing data follow\nsimilar distributions, is violated in an adversarial domain. In this paper, an\nadversary's view point of a classification based system, is presented. Based on\na formal adversarial model, the Seed-Explore-Exploit framework is presented,\nfor simulating the generation of data driven and reverse engineering attacks on\nclassifiers. Experimental evaluation, on 10 real world datasets and using the\nGoogle Cloud Prediction Platform, demonstrates the innate vulnerability of\nclassifiers and the ease with which evasion can be carried out, without any\nexplicit information about the classifier type, the training data or the\napplication domain. The proposed framework, algorithms and empirical\nevaluation, serve as a white hat analysis of the vulnerabilities, and aim to\nfoster the development of secure machine learning frameworks.\n"], ["2017-03-20", "http://arxiv.org/abs/1703.06857", "On the Limitation of Convolutional Neural Networks in Recognizing Negative Images.", ["Hossein Hosseini", " Baicen Xiao", " Mayoore Jaiswal", " Radha Poovendran"], "  Convolutional Neural Networks (CNNs) have achieved state-of-the-art\nperformance on a variety of computer vision tasks, particularly visual\nclassification problems, where new algorithms reported to achieve or even\nsurpass the human performance. In this paper, we examine whether CNNs are\ncapable of learning the semantics of training data. To this end, we evaluate\nCNNs on negative images, since they share the same structure and semantics as\nregular images and humans can classify them correctly. Our experimental results\nindicate that when training on regular images and testing on negative images,\nthe model accuracy is significantly lower than when it is tested on regular\nimages. This leads us to the conjecture that current training methods do not\neffectively train models to generalize the concepts. We then introduce the\nnotion of semantic adversarial examples - transformed inputs that semantically\nrepresent the same objects, but the model does not classify them correctly -\nand present negative images as one class of such inputs.\n"], ["2017-03-16", "http://arxiv.org/abs/1703.05561", "Fraternal Twins: Unifying Attacks on Machine Learning and Digital Watermarking.", ["Erwin Quiring", " Daniel Arp", " Konrad Rieck"], "  Machine learning is increasingly used in security-critical applications, such\nas autonomous driving, face recognition and malware detection. Most learning\nmethods, however, have not been designed with security in mind and thus are\nvulnerable to different types of attacks. This problem has motivated the\nresearch field of adversarial machine learning that is concerned with attacking\nand defending learning methods. Concurrently, a different line of research has\ntackled a very similar problem: In digital watermarking information are\nembedded in a signal in the presence of an adversary. As a consequence, this\nresearch field has also extensively studied techniques for attacking and\ndefending watermarking methods.\n  The two research communities have worked in parallel so far, unnoticeably\ndeveloping similar attack and defense strategies. This paper is a first effort\nto bring these communities together. To this end, we present a unified notation\nof black-box attacks against machine learning and watermarking that reveals the\nsimilarity of both settings. To demonstrate the efficacy of this unified view,\nwe apply concepts from watermarking to machine learning and vice versa. We show\nthat countermeasures from watermarking can mitigate recent model-extraction\nattacks and, similarly, that techniques for hardening machine learning can fend\noff oracle attacks against watermarks. Our work provides a conceptual link\nbetween two research fields and thereby opens novel directions for improving\nthe security of both, machine learning and digital watermarking.\n"], ["2017-03-13", "http://arxiv.org/abs/1703.04318", "Blocking Transferability of Adversarial Examples in Black-Box Learning Systems.", ["Hossein Hosseini", " Yize Chen", " Sreeram Kannan", " Baosen Zhang", " Radha Poovendran"], "  Advances in Machine Learning (ML) have led to its adoption as an integral\ncomponent in many applications, including banking, medical diagnosis, and\ndriverless cars. To further broaden the use of ML models, cloud-based services\noffered by Microsoft, Amazon, Google, and others have developed ML-as-a-service\ntools as black-box systems. However, ML classifiers are vulnerable to\nadversarial examples: inputs that are maliciously modified can cause the\nclassifier to provide adversary-desired outputs. Moreover, it is known that\nadversarial examples generated on one classifier are likely to cause another\nclassifier to make the same mistake, even if the classifiers have different\narchitectures or are trained on disjoint datasets. This property, which is\nknown as transferability, opens up the possibility of attacking black-box\nsystems by generating adversarial examples on a substitute classifier and\ntransferring the examples to the target classifier. Therefore, the key to\nprotect black-box learning systems against the adversarial examples is to block\ntheir transferability. To this end, we propose a training method that, as the\ninput is more perturbed, the classifier smoothly outputs lower confidence on\nthe original label and instead predicts that the input is \"invalid\". In\nessence, we augment the output class set with a NULL label and train the\nclassifier to reject the adversarial examples by classifying them as NULL. In\nexperiments, we apply a wide range of attacks based on adversarial examples on\nthe black-box systems. We show that a classifier trained with the proposed\nmethod effectively resists against the adversarial examples, while maintaining\nthe accuracy on clean data.\n"], ["2017-03-07", "http://arxiv.org/abs/1703.06748", "Tactics of Adversarial Attack on Deep Reinforcement Learning Agents.", ["Yen-Chen Lin", " Zhang-Wei Hong", " Yuan-Hong Liao", " Meng-Li Shih", " Ming-Yu Liu", " Min Sun"], "  We introduce two tactics to attack agents trained by deep reinforcement\nlearning algorithms using adversarial examples, namely the strategically-timed\nattack and the enchanting attack. In the strategically-timed attack, the\nadversary aims at minimizing the agent's reward by only attacking the agent at\na small subset of time steps in an episode. Limiting the attack activity to\nthis subset helps prevent detection of the attack by the agent. We propose a\nnovel method to determine when an adversarial example should be crafted and\napplied. In the enchanting attack, the adversary aims at luring the agent to a\ndesignated target state. This is achieved by combining a generative model and a\nplanning algorithm: while the generative model predicts the future states, the\nplanning algorithm generates a preferred sequence of actions for luring the\nagent. A sequence of adversarial examples is then crafted to lure the agent to\ntake the preferred sequence of actions. We apply the two tactics to the agents\ntrained by the state-of-the-art deep reinforcement learning algorithm including\nDQN and A3C. In 5 Atari games, our strategically timed attack reduces as much\nreward as the uniform attack (i.e., attacking at every time step) does by\nattacking the agent 4 times less often. Our enchanting attack lures the agent\ntoward designated target states with a more than 70% success rate. Videos are\navailable at http://yclin.me/adversarial_attack_RL/\n"], ["2017-03-03", "http://arxiv.org/abs/1703.01101", "Adversarial Examples for Semantic Image Segmentation.", ["Volker Fischer", " Mummadi Chaithanya Kumar", " Jan Hendrik Metzen", " Thomas Brox"], "  Machine learning methods in general and Deep Neural Networks in particular\nhave shown to be vulnerable to adversarial perturbations. So far this\nphenomenon has mainly been studied in the context of whole-image\nclassification. In this contribution, we analyse how adversarial perturbations\ncan affect the task of semantic segmentation. We show how existing adversarial\nattackers can be transferred to this task and that it is possible to create\nimperceptible adversarial perturbations that lead a deep network to misclassify\nalmost all pixels of a chosen class while leaving network prediction nearly\nunchanged outside this class.\n"], ["2017-03-02", "http://arxiv.org/abs/1703.00978", "Compositional Falsification of Cyber-Physical Systems with Machine Learning Components.", ["Tommaso Dreossi", " Alexandre Donz\u00e9", " Sanjit A. Seshia"], "  Cyber-physical systems (CPS), such as automotive systems, are starting to\ninclude sophisticated machine learning (ML) components. Their correctness,\ntherefore, depends on properties of the inner ML modules. While learning\nalgorithms aim to generalize from examples, they are only as good as the\nexamples provided, and recent efforts have shown that they can produce\ninconsistent output under small adversarial perturbations. This raises the\nquestion: can the output from learning components can lead to a failure of the\nentire CPS? In this work, we address this question by formulating it as a\nproblem of falsifying signal temporal logic (STL) specifications for CPS with\nML components. We propose a compositional falsification framework where a\ntemporal logic falsifier and a machine learning analyzer cooperate with the aim\nof finding falsifying executions of the considered model. The efficacy of the\nproposed technique is shown on an automatic emergency braking system model with\na perception component based on deep neural networks.\n"], ["2017-03-01", "http://arxiv.org/abs/1703.00410", "Detecting Adversarial Samples from Artifacts.", ["Reuben Feinman", " Ryan R. Curtin", " Saurabh Shintre", " Andrew B. Gardner"], "  Deep neural networks (DNNs) are powerful nonlinear architectures that are\nknown to be robust to random perturbations of the input. However, these models\nare vulnerable to adversarial perturbations--small input changes crafted\nexplicitly to fool the model. In this paper, we ask whether a DNN can\ndistinguish adversarial samples from their normal and noisy counterparts. We\ninvestigate model confidence on adversarial samples by looking at Bayesian\nuncertainty estimates, available in dropout neural networks, and by performing\ndensity estimation in the subspace of deep features learned by the model. The\nresult is a method for implicit adversarial detection that is oblivious to the\nattack algorithm. We evaluate this method on a variety of standard datasets\nincluding MNIST and CIFAR-10 and show that it generalizes well across different\narchitectures and attacks. Our findings report that 85-93% ROC-AUC can be\nachieved on a number of standard classification tasks with a negative class\nthat consists of both normal and noisy samples.\n"], ["2017-02-26", "http://arxiv.org/abs/1702.08138", "Deceiving Google's Perspective API Built for Detecting Toxic Comments.", ["Hossein Hosseini", " Sreeram Kannan", " Baosen Zhang", " Radha Poovendran"], "  Social media platforms provide an environment where people can freely engage\nin discussions. Unfortunately, they also enable several problems, such as\nonline harassment. Recently, Google and Jigsaw started a project called\nPerspective, which uses machine learning to automatically detect toxic\nlanguage. A demonstration website has been also launched, which allows anyone\nto type a phrase in the interface and instantaneously see the toxicity score\n[1]. In this paper, we propose an attack on the Perspective toxic detection\nsystem based on the adversarial examples. We show that an adversary can subtly\nmodify a highly toxic phrase in a way that the system assigns significantly\nlower toxicity score to it. We apply the attack on the sample phrases provided\nin the Perspective website and show that we can consistently reduce the\ntoxicity scores to the level of the non-toxic phrases. The existence of such\nadversarial examples is very harmful for toxic detection systems and seriously\nundermines their usability.\n"], ["2017-02-22", "http://arxiv.org/abs/1702.06856", "Robustness to Adversarial Examples through an Ensemble of Specialists.", ["Mahdieh Abbasi", " Christian Gagn\u00e9"], "  We are proposing to use an ensemble of diverse specialists, where speciality\nis defined according to the confusion matrix. Indeed, we observed that for\nadversarial instances originating from a given class, labeling tend to be done\ninto a small subset of (incorrect) classes. Therefore, we argue that an\nensemble of specialists should be better able to identify and reject fooling\ninstances, with a high entropy (i.e., disagreement) over the decisions in the\npresence of adversaries. Experimental results obtained confirm that\ninterpretation, opening a way to make the system more robust to adversarial\nexamples through a rejection mechanism, rather than trying to classify them\nproperly at any cost.\n"], ["2017-02-22", "http://arxiv.org/abs/1702.06832", "Adversarial examples for generative models.", ["Jernej Kos", " Ian Fischer", " Dawn Song"], "  We explore methods of producing adversarial examples on deep generative\nmodels such as the variational autoencoder (VAE) and the VAE-GAN. Deep learning\narchitectures are known to be vulnerable to adversarial examples, but previous\nwork has focused on the application of adversarial examples to classification\ntasks. Deep generative models have recently become popular due to their ability\nto model input data distributions and generate realistic examples from those\ndistributions. We present three classes of attacks on the VAE and VAE-GAN\narchitectures and demonstrate them against networks trained on MNIST, SVHN and\nCelebA. Our first attack leverages classification-based adversaries by\nattaching a classifier to the trained encoder of the target generative model,\nwhich can then be used to indirectly manipulate the latent representation. Our\nsecond attack directly uses the VAE loss function to generate a target\nreconstruction image from the adversarial example. Our third attack moves\nbeyond relying on classification or the standard loss for the gradient and\ndirectly optimizes against differences in source and target latent\nrepresentations. We also motivate why an attacker might be interested in\ndeploying such techniques against a target generative network.\n"], ["2017-02-22", "http://arxiv.org/abs/1702.06763", "DeepCloak: Masking Deep Neural Network Models for Robustness Against Adversarial Samples.", ["Ji Gao", " Beilun Wang", " Zeming Lin", " Weilin Xu", " Yanjun Qi"], "  Recent studies have shown that deep neural networks (DNN) are vulnerable to\nadversarial samples: maliciously-perturbed samples crafted to yield incorrect\nmodel outputs. Such attacks can severely undermine DNN systems, particularly in\nsecurity-sensitive settings. It was observed that an adversary could easily\ngenerate adversarial samples by making a small perturbation on irrelevant\nfeature dimensions that are unnecessary for the current classification task. To\novercome this problem, we introduce a defensive mechanism called DeepCloak. By\nidentifying and removing unnecessary features in a DNN model, DeepCloak limits\nthe capacity an attacker can use generating adversarial samples and therefore\nincrease the robustness against such inputs. Comparing with other defensive\napproaches, DeepCloak is easy to implement and computationally efficient.\nExperimental results show that DeepCloak can increase the performance of\nstate-of-the-art DNN models against adversarial samples.\n"], ["2017-02-21", "http://arxiv.org/abs/1702.06280", "On the (Statistical) Detection of Adversarial Examples.", ["Kathrin Grosse", " Praveen Manoharan", " Nicolas Papernot", " Michael Backes", " Patrick McDaniel"], "  Machine Learning (ML) models are applied in a variety of tasks such as\nnetwork intrusion detection or Malware classification. Yet, these models are\nvulnerable to a class of malicious inputs known as adversarial examples. These\nare slightly perturbed inputs that are classified incorrectly by the ML model.\nThe mitigation of these adversarial inputs remains an open problem. As a step\ntowards understanding adversarial examples, we show that they are not drawn\nfrom the same distribution than the original data, and can thus be detected\nusing statistical tests. Using thus knowledge, we introduce a complimentary\napproach to identify specific inputs that are adversarial. Specifically, we\naugment our ML model with an additional output, in which the model is trained\nto classify all adversarial inputs. We evaluate our approach on multiple\nadversarial example crafting methods (including the fast gradient sign and\nsaliency map methods) with several datasets. The statistical test flags sample\nsets containing adversarial inputs confidently at sample sizes between 10 and\n100 data points. Furthermore, our augmented model either detects adversarial\nexamples as outliers with high accuracy (> 80%) or increases the adversary's\ncost - the perturbation added - by more than 150%. In this way, we show that\nstatistical properties of adversarial examples are essential to their\ndetection.\n"], ["2017-02-20", "http://arxiv.org/abs/1702.05983", "Generating Adversarial Malware Examples for Black-Box Attacks Based on GAN.", ["Weiwei Hu", " Ying Tan"], "  Machine learning has been used to detect new malware in recent years, while\nmalware authors have strong motivation to attack such algorithms. Malware\nauthors usually have no access to the detailed structures and parameters of the\nmachine learning models used by malware detection systems, and therefore they\ncan only perform black-box attacks. This paper proposes a generative\nadversarial network (GAN) based algorithm named MalGAN to generate adversarial\nmalware examples, which are able to bypass black-box machine learning based\ndetection models. MalGAN uses a substitute detector to fit the black-box\nmalware detection system. A generative network is trained to minimize the\ngenerated adversarial examples' malicious probabilities predicted by the\nsubstitute detector. The superiority of MalGAN over traditional gradient based\nadversarial example generation algorithms is that MalGAN is able to decrease\nthe detection rate to nearly zero and make the retraining based defensive\nmethod against adversarial examples hard to work.\n"], ["2017-02-14", "http://arxiv.org/abs/1702.04267", "On Detecting Adversarial Perturbations.", ["Jan Hendrik Metzen", " Tim Genewein", " Volker Fischer", " Bastian Bischoff"], "  Machine learning and deep learning in particular has advanced tremendously on\nperceptual tasks in recent years. However, it remains vulnerable against\nadversarial perturbations of the input that have been crafted specifically to\nfool the system while being quasi-imperceptible to a human. In this work, we\npropose to augment deep neural networks with a small \"detector\" subnetwork\nwhich is trained on the binary classification task of distinguishing genuine\ndata from data containing adversarial perturbations. Our method is orthogonal\nto prior work on addressing adversarial perturbations, which has mostly focused\non making the classification network itself more robust. We show empirically\nthat adversarial perturbations can be detected surprisingly well even though\nthey are quasi-imperceptible to humans. Moreover, while the detectors have been\ntrained to detect only a specific adversary, they generalize to similar and\nweaker adversaries. In addition, we propose an adversarial attack that fools\nboth the classifier and the detector and a novel training procedure for the\ndetector that counteracts this attack.\n"], ["2017-02-07", "http://arxiv.org/abs/1702.02284", "Adversarial Attacks on Neural Network Policies.", ["Sandy Huang", " Nicolas Papernot", " Ian Goodfellow", " Yan Duan", " Pieter Abbeel"], "  Machine learning classifiers are known to be vulnerable to inputs maliciously\nconstructed by adversaries to force misclassification. Such adversarial\nexamples have been extensively studied in the context of computer vision\napplications. In this work, we show adversarial attacks are also effective when\ntargeting neural network policies in reinforcement learning. Specifically, we\nshow existing adversarial example crafting techniques can be used to\nsignificantly degrade test-time performance of trained policies. Our threat\nmodel considers adversaries capable of introducing small perturbations to the\nraw input of the policy. We characterize the degree of vulnerability across\ntasks and training algorithms, for a subclass of adversarial-example attacks in\nwhite-box and black-box settings. Regardless of the learned task or training\nalgorithm, we observe a significant drop in performance, even with small\nadversarial perturbations that do not interfere with human perception. Videos\nare available at http://rll.berkeley.edu/adversarial.\n"], ["2017-02-03", "http://arxiv.org/abs/1702.01135", "Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks.", ["Guy Katz", " Clark Barrett", " David Dill", " Kyle Julian", " Mykel Kochenderfer"], "  Deep neural networks have emerged as a widely used and effective means for\ntackling complex, real-world problems. However, a major obstacle in applying\nthem to safety-critical systems is the great difficulty in providing formal\nguarantees about their behavior. We present a novel, scalable, and efficient\ntechnique for verifying properties of deep neural networks (or providing\ncounter-examples). The technique is based on the simplex method, extended to\nhandle the non-convex Rectified Linear Unit (ReLU) activation function, which\nis a crucial ingredient in many modern neural networks. The verification\nprocedure tackles neural networks as a whole, without making any simplifying\nassumptions. We evaluated our technique on a prototype deep neural network\nimplementation of the next-generation airborne collision avoidance system for\nunmanned aircraft (ACAS Xu). Results show that our technique can successfully\nprove properties of networks that are an order of magnitude larger than the\nlargest networks verified using existing methods.\n"], ["2017-01-15", "http://arxiv.org/abs/1701.04143", "Vulnerability of Deep Reinforcement Learning to Policy Induction Attacks.", ["Vahid Behzadan", " Arslan Munir"], "  Deep learning classifiers are known to be inherently vulnerable to\nmanipulation by intentionally perturbed inputs, named adversarial examples. In\nthis work, we establish that reinforcement learning techniques based on Deep\nQ-Networks (DQNs) are also vulnerable to adversarial input perturbations, and\nverify the transferability of adversarial examples across different DQN models.\nFurthermore, we present a novel class of attacks based on this vulnerability\nthat enable policy manipulation and induction in the learning process of DQNs.\nWe propose an attack mechanism that exploits the transferability of adversarial\nexamples to implement policy induction attacks on DQNs, and demonstrate its\nefficacy and impact through experimental study of a game-learning scenario.\n"], ["2017-01-04", "http://arxiv.org/abs/1701.00939", "Dense Associative Memory is Robust to Adversarial Inputs.", ["Dmitry Krotov", " John J Hopfield"], "  Deep neural networks (DNN) trained in a supervised way suffer from two known\nproblems. First, the minima of the objective function used in learning\ncorrespond to data points (also known as rubbish examples or fooling images)\nthat lack semantic similarity with the training data. Second, a clean input can\nbe changed by a small, and often imperceptible for human vision, perturbation,\nso that the resulting deformed input is misclassified by the network. These\nfindings emphasize the differences between the ways DNN and humans classify\npatterns, and raise a question of designing learning algorithms that more\naccurately mimic human perception compared to the existing methods.\n  Our paper examines these questions within the framework of Dense Associative\nMemory (DAM) models. These models are defined by the energy function, with\nhigher order (higher than quadratic) interactions between the neurons. We show\nthat in the limit when the power of the interaction vertex in the energy\nfunction is sufficiently large, these models have the following three\nproperties. First, the minima of the objective function are free from rubbish\nimages, so that each minimum is a semantically meaningful pattern. Second,\nartificial patterns poised precisely at the decision boundary look ambiguous to\nhuman subjects and share aspects of both classes that are separated by that\ndecision boundary. Third, adversarial images constructed by models with small\npower of the interaction vertex, which are equivalent to DNN with rectified\nlinear units (ReLU), fail to transfer to and fool the models with higher order\ninteractions. This opens up a possibility to use higher order models for\ndetecting and stopping malicious adversarial attacks. The presented results\nsuggest that DAM with higher order energy functions are closer to human visual\nperception than DNN with ReLUs.\n"], ["2016-12-22", "http://arxiv.org/abs/1612.07767", "Adversarial Examples Detection in Deep Networks with Convolutional Filter Statistics.", ["Xin Li", " Fuxin Li"], "  Deep learning has greatly improved visual recognition in recent years.\nHowever, recent research has shown that there exist many adversarial examples\nthat can negatively impact the performance of such an architecture. This paper\nfocuses on detecting those adversarial examples by analyzing whether they come\nfrom the same distribution as the normal examples. Instead of directly training\na deep neural network to detect adversarials, a much simpler approach was\nproposed based on statistics on outputs from convolutional layers. A cascade\nclassifier was designed to efficiently detect adversarials. Furthermore,\ntrained from one particular adversarial generating mechanism, the resulting\nclassifier can successfully detect adversarials from a completely different\nmechanism as well. The resulting classifier is non-subdifferentiable, hence\ncreates a difficulty for adversaries to attack by using the gradient of the\nclassifier. After detecting adversarial examples, we show that many of them can\nbe recovered by simply performing a small average filter on the image. Those\nfindings should lead to more insights about the classification mechanisms in\ndeep convolutional neural networks.\n"], ["2016-12-19", "http://arxiv.org/abs/1612.06299", "Simple Black-Box Adversarial Perturbations for Deep Networks.", ["Nina Narodytska", " Shiva Prasad Kasiviswanathan"], "  Deep neural networks are powerful and popular learning models that achieve\nstate-of-the-art pattern recognition performance on many computer vision,\nspeech, and language processing tasks. However, these networks have also been\nshown susceptible to carefully crafted adversarial perturbations which force\nmisclassification of the inputs. Adversarial examples enable adversaries to\nsubvert the expected system behavior leading to undesired consequences and\ncould pose a security risk when these systems are deployed in the real world.\n  In this work, we focus on deep convolutional neural networks and demonstrate\nthat adversaries can easily craft adversarial examples even without any\ninternal knowledge of the target network. Our attacks treat the network as an\noracle (black-box) and only assume that the output of the network can be\nobserved on the probed inputs. Our first attack is based on a simple idea of\nadding perturbation to a randomly selected single pixel or a small set of them.\nWe then improve the effectiveness of this attack by carefully constructing a\nsmall set of pixels to perturb by using the idea of greedy local-search. Our\nproposed attacks also naturally extend to a stronger notion of\nmisclassification. Our extensive experimental results illustrate that even\nthese elementary attacks can reveal a deep neural network's vulnerabilities.\nThe simplicity and effectiveness of our proposed schemes mean that they could\nserve as a litmus test for designing robust networks.\n"], ["2016-12-05", "http://arxiv.org/abs/1612.01401", "Learning Adversary-Resistant Deep Neural Networks.", ["Qinglong Wang", " Wenbo Guo", " Kaixuan Zhang", " Alexander G. II Ororbia", " Xinyu Xing", " Xue Liu", " C. Lee Giles"], "  Deep neural networks (DNNs) have proven to be quite effective in a vast array\nof machine learning tasks, with recent examples in cyber security and\nautonomous vehicles. Despite the superior performance of DNNs in these\napplications, it has been recently shown that these models are susceptible to a\nparticular type of attack that exploits a fundamental flaw in their design.\nThis attack consists of generating particular synthetic examples referred to as\nadversarial samples. These samples are constructed by slightly manipulating\nreal data-points in order to \"fool\" the original DNN model, forcing it to\nmis-classify previously correctly classified samples with high confidence.\nAddressing this flaw in the model is essential if DNNs are to be used in\ncritical applications such as those in cyber security.\n  Previous work has provided various learning algorithms to enhance the\nrobustness of DNN models, and they all fall into the tactic of \"security\nthrough obscurity\". This means security can be guaranteed only if one can\nobscure the learning algorithms from adversaries. Once the learning technique\nis disclosed, DNNs protected by these defense mechanisms are still susceptible\nto adversarial samples. In this work, we investigate this issue shared across\nprevious research work and propose a generic approach to escalate a DNN's\nresistance to adversarial samples. More specifically, our approach integrates a\ndata transformation module with a DNN, making it robust even if we reveal the\nunderlying learning algorithm. To demonstrate the generality of our proposed\napproach and its potential for handling cyber security applications, we\nevaluate our method and several other existing solutions on datasets publicly\navailable. Our results indicate that our approach typically provides superior\nclassification performance and resistance in comparison with state-of-art\nsolutions.\n"], ["2016-12-01", "http://arxiv.org/abs/1612.00334", "A Theoretical Framework for Robustness of (Deep) Classifiers against Adversarial Examples.", ["Beilun Wang", " Ji Gao", " Yanjun Qi"], "  Most machine learning classifiers, including deep neural networks, are\nvulnerable to adversarial examples. Such inputs are typically generated by\nadding small but purposeful modifications that lead to incorrect outputs while\nimperceptible to human eyes. The goal of this paper is not to introduce a\nsingle method, but to make theoretical steps towards fully understanding\nadversarial examples. By using concepts from topology, our theoretical analysis\nbrings forth the key reasons why an adversarial example can fool a classifier\n($f_1$) and adds its oracle ($f_2$, like human eyes) in such analysis. By\ninvestigating the topological relationship between two (pseudo)metric spaces\ncorresponding to predictor $f_1$ and oracle $f_2$, we develop necessary and\nsufficient conditions that can determine if $f_1$ is always robust\n(strong-robust) against adversarial examples according to $f_2$. Interestingly\nour theorems indicate that just one unnecessary feature can make $f_1$ not\nstrong-robust, and the right feature representation learning is the key to\ngetting a classifier that is both accurate and strong-robust.\n"], ["2016-12-01", "http://arxiv.org/abs/1612.00155", "Adversarial Images for Variational Autoencoders.", ["Pedro Tabacof", " Julia Tavares", " Eduardo Valle"], "  We investigate adversarial attacks for autoencoders. We propose a procedure\nthat distorts the input image to mislead the autoencoder in reconstructing a\ncompletely different target image. We attack the internal latent\nrepresentations, attempting to make the adversarial input produce an internal\nrepresentation as similar as possible as the target's. We find that\nautoencoders are much more robust to the attack than classifiers: while some\nexamples have tolerably small input distortion, and reasonable similarity to\nthe target image, there is a quasi-linear trade-off between those aims. We\nreport results on MNIST and SVHN datasets, and also test regular deterministic\nautoencoders, reaching similar conclusions in all cases. Finally, we show that\nthe usual adversarial attack for classifiers, while being much easier, also\npresents a direct proportion between distortion on the input, and misdirection\non the output. That proportionality however is hidden by the normalization of\nthe output, which maps a linear layer into non-linear probabilities.\n"], ["2016-12-01", "http://arxiv.org/abs/1612.00410", "Deep Variational Information Bottleneck.", ["Alexander A. Alemi", " Ian Fischer", " Joshua V. Dillon", " Kevin Murphy"], "  We present a variational approximation to the information bottleneck of\nTishby et al. (1999). This variational approach allows us to parameterize the\ninformation bottleneck model using a neural network and leverage the\nreparameterization trick for efficient training. We call this method \"Deep\nVariational Information Bottleneck\", or Deep VIB. We show that models trained\nwith the VIB objective outperform those that are trained with other forms of\nregularization, in terms of generalization performance and robustness to\nadversarial attack.\n"], ["2016-11-30", "http://arxiv.org/abs/1612.00138", "Towards Robust Deep Neural Networks with BANG.", ["Andras Rozsa", " Manuel Gunther", " Terrance E. Boult"], "  Machine learning models, including state-of-the-art deep neural networks, are\nvulnerable to small perturbations that cause unexpected classification errors.\nThis unexpected lack of robustness raises fundamental questions about their\ngeneralization properties and poses a serious concern for practical\ndeployments. As such perturbations can remain imperceptible - the formed\nadversarial examples demonstrate an inherent inconsistency between vulnerable\nmachine learning models and human perception - some prior work casts this\nproblem as a security issue. Despite the significance of the discovered\ninstabilities and ensuing research, their cause is not well understood and no\neffective method has been developed to address the problem. In this paper, we\npresent a novel theory to explain why this unpleasant phenomenon exists in deep\nneural networks. Based on that theory, we introduce a simple, efficient, and\neffective training approach, Batch Adjusted Network Gradients (BANG), which\nsignificantly improves the robustness of machine learning models. While the\nBANG technique does not rely on any form of data augmentation or the\nutilization of adversarial images for training, the resultant classifiers are\nmore resistant to adversarial perturbations while maintaining or even enhancing\nthe overall classification performance.\n"], ["2016-11-18", "http://arxiv.org/abs/1611.06179", "LOTS about Attacking Deep Features.", ["Andras Rozsa", " Manuel G\u00fcnther", " Terrance E. Boult"], "  Deep neural networks provide state-of-the-art performance on various tasks\nand are, therefore, widely used in real world applications. DNNs are becoming\nfrequently utilized in biometrics for extracting deep features, which can be\nused in recognition systems for enrolling and recognizing new individuals. It\nwas revealed that deep neural networks suffer from a fundamental problem,\nnamely, they can unexpectedly misclassify examples formed by slightly\nperturbing correctly recognized inputs. Various approaches have been developed\nfor generating these so-called adversarial examples, but they aim at attacking\nend-to-end networks. For biometrics, it is natural to ask whether systems using\ndeep features are immune to or, at least, more resilient to attacks than\nend-to-end networks. In this paper, we introduce a general technique called the\nlayerwise origin-target synthesis (LOTS) that can be efficiently used to form\nadversarial examples that mimic the deep features of the target. We analyze and\ncompare the adversarial robustness of the end-to-end VGG Face network with\nsystems that use Euclidean or cosine distance between gallery templates and\nextracted deep features. We demonstrate that iterative LOTS is very effective\nand show that systems utilizing deep features are easier to attack than the\nend-to-end network.\n"], ["2016-11-15", "http://arxiv.org/abs/1611.04786", "AdversariaLib: An Open-source Library for the Security Evaluation of Machine Learning Algorithms Under Attack.", ["Igino Corona", " Battista Biggio", " Davide Maiorca"], "  We present AdversariaLib, an open-source python library for the security\nevaluation of machine learning (ML) against carefully-targeted attacks. It\nsupports the implementation of several attacks proposed thus far in the\nliterature of adversarial learning, allows for the evaluation of a wide range\nof ML algorithms, runs on multiple platforms, and has multi-processing enabled.\nThe library has a modular architecture that makes it easy to use and to extend\nby implementing novel attacks and countermeasures. It relies on other\nwidely-used open-source ML libraries, including scikit-learn and FANN.\nClassification algorithms are implemented and optimized in C/C++, allowing for\na fast evaluation of the simulated attacks. The package is distributed under\nthe GNU General Public License v3, and it is available for download at\nhttp://sourceforge.net/projects/adversarialib.\n"], ["2016-11-11", "http://arxiv.org/abs/1611.03814", "Towards the Science of Security and Privacy in Machine Learning.", ["Nicolas Papernot", " Patrick McDaniel", " Arunesh Sinha", " Michael Wellman"], "  Advances in machine learning (ML) in recent years have enabled a dizzying\narray of applications such as data analytics, autonomous systems, and security\ndiagnostics. ML is now pervasive---new systems and models are being deployed in\nevery domain imaginable, leading to rapid and widespread deployment of software\nbased inference and decision making. There is growing recognition that ML\nexposes new vulnerabilities in software systems, yet the technical community's\nunderstanding of the nature and extent of these vulnerabilities remains\nlimited. We systematize recent findings on ML security and privacy, focusing on\nattacks identified on these systems and defenses crafted to date. We articulate\na comprehensive threat model for ML, and categorize attacks and defenses within\nan adversarial framework. Key insights resulting from works both in the ML and\nsecurity communities are identified and the effectiveness of approaches are\nrelated to structural elements of ML algorithms and the data used to train\nthem. We conclude by formally exploring the opposing relationship between model\naccuracy and resilience to adversarial manipulation. Through these\nexplorations, we show that there are (possibly unavoidable) tensions between\nmodel complexity, accuracy, and resilience that must be calibrated for the\nenvironments in which they will be used.\n"], ["2016-11-08", "http://arxiv.org/abs/1611.02770", "Delving into Transferable Adversarial Examples and Black-box Attacks.", ["Yanpei Liu", " Xinyun Chen", " Chang Liu", " Dawn Song"], "  An intriguing property of deep neural networks is the existence of\nadversarial examples, which can transfer among different architectures. These\ntransferable adversarial examples may severely hinder deep neural network-based\napplications. Previous works mostly study the transferability using small scale\ndatasets. In this work, we are the first to conduct an extensive study of the\ntransferability over large models and a large scale dataset, and we are also\nthe first to study the transferability of targeted adversarial examples with\ntheir target labels. We study both non-targeted and targeted adversarial\nexamples, and show that while transferable non-targeted adversarial examples\nare easy to find, targeted adversarial examples generated using existing\napproaches almost never transfer with their target labels. Therefore, we\npropose novel ensemble-based approaches to generating transferable adversarial\nexamples. Using such approaches, we observe a large proportion of targeted\nadversarial examples that are able to transfer with their target labels for the\nfirst time. We also present some geometric studies to help understanding the\ntransferable adversarial examples. Finally, we show that the adversarial\nexamples generated using ensemble-based approaches can successfully attack\nClarifai.com, which is a black-box image classification system.\n"], ["2016-11-03", "http://arxiv.org/abs/1611.01236", "Adversarial Machine Learning at Scale.", ["Alexey Kurakin", " Ian Goodfellow", " Samy Bengio"], "  Adversarial examples are malicious inputs designed to fool machine learning\nmodels. They often transfer from one model to another, allowing attackers to\nmount black box attacks without knowledge of the target model's parameters.\nAdversarial training is the process of explicitly training a model on\nadversarial examples, in order to make it more robust to attack or to reduce\nits test error on clean inputs. So far, adversarial training has primarily been\napplied to small problems. In this research, we apply adversarial training to\nImageNet. Our contributions include: (1) recommendations for how to succesfully\nscale adversarial training to large models and datasets, (2) the observation\nthat adversarial training confers robustness to single-step attack methods, (3)\nthe finding that multi-step attack methods are somewhat less transferable than\nsingle-step attack methods, so single-step attacks are the best for mounting\nblack-box attacks, and (4) resolution of a \"label leaking\" effect that causes\nadversarially trained models to perform better on adversarial examples than on\nclean examples, because the adversarial example construction process uses the\ntrue label and the model can learn to exploit regularities in the construction\nprocess.\n"], ["2016-10-26", "http://arxiv.org/abs/1610.08401", "Universal adversarial perturbations.", ["Seyed-Mohsen Moosavi-Dezfooli", " Alhussein Fawzi", " Omar Fawzi", " Pascal Frossard"], "  Given a state-of-the-art deep neural network classifier, we show the\nexistence of a universal (image-agnostic) and very small perturbation vector\nthat causes natural images to be misclassified with high probability. We\npropose a systematic algorithm for computing universal perturbations, and show\nthat state-of-the-art deep neural networks are highly vulnerable to such\nperturbations, albeit being quasi-imperceptible to the human eye. We further\nempirically analyze these universal perturbations and show, in particular, that\nthey generalize very well across neural networks. The surprising existence of\nuniversal perturbations reveals important geometric correlations among the\nhigh-dimensional decision boundary of classifiers. It further outlines\npotential security breaches with the existence of single directions in the\ninput space that adversaries can possibly exploit to break a classifier on most\nnatural images.\n"], ["2016-10-21", "http://arxiv.org/abs/1610.06940", "Safety Verification of Deep Neural Networks.", ["Xiaowei Huang", " Marta Kwiatkowska", " Sen Wang", " Min Wu"], "  Deep neural networks have achieved impressive experimental results in image\nclassification, but can surprisingly be unstable with respect to adversarial\nperturbations, that is, minimal changes to the input image that cause the\nnetwork to misclassify it. With potential applications including perception\nmodules and end-to-end controllers for self-driving cars, this raises concerns\nabout their safety. We develop a novel automated verification framework for\nfeed-forward multi-layer neural networks based on Satisfiability Modulo Theory\n(SMT). We focus on safety of image classification decisions with respect to\nimage manipulations, such as scratches or changes to camera angle or lighting\nconditions that would result in the same class being assigned by a human, and\ndefine safety for an individual decision in terms of invariance of the\nclassification within a small neighbourhood of the original image. We enable\nexhaustive search of the region by employing discretisation, and propagate the\nanalysis layer by layer. Our method works directly with the network code and,\nin contrast to existing methods, can guarantee that adversarial examples, if\nthey exist, are found for the given region and family of manipulations. If\nfound, adversarial examples can be shown to human testers and/or used to\nfine-tune the network. We implement the techniques using Z3 and evaluate them\non state-of-the-art networks, including regularised and deep learning networks.\nWe also compare against existing techniques to search for adversarial examples\nand estimate network robustness.\n"], ["2016-10-14", "http://arxiv.org/abs/1610.04563", "Are Accuracy and Robustness Correlated?.", ["Andras Rozsa", " Manuel G\u00fcnther", " Terrance E. Boult"], "  Machine learning models are vulnerable to adversarial examples formed by\napplying small carefully chosen perturbations to inputs that cause unexpected\nclassification errors. In this paper, we perform experiments on various\nadversarial example generation approaches with multiple deep convolutional\nneural networks including Residual Networks, the best performing models on\nImageNet Large-Scale Visual Recognition Challenge 2015. We compare the\nadversarial example generation techniques with respect to the quality of the\nproduced images, and measure the robustness of the tested machine learning\nmodels to adversarial examples. Finally, we conduct large-scale experiments on\ncross-model adversarial portability. We find that adversarial examples are\nmostly transferable across similar network topologies, and we demonstrate that\nbetter machine learning models are less vulnerable to adversarial examples.\n"], ["2016-10-13", "http://arxiv.org/abs/1610.04256", "Assessing Threat of Adversarial Examples on Deep Neural Networks.", ["Abigail Graese", " Andras Rozsa", " Terrance E. Boult"], "  Deep neural networks are facing a potential security threat from adversarial\nexamples, inputs that look normal but cause an incorrect classification by the\ndeep neural network. For example, the proposed threat could result in\nhand-written digits on a scanned check being incorrectly classified but looking\nnormal when humans see them. This research assesses the extent to which\nadversarial examples pose a security threat, when one considers the normal\nimage acquisition process. This process is mimicked by simulating the\ntransformations that normally occur in acquiring the image in a real world\napplication, such as using a scanner to acquire digits for a check amount or\nusing a camera in an autonomous car. These small transformations negate the\neffect of the carefully crafted perturbations of adversarial examples,\nresulting in a correct classification by the deep neural network. Thus just\nacquiring the image decreases the potential impact of the proposed security\nthreat. We also show that the already widely used process of averaging over\nmultiple crops neutralizes most adversarial examples. Normal preprocessing,\nsuch as text binarization, almost completely neutralizes adversarial examples.\nThis is the first paper to show that for text driven classification,\nadversarial examples are an academic curiosity, not a security threat.\n"], ["2016-10-06", "http://arxiv.org/abs/1610.01934", "Using Non-invertible Data Transformations to Build Adversarial-Robust Neural Networks.", ["Qinglong Wang", " Wenbo Guo", " Alexander G. II Ororbia", " Xinyu Xing", " Lin Lin", " C. Lee Giles", " Xue Liu", " Peng Liu", " Gang Xiong"], "  Deep neural networks have proven to be quite effective in a wide variety of\nmachine learning tasks, ranging from improved speech recognition systems to\nadvancing the development of autonomous vehicles. However, despite their\nsuperior performance in many applications, these models have been recently\nshown to be susceptible to a particular type of attack possible through the\ngeneration of particular synthetic examples referred to as adversarial samples.\nThese samples are constructed by manipulating real examples from the training\ndata distribution in order to \"fool\" the original neural model, resulting in\nmisclassification (with high confidence) of previously correctly classified\nsamples. Addressing this weakness is of utmost importance if deep neural\narchitectures are to be applied to critical applications, such as those in the\ndomain of cybersecurity. In this paper, we present an analysis of this\nfundamental flaw lurking in all neural architectures to uncover limitations of\npreviously proposed defense mechanisms. More importantly, we present a unifying\nframework for protecting deep neural models using a non-invertible data\ntransformation--developing two adversary-resilient architectures utilizing both\nlinear and nonlinear dimensionality reduction. Empirical results indicate that\nour framework provides better robustness compared to state-of-art solutions\nwhile having negligible degradation in accuracy.\n"], ["2016-10-04", "http://arxiv.org/abs/1610.01239", "Adversary Resistant Deep Neural Networks with an Application to Malware Detection.", ["Qinglong Wang", " Wenbo Guo", " Kaixuan Zhang", " Alexander G. II Ororbia", " Xinyu Xing", " C. Lee Giles", " Xue Liu"], "  Beyond its highly publicized victories in Go, there have been numerous\nsuccessful applications of deep learning in information retrieval, computer\nvision and speech recognition. In cybersecurity, an increasing number of\ncompanies have become excited about the potential of deep learning, and have\nstarted to use it for various security incidents, the most popular being\nmalware detection. These companies assert that deep learning (DL) could help\nturn the tide in the battle against malware infections. However, deep neural\nnetworks (DNNs) are vulnerable to adversarial samples, a flaw that plagues most\nif not all statistical learning models. Recent research has demonstrated that\nthose with malicious intent can easily circumvent deep learning-powered malware\ndetection by exploiting this flaw.\n  In order to address this problem, previous work has developed various defense\nmechanisms that either augmenting training data or enhance model's complexity.\nHowever, after a thorough analysis of the fundamental flaw in DNNs, we discover\nthat the effectiveness of current defenses is limited and, more importantly,\ncannot provide theoretical guarantees as to their robustness against\nadversarial sampled-based attacks. As such, we propose a new adversary\nresistant technique that obstructs attackers from constructing impactful\nadversarial samples by randomly nullifying features within samples. In this\nwork, we evaluate our proposed technique against a real world dataset with\n14,679 malware variants and 17,399 benign programs. We theoretically validate\nthe robustness of our technique, and empirically show that our technique\nsignificantly boosts DNN robustness to adversarial samples while maintaining\nhigh accuracy in classification. To demonstrate the general applicability of\nour proposed method, we also conduct experiments using the MNIST and CIFAR-10\ndatasets, generally used in image recognition research.\n"], ["2016-10-03", "http://arxiv.org/abs/1610.00768", "Technical Report on the CleverHans v2.1.0 Adversarial Examples Library.", ["Nicolas Papernot", " Fartash Faghri", " Nicholas Carlini", " Ian Goodfellow", " Reuben Feinman", " Alexey Kurakin", " Cihang Xie", " Yash Sharma", " Tom Brown", " Aurko Roy", " Alexander Matyasko", " Vahid Behzadan", " Karen Hambardzumyan", " Zhishuai Zhang", " Yi-Lin Juang", " Zhi Li", " Ryan Sheatsley", " Abhibhav Garg", " Jonathan Uesato", " Willi Gierke", " Yinpeng Dong", " David Berthelot", " Paul Hendricks", " Jonas Rauber", " Rujun Long", " Patrick McDaniel"], "  CleverHans is a software library that provides standardized reference\nimplementations of adversarial example construction techniques and adversarial\ntraining. The library may be used to develop more robust machine learning\nmodels and to provide standardized benchmarks of models' performance in the\nadversarial setting. Benchmarks constructed without a standardized\nimplementation of adversarial example construction are not comparable to each\nother, because a good result may indicate a robust model or it may merely\nindicate a weak implementation of the adversarial example construction\nprocedure.\n  This technical report is structured as follows. Section 1 provides an\noverview of adversarial examples in machine learning and of the CleverHans\nsoftware. Section 2 presents the core functionalities of the library: namely\nthe attacks based on adversarial examples and defenses to improve the\nrobustness of machine learning models to these attacks. Section 3 describes how\nto report benchmark results using the library. Section 4 describes the\nversioning system.\n"], ["2016-09-06", "http://arxiv.org/abs/1609.01461", "Statistical Meta-Analysis of Presentation Attacks for Secure Multibiometric Systems.", ["Battista Biggio", " Giorgio Fumera", " Gian Luca Marcialis", " Fabio Roli"], "  Prior work has shown that multibiometric systems are vulnerable to\npresentation attacks, assuming that their matching score distribution is\nidentical to that of genuine users, without fabricating any fake trait. We have\nrecently shown that this assumption is not representative of current\nfingerprint and face presentation attacks, leading one to overestimate the\nvulnerability of multibiometric systems, and to design less effective fusion\nrules. In this paper, we overcome these limitations by proposing a statistical\nmeta-model of face and fingerprint presentation attacks that characterizes a\nwider family of fake score distributions, including distributions of known and,\npotentially, unknown attacks. This allows us to perform a thorough security\nevaluation of multibiometric systems against presentation attacks, quantifying\nhow their vulnerability may vary also under attacks that are different from\nthose considered during design, through an uncertainty analysis. We empirically\nshow that our approach can reliably predict the performance of multibiometric\nsystems even under never-before-seen face and fingerprint presentation attacks,\nand that the secure fusion rules designed using our approach can exhibit an\nimproved trade-off between the performance in the absence and in the presence\nof attack. We finally argue that our method can be extended to other biometrics\nbesides faces and fingerprints.\n"], ["2016-09-03", "http://arxiv.org/abs/1609.00804", "Randomized Prediction Games for Adversarial Machine Learning.", ["Samuel Rota Bul\u00f2", " Battista Biggio", " Ignazio Pillai", " Marcello Pelillo", " Fabio Roli"], "  In spam and malware detection, attackers exploit randomization to obfuscate\nmalicious data and increase their chances of evading detection at test time;\ne.g., malware code is typically obfuscated using random strings or byte\nsequences to hide known exploits. Interestingly, randomization has also been\nproposed to improve security of learning algorithms against evasion attacks, as\nit results in hiding information about the classifier to the attacker. Recent\nwork has proposed game-theoretical formulations to learn secure classifiers, by\nsimulating different evasion attacks and modifying the classification function\naccordingly. However, both the classification function and the simulated data\nmanipulations have been modeled in a deterministic manner, without accounting\nfor any form of randomization. In this work, we overcome this limitation by\nproposing a randomized prediction game, namely, a non-cooperative\ngame-theoretic formulation in which the classifier and the attacker make\nrandomized strategy selections according to some probability distribution\ndefined over the respective strategy set. We show that our approach allows one\nto improve the trade-off between attack detection and false alarms with respect\nto state-of-the-art secure classifiers, even against attacks that are different\nfrom those hypothesized during design, on application examples including\nhandwritten digit recognition, spam and malware detection.\n"], ["2016-08-31", "http://arxiv.org/abs/1608.08967", "Robustness of classifiers: from adversarial to random noise.", ["Alhussein Fawzi", " Seyed-Mohsen Moosavi-Dezfooli", " Pascal Frossard"], "  Several recent works have shown that state-of-the-art classifiers are\nvulnerable to worst-case (i.e., adversarial) perturbations of the datapoints.\nOn the other hand, it has been empirically observed that these same classifiers\nare relatively robust to random noise. In this paper, we propose to study a\n\\textit{semi-random} noise regime that generalizes both the random and\nworst-case noise regimes. We propose the first quantitative analysis of the\nrobustness of nonlinear classifiers in this general noise regime. We establish\nprecise theoretical bounds on the robustness of classifiers in this general\nregime, which depend on the curvature of the classifier's decision boundary.\nOur bounds confirm and quantify the empirical observations that classifiers\nsatisfying curvature constraints are robust to random noise. Moreover, we\nquantify the robustness of classifiers in terms of the subspace dimension in\nthe semi-random noise regime, and show that our bounds remarkably interpolate\nbetween the worst-case and random noise regimes. We perform experiments and\nshow that the derived bounds provide very accurate estimates when applied to\nvarious state-of-the-art deep neural networks and datasets. This result\nsuggests bounds on the curvature of the classifiers' decision boundaries that\nwe support experimentally, and more generally offers important insights onto\nthe geometry of high dimensional classification problems.\n"], ["2016-08-27", "http://arxiv.org/abs/1608.07690", "A Boundary Tilting Persepective on the Phenomenon of Adversarial Examples.", ["Thomas Tanay", " Lewis Griffin"], "  Deep neural networks have been shown to suffer from a surprising weakness:\ntheir classification outputs can be changed by small, non-random perturbations\nof their inputs. This adversarial example phenomenon has been explained as\noriginating from deep networks being \"too linear\" (Goodfellow et al., 2014). We\nshow here that the linear explanation of adversarial examples presents a number\nof limitations: the formal argument is not convincing, linear classifiers do\nnot always suffer from the phenomenon, and when they do their adversarial\nexamples are different from the ones affecting deep networks.\n  We propose a new perspective on the phenomenon. We argue that adversarial\nexamples exist when the classification boundary lies close to the submanifold\nof sampled data, and present a mathematical analysis of this new perspective in\nthe linear case. We define the notion of adversarial strength and show that it\ncan be reduced to the deviation angle between the classifier considered and the\nnearest centroid classifier. Then, we show that the adversarial strength can be\nmade arbitrarily high independently of the classification performance due to a\nmechanism that we call boundary tilting. This result leads us to defining a new\ntaxonomy of adversarial examples. Finally, we show that the adversarial\nstrength observed in practice is directly dependent on the level of\nregularisation used and the strongest adversarial examples, symptomatic of\noverfitting, can be avoided by using a proper level of regularisation.\n"], ["2016-08-16", "http://arxiv.org/abs/1608.04644", "Towards Evaluating the Robustness of Neural Networks.", ["Nicholas Carlini", " David Wagner"], "  Neural networks provide state-of-the-art results for most machine learning\ntasks. Unfortunately, neural networks are vulnerable to adversarial examples:\ngiven an input $x$ and any target classification $t$, it is possible to find a\nnew input $x'$ that is similar to $x$ but classified as $t$. This makes it\ndifficult to apply neural networks in security-critical areas. Defensive\ndistillation is a recently proposed approach that can take an arbitrary neural\nnetwork, and increase its robustness, reducing the success rate of current\nattacks' ability to find adversarial examples from $95\\%$ to $0.5\\%$.\n  In this paper, we demonstrate that defensive distillation does not\nsignificantly increase the robustness of neural networks by introducing three\nnew attack algorithms that are successful on both distilled and undistilled\nneural networks with $100\\%$ probability. Our attacks are tailored to three\ndistance metrics used previously in the literature, and when compared to\nprevious adversarial example generation algorithms, our attacks are often much\nmore effective (and never worse). Furthermore, we propose using high-confidence\nadversarial examples in a simple transferability test we show can also be used\nto break defensive distillation. We hope our attacks will be used as a\nbenchmark in future defense attempts to create neural networks that resist\nadversarial examples.\n"], ["2016-08-02", "http://arxiv.org/abs/1608.00853", "A study of the effect of JPG compression on adversarial images.", ["Gintare Karolina Dziugaite", " Zoubin Ghahramani", " Daniel M. Roy"], "  Neural network image classifiers are known to be vulnerable to adversarial\nimages, i.e., natural images which have been modified by an adversarial\nperturbation specifically designed to be imperceptible to humans yet fool the\nclassifier. Not only can adversarial images be generated easily, but these\nimages will often be adversarial for networks trained on disjoint subsets of\ndata or with different architectures. Adversarial images represent a potential\nsecurity risk as well as a serious machine learning challenge---it is clear\nthat vulnerable neural networks perceive images very differently from humans.\nNoting that virtually every image classification data set is composed of JPG\nimages, we evaluate the effect of JPG compression on the classification of\nadversarial images. For Fast-Gradient-Sign perturbations of small magnitude, we\nfound that JPG compression often reverses the drop in classification accuracy\nto a large extent, but not always. As the magnitude of the perturbations\nincreases, JPG recompression alone is insufficient to reverse the effect.\n"], ["2016-08-01", "http://arxiv.org/abs/1608.00530", "Early Methods for Detecting Adversarial Images.", ["Dan Hendrycks", " Kevin Gimpel"], "  Many machine learning classifiers are vulnerable to adversarial\nperturbations. An adversarial perturbation modifies an input to change a\nclassifier's prediction without causing the input to seem substantially\ndifferent to human perception. We deploy three methods to detect adversarial\nimages. Adversaries trying to bypass our detectors must make the adversarial\nimage less pathological or they will fail trying. Our best detection method\nreveals that adversarial images place abnormal emphasis on the lower-ranked\nprincipal components from PCA. Other detectors and a colorful saliency map are\nin an appendix.\n"], ["2016-07-18", "http://arxiv.org/abs/1607.05113", "On the Effectiveness of Defensive Distillation.", ["Nicolas Papernot", " Patrick McDaniel"], "  We report experimental results indicating that defensive distillation\nsuccessfully mitigates adversarial samples crafted using the fast gradient sign\nmethod, in addition to those crafted using the Jacobian-based iterative attack\non which the defense mechanism was originally evaluated.\n"], ["2016-07-14", "http://arxiv.org/abs/1607.04311", "Defensive Distillation is Not Robust to Adversarial Examples.", ["Nicholas Carlini", " David Wagner"], "  We show that defensive distillation is not secure: it is no more resistant to\ntargeted misclassification attacks than unprotected neural networks.\n"], ["2016-07-08", "http://arxiv.org/abs/1607.02533", "Adversarial examples in the physical world.", ["Alexey Kurakin", " Ian Goodfellow", " Samy Bengio"], "  Most existing machine learning classifiers are highly vulnerable to\nadversarial examples. An adversarial example is a sample of input data which\nhas been modified very slightly in a way that is intended to cause a machine\nlearning classifier to misclassify it. In many cases, these modifications can\nbe so subtle that a human observer does not even notice the modification at\nall, yet the classifier still makes a mistake. Adversarial examples pose\nsecurity concerns because they could be used to perform an attack on machine\nlearning systems, even if the adversary has no access to the underlying model.\nUp to now, all previous work have assumed a threat model in which the adversary\ncan feed data directly into the machine learning classifier. This is not always\nthe case for systems operating in the physical world, for example those which\nare using signals from cameras and other sensors as an input. This paper shows\nthat even in such physical world scenarios, machine learning systems are\nvulnerable to adversarial examples. We demonstrate this by feeding adversarial\nimages obtained from cell-phone camera to an ImageNet Inception classifier and\nmeasuring the classification accuracy of the system. We find that a large\nfraction of adversarial examples are classified incorrectly even when perceived\nthrough the camera.\n"], ["2016-06-14", "http://arxiv.org/abs/1606.04435", "Adversarial Perturbations Against Deep Neural Networks for Malware Classification.", ["Kathrin Grosse", " Nicolas Papernot", " Praveen Manoharan", " Michael Backes", " Patrick McDaniel"], "  Deep neural networks, like many other machine learning models, have recently\nbeen shown to lack robustness against adversarially crafted inputs. These\ninputs are derived from regular inputs by minor yet carefully selected\nperturbations that deceive machine learning models into desired\nmisclassifications. Existing work in this emerging field was largely specific\nto the domain of image classification, since the high-entropy of images can be\nconveniently manipulated without changing the images' overall visual\nappearance. Yet, it remains unclear how such attacks translate to more\nsecurity-sensitive applications such as malware detection - which may pose\nsignificant challenges in sample generation and arguably grave consequences for\nfailure.\n  In this paper, we show how to construct highly-effective adversarial sample\ncrafting attacks for neural networks used as malware classifiers. The\napplication domain of malware classification introduces additional constraints\nin the adversarial sample crafting problem when compared to the computer vision\ndomain: (i) continuous, differentiable input domains are replaced by discrete,\noften binary inputs; and (ii) the loose condition of leaving visual appearance\nunchanged is replaced by requiring equivalent functional behavior. We\ndemonstrate the feasibility of these attacks on many different instances of\nmalware classifiers that we trained using the DREBIN Android malware data set.\nWe furthermore evaluate to which extent potential defensive mechanisms against\nadversarial crafting can be leveraged to the setting of malware classification.\nWhile feature reduction did not prove to have a positive impact, distillation\nand re-training on adversarially crafted samples show promising results.\n"], ["2016-05-23", "http://arxiv.org/abs/1605.07277", "Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples.", ["Nicolas Papernot", " Patrick McDaniel", " Ian Goodfellow"], "  Many machine learning models are vulnerable to adversarial examples: inputs\nthat are specially crafted to cause a machine learning model to produce an\nincorrect output. Adversarial examples that affect one model often affect\nanother model, even if the two models have different architectures or were\ntrained on different training sets, so long as both models were trained to\nperform the same task. An attacker may therefore train their own substitute\nmodel, craft adversarial examples against the substitute, and transfer them to\na victim model, with very little information about the victim. Recent work has\nfurther developed a technique that uses the victim model as an oracle to label\na synthetic training set for the substitute, so the attacker need not even\ncollect a training set to mount the attack. We extend these recent techniques\nusing reservoir sampling to greatly enhance the efficiency of the training\nprocedure for the substitute model. We introduce new transferability attacks\nbetween previously unexplored (substitute, victim) pairs of machine learning\nmodel classes, most notably SVMs and decision trees. We demonstrate our attacks\non two commercial machine learning classification systems from Amazon (96.19%\nmisclassification rate) and Google (88.94%) using only 800 queries of the\nvictim model, thereby showing that existing machine learning approaches are in\ngeneral vulnerable to systematic black-box attacks regardless of their\nstructure.\n"], ["2016-05-23", "http://arxiv.org/abs/1605.07262", "Measuring Neural Net Robustness with Constraints.", ["Osbert Bastani", " Yani Ioannou", " Leonidas Lampropoulos", " Dimitrios Vytiniotis", " Aditya Nori", " Antonio Criminisi"], "  Despite having high accuracy, neural nets have been shown to be susceptible\nto adversarial examples, where a small perturbation to an input can cause it to\nbecome mislabeled. We propose metrics for measuring the robustness of a neural\nnet and devise a novel algorithm for approximating these metrics based on an\nencoding of robustness as a linear program. We show how our metrics can be used\nto evaluate the robustness of deep neural nets with experiments on the MNIST\nand CIFAR-10 datasets. Our algorithm generates more informative estimates of\nrobustness metrics compared to estimates based on existing algorithms.\nFurthermore, we show how existing approaches to improving robustness \"overfit\"\nto adversarial examples generated using a specific algorithm. Finally, we show\nthat our techniques can be used to additionally improve neural net robustness\nboth according to the metrics that we propose, but also according to previously\nproposed metrics.\n"], ["2016-05-17", "http://arxiv.org/abs/1605.05411", "Are Facial Attributes Adversarially Robust?.", ["Andras Rozsa", " Manuel G\u00fcnther", " Ethan M. Rudd", " Terrance E. Boult"], "  Facial attributes are emerging soft biometrics that have the potential to\nreject non-matches, for example, based on mismatching gender. To be usable in\nstand-alone systems, facial attributes must be extracted from images\nautomatically and reliably. In this paper, we propose a simple yet effective\nsolution for automatic facial attribute extraction by training a deep\nconvolutional neural network (DCNN) for each facial attribute separately,\nwithout using any pre-training or dataset augmentation, and we obtain new\nstate-of-the-art facial attribute classification results on the CelebA\nbenchmark. To test the stability of the networks, we generated adversarial\nimages -- formed by adding imperceptible non-random perturbations to original\ninputs which result in classification errors -- via a novel fast flipping\nattribute (FFA) technique. We show that FFA generates more adversarial examples\nthan other related algorithms, and that DCNNs for certain attributes are\ngenerally robust to adversarial inputs, while DCNNs for other attributes are\nnot. This result is surprising because no DCNNs tested to date have exhibited\nrobustness to adversarial images without explicit augmentation in the training\nprocedure to account for adversarial examples. Finally, we introduce the\nconcept of natural adversarial samples, i.e., images that are misclassified but\ncan be easily turned into correctly classified images by applying small\nperturbations. We demonstrate that natural adversarial samples commonly occur,\neven within the training set, and show that many of these images remain\nmisclassified even with additional training epochs. This phenomenon is\nsurprising because correcting the misclassification, particularly when guided\nby training data, should require only a small adjustment to the DCNN\nparameters.\n"], ["2016-05-05", "http://arxiv.org/abs/1605.01775", "Adversarial Diversity and Hard Positive Generation.", ["Andras Rozsa", " Ethan M. Rudd", " Terrance E. Boult"], "  State-of-the-art deep neural networks suffer from a fundamental problem -\nthey misclassify adversarial examples formed by applying small perturbations to\ninputs. In this paper, we present a new psychometric perceptual adversarial\nsimilarity score (PASS) measure for quantifying adversarial images, introduce\nthe notion of hard positive generation, and use a diverse set of adversarial\nperturbations - not just the closest ones - for data augmentation. We introduce\na novel hot/cold approach for adversarial example generation, which provides\nmultiple possible adversarial perturbations for every single image. The\nperturbations generated by our novel approach often correspond to semantically\nmeaningful image structures, and allow greater flexibility to scale\nperturbation-amplitudes, which yields an increased diversity of adversarial\nimages. We present adversarial images on several network topologies and\ndatasets, including LeNet on the MNIST dataset, and GoogLeNet and ResidualNet\non the ImageNet dataset. Finally, we demonstrate on LeNet and GoogLeNet that\nfine-tuning with a diverse set of hard positives improves the robustness of\nthese networks compared to training with prior methods of generating\nadversarial images.\n"], ["2016-04-27", "http://arxiv.org/abs/1604.08275", "Crafting Adversarial Input Sequences for Recurrent Neural Networks.", ["Nicolas Papernot", " Patrick McDaniel", " Ananthram Swami", " Richard Harang"], "  Machine learning models are frequently used to solve complex security\nproblems, as well as to make decisions in sensitive situations like guiding\nautonomous vehicles or predicting financial market behaviors. Previous efforts\nhave shown that numerous machine learning models were vulnerable to adversarial\nmanipulations of their inputs taking the form of adversarial samples. Such\ninputs are crafted by adding carefully selected perturbations to legitimate\ninputs so as to force the machine learning model to misbehave, for instance by\noutputting a wrong class if the machine learning task of interest is\nclassification. In fact, to the best of our knowledge, all previous work on\nadversarial samples crafting for neural network considered models used to solve\nclassification tasks, most frequently in computer vision applications. In this\npaper, we contribute to the field of adversarial machine learning by\ninvestigating adversarial input sequences for recurrent neural networks\nprocessing sequential data. We show that the classes of algorithms introduced\npreviously to craft adversarial samples misclassified by feed-forward neural\nnetworks can be adapted to recurrent neural networks. In a experiment, we show\nthat adversaries can craft adversarial sequences misleading both categorical\nand sequential recurrent neural networks.\n"], ["2016-04-14", "http://arxiv.org/abs/1604.04326", "Improving the Robustness of Deep Neural Networks via Stability Training.", ["Stephan Zheng", " Yang Song", " Thomas Leung", " Ian Goodfellow"], "  In this paper we address the issue of output instability of deep neural\nnetworks: small perturbations in the visual input can significantly distort the\nfeature embeddings and output of a neural network. Such instability affects\nmany deep architectures with state-of-the-art performance on a wide range of\ncomputer vision tasks. We present a general stability training method to\nstabilize deep networks against small input distortions that result from\nvarious types of common image processing, such as compression, rescaling, and\ncropping. We validate our method by stabilizing the state-of-the-art Inception\narchitecture against these types of distortions. In addition, we demonstrate\nthat our stabilized model gives robust state-of-the-art performance on\nlarge-scale near-duplicate detection, similar-image ranking, and classification\non noisy datasets.\n"], ["2016-04-09", "http://arxiv.org/abs/1604.02606", "A General Retraining Framework for Scalable Adversarial Classification.", ["Bo Li", " Yevgeniy Vorobeychik", " Xinyun Chen"], "  Traditional classification algorithms assume that training and test data come\nfrom similar distributions. This assumption is violated in adversarial\nsettings, where malicious actors modify instances to evade detection. A number\nof custom methods have been developed for both adversarial evasion attacks and\nrobust learning. We propose the first systematic and general-purpose retraining\nframework which can: a) boost robustness of an \\emph{arbitrary} learning\nalgorithm, in the face of b) a broader class of adversarial models than any\nprior methods. We show that, under natural conditions, the retraining framework\nminimizes an upper bound on optimal adversarial risk, and show how to extend\nthis result to account for approximations of evasion attacks. Extensive\nexperimental evaluation demonstrates that our retraining methods are nearly\nindistinguishable from state-of-the-art algorithms for optimizing adversarial\nrisk, but are more general and far more scalable. The experiments also confirm\nthat without retraining, our adversarial framework dramatically reduces the\neffectiveness of learning. In contrast, retraining significantly boosts\nrobustness to evasion attacks without significantly compromising overall\naccuracy.\n"], ["2016-03-16", "http://arxiv.org/abs/1603.05145", "Suppressing the Unusual: towards Robust CNNs using Symmetric Activation Functions.", ["Qiyang Zhao", " Lewis D Griffin"], "  Many deep Convolutional Neural Networks (CNN) make incorrect predictions on\nadversarial samples obtained by imperceptible perturbations of clean samples.\nWe hypothesize that this is caused by a failure to suppress unusual signals\nwithin network layers. As remedy we propose the use of Symmetric Activation\nFunctions (SAF) in non-linear signal transducer units. These units suppress\nsignals of exceptional magnitude. We prove that SAF networks can perform\nclassification tasks to arbitrary precision in a simplified situation. In\npractice, rather than use SAFs alone, we add them into CNNs to improve their\nrobustness. The modified CNNs can be easily trained using popular strategies\nwith the moderate training load. Our experiments on MNIST and CIFAR-10 show\nthat the modified CNNs perform similarly to plain ones on clean samples, and\nare remarkably more robust against adversarial and nonsense samples.\n"], ["2016-02-08", "http://arxiv.org/abs/1602.02697", "Practical Black-Box Attacks against Machine Learning.", ["Nicolas Papernot", " Patrick McDaniel", " Ian Goodfellow", " Somesh Jha", " Z. Berkay Celik", " Ananthram Swami"], "  Machine learning (ML) models, e.g., deep neural networks (DNNs), are\nvulnerable to adversarial examples: malicious inputs modified to yield\nerroneous model outputs, while appearing unmodified to human observers.\nPotential attacks include having malicious content like malware identified as\nlegitimate or controlling vehicle behavior. Yet, all existing adversarial\nexample attacks require knowledge of either the model internals or its training\ndata. We introduce the first practical demonstration of an attacker controlling\na remotely hosted DNN with no such knowledge. Indeed, the only capability of\nour black-box adversary is to observe labels given by the DNN to chosen inputs.\nOur attack strategy consists in training a local model to substitute for the\ntarget DNN, using inputs synthetically generated by an adversary and labeled by\nthe target DNN. We use the local substitute to craft adversarial examples, and\nfind that they are misclassified by the targeted DNN. To perform a real-world\nand properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online\ndeep learning API. We find that their DNN misclassifies 84.24% of the\nadversarial examples crafted with our substitute. We demonstrate the general\napplicability of our strategy to many ML techniques by conducting the same\nattack against models hosted by Amazon and Google, using logistic regression\nsubstitutes. They yield adversarial examples misclassified by Amazon and Google\nat rates of 96.19% and 88.94%. We also find that this black-box attack strategy\nis capable of evading defense strategies previously found to make adversarial\nexample crafting harder.\n"], ["2016-02-07", "http://arxiv.org/abs/1602.02389", "Ensemble Robustness and Generalization of Stochastic Deep Learning Algorithms.", ["Tom Zahavy", " Bingyi Kang", " Alex Sivak", " Jiashi Feng", " Huan Xu", " Shie Mannor"], "  The question why deep learning algorithms generalize so well has attracted\nincreasing research interest. However, most of the well-established approaches,\nsuch as hypothesis capacity, stability or sparseness, have not provided\ncomplete explanations (Zhang et al., 2016; Kawaguchi et al., 2017). In this\nwork, we focus on the robustness approach (Xu & Mannor, 2012), i.e., if the\nerror of a hypothesis will not change much due to perturbations of its training\nexamples, then it will also generalize well. As most deep learning algorithms\nare stochastic (e.g., Stochastic Gradient Descent, Dropout, and\nBayes-by-backprop), we revisit the robustness arguments of Xu & Mannor, and\nintroduce a new approach, ensemble robustness, that concerns the robustness of\na population of hypotheses. Through the lens of ensemble robustness, we reveal\nthat a stochastic learning algorithm can generalize well as long as its\nsensitiveness to adversarial perturbations is bounded in average over training\nexamples. Moreover, an algorithm may be sensitive to some adversarial examples\n(Goodfellow et al., 2015) but still generalize well. To support our claims, we\nprovide extensive simulations for different deep learning algorithms and\ndifferent network architectures exhibiting a strong correlation between\nensemble robustness and the ability to generalize.\n"], ["2016-01-26", "http://arxiv.org/abs/1601.07213", "Unifying Adversarial Training Algorithms with Flexible Deep Data Gradient Regularization.", ["Alexander G. II Ororbia", " C. Lee Giles", " Daniel Kifer"], "  Many previous proposals for adversarial training of deep neural nets have\nincluded di- rectly modifying the gradient, training on a mix of original and\nadversarial examples, using contractive penalties, and approximately optimizing\nconstrained adversarial ob- jective functions. In this paper, we show these\nproposals are actually all instances of optimizing a general, regularized\nobjective we call DataGrad. Our proposed DataGrad framework, which can be\nviewed as a deep extension of the layerwise contractive au- toencoder penalty,\ncleanly simplifies prior work and easily allows extensions such as adversarial\ntraining with multi-task cues. In our experiments, we find that the deep gra-\ndient regularization of DataGrad (which also has L1 and L2 flavors of\nregularization) outperforms alternative forms of regularization, including\nclassical L1, L2, and multi- task, both on the original dataset as well as on\nadversarial sets. Furthermore, we find that combining multi-task optimization\nwith DataGrad adversarial training results in the most robust performance.\n"], ["2015-11-23", "http://arxiv.org/abs/1511.07528", "The Limitations of Deep Learning in Adversarial Settings.", ["Nicolas Papernot", " Patrick McDaniel", " Somesh Jha", " Matt Fredrikson", " Z. Berkay Celik", " Ananthram Swami"], "  Deep learning takes advantage of large datasets and computationally efficient\ntraining algorithms to outperform other approaches at various machine learning\ntasks. However, imperfections in the training phase of deep neural networks\nmake them vulnerable to adversarial samples: inputs crafted by adversaries with\nthe intent of causing deep neural networks to misclassify. In this work, we\nformalize the space of adversaries against deep neural networks (DNNs) and\nintroduce a novel class of algorithms to craft adversarial samples based on a\nprecise understanding of the mapping between inputs and outputs of DNNs. In an\napplication to computer vision, we show that our algorithms can reliably\nproduce samples correctly classified by human subjects but misclassified in\nspecific targets by a DNN with a 97% adversarial success rate while only\nmodifying on average 4.02% of the input features per sample. We then evaluate\nthe vulnerability of different sample classes to adversarial perturbations by\ndefining a hardness measure. Finally, we describe preliminary work outlining\ndefenses against adversarial samples by defining a predictive measure of\ndistance between a benign input and a target classification.\n"], ["2015-11-19", "http://arxiv.org/abs/1511.06385", "A Unified Gradient Regularization Family for Adversarial Examples.", ["Chunchuan Lyu", " Kaizhu Huang", " Hai-Ning Liang"], "  Adversarial examples are augmented data points generated by imperceptible\nperturbation of input samples. They have recently drawn much attention with the\nmachine learning and data mining community. Being difficult to distinguish from\nreal examples, such adversarial examples could change the prediction of many of\nthe best learning models including the state-of-the-art deep learning models.\nRecent attempts have been made to build robust models that take into account\nadversarial examples. However, these methods can either lead to performance\ndrops or lack mathematical motivations. In this paper, we propose a unified\nframework to build robust machine learning models against adversarial examples.\nMore specifically, using the unified framework, we develop a family of gradient\nregularization methods that effectively penalize the gradient of loss function\nw.r.t. inputs. Our proposed framework is appealing in that it offers a unified\nview to deal with adversarial examples. It incorporates another\nrecently-proposed perturbation based approach as a special case. In addition,\nwe present some visual effects that reveals semantic meaning in those\nperturbations, and thus support our regularization method and provide another\nexplanation for generalizability of adversarial examples. By applying this\ntechnique to Maxout networks, we conduct a series of experiments and achieve\nencouraging results on two benchmark datasets. In particular,we attain the best\naccuracy on MNIST data (without data augmentation) and competitive performance\non CIFAR-10 data.\n"], ["2015-11-19", "http://arxiv.org/abs/1511.06381", "Manifold Regularized Deep Neural Networks using Adversarial Examples.", ["Taehoon Lee", " Minsuk Choi", " Sungroh Yoon"], "  Learning meaningful representations using deep neural networks involves\ndesigning efficient training schemes and well-structured networks. Currently,\nthe method of stochastic gradient descent that has a momentum with dropout is\none of the most popular training protocols. Based on that, more advanced\nmethods (i.e., Maxout and Batch Normalization) have been proposed in recent\nyears, but most still suffer from performance degradation caused by small\nperturbations, also known as adversarial examples. To address this issue, we\npropose manifold regularized networks (MRnet) that utilize a novel training\nobjective function that minimizes the difference between multi-layer embedding\nresults of samples and those adversarial. Our experimental results demonstrated\nthat MRnet is more resilient to adversarial examples and helps us to generalize\nrepresentations on manifolds. Furthermore, combining MRnet and dropout allowed\nus to achieve competitive classification performances for three well-known\nbenchmarks: MNIST, CIFAR-10, and SVHN.\n"], ["2015-11-19", "http://arxiv.org/abs/1511.06306", "Robust Convolutional Neural Networks under Adversarial Noise.", ["Jonghoon Jin", " Aysegul Dundar", " Eugenio Culurciello"], "  Recent studies have shown that Convolutional Neural Networks (CNNs) are\nvulnerable to a small perturbation of input called \"adversarial examples\". In\nthis work, we propose a new feedforward CNN that improves robustness in the\npresence of adversarial noise. Our model uses stochastic additive noise added\nto the input image and to the CNN models. The proposed model operates in\nconjunction with a CNN trained with either standard or adversarial objective\nfunction. In particular, convolution, max-pooling, and ReLU layers are modified\nto benefit from the noise model. Our feedforward model is parameterized by only\na mean and variance per pixel which simplifies computations and makes our\nmethod scalable to a deep architecture. From CIFAR-10 and ImageNet test, the\nproposed model outperforms other methods and the improvement is more evident\nfor difficult classification tasks or stronger adversarial noise.\n"], ["2015-11-19", "http://arxiv.org/abs/1511.06292", "Foveation-based Mechanisms Alleviate Adversarial Examples.", ["Yan Luo", " Xavier Boix", " Gemma Roig", " Tomaso Poggio", " Qi Zhao"], "  We show that adversarial examples, i.e., the visually imperceptible\nperturbations that result in Convolutional Neural Networks (CNNs) fail, can be\nalleviated with a mechanism based on foveations---applying the CNN in different\nimage regions. To see this, first, we report results in ImageNet that lead to a\nrevision of the hypothesis that adversarial perturbations are a consequence of\nCNNs acting as a linear classifier: CNNs act locally linearly to changes in the\nimage regions with objects recognized by the CNN, and in other regions the CNN\nmay act non-linearly. Then, we corroborate that when the neural responses are\nlinear, applying the foveation mechanism to the adversarial example tends to\nsignificantly reduce the effect of the perturbation. This is because,\nhypothetically, the CNNs for ImageNet are robust to changes of scale and\ntranslation of the object produced by the foveation, but this property does not\ngeneralize to transformations of the perturbation. As a result, the accuracy\nafter a foveation is almost the same as the accuracy of the CNN without the\nadversarial perturbation, even if the adversarial perturbation is calculated\ntaking into account a foveation.\n"], ["2015-11-19", "http://arxiv.org/abs/1511.06233", "Towards Open Set Deep Networks.", ["Abhijit Bendale", " Terrance Boult"], "  Deep networks have produced significant gains for various visual recognition\nproblems, leading to high impact academic and commercial applications. Recent\nwork in deep networks highlighted that it is easy to generate images that\nhumans would never classify as a particular object class, yet networks classify\nsuch images high confidence as that given class - deep network are easily\nfooled with images humans do not consider meaningful. The closed set nature of\ndeep networks forces them to choose from one of the known classes leading to\nsuch artifacts. Recognition in the real world is open set, i.e. the recognition\nsystem should reject unknown/unseen classes at test time. We present a\nmethodology to adapt deep networks for open set recognition, by introducing a\nnew model layer, OpenMax, which estimates the probability of an input being\nfrom an unknown class. A key element of estimating the unknown probability is\nadapting Meta-Recognition concepts to the activation patterns in the\npenultimate layer of the network. OpenMax allows rejection of \"fooling\" and\nunrelated open set images presented to the system; OpenMax greatly reduces the\nnumber of obvious errors made by a deep network. We prove that the OpenMax\nconcept provides bounded open space risk, thereby formally providing an open\nset recognition solution. We evaluate the resulting open set deep networks\nusing pre-trained networks from the Caffe Model-zoo on ImageNet 2012 validation\ndata, and thousands of fooling and open set images. The proposed OpenMax model\nsignificantly outperforms open set recognition accuracy of basic deep networks\nas well as deep networks with thresholding of SoftMax probabilities.\n"], ["2015-11-17", "http://arxiv.org/abs/1511.05432", "Understanding Adversarial Training: Increasing Local Stability of Neural Nets through Robust Optimization.", ["Uri Shaham", " Yutaro Yamada", " Sahand Negahban"], "  We propose a general framework for increasing local stability of Artificial\nNeural Nets (ANNs) using Robust Optimization (RO). We achieve this through an\nalternating minimization-maximization procedure, in which the loss of the\nnetwork is minimized over perturbed examples that are generated at each\nparameter update. We show that adversarial training of ANNs is in fact\nrobustification of the network optimization, and that our proposed framework\ngeneralizes previous approaches for increasing local stability of ANNs.\nExperimental results reveal that our approach increases the robustness of the\nnetwork to existing adversarial examples, while making it harder to generate\nnew ones. Furthermore, our algorithm improves the accuracy of the network also\non the original test data.\n"], ["2015-11-16", "http://arxiv.org/abs/1511.05122", "Adversarial Manipulation of Deep Representations.", ["Sara Sabour", " Yanshuai Cao", " Fartash Faghri", " David J. Fleet"], "  We show that the representation of an image in a deep neural network (DNN)\ncan be manipulated to mimic those of other natural images, with only minor,\nimperceptible perturbations to the original image. Previous methods for\ngenerating adversarial images focused on image perturbations designed to\nproduce erroneous class labels, while we concentrate on the internal layers of\nDNN representations. In this way our new class of adversarial images differs\nqualitatively from others. While the adversary is perceptually similar to one\nimage, its internal representation appears remarkably similar to a different\nimage, one from a different class, bearing little if any apparent similarity to\nthe input; they appear generic and consistent with the space of natural images.\nThis phenomenon raises questions about DNN representations, as well as the\nproperties of natural images themselves.\n"], ["2015-11-14", "http://arxiv.org/abs/1511.04599", "DeepFool: a simple and accurate method to fool deep neural networks.", ["Seyed-Mohsen Moosavi-Dezfooli", " Alhussein Fawzi", " Pascal Frossard"], "  State-of-the-art deep neural networks have achieved impressive results on\nmany image classification tasks. However, these same architectures have been\nshown to be unstable to small, well sought, perturbations of the images.\nDespite the importance of this phenomenon, no effective methods have been\nproposed to accurately compute the robustness of state-of-the-art deep\nclassifiers to such perturbations on large-scale datasets. In this paper, we\nfill this gap and propose the DeepFool algorithm to efficiently compute\nperturbations that fool deep networks, and thus reliably quantify the\nrobustness of these classifiers. Extensive experimental results show that our\napproach outperforms recent methods in the task of computing adversarial\nperturbations and making classifiers more robust.\n"], ["2015-11-13", "http://arxiv.org/abs/1511.04508", "Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks.", ["Nicolas Papernot", " Patrick McDaniel", " Xi Wu", " Somesh Jha", " Ananthram Swami"], "  Deep learning algorithms have been shown to perform extremely well on many\nclassical machine learning problems. However, recent studies have shown that\ndeep learning, like other machine learning techniques, is vulnerable to\nadversarial samples: inputs crafted to force a deep neural network (DNN) to\nprovide adversary-selected outputs. Such attacks can seriously undermine the\nsecurity of the system supported by the DNN, sometimes with devastating\nconsequences. For example, autonomous vehicles can be crashed, illicit or\nillegal content can bypass content filters, or biometric authentication systems\ncan be manipulated to allow improper access. In this work, we introduce a\ndefensive mechanism called defensive distillation to reduce the effectiveness\nof adversarial samples on DNNs. We analytically investigate the\ngeneralizability and robustness properties granted by the use of defensive\ndistillation when training DNNs. We also empirically study the effectiveness of\nour defense mechanisms on two DNNs placed in adversarial settings. The study\nshows that defensive distillation can reduce effectiveness of sample creation\nfrom 95% to less than 0.5% on a studied DNN. Such dramatic gains can be\nexplained by the fact that distillation leads gradients used in adversarial\nsample creation to be reduced by a factor of 10^30. We also find that\ndistillation increases the average minimum number of features that need to be\nmodified to create adversarial samples by about 800% on one of the DNNs we\ntested.\n"], ["2015-11-10", "http://arxiv.org/abs/1511.03034", "Learning with a Strong Adversary.", ["Ruitong Huang", " Bing Xu", " Dale Schuurmans", " Csaba Szepesvari"], "  The robustness of neural networks to intended perturbations has recently\nattracted significant attention. In this paper, we propose a new method,\n\\emph{learning with a strong adversary}, that learns robust classifiers from\nsupervised data. The proposed method takes finding adversarial examples as an\nintermediate step. A new and simple way of finding adversarial examples is\npresented and experimentally shown to be efficient. Experimental results\ndemonstrate that resulting learning method greatly improves the robustness of\nthe classification models produced.\n"], ["2015-10-18", "http://arxiv.org/abs/1510.05328", "Exploring the Space of Adversarial Images.", ["Pedro Tabacof", " Eduardo Valle"], "  Adversarial examples have raised questions regarding the robustness and\nsecurity of deep neural networks. In this work we formalize the problem of\nadversarial images given a pretrained classifier, showing that even in the\nlinear case the resulting optimization problem is nonconvex. We generate\nadversarial images using shallow and deep classifiers on the MNIST and ImageNet\ndatasets. We probe the pixel space of adversarial images using noise of varying\nintensity and distribution. We bring novel visualizations that showcase the\nphenomenon and its high variability. We show that adversarial images appear in\nlarge regions in the pixel space, but that, for the same task, a shallow\nclassifier seems more robust to adversarial images than a deep convolutional\nnetwork.\n"], ["2015-10-14", "http://arxiv.org/abs/1510.04189", "Improving Back-Propagation by Adding an Adversarial Gradient.", ["Arild N\u00f8kland"], "  The back-propagation algorithm is widely used for learning in artificial\nneural networks. A challenge in machine learning is to create models that\ngeneralize to new data samples not seen in the training data. Recently, a\ncommon flaw in several machine learning algorithms was discovered: small\nperturbations added to the input data lead to consistent misclassification of\ndata samples. Samples that easily mislead the model are called adversarial\nexamples. Training a \"maxout\" network on adversarial examples has shown to\ndecrease this vulnerability, but also increase classification performance. This\npaper shows that adversarial training has a regularizing effect also in\nnetworks with logistic, hyperbolic tangent and rectified linear units. A simple\nextension to the back-propagation method is proposed, that adds an adversarial\ngradient to the training. The extension requires an additional forward and\nbackward pass to calculate a modified input sample, or mini batch, used as\ninput for standard back-propagation learning. The first experimental results on\nMNIST show that the \"adversarial back-propagation\" method increases the\nresistance to adversarial examples and boosts the classification performance.\nThe extension reduces the classification error on the permutation invariant\nMNIST from 1.60% to 0.95% in a logistic network, and from 1.40% to 0.78% in a\nnetwork with rectified linear units. Results on CIFAR-10 indicate that the\nmethod has a regularizing effect similar to dropout in fully connected\nnetworks. Based on these promising results, adversarial back-propagation is\nproposed as a stand-alone regularizing method that should be further\ninvestigated.\n"], ["2015-07-16", "http://arxiv.org/abs/1507.04761", "Deep Learning and Music Adversaries.", ["Corey Kereliuk", " Bob L. Sturm", " Jan Larsen"], "  An adversary is essentially an algorithm intent on making a classification\nsystem perform in some particular way given an input, e.g., increase the\nprobability of a false negative. Recent work builds adversaries for deep\nlearning systems applied to image object recognition, which exploits the\nparameters of the system to find the minimal perturbation of the input image\nsuch that the network misclassifies it with high confidence. We adapt this\napproach to construct and deploy an adversary of deep learning systems applied\nto music content analysis. In our case, however, the input to the systems is\nmagnitude spectral frames, which requires special care in order to produce\nvalid input audio signals from network-derived perturbations. For two different\ntrain-test partitionings of two benchmark datasets, and two different deep\narchitectures, we find that this adversary is very effective in defeating the\nresulting systems. We find the convolutional networks are more robust, however,\ncompared with systems based on a majority vote over individually classified\naudio frames. Furthermore, we integrate the adversary into the training of new\ndeep systems, but do not find that this improves their resilience against the\nsame adversary.\n"], ["2015-02-09", "http://arxiv.org/abs/1502.02590", "Analysis of classifiers' robustness to adversarial perturbations.", ["Alhussein Fawzi", " Omar Fawzi", " Pascal Frossard"], "  The goal of this paper is to analyze an intriguing phenomenon recently\ndiscovered in deep networks, namely their instability to adversarial\nperturbations (Szegedy et. al., 2014). We provide a theoretical framework for\nanalyzing the robustness of classifiers to adversarial perturbations, and show\nfundamental upper bounds on the robustness of classifiers. Specifically, we\nestablish a general upper bound on the robustness of classifiers to adversarial\nperturbations, and then illustrate the obtained upper bound on the families of\nlinear and quadratic classifiers. In both cases, our upper bound depends on a\ndistinguishability measure that captures the notion of difficulty of the\nclassification task. Our results for both classes imply that in tasks involving\nsmall distinguishability, no classifier in the considered set will be robust to\nadversarial perturbations, even if a good accuracy is achieved. Our theoretical\nframework moreover suggests that the phenomenon of adversarial instability is\ndue to the low flexibility of classifiers, compared to the difficulty of the\nclassification task (captured by the distinguishability). Moreover, we show the\nexistence of a clear distinction between the robustness of a classifier to\nrandom noise and its robustness to adversarial perturbations. Specifically, the\nformer is shown to be larger than the latter by a factor that is proportional\nto \\sqrt{d} (with d being the signal dimension) for linear classifiers. This\nresult gives a theoretical explanation for the discrepancy between the two\nrobustness properties in high dimensional problems, which was empirically\nobserved in the context of neural networks. To the best of our knowledge, our\nresults provide the first theoretical work that addresses the phenomenon of\nadversarial instability recently observed for deep networks. Our analysis is\ncomplemented by experimental results on controlled and real-world data.\n"], ["2014-12-19", "http://arxiv.org/abs/1412.6572", "Explaining and Harnessing Adversarial Examples.", ["Ian J. Goodfellow", " Jonathon Shlens", " Christian Szegedy"], "  Several machine learning models, including neural networks, consistently\nmisclassify adversarial examples---inputs formed by applying small but\nintentionally worst-case perturbations to examples from the dataset, such that\nthe perturbed input results in the model outputting an incorrect answer with\nhigh confidence. Early attempts at explaining this phenomenon focused on\nnonlinearity and overfitting. We argue instead that the primary cause of neural\nnetworks' vulnerability to adversarial perturbation is their linear nature.\nThis explanation is supported by new quantitative results while giving the\nfirst explanation of the most intriguing fact about them: their generalization\nacross architectures and training sets. Moreover, this view yields a simple and\nfast method of generating adversarial examples. Using this approach to provide\nexamples for adversarial training, we reduce the test set error of a maxout\nnetwork on the MNIST dataset.\n"], ["2014-12-11", "http://arxiv.org/abs/1412.5068", "Towards Deep Neural Network Architectures Robust to Adversarial Examples.", ["Shixiang Gu", " Luca Rigazio"], "  Recent work has shown deep neural networks (DNNs) to be highly susceptible to\nwell-designed, small perturbations at the input layer, or so-called adversarial\nexamples. Taking images as an example, such distortions are often\nimperceptible, but can result in 100% mis-classification for a state of the art\nDNN. We study the structure of adversarial examples and explore network\ntopology, pre-processing and training strategies to improve the robustness of\nDNNs. We perform various experiments to assess the removability of adversarial\nexamples by corrupting with additional noise and pre-processing with denoising\nautoencoders (DAEs). We find that DAEs can remove substantial amounts of the\nadversarial noise. How- ever, when stacking the DAE with the original DNN, the\nresulting network can again be attacked by new adversarial examples with even\nsmaller distortion. As a solution, we propose Deep Contractive Network, a model\nwith a new end-to-end training procedure that includes a smoothness penalty\ninspired by the contractive autoencoder (CAE). This increases the network\nrobustness to adversarial examples, without a significant performance penalty.\n"], ["2014-12-05", "http://arxiv.org/abs/1412.1897", "Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images.", ["Anh Nguyen", " Jason Yosinski", " Jeff Clune"], "  Deep neural networks (DNNs) have recently been achieving state-of-the-art\nperformance on a variety of pattern-recognition tasks, most notably visual\nclassification problems. Given that DNNs are now able to classify objects in\nimages with near-human-level performance, questions naturally arise as to what\ndifferences remain between computer and human vision. A recent study revealed\nthat changing an image (e.g. of a lion) in a way imperceptible to humans can\ncause a DNN to label the image as something else entirely (e.g. mislabeling a\nlion a library). Here we show a related result: it is easy to produce images\nthat are completely unrecognizable to humans, but that state-of-the-art DNNs\nbelieve to be recognizable objects with 99.99% confidence (e.g. labeling with\ncertainty that white noise static is a lion). Specifically, we take\nconvolutional neural networks trained to perform well on either the ImageNet or\nMNIST datasets and then find images with evolutionary algorithms or gradient\nascent that DNNs label with high confidence as belonging to each dataset class.\nIt is possible to produce images totally unrecognizable to human eyes that DNNs\nbelieve with near certainty are familiar objects, which we call \"fooling\nimages\" (more generally, fooling examples). Our results shed light on\ninteresting differences between human vision and current DNNs, and raise\nquestions about the generality of DNN computer vision.\n"], ["2014-01-29", "http://arxiv.org/abs/1401.7727", "Security Evaluation of Support Vector Machines in Adversarial Environments.", ["Battista Biggio", " Igino Corona", " Blaine Nelson", " Benjamin I. P. Rubinstein", " Davide Maiorca", " Giorgio Fumera", " Giorgio Giacinto", " and Fabio Roli"], "  Support Vector Machines (SVMs) are among the most popular classification\ntechniques adopted in security applications like malware detection, intrusion\ndetection, and spam filtering. However, if SVMs are to be incorporated in\nreal-world security systems, they must be able to cope with attack patterns\nthat can either mislead the learning algorithm (poisoning), evade detection\n(evasion), or gain information about their internal parameters (privacy\nbreaches). The main contributions of this chapter are twofold. First, we\nintroduce a formal general framework for the empirical evaluation of the\nsecurity of machine-learning systems. Second, according to our framework, we\ndemonstrate the feasibility of evasion, poisoning and privacy attacks against\nSVMs in real-world security problems. For each attack technique, we evaluate\nits impact and discuss whether (and how) it can be countered through an\nadversary-aware design of SVMs. Our experiments are easily reproducible thanks\nto open-source code that we have made available, together with all the employed\ndatasets, on a public repository.\n"], ["2013-12-20", "http://arxiv.org/abs/1312.6199", "Intriguing properties of neural networks.", ["Christian Szegedy", " Wojciech Zaremba", " Ilya Sutskever", " Joan Bruna", " Dumitru Erhan", " Ian Goodfellow", " Rob Fergus"], "  Deep neural networks are highly expressive models that have recently achieved\nstate of the art performance on speech and visual recognition tasks. While\ntheir expressiveness is the reason they succeed, it also causes them to learn\nuninterpretable solutions that could have counter-intuitive properties. In this\npaper we report two such properties.\n  First, we find that there is no distinction between individual high level\nunits and random linear combinations of high level units, according to various\nmethods of unit analysis. It suggests that it is the space, rather than the\nindividual units, that contains of the semantic information in the high layers\nof neural networks.\n  Second, we find that deep neural networks learn input-output mappings that\nare fairly discontinuous to a significant extend. We can cause the network to\nmisclassify an image by applying a certain imperceptible perturbation, which is\nfound by maximizing the network's prediction error. In addition, the specific\nnature of these perturbations is not a random artifact of learning: the same\nperturbation can cause a different network, that was trained on a different\nsubset of the dataset, to misclassify the same input.\n"]]